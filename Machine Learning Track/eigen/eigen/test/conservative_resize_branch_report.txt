/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/GenericPacketMath.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |// Copyright (C) 2006-2008 Benoit Jacob <jacob.benoit.1@gmail.com>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_GENERIC_PACKET_MATH_H
   12|       |#define EIGEN_GENERIC_PACKET_MATH_H
   13|       |
   14|       |// IWYU pragma: private
   15|       |#include "./InternalHeaderCheck.h"
   16|       |
   17|       |namespace Eigen {
   18|       |
   19|       |namespace internal {
   20|       |
   21|       |/** \internal
   22|       | * \file GenericPacketMath.h
   23|       | *
   24|       | * Default implementation for types not supported by the vectorization.
   25|       | * In practice these functions are provided to make easier the writing
   26|       | * of generic vectorized code.
   27|       | */
   28|       |
   29|       |#ifndef EIGEN_DEBUG_ALIGNED_LOAD
   30|       |#define EIGEN_DEBUG_ALIGNED_LOAD
   31|       |#endif
   32|       |
   33|       |#ifndef EIGEN_DEBUG_UNALIGNED_LOAD
   34|       |#define EIGEN_DEBUG_UNALIGNED_LOAD
   35|       |#endif
   36|       |
   37|       |#ifndef EIGEN_DEBUG_ALIGNED_STORE
   38|       |#define EIGEN_DEBUG_ALIGNED_STORE
   39|       |#endif
   40|       |
   41|       |#ifndef EIGEN_DEBUG_UNALIGNED_STORE
   42|       |#define EIGEN_DEBUG_UNALIGNED_STORE
   43|       |#endif
   44|       |
   45|       |struct default_packet_traits {
   46|       |  enum {
   47|       |    // Ops that are implemented for most types.
   48|       |    HasAdd = 1,
   49|       |    HasSub = 1,
   50|       |    HasShift = 1,
   51|       |    HasMul = 1,
   52|       |    HasNegate = 1,
   53|       |    HasAbs = 1,
   54|       |    HasAbs2 = 1,
   55|       |    HasMin = 1,
   56|       |    HasMax = 1,
   57|       |    HasConj = 1,
   58|       |    HasSetLinear = 1,
   59|       |    HasSign = 1,
   60|       |    // By default, the nearest integer functions (rint, round, floor, ceil, trunc) are enabled for all scalar and packet
   61|       |    // types
   62|       |    HasRound = 1,
   63|       |
   64|       |    HasArg = 0,
   65|       |    HasAbsDiff = 0,
   66|       |    HasBlend = 0,
   67|       |    // This flag is used to indicate whether packet comparison is supported.
   68|       |    // pcmp_eq, pcmp_lt and pcmp_le should be defined for it to be true.
   69|       |    HasCmp = 0,
   70|       |
   71|       |    HasDiv = 0,
   72|       |    HasReciprocal = 0,
   73|       |    HasSqrt = 0,
   74|       |    HasRsqrt = 0,
   75|       |    HasExp = 0,
   76|       |    HasExpm1 = 0,
   77|       |    HasLog = 0,
   78|       |    HasLog1p = 0,
   79|       |    HasLog10 = 0,
   80|       |    HasPow = 0,
   81|       |    HasSin = 0,
   82|       |    HasCos = 0,
   83|       |    HasTan = 0,
   84|       |    HasASin = 0,
   85|       |    HasACos = 0,
   86|       |    HasATan = 0,
   87|       |    HasATanh = 0,
   88|       |    HasSinh = 0,
   89|       |    HasCosh = 0,
   90|       |    HasTanh = 0,
   91|       |    HasLGamma = 0,
   92|       |    HasDiGamma = 0,
   93|       |    HasZeta = 0,
   94|       |    HasPolygamma = 0,
   95|       |    HasErf = 0,
   96|       |    HasErfc = 0,
   97|       |    HasNdtri = 0,
   98|       |    HasBessel = 0,
   99|       |    HasIGamma = 0,
  100|       |    HasIGammaDerA = 0,
  101|       |    HasGammaSampleDerAlpha = 0,
  102|       |    HasIGammac = 0,
  103|       |    HasBetaInc = 0
  104|       |  };
  105|       |};
  106|       |
  107|       |template <typename T>
  108|       |struct packet_traits : default_packet_traits {
  109|       |  typedef T type;
  110|       |  typedef T half;
  111|       |  enum {
  112|       |    Vectorizable = 0,
  113|       |    size = 1,
  114|       |    AlignedOnScalar = 0,
  115|       |  };
  116|       |  enum {
  117|       |    HasAdd = 0,
  118|       |    HasSub = 0,
  119|       |    HasMul = 0,
  120|       |    HasNegate = 0,
  121|       |    HasAbs = 0,
  122|       |    HasAbs2 = 0,
  123|       |    HasMin = 0,
  124|       |    HasMax = 0,
  125|       |    HasConj = 0,
  126|       |    HasSetLinear = 0
  127|       |  };
  128|       |};
  129|       |
  130|       |template <typename T>
  131|       |struct packet_traits<const T> : packet_traits<T> {};
  132|       |
  133|       |template <typename T>
  134|       |struct unpacket_traits {
  135|       |  typedef T type;
  136|       |  typedef T half;
  137|       |  typedef typename numext::get_integer_by_size<sizeof(T)>::signed_type integer_packet;
  138|       |  enum {
  139|       |    size = 1,
  140|       |    alignment = alignof(T),
  141|       |    vectorizable = false,
  142|       |    masked_load_available = false,
  143|       |    masked_store_available = false
  144|       |  };
  145|       |};
  146|       |
  147|       |template <typename T>
  148|       |struct unpacket_traits<const T> : unpacket_traits<T> {};
  149|       |
  150|       |/** \internal A convenience utility for determining if the type is a scalar.
  151|       | * This is used to enable some generic packet implementations.
  152|       | */
  153|       |template <typename Packet>
  154|       |struct is_scalar {
  155|       |  using Scalar = typename unpacket_traits<Packet>::type;
  156|       |  enum { value = internal::is_same<Packet, Scalar>::value };
  157|       |};
  158|       |
  159|       |// automatically and succinctly define combinations of pcast<SrcPacket,TgtPacket> when
  160|       |// 1) the packets are the same type, or
  161|       |// 2) the packets differ only in sign.
  162|       |// In both of these cases, preinterpret (bit_cast) is equivalent to pcast (static_cast)
  163|       |template <typename SrcPacket, typename TgtPacket,
  164|       |          bool Scalar = is_scalar<SrcPacket>::value && is_scalar<TgtPacket>::value>
  165|       |struct is_degenerate_helper : is_same<SrcPacket, TgtPacket> {};
  166|       |template <>
  167|       |struct is_degenerate_helper<int8_t, uint8_t, true> : std::true_type {};
  168|       |template <>
  169|       |struct is_degenerate_helper<int16_t, uint16_t, true> : std::true_type {};
  170|       |template <>
  171|       |struct is_degenerate_helper<int32_t, uint32_t, true> : std::true_type {};
  172|       |template <>
  173|       |struct is_degenerate_helper<int64_t, uint64_t, true> : std::true_type {};
  174|       |
  175|       |template <typename SrcPacket, typename TgtPacket>
  176|       |struct is_degenerate_helper<SrcPacket, TgtPacket, false> {
  177|       |  using SrcScalar = typename unpacket_traits<SrcPacket>::type;
  178|       |  static constexpr int SrcSize = unpacket_traits<SrcPacket>::size;
  179|       |  using TgtScalar = typename unpacket_traits<TgtPacket>::type;
  180|       |  static constexpr int TgtSize = unpacket_traits<TgtPacket>::size;
  181|       |  static constexpr bool value = is_degenerate_helper<SrcScalar, TgtScalar, true>::value && (SrcSize == TgtSize);
  182|       |};
  183|       |
  184|       |// is_degenerate<T1,T2>::value == is_degenerate<T2,T1>::value
  185|       |template <typename SrcPacket, typename TgtPacket>
  186|       |struct is_degenerate {
  187|       |  static constexpr bool value =
  188|       |      is_degenerate_helper<SrcPacket, TgtPacket>::value || is_degenerate_helper<TgtPacket, SrcPacket>::value;
  189|       |};
  190|       |
  191|       |template <typename Packet>
  192|       |struct is_half {
  193|       |  using Scalar = typename unpacket_traits<Packet>::type;
  194|       |  static constexpr int Size = unpacket_traits<Packet>::size;
  195|       |  using DefaultPacket = typename packet_traits<Scalar>::type;
  196|       |  static constexpr int DefaultSize = unpacket_traits<DefaultPacket>::size;
  197|       |  static constexpr bool value = Size != 1 && Size < DefaultSize;
  198|       |};
  199|       |
  200|       |template <typename Src, typename Tgt>
  201|       |struct type_casting_traits {
  202|       |  enum {
  203|       |    VectorizedCast =
  204|       |        is_degenerate<Src, Tgt>::value && packet_traits<Src>::Vectorizable && packet_traits<Tgt>::Vectorizable,
  205|       |    SrcCoeffRatio = 1,
  206|       |    TgtCoeffRatio = 1
  207|       |  };
  208|       |};
  209|       |
  210|       |// provides a succinct template to define vectorized casting traits with respect to the largest accessible packet types
  211|       |template <typename Src, typename Tgt>
  212|       |struct vectorized_type_casting_traits {
  213|       |  enum : int {
  214|       |    DefaultSrcPacketSize = packet_traits<Src>::size,
  215|       |    DefaultTgtPacketSize = packet_traits<Tgt>::size,
  216|       |    VectorizedCast = 1,
  217|       |    SrcCoeffRatio = plain_enum_max(DefaultTgtPacketSize / DefaultSrcPacketSize, 1),
  218|       |    TgtCoeffRatio = plain_enum_max(DefaultSrcPacketSize / DefaultTgtPacketSize, 1)
  219|       |  };
  220|       |};
  221|       |
  222|       |/** \internal Wrapper to ensure that multiple packet types can map to the same
  223|       |    same underlying vector type. */
  224|       |template <typename T, int unique_id = 0>
  225|       |struct eigen_packet_wrapper {
  226|      0|  EIGEN_ALWAYS_INLINE operator T&() { return m_val; }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi0EEcvRS2_Ev
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi5EEcvRS2_Ev
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi4EEcvRS2_Ev
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi1EEcvRS2_Ev
  ------------------
  227|      0|  EIGEN_ALWAYS_INLINE operator const T&() const { return m_val; }
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal20eigen_packet_wrapperIDv2_xLi5EEcvRKS2_Ev
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal20eigen_packet_wrapperIDv2_xLi0EEcvRKS2_Ev
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal20eigen_packet_wrapperIDv2_xLi4EEcvRKS2_Ev
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal20eigen_packet_wrapperIDv2_xLi1EEcvRKS2_Ev
  ------------------
  228|       |  EIGEN_ALWAYS_INLINE eigen_packet_wrapper() = default;
  229|       |  EIGEN_ALWAYS_INLINE eigen_packet_wrapper(const T& v) : m_val(v) {}
  230|      0|  EIGEN_ALWAYS_INLINE eigen_packet_wrapper& operator=(const T& v) {
  231|      0|    m_val = v;
  232|      0|    return *this;
  233|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi0EEaSERKS2_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi4EEaSERKS2_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi1EEaSERKS2_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal20eigen_packet_wrapperIDv2_xLi5EEaSERKS2_
  ------------------
  234|       |
  235|       |  T m_val;
  236|       |};
  237|       |
  238|       |template <typename Target, typename Packet, bool IsSame = is_same<Target, Packet>::value>
  239|       |struct preinterpret_generic;
  240|       |
  241|       |template <typename Target, typename Packet>
  242|       |struct preinterpret_generic<Target, Packet, false> {
  243|       |  // the packets are not the same, attempt scalar bit_cast
  244|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Target run(const Packet& a) {
  245|       |    return numext::bit_cast<Target, Packet>(a);
  246|       |  }
  247|       |};
  248|       |
  249|       |template <typename Packet>
  250|       |struct preinterpret_generic<Packet, Packet, true> {
  251|       |  // the packets are the same type: do nothing
  252|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& a) { return a; }
  253|       |};
  254|       |
  255|       |/** \internal \returns reinterpret_cast<Target>(a) */
  256|       |template <typename Target, typename Packet>
  257|       |EIGEN_DEVICE_FUNC inline Target preinterpret(const Packet& a) {
  258|       |  return preinterpret_generic<Target, Packet>::run(a);
  259|       |}
  260|       |
  261|       |template <typename SrcPacket, typename TgtPacket, bool Degenerate = is_degenerate<SrcPacket, TgtPacket>::value,
  262|       |          bool TgtIsHalf = is_half<TgtPacket>::value>
  263|       |struct pcast_generic;
  264|       |
  265|       |template <typename SrcPacket, typename TgtPacket>
  266|       |struct pcast_generic<SrcPacket, TgtPacket, false, false> {
  267|       |  // the packets are not degenerate: attempt scalar static_cast
  268|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TgtPacket run(const SrcPacket& a) {
  269|       |    return cast_impl<SrcPacket, TgtPacket>::run(a);
  270|       |  }
  271|       |};
  272|       |
  273|       |template <typename Packet>
  274|       |struct pcast_generic<Packet, Packet, true, false> {
  275|       |  // the packets are the same: do nothing
  276|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& a) { return a; }
  277|       |};
  278|       |
  279|       |template <typename SrcPacket, typename TgtPacket, bool TgtIsHalf>
  280|       |struct pcast_generic<SrcPacket, TgtPacket, true, TgtIsHalf> {
  281|       |  // the packets are degenerate: preinterpret is equivalent to pcast
  282|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TgtPacket run(const SrcPacket& a) { return preinterpret<TgtPacket>(a); }
  283|       |};
  284|       |
  285|       |/** \internal \returns static_cast<TgtType>(a) (coeff-wise) */
  286|       |template <typename SrcPacket, typename TgtPacket>
  287|       |EIGEN_DEVICE_FUNC inline TgtPacket pcast(const SrcPacket& a) {
  288|       |  return pcast_generic<SrcPacket, TgtPacket>::run(a);
  289|       |}
  290|       |template <typename SrcPacket, typename TgtPacket>
  291|       |EIGEN_DEVICE_FUNC inline TgtPacket pcast(const SrcPacket& a, const SrcPacket& b) {
  292|       |  return pcast_generic<SrcPacket, TgtPacket>::run(a, b);
  293|       |}
  294|       |template <typename SrcPacket, typename TgtPacket>
  295|       |EIGEN_DEVICE_FUNC inline TgtPacket pcast(const SrcPacket& a, const SrcPacket& b, const SrcPacket& c,
  296|       |                                         const SrcPacket& d) {
  297|       |  return pcast_generic<SrcPacket, TgtPacket>::run(a, b, c, d);
  298|       |}
  299|       |template <typename SrcPacket, typename TgtPacket>
  300|       |EIGEN_DEVICE_FUNC inline TgtPacket pcast(const SrcPacket& a, const SrcPacket& b, const SrcPacket& c, const SrcPacket& d,
  301|       |                                         const SrcPacket& e, const SrcPacket& f, const SrcPacket& g,
  302|       |                                         const SrcPacket& h) {
  303|       |  return pcast_generic<SrcPacket, TgtPacket>::run(a, b, c, d, e, f, g, h);
  304|       |}
  305|       |
  306|       |template <typename SrcPacket, typename TgtPacket>
  307|       |struct pcast_generic<SrcPacket, TgtPacket, false, true> {
  308|       |  // TgtPacket is a half packet of some other type
  309|       |  // perform cast and truncate result
  310|       |  using DefaultTgtPacket = typename is_half<TgtPacket>::DefaultPacket;
  311|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE TgtPacket run(const SrcPacket& a) {
  312|       |    return preinterpret<TgtPacket>(pcast<SrcPacket, DefaultTgtPacket>(a));
  313|       |  }
  314|       |};
  315|       |
  316|       |/** \internal \returns a + b (coeff-wise) */
  317|       |template <typename Packet>
  318|      0|EIGEN_DEVICE_FUNC inline Packet padd(const Packet& a, const Packet& b) {
  319|      0|  return a + b;
  320|      0|}
  321|       |// Avoid compiler warning for boolean algebra.
  322|       |template <>
  323|      0|EIGEN_DEVICE_FUNC inline bool padd(const bool& a, const bool& b) {
  324|      0|  return a || b;
  325|      0|}
  326|       |
  327|       |/** \internal \returns a packet version of \a *from, (un-aligned masked add)
  328|       | * There is no generic implementation. We only have implementations for specialized
  329|       | * cases. Generic case should not be called.
  330|       | */
  331|       |template <typename Packet>
  332|       |EIGEN_DEVICE_FUNC inline std::enable_if_t<unpacket_traits<Packet>::masked_fpops_available, Packet> padd(
  333|       |    const Packet& a, const Packet& b, typename unpacket_traits<Packet>::mask_t umask);
  334|       |
  335|       |/** \internal \returns a - b (coeff-wise) */
  336|       |template <typename Packet>
  337|       |EIGEN_DEVICE_FUNC inline Packet psub(const Packet& a, const Packet& b) {
  338|       |  return a - b;
  339|       |}
  340|       |
  341|       |/** \internal \returns -a (coeff-wise) */
  342|       |template <typename Packet>
  343|       |EIGEN_DEVICE_FUNC inline Packet pnegate(const Packet& a) {
  344|       |  EIGEN_STATIC_ASSERT((!is_same<typename unpacket_traits<Packet>::type, bool>::value),
  345|       |                      NEGATE IS NOT DEFINED FOR BOOLEAN TYPES)
  346|       |  return numext::negate(a);
  347|       |}
  348|       |
  349|       |/** \internal \returns conj(a) (coeff-wise) */
  350|       |template <typename Packet>
  351|       |EIGEN_DEVICE_FUNC inline Packet pconj(const Packet& a) {
  352|       |  return numext::conj(a);
  353|       |}
  354|       |
  355|       |/** \internal \returns a * b (coeff-wise) */
  356|       |template <typename Packet>
  357|      0|EIGEN_DEVICE_FUNC inline Packet pmul(const Packet& a, const Packet& b) {
  358|      0|  return a * b;
  359|      0|}
  360|       |// Avoid compiler warning for boolean algebra.
  361|       |template <>
  362|      0|EIGEN_DEVICE_FUNC inline bool pmul(const bool& a, const bool& b) {
  363|      0|  return a && b;
  364|      0|}
  365|       |
  366|       |/** \internal \returns a / b (coeff-wise) */
  367|       |template <typename Packet>
  368|      0|EIGEN_DEVICE_FUNC inline Packet pdiv(const Packet& a, const Packet& b) {
  369|      0|  return a / b;
  370|      0|}
  371|       |
  372|       |// In the generic case, memset to all one bits.
  373|       |template <typename Packet, typename EnableIf = void>
  374|       |struct ptrue_impl {
  375|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& /*a*/) {
  376|       |    Packet b;
  377|       |    memset(static_cast<void*>(&b), 0xff, sizeof(Packet));
  378|       |    return b;
  379|       |  }
  380|       |};
  381|       |
  382|       |// For booleans, we can only directly set a valid `bool` value to avoid UB.
  383|       |template <>
  384|       |struct ptrue_impl<bool, void> {
  385|      0|  static EIGEN_DEVICE_FUNC inline bool run(const bool& /*a*/) { return true; }
  386|       |};
  387|       |
  388|       |// For non-trivial scalars, set to Scalar(1) (i.e. a non-zero value).
  389|       |// Although this is technically not a valid bitmask, the scalar path for pselect
  390|       |// uses a comparison to zero, so this should still work in most cases. We don't
  391|       |// have another option, since the scalar type requires initialization.
  392|       |template <typename T>
  393|       |struct ptrue_impl<T, std::enable_if_t<is_scalar<T>::value && NumTraits<T>::RequireInitialization>> {
  394|       |  static EIGEN_DEVICE_FUNC inline T run(const T& /*a*/) { return T(1); }
  395|       |};
  396|       |
  397|       |/** \internal \returns one bits. */
  398|       |template <typename Packet>
  399|       |EIGEN_DEVICE_FUNC inline Packet ptrue(const Packet& a) {
  400|       |  return ptrue_impl<Packet>::run(a);
  401|       |}
  402|       |
  403|       |// In the general case, memset to zero.
  404|       |template <typename Packet, typename EnableIf = void>
  405|       |struct pzero_impl {
  406|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& /*a*/) {
  407|       |    Packet b;
  408|       |    memset(static_cast<void*>(&b), 0x00, sizeof(Packet));
  409|       |    return b;
  410|       |  }
  411|       |};
  412|       |
  413|       |// For scalars, explicitly set to Scalar(0), since the underlying representation
  414|       |// for zero may not consist of all-zero bits.
  415|       |template <typename T>
  416|       |struct pzero_impl<T, std::enable_if_t<is_scalar<T>::value>> {
  417|       |  static EIGEN_DEVICE_FUNC inline T run(const T& /*a*/) { return T(0); }
  418|       |};
  419|       |
  420|       |/** \internal \returns packet of zeros */
  421|       |template <typename Packet>
  422|       |EIGEN_DEVICE_FUNC inline Packet pzero(const Packet& a) {
  423|       |  return pzero_impl<Packet>::run(a);
  424|       |}
  425|       |
  426|       |/** \internal \returns a <= b as a bit mask */
  427|       |template <typename Packet>
  428|       |EIGEN_DEVICE_FUNC inline Packet pcmp_le(const Packet& a, const Packet& b) {
  429|       |  return a <= b ? ptrue(a) : pzero(a);
  430|       |}
  431|       |
  432|       |/** \internal \returns a < b as a bit mask */
  433|       |template <typename Packet>
  434|       |EIGEN_DEVICE_FUNC inline Packet pcmp_lt(const Packet& a, const Packet& b) {
  435|       |  return a < b ? ptrue(a) : pzero(a);
  436|       |}
  437|       |
  438|       |/** \internal \returns a == b as a bit mask */
  439|       |template <typename Packet>
  440|       |EIGEN_DEVICE_FUNC inline Packet pcmp_eq(const Packet& a, const Packet& b) {
  441|       |  return a == b ? ptrue(a) : pzero(a);
  442|       |}
  443|       |
  444|       |/** \internal \returns a < b or a==NaN or b==NaN as a bit mask */
  445|       |template <typename Packet>
  446|       |EIGEN_DEVICE_FUNC inline Packet pcmp_lt_or_nan(const Packet& a, const Packet& b) {
  447|       |  return a >= b ? pzero(a) : ptrue(a);
  448|       |}
  449|       |
  450|       |template <typename T>
  451|       |struct bit_and {
  452|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE T operator()(const T& a, const T& b) const { return a & b; }
  453|       |};
  454|       |
  455|       |template <typename T>
  456|       |struct bit_or {
  457|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE T operator()(const T& a, const T& b) const { return a | b; }
  458|       |};
  459|       |
  460|       |template <typename T>
  461|       |struct bit_xor {
  462|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE T operator()(const T& a, const T& b) const { return a ^ b; }
  463|       |};
  464|       |
  465|       |template <typename T>
  466|       |struct bit_not {
  467|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE T operator()(const T& a) const { return ~a; }
  468|       |};
  469|       |
  470|       |template <>
  471|       |struct bit_and<bool> {
  472|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE bool operator()(const bool& a, const bool& b) const {
  473|      0|    return a && b;
  474|      0|  }
  475|       |};
  476|       |
  477|       |template <>
  478|       |struct bit_or<bool> {
  479|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE bool operator()(const bool& a, const bool& b) const {
  480|      0|    return a || b;
  481|      0|  }
  482|       |};
  483|       |
  484|       |template <>
  485|       |struct bit_xor<bool> {
  486|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE bool operator()(const bool& a, const bool& b) const {
  487|      0|    return a != b;
  488|      0|  }
  489|       |};
  490|       |
  491|       |template <>
  492|       |struct bit_not<bool> {
  493|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR EIGEN_ALWAYS_INLINE bool operator()(const bool& a) const { return !a; }
  494|       |};
  495|       |
  496|       |// Use operators &, |, ^, ~.
  497|       |template <typename T>
  498|       |struct operator_bitwise_helper {
  499|       |  EIGEN_DEVICE_FUNC static inline T bitwise_and(const T& a, const T& b) { return bit_and<T>()(a, b); }
  500|       |  EIGEN_DEVICE_FUNC static inline T bitwise_or(const T& a, const T& b) { return bit_or<T>()(a, b); }
  501|       |  EIGEN_DEVICE_FUNC static inline T bitwise_xor(const T& a, const T& b) { return bit_xor<T>()(a, b); }
  502|       |  EIGEN_DEVICE_FUNC static inline T bitwise_not(const T& a) { return bit_not<T>()(a); }
  503|       |};
  504|       |
  505|       |// Apply binary operations byte-by-byte
  506|       |template <typename T>
  507|       |struct bytewise_bitwise_helper {
  508|       |  EIGEN_DEVICE_FUNC static inline T bitwise_and(const T& a, const T& b) {
  509|       |    return binary(a, b, bit_and<unsigned char>());
  510|       |  }
  511|       |  EIGEN_DEVICE_FUNC static inline T bitwise_or(const T& a, const T& b) { return binary(a, b, bit_or<unsigned char>()); }
  512|       |  EIGEN_DEVICE_FUNC static inline T bitwise_xor(const T& a, const T& b) {
  513|       |    return binary(a, b, bit_xor<unsigned char>());
  514|       |  }
  515|       |  EIGEN_DEVICE_FUNC static inline T bitwise_not(const T& a) { return unary(a, bit_not<unsigned char>()); }
  516|       |
  517|       | private:
  518|       |  template <typename Op>
  519|       |  EIGEN_DEVICE_FUNC static inline T unary(const T& a, Op op) {
  520|       |    const unsigned char* a_ptr = reinterpret_cast<const unsigned char*>(&a);
  521|       |    T c;
  522|       |    unsigned char* c_ptr = reinterpret_cast<unsigned char*>(&c);
  523|       |    for (size_t i = 0; i < sizeof(T); ++i) {
  524|       |      *c_ptr++ = op(*a_ptr++);
  525|       |    }
  526|       |    return c;
  527|       |  }
  528|       |
  529|       |  template <typename Op>
  530|       |  EIGEN_DEVICE_FUNC static inline T binary(const T& a, const T& b, Op op) {
  531|       |    const unsigned char* a_ptr = reinterpret_cast<const unsigned char*>(&a);
  532|       |    const unsigned char* b_ptr = reinterpret_cast<const unsigned char*>(&b);
  533|       |    T c;
  534|       |    unsigned char* c_ptr = reinterpret_cast<unsigned char*>(&c);
  535|       |    for (size_t i = 0; i < sizeof(T); ++i) {
  536|       |      *c_ptr++ = op(*a_ptr++, *b_ptr++);
  537|       |    }
  538|       |    return c;
  539|       |  }
  540|       |};
  541|       |
  542|       |// In the general case, use byte-by-byte manipulation.
  543|       |template <typename T, typename EnableIf = void>
  544|       |struct bitwise_helper : public bytewise_bitwise_helper<T> {};
  545|       |
  546|       |// For integers or non-trivial scalars, use binary operators.
  547|       |template <typename T>
  548|       |struct bitwise_helper<T, typename std::enable_if_t<is_scalar<T>::value &&
  549|       |                                                   (NumTraits<T>::IsInteger || NumTraits<T>::RequireInitialization)>>
  550|       |    : public operator_bitwise_helper<T> {};
  551|       |
  552|       |/** \internal \returns the bitwise and of \a a and \a b */
  553|       |template <typename Packet>
  554|       |EIGEN_DEVICE_FUNC inline Packet pand(const Packet& a, const Packet& b) {
  555|       |  return bitwise_helper<Packet>::bitwise_and(a, b);
  556|       |}
  557|       |
  558|       |/** \internal \returns the bitwise or of \a a and \a b */
  559|       |template <typename Packet>
  560|       |EIGEN_DEVICE_FUNC inline Packet por(const Packet& a, const Packet& b) {
  561|       |  return bitwise_helper<Packet>::bitwise_or(a, b);
  562|       |}
  563|       |
  564|       |/** \internal \returns the bitwise xor of \a a and \a b */
  565|       |template <typename Packet>
  566|       |EIGEN_DEVICE_FUNC inline Packet pxor(const Packet& a, const Packet& b) {
  567|       |  return bitwise_helper<Packet>::bitwise_xor(a, b);
  568|       |}
  569|       |
  570|       |/** \internal \returns the bitwise not of \a a */
  571|       |template <typename Packet>
  572|       |EIGEN_DEVICE_FUNC inline Packet pnot(const Packet& a) {
  573|       |  return bitwise_helper<Packet>::bitwise_not(a);
  574|       |}
  575|       |
  576|       |/** \internal \returns the bitwise and of \a a and not \a b */
  577|       |template <typename Packet>
  578|       |EIGEN_DEVICE_FUNC inline Packet pandnot(const Packet& a, const Packet& b) {
  579|       |  return pand(a, pnot(b));
  580|       |}
  581|       |
  582|       |// In the general case, use bitwise select.
  583|       |template <typename Packet, typename EnableIf = void>
  584|       |struct pselect_impl {
  585|      0|  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& mask, const Packet& a, const Packet& b) {
  586|      0|    return por(pand(a, mask), pandnot(b, mask));
  587|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pselect_implINS0_20eigen_packet_wrapperIDv2_xLi5EEEvE3runERKS4_S7_S7_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pselect_implINS0_20eigen_packet_wrapperIDv2_xLi0EEEvE3runERKS4_S7_S7_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pselect_implIDv4_fvE3runERKS2_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pselect_implIDv2_dvE3runERKS2_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pselect_implINS0_9Packet1cdEvE3runERKS2_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pselect_implINS0_9Packet2cfEvE3runERKS2_S5_S5_
  ------------------
  588|       |};
  589|       |
  590|       |// For scalars, use ternary select.
  591|       |template <typename Packet>
  592|       |struct pselect_impl<Packet, std::enable_if_t<is_scalar<Packet>::value>> {
  593|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& mask, const Packet& a, const Packet& b) {
  594|       |    return numext::equal_strict(mask, Packet(0)) ? b : a;
  595|       |  }
  596|       |};
  597|       |
  598|       |/** \internal \returns \a or \b for each field in packet according to \mask */
  599|       |template <typename Packet>
  600|      0|EIGEN_DEVICE_FUNC inline Packet pselect(const Packet& mask, const Packet& a, const Packet& b) {
  601|      0|  return pselect_impl<Packet>::run(mask, a, b);
  602|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7pselectINS0_20eigen_packet_wrapperIDv2_xLi5EEEEET_RKS5_S7_S7_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7pselectINS0_20eigen_packet_wrapperIDv2_xLi0EEEEET_RKS5_S7_S7_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7pselectIDv4_fEET_RKS3_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7pselectIDv2_dEET_RKS3_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7pselectINS0_9Packet1cdEEET_RKS3_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7pselectINS0_9Packet2cfEEET_RKS3_S5_S5_
  ------------------
  603|       |
  604|       |template <>
  605|      0|EIGEN_DEVICE_FUNC inline bool pselect<bool>(const bool& cond, const bool& a, const bool& b) {
  606|      0|  return cond ? a : b;
  607|      0|}
  608|       |
  609|       |/** \internal \returns the min or of \a a and \a b (coeff-wise)
  610|       |    If either \a a or \a b are NaN, the result is implementation defined. */
  611|       |template <int NaNPropagation>
  612|       |struct pminmax_impl {
  613|       |  template <typename Packet, typename Op>
  614|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a, const Packet& b, Op op) {
  615|       |    return op(a, b);
  616|       |  }
  617|       |};
  618|       |
  619|       |/** \internal \returns the min or max of \a a and \a b (coeff-wise)
  620|       |    If either \a a or \a b are NaN, NaN is returned. */
  621|       |template <>
  622|       |struct pminmax_impl<PropagateNaN> {
  623|       |  template <typename Packet, typename Op>
  624|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a, const Packet& b, Op op) {
  625|       |    Packet not_nan_mask_a = pcmp_eq(a, a);
  626|       |    Packet not_nan_mask_b = pcmp_eq(b, b);
  627|       |    return pselect(not_nan_mask_a, pselect(not_nan_mask_b, op(a, b), b), a);
  628|       |  }
  629|       |};
  630|       |
  631|       |/** \internal \returns the min or max of \a a and \a b (coeff-wise)
  632|       |    If both \a a and \a b are NaN, NaN is returned.
  633|       |    Equivalent to std::fmin(a, b).  */
  634|       |template <>
  635|       |struct pminmax_impl<PropagateNumbers> {
  636|       |  template <typename Packet, typename Op>
  637|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a, const Packet& b, Op op) {
  638|       |    Packet not_nan_mask_a = pcmp_eq(a, a);
  639|       |    Packet not_nan_mask_b = pcmp_eq(b, b);
  640|       |    return pselect(not_nan_mask_a, pselect(not_nan_mask_b, op(a, b), a), b);
  641|       |  }
  642|       |};
  643|       |
  644|       |#define EIGEN_BINARY_OP_NAN_PROPAGATION(Type, Func) [](const Type& a, const Type& b) { return Func(a, b); }
  645|       |
  646|       |/** \internal \returns the min of \a a and \a b  (coeff-wise).
  647|       |    If \a a or \b b is NaN, the return value is implementation defined. */
  648|       |template <typename Packet>
  649|      0|EIGEN_DEVICE_FUNC inline Packet pmin(const Packet& a, const Packet& b) {
  650|      0|  return numext::mini(a, b);
  651|      0|}
  652|       |
  653|       |/** \internal \returns the min of \a a and \a b  (coeff-wise).
  654|       |    NaNPropagation determines the NaN propagation semantics. */
  655|       |template <int NaNPropagation, typename Packet>
  656|       |EIGEN_DEVICE_FUNC inline Packet pmin(const Packet& a, const Packet& b) {
  657|       |  return pminmax_impl<NaNPropagation>::run(a, b, EIGEN_BINARY_OP_NAN_PROPAGATION(Packet, (pmin<Packet>)));
  658|       |}
  659|       |
  660|       |/** \internal \returns the max of \a a and \a b  (coeff-wise)
  661|       |    If \a a or \b b is NaN, the return value is implementation defined. */
  662|       |template <typename Packet>
  663|      0|EIGEN_DEVICE_FUNC inline Packet pmax(const Packet& a, const Packet& b) {
  664|      0|  return numext::maxi(a, b);
  665|      0|}
  666|       |
  667|       |/** \internal \returns the max of \a a and \a b  (coeff-wise).
  668|       |    NaNPropagation determines the NaN propagation semantics. */
  669|       |template <int NaNPropagation, typename Packet>
  670|       |EIGEN_DEVICE_FUNC inline Packet pmax(const Packet& a, const Packet& b) {
  671|       |  return pminmax_impl<NaNPropagation>::run(a, b, EIGEN_BINARY_OP_NAN_PROPAGATION(Packet, (pmax<Packet>)));
  672|       |}
  673|       |
  674|       |/** \internal \returns the absolute value of \a a */
  675|       |template <typename Packet>
  676|       |EIGEN_DEVICE_FUNC inline Packet pabs(const Packet& a) {
  677|       |  return numext::abs(a);
  678|       |}
  679|       |template <>
  680|      0|EIGEN_DEVICE_FUNC inline unsigned int pabs(const unsigned int& a) {
  681|      0|  return a;
  682|      0|}
  683|       |template <>
  684|      0|EIGEN_DEVICE_FUNC inline unsigned long pabs(const unsigned long& a) {
  685|      0|  return a;
  686|      0|}
  687|       |template <>
  688|      0|EIGEN_DEVICE_FUNC inline unsigned long long pabs(const unsigned long long& a) {
  689|      0|  return a;
  690|      0|}
  691|       |
  692|       |/** \internal \returns the addsub value of \a a,b */
  693|       |template <typename Packet>
  694|       |EIGEN_DEVICE_FUNC inline Packet paddsub(const Packet& a, const Packet& b) {
  695|       |  return pselect(peven_mask(a), padd(a, b), psub(a, b));
  696|       |}
  697|       |
  698|       |/** \internal \returns the phase angle of \a a */
  699|       |template <typename Packet>
  700|       |EIGEN_DEVICE_FUNC inline Packet parg(const Packet& a) {
  701|       |  using numext::arg;
  702|       |  return arg(a);
  703|       |}
  704|       |
  705|       |/** \internal \returns \a a arithmetically shifted by N bits to the right */
  706|       |template <int N, typename T>
  707|       |EIGEN_DEVICE_FUNC inline T parithmetic_shift_right(const T& a) {
  708|       |  return numext::arithmetic_shift_right(a, N);
  709|       |}
  710|       |
  711|       |/** \internal \returns \a a logically shifted by N bits to the right */
  712|       |template <int N, typename T>
  713|       |EIGEN_DEVICE_FUNC inline T plogical_shift_right(const T& a) {
  714|       |  return numext::logical_shift_right(a, N);
  715|       |}
  716|       |
  717|       |/** \internal \returns \a a shifted by N bits to the left */
  718|       |template <int N, typename T>
  719|       |EIGEN_DEVICE_FUNC inline T plogical_shift_left(const T& a) {
  720|       |  return numext::logical_shift_left(a, N);
  721|       |}
  722|       |
  723|       |/** \internal \returns the significant and exponent of the underlying floating point numbers
  724|       | * See https://en.cppreference.com/w/cpp/numeric/math/frexp
  725|       | */
  726|       |template <typename Packet>
  727|       |EIGEN_DEVICE_FUNC inline Packet pfrexp(const Packet& a, Packet& exponent) {
  728|       |  int exp;
  729|       |  EIGEN_USING_STD(frexp);
  730|       |  Packet result = static_cast<Packet>(frexp(a, &exp));
  731|       |  exponent = static_cast<Packet>(exp);
  732|       |  return result;
  733|       |}
  734|       |
  735|       |/** \internal \returns a * 2^((int)exponent)
  736|       | * See https://en.cppreference.com/w/cpp/numeric/math/ldexp
  737|       | */
  738|       |template <typename Packet>
  739|       |EIGEN_DEVICE_FUNC inline Packet pldexp(const Packet& a, const Packet& exponent) {
  740|       |  EIGEN_USING_STD(ldexp)
  741|       |  return static_cast<Packet>(ldexp(a, static_cast<int>(exponent)));
  742|       |}
  743|       |
  744|       |/** \internal \returns the min of \a a and \a b  (coeff-wise) */
  745|       |template <typename Packet>
  746|       |EIGEN_DEVICE_FUNC inline Packet pabsdiff(const Packet& a, const Packet& b) {
  747|       |  return pselect(pcmp_lt(a, b), psub(b, a), psub(a, b));
  748|       |}
  749|       |
  750|       |/** \internal \returns a packet version of \a *from, from must be properly aligned */
  751|       |template <typename Packet>
  752|       |EIGEN_DEVICE_FUNC inline Packet pload(const typename unpacket_traits<Packet>::type* from) {
  753|       |  return *from;
  754|       |}
  755|       |
  756|       |/** \internal \returns n elements of a packet version of \a *from, from must be properly aligned
  757|       | * offset indicates the starting element in which to load and
  758|       | * offset + n <= unpacket_traits::size
  759|       | * All elements before offset and after the last element loaded will initialized with zero */
  760|       |template <typename Packet>
  761|       |EIGEN_DEVICE_FUNC inline Packet pload_partial(const typename unpacket_traits<Packet>::type* from, const Index n,
  762|       |                                              const Index offset = 0) {
  763|       |  const Index packet_size = unpacket_traits<Packet>::size;
  764|       |  eigen_assert(n + offset <= packet_size && "number of elements plus offset will read past end of packet");
  765|       |  typedef typename unpacket_traits<Packet>::type Scalar;
  766|       |  EIGEN_ALIGN_MAX Scalar elements[packet_size] = {Scalar(0)};
  767|       |  for (Index i = offset; i < numext::mini(n + offset, packet_size); i++) {
  768|       |    elements[i] = from[i - offset];
  769|       |  }
  770|       |  return pload<Packet>(elements);
  771|       |}
  772|       |
  773|       |/** \internal \returns a packet version of \a *from, (un-aligned load) */
  774|       |template <typename Packet>
  775|       |EIGEN_DEVICE_FUNC inline Packet ploadu(const typename unpacket_traits<Packet>::type* from) {
  776|       |  return *from;
  777|       |}
  778|       |
  779|       |/** \internal \returns n elements of a packet version of \a *from, (un-aligned load)
  780|       | * All elements after the last element loaded will initialized with zero */
  781|       |template <typename Packet>
  782|       |EIGEN_DEVICE_FUNC inline Packet ploadu_partial(const typename unpacket_traits<Packet>::type* from, const Index n,
  783|       |                                               const Index offset = 0) {
  784|       |  const Index packet_size = unpacket_traits<Packet>::size;
  785|       |  eigen_assert(n + offset <= packet_size && "number of elements plus offset will read past end of packet");
  786|       |  typedef typename unpacket_traits<Packet>::type Scalar;
  787|       |  EIGEN_ALIGN_MAX Scalar elements[packet_size] = {Scalar(0)};
  788|       |  for (Index i = offset; i < numext::mini(n + offset, packet_size); i++) {
  789|       |    elements[i] = from[i - offset];
  790|       |  }
  791|       |  return pload<Packet>(elements);
  792|       |}
  793|       |
  794|       |/** \internal \returns a packet version of \a *from, (un-aligned masked load)
  795|       | * There is no generic implementation. We only have implementations for specialized
  796|       | * cases. Generic case should not be called.
  797|       | */
  798|       |template <typename Packet>
  799|       |EIGEN_DEVICE_FUNC inline std::enable_if_t<unpacket_traits<Packet>::masked_load_available, Packet> ploadu(
  800|       |    const typename unpacket_traits<Packet>::type* from, typename unpacket_traits<Packet>::mask_t umask);
  801|       |
  802|       |/** \internal \returns a packet with constant coefficients \a a, e.g.: (a,a,a,a) */
  803|       |template <typename Packet>
  804|      0|EIGEN_DEVICE_FUNC inline Packet pset1(const typename unpacket_traits<Packet>::type& a) {
  805|      0|  return a;
  806|      0|}
  807|       |
  808|       |/** \internal \returns a packet with constant coefficients set from bits */
  809|       |template <typename Packet, typename BitsType>
  810|       |EIGEN_DEVICE_FUNC inline Packet pset1frombits(BitsType a);
  811|       |
  812|       |/** \internal \returns a packet with constant coefficients \a a[0], e.g.: (a[0],a[0],a[0],a[0]) */
  813|       |template <typename Packet>
  814|      0|EIGEN_DEVICE_FUNC inline Packet pload1(const typename unpacket_traits<Packet>::type* a) {
  815|      0|  return pset1<Packet>(*a);
  816|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pload1IDv2_dEET_PKNS0_15unpacket_traitsIS3_E4typeE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pload1IDv4_fEET_PKNS0_15unpacket_traitsIS3_E4typeE
  ------------------
  817|       |
  818|       |/** \internal \returns a packet with elements of \a *from duplicated.
  819|       | * For instance, for a packet of 8 elements, 4 scalars will be read from \a *from and
  820|       | * duplicated to form: {from[0],from[0],from[1],from[1],from[2],from[2],from[3],from[3]}
  821|       | * Currently, this function is only used for scalar * complex products.
  822|       | */
  823|       |template <typename Packet>
  824|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet ploaddup(const typename unpacket_traits<Packet>::type* from) {
  825|       |  return *from;
  826|       |}
  827|       |
  828|       |/** \internal \returns a packet with elements of \a *from quadrupled.
  829|       | * For instance, for a packet of 8 elements, 2 scalars will be read from \a *from and
  830|       | * replicated to form: {from[0],from[0],from[0],from[0],from[1],from[1],from[1],from[1]}
  831|       | * Currently, this function is only used in matrix products.
  832|       | * For packet-size smaller or equal to 4, this function is equivalent to pload1
  833|       | */
  834|       |template <typename Packet>
  835|       |EIGEN_DEVICE_FUNC inline Packet ploadquad(const typename unpacket_traits<Packet>::type* from) {
  836|       |  return pload1<Packet>(from);
  837|       |}
  838|       |
  839|       |/** \internal equivalent to
  840|       | * \code
  841|       | * a0 = pload1(a+0);
  842|       | * a1 = pload1(a+1);
  843|       | * a2 = pload1(a+2);
  844|       | * a3 = pload1(a+3);
  845|       | * \endcode
  846|       | * \sa pset1, pload1, ploaddup, pbroadcast2
  847|       | */
  848|       |template <typename Packet>
  849|       |EIGEN_DEVICE_FUNC inline void pbroadcast4(const typename unpacket_traits<Packet>::type* a, Packet& a0, Packet& a1,
  850|       |                                          Packet& a2, Packet& a3) {
  851|       |  a0 = pload1<Packet>(a + 0);
  852|       |  a1 = pload1<Packet>(a + 1);
  853|       |  a2 = pload1<Packet>(a + 2);
  854|       |  a3 = pload1<Packet>(a + 3);
  855|       |}
  856|       |
  857|       |/** \internal equivalent to
  858|       | * \code
  859|       | * a0 = pload1(a+0);
  860|       | * a1 = pload1(a+1);
  861|       | * \endcode
  862|       | * \sa pset1, pload1, ploaddup, pbroadcast4
  863|       | */
  864|       |template <typename Packet>
  865|       |EIGEN_DEVICE_FUNC inline void pbroadcast2(const typename unpacket_traits<Packet>::type* a, Packet& a0, Packet& a1) {
  866|       |  a0 = pload1<Packet>(a + 0);
  867|       |  a1 = pload1<Packet>(a + 1);
  868|       |}
  869|       |
  870|       |/** \internal \brief Returns a packet with coefficients (a,a+1,...,a+packet_size-1). */
  871|       |template <typename Packet>
  872|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet plset(const typename unpacket_traits<Packet>::type& a) {
  873|       |  return a;
  874|       |}
  875|       |
  876|       |/** \internal \returns a packet with constant coefficients \a a, e.g.: (x, 0, x, 0),
  877|       |     where x is the value of all 1-bits. */
  878|       |template <typename Packet>
  879|       |EIGEN_DEVICE_FUNC inline Packet peven_mask(const Packet& /*a*/) {
  880|       |  typedef typename unpacket_traits<Packet>::type Scalar;
  881|       |  const size_t n = unpacket_traits<Packet>::size;
  882|       |  EIGEN_ALIGN_TO_BOUNDARY(sizeof(Packet)) Scalar elements[n];
  883|       |  for (size_t i = 0; i < n; ++i) {
  884|       |    memset(elements + i, ((i & 1) == 0 ? 0xff : 0), sizeof(Scalar));
  885|       |  }
  886|       |  return ploadu<Packet>(elements);
  887|       |}
  888|       |
  889|       |/** \internal copy the packet \a from to \a *to, \a to must be properly aligned */
  890|       |template <typename Scalar, typename Packet>
  891|       |EIGEN_DEVICE_FUNC inline void pstore(Scalar* to, const Packet& from) {
  892|       |  (*to) = from;
  893|       |}
  894|       |
  895|       |/** \internal copy n elements of the packet \a from to \a *to, \a to must be properly aligned
  896|       | * offset indicates the starting element in which to store and
  897|       | * offset + n <= unpacket_traits::size */
  898|       |template <typename Scalar, typename Packet>
  899|       |EIGEN_DEVICE_FUNC inline void pstore_partial(Scalar* to, const Packet& from, const Index n, const Index offset = 0) {
  900|       |  const Index packet_size = unpacket_traits<Packet>::size;
  901|       |  eigen_assert(n + offset <= packet_size && "number of elements plus offset will write past end of packet");
  902|       |  EIGEN_ALIGN_MAX Scalar elements[packet_size];
  903|       |  pstore<Scalar>(elements, from);
  904|       |  for (Index i = 0; i < numext::mini(n, packet_size - offset); i++) {
  905|       |    to[i] = elements[i + offset];
  906|       |  }
  907|       |}
  908|       |
  909|       |/** \internal copy the packet \a from to \a *to, (un-aligned store) */
  910|       |template <typename Scalar, typename Packet>
  911|       |EIGEN_DEVICE_FUNC inline void pstoreu(Scalar* to, const Packet& from) {
  912|       |  (*to) = from;
  913|       |}
  914|       |
  915|       |/** \internal copy n elements of the packet \a from to \a *to, (un-aligned store) */
  916|       |template <typename Scalar, typename Packet>
  917|       |EIGEN_DEVICE_FUNC inline void pstoreu_partial(Scalar* to, const Packet& from, const Index n, const Index offset = 0) {
  918|       |  const Index packet_size = unpacket_traits<Packet>::size;
  919|       |  eigen_assert(n + offset <= packet_size && "number of elements plus offset will write past end of packet");
  920|       |  EIGEN_ALIGN_MAX Scalar elements[packet_size];
  921|       |  pstore<Scalar>(elements, from);
  922|       |  for (Index i = 0; i < numext::mini(n, packet_size - offset); i++) {
  923|       |    to[i] = elements[i + offset];
  924|       |  }
  925|       |}
  926|       |
  927|       |/** \internal copy the packet \a from to \a *to, (un-aligned store with a mask)
  928|       | * There is no generic implementation. We only have implementations for specialized
  929|       | * cases. Generic case should not be called.
  930|       | */
  931|       |template <typename Scalar, typename Packet>
  932|       |EIGEN_DEVICE_FUNC inline std::enable_if_t<unpacket_traits<Packet>::masked_store_available, void> pstoreu(
  933|       |    Scalar* to, const Packet& from, typename unpacket_traits<Packet>::mask_t umask);
  934|       |
  935|       |template <typename Scalar, typename Packet>
  936|       |EIGEN_DEVICE_FUNC inline Packet pgather(const Scalar* from, Index /*stride*/) {
  937|       |  return ploadu<Packet>(from);
  938|       |}
  939|       |
  940|       |template <typename Scalar, typename Packet>
  941|       |EIGEN_DEVICE_FUNC inline Packet pgather_partial(const Scalar* from, Index stride, const Index n) {
  942|       |  const Index packet_size = unpacket_traits<Packet>::size;
  943|       |  EIGEN_ALIGN_MAX Scalar elements[packet_size] = {Scalar(0)};
  944|       |  for (Index i = 0; i < numext::mini(n, packet_size); i++) {
  945|       |    elements[i] = from[i * stride];
  946|       |  }
  947|       |  return pload<Packet>(elements);
  948|       |}
  949|       |
  950|       |template <typename Scalar, typename Packet>
  951|       |EIGEN_DEVICE_FUNC inline void pscatter(Scalar* to, const Packet& from, Index /*stride*/) {
  952|       |  pstore(to, from);
  953|       |}
  954|       |
  955|       |template <typename Scalar, typename Packet>
  956|       |EIGEN_DEVICE_FUNC inline void pscatter_partial(Scalar* to, const Packet& from, Index stride, const Index n) {
  957|       |  const Index packet_size = unpacket_traits<Packet>::size;
  958|       |  EIGEN_ALIGN_MAX Scalar elements[packet_size];
  959|       |  pstore<Scalar>(elements, from);
  960|       |  for (Index i = 0; i < numext::mini(n, packet_size); i++) {
  961|       |    to[i * stride] = elements[i];
  962|       |  }
  963|       |}
  964|       |
  965|       |/** \internal tries to do cache prefetching of \a addr */
  966|       |template <typename Scalar>
  967|       |EIGEN_DEVICE_FUNC inline void prefetch(const Scalar* addr) {
  968|       |#if defined(EIGEN_HIP_DEVICE_COMPILE)
  969|       |  // do nothing
  970|       |#elif defined(EIGEN_CUDA_ARCH)
  971|       |#if defined(__LP64__) || EIGEN_OS_WIN64
  972|       |  // 64-bit pointer operand constraint for inlined asm
  973|       |  asm(" prefetch.L1 [ %1 ];" : "=l"(addr) : "l"(addr));
  974|       |#else
  975|       |  // 32-bit pointer operand constraint for inlined asm
  976|       |  asm(" prefetch.L1 [ %1 ];" : "=r"(addr) : "r"(addr));
  977|       |#endif
  978|       |#elif (!EIGEN_COMP_MSVC) && (EIGEN_COMP_GNUC || EIGEN_COMP_CLANG || EIGEN_COMP_ICC)
  979|       |  __builtin_prefetch(addr);
  980|       |#endif
  981|       |}
  982|       |
  983|       |/** \internal \returns the reversed elements of \a a*/
  984|       |template <typename Packet>
  985|       |EIGEN_DEVICE_FUNC inline Packet preverse(const Packet& a) {
  986|       |  return a;
  987|       |}
  988|       |
  989|       |/** \internal \returns \a a with real and imaginary part flipped (for complex type only) */
  990|       |template <typename Packet>
  991|       |EIGEN_DEVICE_FUNC inline Packet pcplxflip(const Packet& a) {
  992|       |  return Packet(numext::imag(a), numext::real(a));
  993|       |}
  994|       |
  995|       |/**************************
  996|       | * Special math functions
  997|       | ***************************/
  998|       |
  999|       |/** \internal \returns isnan(a) */
 1000|       |template <typename Packet>
 1001|      0|EIGEN_DEVICE_FUNC inline Packet pisnan(const Packet& a) {
 1002|      0|  return pandnot(ptrue(a), pcmp_eq(a, a));
 1003|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pisnanIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pisnanINS0_9Packet1cdEEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pisnanINS0_9Packet2cfEEET_RKS3_
  ------------------
 1004|       |
 1005|       |/** \internal \returns isinf(a) */
 1006|       |template <typename Packet>
 1007|       |EIGEN_DEVICE_FUNC inline Packet pisinf(const Packet& a) {
 1008|       |  using Scalar = typename unpacket_traits<Packet>::type;
 1009|       |  constexpr Scalar inf = NumTraits<Scalar>::infinity();
 1010|       |  return pcmp_eq(pabs(a), pset1<Packet>(inf));
 1011|       |}
 1012|       |
 1013|       |/** \internal \returns the sine of \a a (coeff-wise) */
 1014|       |template <typename Packet>
 1015|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psin(const Packet& a) {
 1016|       |  EIGEN_USING_STD(sin);
 1017|       |  return sin(a);
 1018|       |}
 1019|       |
 1020|       |/** \internal \returns the cosine of \a a (coeff-wise) */
 1021|       |template <typename Packet>
 1022|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pcos(const Packet& a) {
 1023|       |  EIGEN_USING_STD(cos);
 1024|       |  return cos(a);
 1025|       |}
 1026|       |
 1027|       |/** \internal \returns the tan of \a a (coeff-wise) */
 1028|       |template <typename Packet>
 1029|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet ptan(const Packet& a) {
 1030|       |  EIGEN_USING_STD(tan);
 1031|       |  return tan(a);
 1032|       |}
 1033|       |
 1034|       |/** \internal \returns the arc sine of \a a (coeff-wise) */
 1035|       |template <typename Packet>
 1036|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pasin(const Packet& a) {
 1037|       |  EIGEN_USING_STD(asin);
 1038|       |  return asin(a);
 1039|       |}
 1040|       |
 1041|       |/** \internal \returns the arc cosine of \a a (coeff-wise) */
 1042|       |template <typename Packet>
 1043|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pacos(const Packet& a) {
 1044|       |  EIGEN_USING_STD(acos);
 1045|       |  return acos(a);
 1046|       |}
 1047|       |
 1048|       |/** \internal \returns the hyperbolic sine of \a a (coeff-wise) */
 1049|       |template <typename Packet>
 1050|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psinh(const Packet& a) {
 1051|       |  EIGEN_USING_STD(sinh);
 1052|       |  return sinh(a);
 1053|       |}
 1054|       |
 1055|       |/** \internal \returns the hyperbolic cosine of \a a (coeff-wise) */
 1056|       |template <typename Packet>
 1057|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pcosh(const Packet& a) {
 1058|       |  EIGEN_USING_STD(cosh);
 1059|       |  return cosh(a);
 1060|       |}
 1061|       |
 1062|       |/** \internal \returns the arc tangent of \a a (coeff-wise) */
 1063|       |template <typename Packet>
 1064|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patan(const Packet& a) {
 1065|       |  EIGEN_USING_STD(atan);
 1066|       |  return atan(a);
 1067|       |}
 1068|       |
 1069|       |/** \internal \returns the hyperbolic tan of \a a (coeff-wise) */
 1070|       |template <typename Packet>
 1071|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet ptanh(const Packet& a) {
 1072|       |  EIGEN_USING_STD(tanh);
 1073|       |  return tanh(a);
 1074|       |}
 1075|       |
 1076|       |/** \internal \returns the arc tangent of \a a (coeff-wise) */
 1077|       |template <typename Packet>
 1078|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patanh(const Packet& a) {
 1079|       |  EIGEN_USING_STD(atanh);
 1080|       |  return atanh(a);
 1081|       |}
 1082|       |
 1083|       |/** \internal \returns the exp of \a a (coeff-wise) */
 1084|       |template <typename Packet>
 1085|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp(const Packet& a) {
 1086|       |  return numext::exp(a);
 1087|       |}
 1088|       |
 1089|       |/** \internal \returns the exp2 of \a a (coeff-wise) */
 1090|       |template <typename Packet>
 1091|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp2(const Packet& a) {
 1092|       |  return numext::exp2(a);
 1093|       |}
 1094|       |
 1095|       |/** \internal \returns the expm1 of \a a (coeff-wise) */
 1096|       |template <typename Packet>
 1097|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexpm1(const Packet& a) {
 1098|       |  return numext::expm1(a);
 1099|       |}
 1100|       |
 1101|       |/** \internal \returns the log of \a a (coeff-wise) */
 1102|       |template <typename Packet>
 1103|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog(const Packet& a) {
 1104|       |  EIGEN_USING_STD(log);
 1105|       |  return log(a);
 1106|       |}
 1107|       |
 1108|       |/** \internal \returns the log1p of \a a (coeff-wise) */
 1109|       |template <typename Packet>
 1110|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog1p(const Packet& a) {
 1111|       |  return numext::log1p(a);
 1112|       |}
 1113|       |
 1114|       |/** \internal \returns the log10 of \a a (coeff-wise) */
 1115|       |template <typename Packet>
 1116|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog10(const Packet& a) {
 1117|       |  EIGEN_USING_STD(log10);
 1118|       |  return log10(a);
 1119|       |}
 1120|       |
 1121|       |/** \internal \returns the log2 of \a a (coeff-wise) */
 1122|       |template <typename Packet>
 1123|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog2(const Packet& a) {
 1124|       |  using Scalar = typename internal::unpacket_traits<Packet>::type;
 1125|       |  using RealScalar = typename NumTraits<Scalar>::Real;
 1126|       |  return pmul(pset1<Packet>(Scalar(RealScalar(EIGEN_LOG2E))), plog(a));
 1127|       |}
 1128|       |
 1129|       |/** \internal \returns the square-root of \a a (coeff-wise) */
 1130|       |template <typename Packet>
 1131|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psqrt(const Packet& a) {
 1132|       |  return numext::sqrt(a);
 1133|       |}
 1134|       |
 1135|       |/** \internal \returns the cube-root of \a a (coeff-wise) */
 1136|       |template <typename Packet>
 1137|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pcbrt(const Packet& a) {
 1138|       |  return numext::cbrt(a);
 1139|       |}
 1140|       |
 1141|       |template <typename Packet, bool IsScalar = is_scalar<Packet>::value,
 1142|       |          bool IsInteger = NumTraits<typename unpacket_traits<Packet>::type>::IsInteger>
 1143|       |struct nearest_integer_packetop_impl {
 1144|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_floor(const Packet& x) { return numext::floor(x); }
 1145|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_ceil(const Packet& x) { return numext::ceil(x); }
 1146|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_rint(const Packet& x) { return numext::rint(x); }
 1147|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_round(const Packet& x) { return numext::round(x); }
 1148|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_trunc(const Packet& x) { return numext::trunc(x); }
 1149|       |};
 1150|       |
 1151|       |/** \internal \returns the rounded value of \a a (coeff-wise) */
 1152|       |template <typename Packet>
 1153|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet pround(const Packet& a) {
 1154|       |  return nearest_integer_packetop_impl<Packet>::run_round(a);
 1155|       |}
 1156|       |
 1157|       |/** \internal \returns the floor of \a a (coeff-wise) */
 1158|       |template <typename Packet>
 1159|      0|EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet pfloor(const Packet& a) {
 1160|      0|  return nearest_integer_packetop_impl<Packet>::run_floor(a);
 1161|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pfloorIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pfloorIDv2_dEET_RKS3_
  ------------------
 1162|       |
 1163|       |/** \internal \returns the rounded value of \a a (coeff-wise) with current
 1164|       | * rounding mode */
 1165|       |template <typename Packet>
 1166|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet print(const Packet& a) {
 1167|       |  return nearest_integer_packetop_impl<Packet>::run_rint(a);
 1168|       |}
 1169|       |
 1170|       |/** \internal \returns the ceil of \a a (coeff-wise) */
 1171|       |template <typename Packet>
 1172|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet pceil(const Packet& a) {
 1173|       |  return nearest_integer_packetop_impl<Packet>::run_ceil(a);
 1174|       |}
 1175|       |
 1176|       |/** \internal \returns the truncation of \a a (coeff-wise) */
 1177|       |template <typename Packet>
 1178|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet ptrunc(const Packet& a) {
 1179|       |  return nearest_integer_packetop_impl<Packet>::run_trunc(a);
 1180|       |}
 1181|       |
 1182|       |template <typename Packet, typename EnableIf = void>
 1183|       |struct psign_impl {
 1184|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a) { return numext::sign(a); }
 1185|       |};
 1186|       |
 1187|       |/** \internal \returns the sign of \a a (coeff-wise) */
 1188|       |template <typename Packet>
 1189|       |EIGEN_DEVICE_FUNC inline Packet psign(const Packet& a) {
 1190|       |  return psign_impl<Packet>::run(a);
 1191|       |}
 1192|       |
 1193|       |template <>
 1194|      0|EIGEN_DEVICE_FUNC inline bool psign(const bool& a) {
 1195|      0|  return a;
 1196|      0|}
 1197|       |
 1198|       |/** \internal \returns the first element of a packet */
 1199|       |template <typename Packet>
 1200|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type pfirst(const Packet& a) {
 1201|       |  return a;
 1202|       |}
 1203|       |
 1204|       |/** \internal \returns the sum of the elements of upper and lower half of \a a if \a a is larger than 4.
 1205|       | * For a packet {a0, a1, a2, a3, a4, a5, a6, a7}, it returns a half packet {a0+a4, a1+a5, a2+a6, a3+a7}
 1206|       | * For packet-size smaller or equal to 4, this boils down to a noop.
 1207|       | */
 1208|       |template <typename Packet>
 1209|       |EIGEN_DEVICE_FUNC inline std::conditional_t<(unpacket_traits<Packet>::size % 8) == 0,
 1210|       |                                            typename unpacket_traits<Packet>::half, Packet>
 1211|       |predux_half_dowto4(const Packet& a) {
 1212|       |  return a;
 1213|       |}
 1214|       |
 1215|       |// Slow generic implementation of Packet reduction.
 1216|       |template <typename Packet, typename Op>
 1217|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type predux_helper(const Packet& a, Op op) {
 1218|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1219|       |  const size_t n = unpacket_traits<Packet>::size;
 1220|       |  EIGEN_ALIGN_TO_BOUNDARY(sizeof(Packet)) Scalar elements[n];
 1221|       |  pstoreu<Scalar>(elements, a);
 1222|       |  for (size_t k = n / 2; k > 0; k /= 2) {
 1223|       |    for (size_t i = 0; i < k; ++i) {
 1224|       |      elements[i] = op(elements[i], elements[i + k]);
 1225|       |    }
 1226|       |  }
 1227|       |  return elements[0];
 1228|       |}
 1229|       |
 1230|       |/** \internal \returns the sum of the elements of \a a*/
 1231|       |template <typename Packet>
 1232|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type predux(const Packet& a) {
 1233|       |  return a;
 1234|       |}
 1235|       |
 1236|       |/** \internal \returns the product of the elements of \a a */
 1237|       |template <typename Packet>
 1238|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type predux_mul(const Packet& a) {
 1239|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1240|       |  return predux_helper(a, EIGEN_BINARY_OP_NAN_PROPAGATION(Scalar, (pmul<Scalar>)));
 1241|       |}
 1242|       |
 1243|       |/** \internal \returns the min of the elements of \a a */
 1244|       |template <typename Packet>
 1245|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type predux_min(const Packet& a) {
 1246|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1247|       |  return predux_helper(a, EIGEN_BINARY_OP_NAN_PROPAGATION(Scalar, (pmin<PropagateFast, Scalar>)));
 1248|       |}
 1249|       |
 1250|       |template <int NaNPropagation, typename Packet>
 1251|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type predux_min(const Packet& a) {
 1252|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1253|       |  return predux_helper(a, EIGEN_BINARY_OP_NAN_PROPAGATION(Scalar, (pmin<NaNPropagation, Scalar>)));
 1254|       |}
 1255|       |
 1256|       |/** \internal \returns the min of the elements of \a a */
 1257|       |template <typename Packet>
 1258|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type predux_max(const Packet& a) {
 1259|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1260|       |  return predux_helper(a, EIGEN_BINARY_OP_NAN_PROPAGATION(Scalar, (pmax<PropagateFast, Scalar>)));
 1261|       |}
 1262|       |
 1263|       |template <int NaNPropagation, typename Packet>
 1264|       |EIGEN_DEVICE_FUNC inline typename unpacket_traits<Packet>::type predux_max(const Packet& a) {
 1265|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1266|       |  return predux_helper(a, EIGEN_BINARY_OP_NAN_PROPAGATION(Scalar, (pmax<NaNPropagation, Scalar>)));
 1267|       |}
 1268|       |
 1269|       |#undef EIGEN_BINARY_OP_NAN_PROPAGATION
 1270|       |
 1271|       |/** \internal \returns true if all coeffs of \a a means "true"
 1272|       | * It is supposed to be called on values returned by pcmp_*.
 1273|       | */
 1274|       |// not needed yet
 1275|       |// template<typename Packet> EIGEN_DEVICE_FUNC inline bool predux_all(const Packet& a)
 1276|       |// { return bool(a); }
 1277|       |
 1278|       |/** \internal \returns true if any coeffs of \a a means "true"
 1279|       | * It is supposed to be called on values returned by pcmp_*.
 1280|       | */
 1281|       |template <typename Packet>
 1282|       |EIGEN_DEVICE_FUNC inline bool predux_any(const Packet& a) {
 1283|       |  // Dirty but generic implementation where "true" is assumed to be non 0 and all the sames.
 1284|       |  // It is expected that "true" is either:
 1285|       |  //  - Scalar(1)
 1286|       |  //  - bits full of ones (NaN for floats),
 1287|       |  //  - or first bit equals to 1 (1 for ints, smallest denormal for floats).
 1288|       |  // For all these cases, taking the sum is just fine, and this boils down to a no-op for scalars.
 1289|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1290|       |  return numext::not_equal_strict(predux(a), Scalar(0));
 1291|       |}
 1292|       |
 1293|       |/***************************************************************************
 1294|       | * The following functions might not have to be overwritten for vectorized types
 1295|       | ***************************************************************************/
 1296|       |
 1297|       |// FMA instructions.
 1298|       |/** \internal \returns a * b + c (coeff-wise) */
 1299|       |template <typename Packet>
 1300|      0|EIGEN_DEVICE_FUNC inline Packet pmadd(const Packet& a, const Packet& b, const Packet& c) {
 1301|      0|  return padd(pmul(a, b), c);
 1302|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5pmaddIfEET_RKS2_S4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5pmaddIDv4_fEET_RKS3_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5pmaddIDv2_dEET_RKS3_S5_S5_
  ------------------
 1303|       |
 1304|       |/** \internal \returns a * b - c (coeff-wise) */
 1305|       |template <typename Packet>
 1306|       |EIGEN_DEVICE_FUNC inline Packet pmsub(const Packet& a, const Packet& b, const Packet& c) {
 1307|       |  return psub(pmul(a, b), c);
 1308|       |}
 1309|       |
 1310|       |/** \internal \returns -(a * b) + c (coeff-wise) */
 1311|       |template <typename Packet>
 1312|      0|EIGEN_DEVICE_FUNC inline Packet pnmadd(const Packet& a, const Packet& b, const Packet& c) {
 1313|      0|  return psub(c, pmul(a, b));
 1314|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pnmaddINS0_20eigen_packet_wrapperIDv2_xLi0EEEEET_RKS5_S7_S7_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pnmaddIDv4_fEET_RKS3_S5_S5_
  ------------------
 1315|       |
 1316|       |/** \internal \returns -((a * b + c) (coeff-wise) */
 1317|       |template <typename Packet>
 1318|       |EIGEN_DEVICE_FUNC inline Packet pnmsub(const Packet& a, const Packet& b, const Packet& c) {
 1319|       |  return pnegate(pmadd(a, b, c));
 1320|       |}
 1321|       |
 1322|       |/** \internal copy a packet with constant coefficient \a a (e.g., [a,a,a,a]) to \a *to. \a to must be 16 bytes aligned
 1323|       | */
 1324|       |// NOTE: this function must really be templated on the packet type (think about different packet types for the same
 1325|       |// scalar type)
 1326|       |template <typename Packet>
 1327|       |inline void pstore1(typename unpacket_traits<Packet>::type* to, const typename unpacket_traits<Packet>::type& a) {
 1328|       |  pstore(to, pset1<Packet>(a));
 1329|       |}
 1330|       |
 1331|       |/** \internal \returns a packet version of \a *from.
 1332|       | * The pointer \a from must be aligned on a \a Alignment bytes boundary. */
 1333|       |template <typename Packet, int Alignment>
 1334|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet ploadt(const typename unpacket_traits<Packet>::type* from) {
 1335|       |  if (Alignment >= unpacket_traits<Packet>::alignment)
 1336|       |    return pload<Packet>(from);
 1337|       |  else
 1338|       |    return ploadu<Packet>(from);
 1339|       |}
 1340|       |
 1341|       |/** \internal \returns n elements of a packet version of \a *from.
 1342|       | * The pointer \a from must be aligned on a \a Alignment bytes boundary. */
 1343|       |template <typename Packet, int Alignment>
 1344|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet ploadt_partial(const typename unpacket_traits<Packet>::type* from,
 1345|       |                                                            const Index n, const Index offset = 0) {
 1346|       |  if (Alignment >= unpacket_traits<Packet>::alignment)
 1347|       |    return pload_partial<Packet>(from, n, offset);
 1348|       |  else
 1349|       |    return ploadu_partial<Packet>(from, n, offset);
 1350|       |}
 1351|       |
 1352|       |/** \internal copy the packet \a from to \a *to.
 1353|       | * The pointer \a from must be aligned on a \a Alignment bytes boundary. */
 1354|       |template <typename Scalar, typename Packet, int Alignment>
 1355|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void pstoret(Scalar* to, const Packet& from) {
 1356|       |  if (Alignment >= unpacket_traits<Packet>::alignment)
 1357|       |    pstore(to, from);
 1358|       |  else
 1359|       |    pstoreu(to, from);
 1360|       |}
 1361|       |
 1362|       |/** \internal copy n elements of the packet \a from to \a *to.
 1363|       | * The pointer \a from must be aligned on a \a Alignment bytes boundary. */
 1364|       |template <typename Scalar, typename Packet, int Alignment>
 1365|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void pstoret_partial(Scalar* to, const Packet& from, const Index n,
 1366|       |                                                           const Index offset = 0) {
 1367|       |  if (Alignment >= unpacket_traits<Packet>::alignment)
 1368|       |    pstore_partial(to, from, n, offset);
 1369|       |  else
 1370|       |    pstoreu_partial(to, from, n, offset);
 1371|       |}
 1372|       |
 1373|       |/** \internal \returns a packet version of \a *from.
 1374|       | * Unlike ploadt, ploadt_ro takes advantage of the read-only memory path on the
 1375|       | * hardware if available to speedup the loading of data that won't be modified
 1376|       | * by the current computation.
 1377|       | */
 1378|       |template <typename Packet, int LoadMode>
 1379|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet ploadt_ro(const typename unpacket_traits<Packet>::type* from) {
 1380|       |  return ploadt<Packet, LoadMode>(from);
 1381|       |}
 1382|       |
 1383|       |/***************************************************************************
 1384|       | * Fast complex products (GCC generates a function call which is very slow)
 1385|       | ***************************************************************************/
 1386|       |
 1387|       |// Eigen+CUDA does not support complexes.
 1388|       |#if !defined(EIGEN_GPUCC)
 1389|       |
 1390|       |template <>
 1391|      0|inline std::complex<float> pmul(const std::complex<float>& a, const std::complex<float>& b) {
 1392|      0|  return std::complex<float>(a.real() * b.real() - a.imag() * b.imag(), a.imag() * b.real() + a.real() * b.imag());
 1393|      0|}
 1394|       |
 1395|       |template <>
 1396|      0|inline std::complex<double> pmul(const std::complex<double>& a, const std::complex<double>& b) {
 1397|      0|  return std::complex<double>(a.real() * b.real() - a.imag() * b.imag(), a.imag() * b.real() + a.real() * b.imag());
 1398|      0|}
 1399|       |
 1400|       |#endif
 1401|       |
 1402|       |/***************************************************************************
 1403|       | * PacketBlock, that is a collection of N packets where the number of words
 1404|       | * in the packet is a multiple of N.
 1405|       | ***************************************************************************/
 1406|       |template <typename Packet, int N = unpacket_traits<Packet>::size>
 1407|       |struct PacketBlock {
 1408|       |  Packet packet[N];
 1409|       |};
 1410|       |
 1411|       |template <typename Packet>
 1412|       |EIGEN_DEVICE_FUNC inline void ptranspose(PacketBlock<Packet, 1>& /*kernel*/) {
 1413|       |  // Nothing to do in the scalar case, i.e. a 1x1 matrix.
 1414|       |}
 1415|       |
 1416|       |/***************************************************************************
 1417|       | * Selector, i.e. vector of N boolean values used to select (i.e. blend)
 1418|       | * words from 2 packets.
 1419|       | ***************************************************************************/
 1420|       |template <size_t N>
 1421|       |struct Selector {
 1422|       |  bool select[N];
 1423|       |};
 1424|       |
 1425|       |template <typename Packet>
 1426|       |EIGEN_DEVICE_FUNC inline Packet pblend(const Selector<unpacket_traits<Packet>::size>& ifPacket,
 1427|       |                                       const Packet& thenPacket, const Packet& elsePacket) {
 1428|       |  return ifPacket.select[0] ? thenPacket : elsePacket;
 1429|       |}
 1430|       |
 1431|       |/** \internal \returns 1 / a (coeff-wise) */
 1432|       |template <typename Packet>
 1433|      0|EIGEN_DEVICE_FUNC inline Packet preciprocal(const Packet& a) {
 1434|      0|  using Scalar = typename unpacket_traits<Packet>::type;
 1435|      0|  return pdiv(pset1<Packet>(Scalar(1)), a);
 1436|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal11preciprocalIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal11preciprocalIDv2_dEET_RKS3_
  ------------------
 1437|       |
 1438|       |/** \internal \returns the reciprocal square-root of \a a (coeff-wise) */
 1439|       |template <typename Packet>
 1440|       |EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet prsqrt(const Packet& a) {
 1441|       |  return preciprocal<Packet>(psqrt(a));
 1442|       |}
 1443|       |
 1444|       |template <typename Packet, bool IsScalar = is_scalar<Packet>::value,
 1445|       |          bool IsInteger = NumTraits<typename unpacket_traits<Packet>::type>::IsInteger>
 1446|       |struct psignbit_impl;
 1447|       |template <typename Packet, bool IsInteger>
 1448|       |struct psignbit_impl<Packet, true, IsInteger> {
 1449|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE static constexpr Packet run(const Packet& a) { return numext::signbit(a); }
 1450|       |};
 1451|       |template <typename Packet>
 1452|       |struct psignbit_impl<Packet, false, false> {
 1453|       |  // generic implementation if not specialized in PacketMath.h
 1454|       |  // slower than arithmetic shift
 1455|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1456|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE static Packet run(const Packet& a) {
 1457|       |    const Packet cst_pos_one = pset1<Packet>(Scalar(1));
 1458|       |    const Packet cst_neg_one = pset1<Packet>(Scalar(-1));
 1459|       |    return pcmp_eq(por(pand(a, cst_neg_one), cst_pos_one), cst_neg_one);
 1460|       |  }
 1461|       |};
 1462|       |template <typename Packet>
 1463|       |struct psignbit_impl<Packet, false, true> {
 1464|       |  // generic implementation for integer packets
 1465|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE static constexpr Packet run(const Packet& a) { return pcmp_lt(a, pzero(a)); }
 1466|       |};
 1467|       |/** \internal \returns the sign bit of \a a as a bitmask*/
 1468|       |template <typename Packet>
 1469|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE constexpr Packet psignbit(const Packet& a) {
 1470|       |  return psignbit_impl<Packet>::run(a);
 1471|       |}
 1472|       |
 1473|       |/** \internal \returns the 2-argument arc tangent of \a y and \a x (coeff-wise) */
 1474|       |template <typename Packet, std::enable_if_t<is_scalar<Packet>::value, int> = 0>
 1475|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet patan2(const Packet& y, const Packet& x) {
 1476|       |  return numext::atan2(y, x);
 1477|       |}
 1478|       |
 1479|       |/** \internal \returns the 2-argument arc tangent of \a y and \a x (coeff-wise) */
 1480|       |template <typename Packet, std::enable_if_t<!is_scalar<Packet>::value, int> = 0>
 1481|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet patan2(const Packet& y, const Packet& x) {
 1482|      0|  typedef typename internal::unpacket_traits<Packet>::type Scalar;
 1483|      0|
 1484|      0|  // See https://en.cppreference.com/w/cpp/numeric/math/atan2
 1485|      0|  // for how corner cases are supposed to be handled according to the
 1486|      0|  // IEEE floating-point standard (IEC 60559).
 1487|      0|  const Packet kSignMask = pset1<Packet>(-Scalar(0));
 1488|      0|  const Packet kZero = pzero(x);
 1489|      0|  const Packet kOne = pset1<Packet>(Scalar(1));
 1490|      0|  const Packet kPi = pset1<Packet>(Scalar(EIGEN_PI));
 1491|      0|
 1492|      0|  const Packet x_has_signbit = psignbit(x);
 1493|      0|  const Packet y_signmask = pand(y, kSignMask);
 1494|      0|  const Packet x_signmask = pand(x, kSignMask);
 1495|      0|  const Packet result_signmask = pxor(y_signmask, x_signmask);
 1496|      0|  const Packet shift = por(pand(x_has_signbit, kPi), y_signmask);
 1497|      0|
 1498|      0|  const Packet x_and_y_are_same = pcmp_eq(pabs(x), pabs(y));
 1499|      0|  const Packet x_and_y_are_zero = pcmp_eq(por(x, y), kZero);
 1500|      0|
 1501|      0|  Packet arg = pdiv(y, x);
 1502|      0|  arg = pselect(x_and_y_are_same, por(kOne, result_signmask), arg);
 1503|      0|  arg = pselect(x_and_y_are_zero, result_signmask, arg);
 1504|      0|
 1505|      0|  Packet result = patan(arg);
 1506|      0|  result = padd(result, shift);
 1507|      0|  return result;
 1508|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6patan2IDv2_dLi0EEET_RKS3_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6patan2IDv4_fLi0EEET_RKS3_S5_
  ------------------
 1509|       |
 1510|       |/** \internal \returns the argument of \a a as a complex number */
 1511|       |template <typename Packet, std::enable_if_t<is_scalar<Packet>::value, int> = 0>
 1512|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet pcarg(const Packet& a) {
 1513|       |  return Packet(numext::arg(a));
 1514|       |}
 1515|       |
 1516|       |/** \internal \returns the argument of \a a as a complex number */
 1517|       |template <typename Packet, std::enable_if_t<!is_scalar<Packet>::value, int> = 0>
 1518|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Packet pcarg(const Packet& a) {
 1519|       |  EIGEN_STATIC_ASSERT(NumTraits<typename unpacket_traits<Packet>::type>::IsComplex,
 1520|       |                      THIS METHOD IS FOR COMPLEX TYPES ONLY)
 1521|       |  using RealPacket = typename unpacket_traits<Packet>::as_real;
 1522|       |  // a                                              // r     i    r     i    ...
 1523|       |  RealPacket aflip = pcplxflip(a).v;                // i     r    i     r    ...
 1524|       |  RealPacket result = patan2(aflip, a.v);           // atan2 crap atan2 crap ...
 1525|       |  return (Packet)pand(result, peven_mask(result));  // atan2 0    atan2 0    ...
 1526|       |}
 1527|       |
 1528|       |}  // end namespace internal
 1529|       |
 1530|       |}  // end namespace Eigen
 1531|       |
 1532|       |#endif  // EIGEN_GENERIC_PACKET_MATH_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/IO.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2006-2008 Benoit Jacob <jacob.benoit.1@gmail.com>
    5|       |// Copyright (C) 2008 Gael Guennebaud <gael.guennebaud@inria.fr>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_IO_H
   12|       |#define EIGEN_IO_H
   13|       |
   14|       |// IWYU pragma: private
   15|       |#include "./InternalHeaderCheck.h"
   16|       |
   17|       |namespace Eigen {
   18|       |
   19|       |enum { DontAlignCols = 1 };
   20|       |enum { StreamPrecision = -1, FullPrecision = -2 };
   21|       |
   22|       |namespace internal {
   23|       |template <typename Derived>
   24|       |std::ostream& print_matrix(std::ostream& s, const Derived& _m, const IOFormat& fmt);
   25|       |}
   26|       |
   27|       |/** \class IOFormat
   28|       | * \ingroup Core_Module
   29|       | *
   30|       | * \brief Stores a set of parameters controlling the way matrices are printed
   31|       | *
   32|       | * List of available parameters:
   33|       | *  - \b precision number of digits for floating point values, or one of the special constants \c StreamPrecision and \c
   34|       | * FullPrecision. The default is the special value \c StreamPrecision which means to use the stream's own precision
   35|       | * setting, as set for instance using \c cout.precision(3). The other special value \c FullPrecision means that the
   36|       | * number of digits will be computed to match the full precision of each floating-point type.
   37|       | *  - \b flags an OR-ed combination of flags, the default value is 0, the only currently available flag is \c
   38|       | * DontAlignCols which allows to disable the alignment of columns, resulting in faster code.
   39|       | *  - \b coeffSeparator string printed between two coefficients of the same row
   40|       | *  - \b rowSeparator string printed between two rows
   41|       | *  - \b rowPrefix string printed at the beginning of each row
   42|       | *  - \b rowSuffix string printed at the end of each row
   43|       | *  - \b matPrefix string printed at the beginning of the matrix
   44|       | *  - \b matSuffix string printed at the end of the matrix
   45|       | *  - \b fill character printed to fill the empty space in aligned columns
   46|       | *
   47|       | * Example: \include IOFormat.cpp
   48|       | * Output: \verbinclude IOFormat.out
   49|       | *
   50|       | * \sa DenseBase::format(), class WithFormat
   51|       | */
   52|       |struct IOFormat {
   53|       |  /** Default constructor, see class IOFormat for the meaning of the parameters */
   54|       |  IOFormat(int _precision = StreamPrecision, int _flags = 0, const std::string& _coeffSeparator = " ",
   55|       |           const std::string& _rowSeparator = "\n", const std::string& _rowPrefix = "",
   56|       |           const std::string& _rowSuffix = "", const std::string& _matPrefix = "", const std::string& _matSuffix = "",
   57|       |           const char _fill = ' ')
   58|       |      : matPrefix(_matPrefix),
   59|       |        matSuffix(_matSuffix),
   60|       |        rowPrefix(_rowPrefix),
   61|       |        rowSuffix(_rowSuffix),
   62|       |        rowSeparator(_rowSeparator),
   63|       |        rowSpacer(""),
   64|       |        coeffSeparator(_coeffSeparator),
   65|       |        fill(_fill),
   66|       |        precision(_precision),
   67|      0|        flags(_flags) {
   68|      0|    // TODO check if rowPrefix, rowSuffix or rowSeparator contains a newline
   69|      0|    // don't add rowSpacer if columns are not to be aligned
   70|      0|    if ((flags & DontAlignCols)) return;
   71|      0|    int i = int(matPrefix.length()) - 1;
   72|      0|    while (i >= 0 && matPrefix[i] != '\n') {
   73|      0|      rowSpacer += ' ';
   74|      0|      i--;
   75|      0|    }
   76|      0|  }
   77|       |  std::string matPrefix, matSuffix;
   78|       |  std::string rowPrefix, rowSuffix, rowSeparator, rowSpacer;
   79|       |  std::string coeffSeparator;
   80|       |  char fill;
   81|       |  int precision;
   82|       |  int flags;
   83|       |};
   84|       |
   85|       |/** \class WithFormat
   86|       | * \ingroup Core_Module
   87|       | *
   88|       | * \brief Pseudo expression providing matrix output with given format
   89|       | *
   90|       | * \tparam ExpressionType the type of the object on which IO stream operations are performed
   91|       | *
   92|       | * This class represents an expression with stream operators controlled by a given IOFormat.
   93|       | * It is the return type of DenseBase::format()
   94|       | * and most of the time this is the only way it is used.
   95|       | *
   96|       | * See class IOFormat for some examples.
   97|       | *
   98|       | * \sa DenseBase::format(), class IOFormat
   99|       | */
  100|       |template <typename ExpressionType>
  101|       |class WithFormat {
  102|       | public:
  103|       |  WithFormat(const ExpressionType& matrix, const IOFormat& format) : m_matrix(matrix), m_format(format) {}
  104|       |
  105|       |  friend std::ostream& operator<<(std::ostream& s, const WithFormat& wf) {
  106|       |    return internal::print_matrix(s, wf.m_matrix.eval(), wf.m_format);
  107|       |  }
  108|       |
  109|       | protected:
  110|       |  typename ExpressionType::Nested m_matrix;
  111|       |  IOFormat m_format;
  112|       |};
  113|       |
  114|       |namespace internal {
  115|       |
  116|       |// NOTE: This helper is kept for backward compatibility with previous code specializing
  117|       |//       this internal::significant_decimals_impl structure. In the future we should directly
  118|       |//       call max_digits10().
  119|       |template <typename Scalar>
  120|       |struct significant_decimals_impl {
  121|       |  static inline int run() { return NumTraits<Scalar>::max_digits10(); }
  122|       |};
  123|       |
  124|       |/** \internal
  125|       | * print the matrix \a _m to the output stream \a s using the output format \a fmt */
  126|       |template <typename Derived>
  127|       |std::ostream& print_matrix(std::ostream& s, const Derived& _m, const IOFormat& fmt) {
  128|       |  using internal::is_same;
  129|       |
  130|       |  if (_m.size() == 0) {
  131|       |    s << fmt.matPrefix << fmt.matSuffix;
  132|       |    return s;
  133|       |  }
  134|       |
  135|       |  typename Derived::Nested m = _m;
  136|       |  typedef typename Derived::Scalar Scalar;
  137|       |  typedef std::conditional_t<is_same<Scalar, char>::value || is_same<Scalar, unsigned char>::value ||
  138|       |                                 is_same<Scalar, numext::int8_t>::value || is_same<Scalar, numext::uint8_t>::value,
  139|       |                             int,
  140|       |                             std::conditional_t<is_same<Scalar, std::complex<char> >::value ||
  141|       |                                                    is_same<Scalar, std::complex<unsigned char> >::value ||
  142|       |                                                    is_same<Scalar, std::complex<numext::int8_t> >::value ||
  143|       |                                                    is_same<Scalar, std::complex<numext::uint8_t> >::value,
  144|       |                                                std::complex<int>, const Scalar&> >
  145|       |      PrintType;
  146|       |
  147|       |  Index width = 0;
  148|       |
  149|       |  std::streamsize explicit_precision;
  150|       |  if (fmt.precision == StreamPrecision) {
  151|       |    explicit_precision = 0;
  152|       |  } else if (fmt.precision == FullPrecision) {
  153|       |    if (NumTraits<Scalar>::IsInteger) {
  154|       |      explicit_precision = 0;
  155|       |    } else {
  156|       |      explicit_precision = significant_decimals_impl<Scalar>::run();
  157|       |    }
  158|       |  } else {
  159|       |    explicit_precision = fmt.precision;
  160|       |  }
  161|       |
  162|       |  std::streamsize old_precision = 0;
  163|       |  if (explicit_precision) old_precision = s.precision(explicit_precision);
  164|       |
  165|       |  bool align_cols = !(fmt.flags & DontAlignCols);
  166|       |  if (align_cols) {
  167|       |    // compute the largest width
  168|       |    for (Index j = 0; j < m.cols(); ++j)
  169|       |      for (Index i = 0; i < m.rows(); ++i) {
  170|       |        std::stringstream sstr;
  171|       |        sstr.copyfmt(s);
  172|       |        sstr << static_cast<PrintType>(m.coeff(i, j));
  173|       |        width = std::max<Index>(width, Index(sstr.str().length()));
  174|       |      }
  175|       |  }
  176|       |  std::streamsize old_width = s.width();
  177|       |  char old_fill_character = s.fill();
  178|       |  s << fmt.matPrefix;
  179|       |  for (Index i = 0; i < m.rows(); ++i) {
  180|       |    if (i) s << fmt.rowSpacer;
  181|       |    s << fmt.rowPrefix;
  182|       |    if (width) {
  183|       |      s.fill(fmt.fill);
  184|       |      s.width(width);
  185|       |    }
  186|       |    s << static_cast<PrintType>(m.coeff(i, 0));
  187|       |    for (Index j = 1; j < m.cols(); ++j) {
  188|       |      s << fmt.coeffSeparator;
  189|       |      if (width) {
  190|       |        s.fill(fmt.fill);
  191|       |        s.width(width);
  192|       |      }
  193|       |      s << static_cast<PrintType>(m.coeff(i, j));
  194|       |    }
  195|       |    s << fmt.rowSuffix;
  196|       |    if (i < m.rows() - 1) s << fmt.rowSeparator;
  197|       |  }
  198|       |  s << fmt.matSuffix;
  199|       |  if (explicit_precision) s.precision(old_precision);
  200|       |  if (width) {
  201|       |    s.fill(old_fill_character);
  202|       |    s.width(old_width);
  203|       |  }
  204|       |  return s;
  205|       |}
  206|       |
  207|       |}  // end namespace internal
  208|       |
  209|       |/** \relates DenseBase
  210|       | *
  211|       | * Outputs the matrix, to the given stream.
  212|       | *
  213|       | * If you wish to print the matrix with a format different than the default, use DenseBase::format().
  214|       | *
  215|       | * It is also possible to change the default format by defining EIGEN_DEFAULT_IO_FORMAT before including Eigen headers.
  216|       | * If not defined, this will automatically be defined to Eigen::IOFormat(), that is the Eigen::IOFormat with default
  217|       | * parameters.
  218|       | *
  219|       | * \sa DenseBase::format()
  220|       | */
  221|       |template <typename Derived>
  222|       |std::ostream& operator<<(std::ostream& s, const DenseBase<Derived>& m) {
  223|       |  return internal::print_matrix(s, m.eval(), EIGEN_DEFAULT_IO_FORMAT);
  224|       |}
  225|       |
  226|       |template <typename Derived>
  227|       |std::ostream& operator<<(std::ostream& s, const DiagonalBase<Derived>& m) {
  228|       |  return internal::print_matrix(s, m.derived(), EIGEN_DEFAULT_IO_FORMAT);
  229|       |}
  230|       |
  231|       |}  // end namespace Eigen
  232|       |
  233|       |#endif  // EIGEN_IO_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/MathFunctions.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2006-2010 Benoit Jacob <jacob.benoit.1@gmail.com>
    5|       |// Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_MATHFUNCTIONS_H
   12|       |#define EIGEN_MATHFUNCTIONS_H
   13|       |
   14|       |// TODO this should better be moved to NumTraits
   15|       |// Source: WolframAlpha
   16|       |#define EIGEN_PI 3.141592653589793238462643383279502884197169399375105820974944592307816406L
   17|       |#define EIGEN_LOG2E 1.442695040888963407359924681001892137426645954152985934135449406931109219L
   18|       |#define EIGEN_LN2 0.693147180559945309417232121458176568075500134360255254120680009493393621L
   19|       |
   20|       |// IWYU pragma: private
   21|       |#include "./InternalHeaderCheck.h"
   22|       |
   23|       |namespace Eigen {
   24|       |
   25|       |namespace internal {
   26|       |
   27|       |/** \internal \class global_math_functions_filtering_base
   28|       | *
   29|       | * What it does:
   30|       | * Defines a typedef 'type' as follows:
   31|       | * - if type T has a member typedef Eigen_BaseClassForSpecializationOfGlobalMathFuncImpl, then
   32|       | *   global_math_functions_filtering_base<T>::type is a typedef for it.
   33|       | * - otherwise, global_math_functions_filtering_base<T>::type is a typedef for T.
   34|       | *
   35|       | * How it's used:
   36|       | * To allow to defined the global math functions (like sin...) in certain cases, like the Array expressions.
   37|       | * When you do sin(array1+array2), the object array1+array2 has a complicated expression type, all what you want to know
   38|       | * is that it inherits ArrayBase. So we implement a partial specialization of sin_impl for ArrayBase<Derived>.
   39|       | * So we must make sure to use sin_impl<ArrayBase<Derived> > and not sin_impl<Derived>, otherwise our partial
   40|       | * specialization won't be used. How does sin know that? That's exactly what global_math_functions_filtering_base tells
   41|       | * it.
   42|       | *
   43|       | * How it's implemented:
   44|       | * SFINAE in the style of enable_if. Highly susceptible of breaking compilers. With GCC, it sure does work, but if you
   45|       | * replace the typename dummy by an integer template parameter, it doesn't work anymore!
   46|       | */
   47|       |
   48|       |template <typename T, typename dummy = void>
   49|       |struct global_math_functions_filtering_base {
   50|       |  typedef T type;
   51|       |};
   52|       |
   53|       |template <typename T>
   54|       |struct always_void {
   55|       |  typedef void type;
   56|       |};
   57|       |
   58|       |template <typename T>
   59|       |struct global_math_functions_filtering_base<
   60|       |    T, typename always_void<typename T::Eigen_BaseClassForSpecializationOfGlobalMathFuncImpl>::type> {
   61|       |  typedef typename T::Eigen_BaseClassForSpecializationOfGlobalMathFuncImpl type;
   62|       |};
   63|       |
   64|       |#define EIGEN_MATHFUNC_IMPL(func, scalar) \
   65|      0|  Eigen::internal::func##_impl<typename Eigen::internal::global_math_functions_filtering_base<scalar>::type>
   66|       |#define EIGEN_MATHFUNC_RETVAL(func, scalar) \
   67|       |  typename Eigen::internal::func##_retval<  \
   68|       |      typename Eigen::internal::global_math_functions_filtering_base<scalar>::type>::type
   69|       |
   70|       |/****************************************************************************
   71|       | * Implementation of real                                                 *
   72|       | ****************************************************************************/
   73|       |
   74|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
   75|       |struct real_default_impl {
   76|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
   77|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) { return x; }
   78|       |};
   79|       |
   80|       |template <typename Scalar>
   81|       |struct real_default_impl<Scalar, true> {
   82|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
   83|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
   84|       |    using std::real;
   85|       |    return real(x);
   86|       |  }
   87|       |};
   88|       |
   89|       |template <typename Scalar>
   90|       |struct real_impl : real_default_impl<Scalar> {};
   91|       |
   92|       |#if defined(EIGEN_GPU_COMPILE_PHASE)
   93|       |template <typename T>
   94|       |struct real_impl<std::complex<T>> {
   95|       |  typedef T RealScalar;
   96|       |  EIGEN_DEVICE_FUNC static inline T run(const std::complex<T>& x) { return x.real(); }
   97|       |};
   98|       |#endif
   99|       |
  100|       |template <typename Scalar>
  101|       |struct real_retval {
  102|       |  typedef typename NumTraits<Scalar>::Real type;
  103|       |};
  104|       |
  105|       |/****************************************************************************
  106|       | * Implementation of imag                                                 *
  107|       | ****************************************************************************/
  108|       |
  109|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
  110|       |struct imag_default_impl {
  111|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  112|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar&) { return RealScalar(0); }
  113|       |};
  114|       |
  115|       |template <typename Scalar>
  116|       |struct imag_default_impl<Scalar, true> {
  117|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  118|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
  119|       |    using std::imag;
  120|       |    return imag(x);
  121|       |  }
  122|       |};
  123|       |
  124|       |template <typename Scalar>
  125|       |struct imag_impl : imag_default_impl<Scalar> {};
  126|       |
  127|       |#if defined(EIGEN_GPU_COMPILE_PHASE)
  128|       |template <typename T>
  129|       |struct imag_impl<std::complex<T>> {
  130|       |  typedef T RealScalar;
  131|       |  EIGEN_DEVICE_FUNC static inline T run(const std::complex<T>& x) { return x.imag(); }
  132|       |};
  133|       |#endif
  134|       |
  135|       |template <typename Scalar>
  136|       |struct imag_retval {
  137|       |  typedef typename NumTraits<Scalar>::Real type;
  138|       |};
  139|       |
  140|       |/****************************************************************************
  141|       | * Implementation of real_ref                                             *
  142|       | ****************************************************************************/
  143|       |
  144|       |template <typename Scalar>
  145|       |struct real_ref_impl {
  146|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  147|      0|  EIGEN_DEVICE_FUNC static inline RealScalar& run(Scalar& x) { return reinterpret_cast<RealScalar*>(&x)[0]; }
  148|      0|  EIGEN_DEVICE_FUNC static inline const RealScalar& run(const Scalar& x) {
  149|      0|    return reinterpret_cast<const RealScalar*>(&x)[0];
  150|      0|  }
  151|       |};
  152|       |
  153|       |template <typename Scalar>
  154|       |struct real_ref_retval {
  155|       |  typedef typename NumTraits<Scalar>::Real& type;
  156|       |};
  157|       |
  158|       |/****************************************************************************
  159|       | * Implementation of imag_ref                                             *
  160|       | ****************************************************************************/
  161|       |
  162|       |template <typename Scalar, bool IsComplex>
  163|       |struct imag_ref_default_impl {
  164|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  165|       |  EIGEN_DEVICE_FUNC static inline RealScalar& run(Scalar& x) { return reinterpret_cast<RealScalar*>(&x)[1]; }
  166|       |  EIGEN_DEVICE_FUNC static inline const RealScalar& run(const Scalar& x) {
  167|       |    return reinterpret_cast<const RealScalar*>(&x)[1];
  168|       |  }
  169|       |};
  170|       |
  171|       |template <typename Scalar>
  172|       |struct imag_ref_default_impl<Scalar, false> {
  173|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline Scalar run(Scalar&) { return Scalar(0); }
  174|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline const Scalar run(const Scalar&) { return Scalar(0); }
  175|       |};
  176|       |
  177|       |template <typename Scalar>
  178|       |struct imag_ref_impl : imag_ref_default_impl<Scalar, NumTraits<Scalar>::IsComplex> {};
  179|       |
  180|       |template <typename Scalar>
  181|       |struct imag_ref_retval {
  182|       |  typedef typename NumTraits<Scalar>::Real& type;
  183|       |};
  184|       |
  185|       |/****************************************************************************
  186|       | * Implementation of conj                                                 *
  187|       | ****************************************************************************/
  188|       |
  189|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
  190|       |struct conj_default_impl {
  191|       |  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& x) { return x; }
  192|       |};
  193|       |
  194|       |template <typename Scalar>
  195|       |struct conj_default_impl<Scalar, true> {
  196|       |  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& x) {
  197|       |    using std::conj;
  198|       |    return conj(x);
  199|       |  }
  200|       |};
  201|       |
  202|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
  203|       |struct conj_impl : conj_default_impl<Scalar, IsComplex> {};
  204|       |
  205|       |template <typename Scalar>
  206|       |struct conj_retval {
  207|       |  typedef Scalar type;
  208|       |};
  209|       |
  210|       |/****************************************************************************
  211|       | * Implementation of abs2                                                 *
  212|       | ****************************************************************************/
  213|       |
  214|       |template <typename Scalar, bool IsComplex>
  215|       |struct abs2_impl_default {
  216|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  217|      0|  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) { return x * x; }
  218|       |};
  219|       |
  220|       |template <typename Scalar>
  221|       |struct abs2_impl_default<Scalar, true>  // IsComplex
  222|       |{
  223|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  224|      0|  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) { return x.real() * x.real() + x.imag() * x.imag(); }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17abs2_impl_defaultISt7complexIfELb1EE3runERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17abs2_impl_defaultISt7complexIdELb1EE3runERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17abs2_impl_defaultISt7complexIeELb1EE3runERKS3_
  ------------------
  225|       |};
  226|       |
  227|       |template <typename Scalar>
  228|       |struct abs2_impl {
  229|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  230|      0|  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
  231|      0|    return abs2_impl_default<Scalar, NumTraits<Scalar>::IsComplex>::run(x);
  232|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal9abs2_implIfE3runERKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal9abs2_implISt7complexIfEE3runERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal9abs2_implISt7complexIdEE3runERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal9abs2_implISt7complexIeEE3runERKS3_
  ------------------
  233|       |};
  234|       |
  235|       |template <typename Scalar>
  236|       |struct abs2_retval {
  237|       |  typedef typename NumTraits<Scalar>::Real type;
  238|       |};
  239|       |
  240|       |/****************************************************************************
  241|       | * Implementation of sqrt/rsqrt                                             *
  242|       | ****************************************************************************/
  243|       |
  244|       |template <typename Scalar>
  245|       |struct sqrt_impl {
  246|       |  EIGEN_DEVICE_FUNC static EIGEN_ALWAYS_INLINE Scalar run(const Scalar& x) {
  247|       |    EIGEN_USING_STD(sqrt);
  248|       |    return sqrt(x);
  249|       |  }
  250|       |};
  251|       |
  252|       |// Complex sqrt defined in MathFunctionsImpl.h.
  253|       |template <typename T>
  254|       |EIGEN_DEVICE_FUNC std::complex<T> complex_sqrt(const std::complex<T>& a_x);
  255|       |
  256|       |// Custom implementation is faster than `std::sqrt`, works on
  257|       |// GPU, and correctly handles special cases (unlike MSVC).
  258|       |template <typename T>
  259|       |struct sqrt_impl<std::complex<T>> {
  260|       |  EIGEN_DEVICE_FUNC static EIGEN_ALWAYS_INLINE std::complex<T> run(const std::complex<T>& x) {
  261|       |    return complex_sqrt<T>(x);
  262|       |  }
  263|       |};
  264|       |
  265|       |template <typename Scalar>
  266|       |struct sqrt_retval {
  267|       |  typedef Scalar type;
  268|       |};
  269|       |
  270|       |// Default implementation relies on numext::sqrt, at bottom of file.
  271|       |template <typename T>
  272|       |struct rsqrt_impl;
  273|       |
  274|       |// Complex rsqrt defined in MathFunctionsImpl.h.
  275|       |template <typename T>
  276|       |EIGEN_DEVICE_FUNC std::complex<T> complex_rsqrt(const std::complex<T>& a_x);
  277|       |
  278|       |template <typename T>
  279|       |struct rsqrt_impl<std::complex<T>> {
  280|       |  EIGEN_DEVICE_FUNC static EIGEN_ALWAYS_INLINE std::complex<T> run(const std::complex<T>& x) {
  281|       |    return complex_rsqrt<T>(x);
  282|       |  }
  283|       |};
  284|       |
  285|       |template <typename Scalar>
  286|       |struct rsqrt_retval {
  287|       |  typedef Scalar type;
  288|       |};
  289|       |
  290|       |/****************************************************************************
  291|       | * Implementation of norm1                                                *
  292|       | ****************************************************************************/
  293|       |
  294|       |template <typename Scalar, bool IsComplex>
  295|       |struct norm1_default_impl;
  296|       |
  297|       |template <typename Scalar>
  298|       |struct norm1_default_impl<Scalar, true> {
  299|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  300|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
  301|       |    EIGEN_USING_STD(abs);
  302|       |    return abs(x.real()) + abs(x.imag());
  303|       |  }
  304|       |};
  305|       |
  306|       |template <typename Scalar>
  307|       |struct norm1_default_impl<Scalar, false> {
  308|       |  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& x) {
  309|       |    EIGEN_USING_STD(abs);
  310|       |    return abs(x);
  311|       |  }
  312|       |};
  313|       |
  314|       |template <typename Scalar>
  315|       |struct norm1_impl : norm1_default_impl<Scalar, NumTraits<Scalar>::IsComplex> {};
  316|       |
  317|       |template <typename Scalar>
  318|       |struct norm1_retval {
  319|       |  typedef typename NumTraits<Scalar>::Real type;
  320|       |};
  321|       |
  322|       |/****************************************************************************
  323|       | * Implementation of hypot                                                *
  324|       | ****************************************************************************/
  325|       |
  326|       |template <typename Scalar>
  327|       |struct hypot_impl;
  328|       |
  329|       |template <typename Scalar>
  330|       |struct hypot_retval {
  331|       |  typedef typename NumTraits<Scalar>::Real type;
  332|       |};
  333|       |
  334|       |/****************************************************************************
  335|       | * Implementation of cast                                                 *
  336|       | ****************************************************************************/
  337|       |
  338|       |template <typename OldType, typename NewType, typename EnableIf = void>
  339|       |struct cast_impl {
  340|       |  EIGEN_DEVICE_FUNC static inline NewType run(const OldType& x) { return static_cast<NewType>(x); }
  341|       |};
  342|       |
  343|       |template <typename OldType>
  344|       |struct cast_impl<OldType, bool> {
  345|       |  EIGEN_DEVICE_FUNC static inline bool run(const OldType& x) { return x != OldType(0); }
  346|       |};
  347|       |
  348|       |// Casting from S -> Complex<T> leads to an implicit conversion from S to T,
  349|       |// generating warnings on clang.  Here we explicitly cast the real component.
  350|       |template <typename OldType, typename NewType>
  351|       |struct cast_impl<OldType, NewType,
  352|       |                 typename std::enable_if_t<!NumTraits<OldType>::IsComplex && NumTraits<NewType>::IsComplex>> {
  353|       |  EIGEN_DEVICE_FUNC static inline NewType run(const OldType& x) {
  354|       |    typedef typename NumTraits<NewType>::Real NewReal;
  355|       |    return static_cast<NewType>(static_cast<NewReal>(x));
  356|       |  }
  357|       |};
  358|       |
  359|       |// here, for once, we're plainly returning NewType: we don't want cast to do weird things.
  360|       |
  361|       |template <typename OldType, typename NewType>
  362|       |EIGEN_DEVICE_FUNC inline NewType cast(const OldType& x) {
  363|       |  return cast_impl<OldType, NewType>::run(x);
  364|       |}
  365|       |
  366|       |/****************************************************************************
  367|       | * Implementation of arg                                                     *
  368|       | ****************************************************************************/
  369|       |
  370|       |// Visual Studio 2017 has a bug where arg(float) returns 0 for negative inputs.
  371|       |// This seems to be fixed in VS 2019.
  372|       |#if (!EIGEN_COMP_MSVC || EIGEN_COMP_MSVC >= 1920)
  373|       |// std::arg is only defined for types of std::complex, or integer types or float/double/long double
  374|       |template <typename Scalar, bool HasStdImpl = NumTraits<Scalar>::IsComplex || is_integral<Scalar>::value ||
  375|       |                                             is_same<Scalar, float>::value || is_same<Scalar, double>::value ||
  376|       |                                             is_same<Scalar, long double>::value>
  377|       |struct arg_default_impl;
  378|       |
  379|       |template <typename Scalar>
  380|       |struct arg_default_impl<Scalar, true> {
  381|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  382|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
  383|       |    // There is no official ::arg on device in CUDA/HIP, so we always need to use std::arg.
  384|       |    using std::arg;
  385|       |    return static_cast<RealScalar>(arg(x));
  386|       |  }
  387|       |};
  388|       |
  389|       |// Must be non-complex floating-point type (e.g. half/bfloat16).
  390|       |template <typename Scalar>
  391|       |struct arg_default_impl<Scalar, false> {
  392|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  393|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
  394|       |    return (x < Scalar(0)) ? RealScalar(EIGEN_PI) : RealScalar(0);
  395|       |  }
  396|       |};
  397|       |#else
  398|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
  399|       |struct arg_default_impl {
  400|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  401|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
  402|       |    return (x < RealScalar(0)) ? RealScalar(EIGEN_PI) : RealScalar(0);
  403|       |  }
  404|       |};
  405|       |
  406|       |template <typename Scalar>
  407|       |struct arg_default_impl<Scalar, true> {
  408|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  409|       |  EIGEN_DEVICE_FUNC static inline RealScalar run(const Scalar& x) {
  410|       |    EIGEN_USING_STD(arg);
  411|       |    return arg(x);
  412|       |  }
  413|       |};
  414|       |#endif
  415|       |template <typename Scalar>
  416|       |struct arg_impl : arg_default_impl<Scalar> {};
  417|       |
  418|       |template <typename Scalar>
  419|       |struct arg_retval {
  420|       |  typedef typename NumTraits<Scalar>::Real type;
  421|       |};
  422|       |
  423|       |/****************************************************************************
  424|       | * Implementation of expm1                                                   *
  425|       | ****************************************************************************/
  426|       |
  427|       |// This implementation is based on GSL Math's expm1.
  428|       |namespace std_fallback {
  429|       |// fallback expm1 implementation in case there is no expm1(Scalar) function in namespace of Scalar,
  430|       |// or that there is no suitable std::expm1 function available. Implementation
  431|       |// attributed to Kahan. See: http://www.plunk.org/~hatch/rightway.php.
  432|       |template <typename Scalar>
  433|       |EIGEN_DEVICE_FUNC inline Scalar expm1(const Scalar& x) {
  434|       |  EIGEN_STATIC_ASSERT_NON_INTEGER(Scalar)
  435|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  436|       |
  437|       |  EIGEN_USING_STD(exp);
  438|       |  Scalar u = exp(x);
  439|       |  if (numext::equal_strict(u, Scalar(1))) {
  440|       |    return x;
  441|       |  }
  442|       |  Scalar um1 = u - RealScalar(1);
  443|       |  if (numext::equal_strict(um1, Scalar(-1))) {
  444|       |    return RealScalar(-1);
  445|       |  }
  446|       |
  447|       |  EIGEN_USING_STD(log);
  448|       |  Scalar logu = log(u);
  449|       |  return numext::equal_strict(u, logu) ? u : (u - RealScalar(1)) * x / logu;
  450|       |}
  451|       |}  // namespace std_fallback
  452|       |
  453|       |template <typename Scalar>
  454|       |struct expm1_impl {
  455|      0|  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& x) {
  456|      0|    EIGEN_STATIC_ASSERT_NON_INTEGER(Scalar)
  457|      0|    EIGEN_USING_STD(expm1);
  458|      0|    return expm1(x);
  459|      0|  }
  460|       |};
  461|       |
  462|       |template <typename Scalar>
  463|       |struct expm1_retval {
  464|       |  typedef Scalar type;
  465|       |};
  466|       |
  467|       |/****************************************************************************
  468|       | * Implementation of log                                                     *
  469|       | ****************************************************************************/
  470|       |
  471|       |// Complex log defined in MathFunctionsImpl.h.
  472|       |template <typename T>
  473|       |EIGEN_DEVICE_FUNC std::complex<T> complex_log(const std::complex<T>& z);
  474|       |
  475|       |template <typename Scalar>
  476|       |struct log_impl {
  477|       |  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& x) {
  478|       |    EIGEN_USING_STD(log);
  479|       |    return static_cast<Scalar>(log(x));
  480|       |  }
  481|       |};
  482|       |
  483|       |template <typename Scalar>
  484|       |struct log_impl<std::complex<Scalar>> {
  485|       |  EIGEN_DEVICE_FUNC static inline std::complex<Scalar> run(const std::complex<Scalar>& z) { return complex_log(z); }
  486|       |};
  487|       |
  488|       |/****************************************************************************
  489|       | * Implementation of log1p                                                   *
  490|       | ****************************************************************************/
  491|       |
  492|       |namespace std_fallback {
  493|       |// fallback log1p implementation in case there is no log1p(Scalar) function in namespace of Scalar,
  494|       |// or that there is no suitable std::log1p function available
  495|       |template <typename Scalar>
  496|       |EIGEN_DEVICE_FUNC inline Scalar log1p(const Scalar& x) {
  497|       |  EIGEN_STATIC_ASSERT_NON_INTEGER(Scalar)
  498|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  499|       |  EIGEN_USING_STD(log);
  500|       |  Scalar x1p = RealScalar(1) + x;
  501|       |  Scalar log_1p = log_impl<Scalar>::run(x1p);
  502|       |  const bool is_small = numext::equal_strict(x1p, Scalar(1));
  503|       |  const bool is_inf = numext::equal_strict(x1p, log_1p);
  504|       |  return (is_small || is_inf) ? x : x * (log_1p / (x1p - RealScalar(1)));
  505|       |}
  506|       |}  // namespace std_fallback
  507|       |
  508|       |template <typename Scalar>
  509|       |struct log1p_impl {
  510|       |  EIGEN_STATIC_ASSERT_NON_INTEGER(Scalar)
  511|       |
  512|      0|  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& x) {
  513|      0|    EIGEN_USING_STD(log1p);
  514|      0|    return log1p(x);
  515|      0|  }
  516|       |};
  517|       |
  518|       |// Specialization for complex types that are not supported by std::log1p.
  519|       |template <typename RealScalar>
  520|       |struct log1p_impl<std::complex<RealScalar>> {
  521|       |  EIGEN_STATIC_ASSERT_NON_INTEGER(RealScalar)
  522|       |
  523|       |  EIGEN_DEVICE_FUNC static inline std::complex<RealScalar> run(const std::complex<RealScalar>& x) {
  524|       |    return std_fallback::log1p(x);
  525|       |  }
  526|       |};
  527|       |
  528|       |template <typename Scalar>
  529|       |struct log1p_retval {
  530|       |  typedef Scalar type;
  531|       |};
  532|       |
  533|       |/****************************************************************************
  534|       | * Implementation of pow                                                  *
  535|       | ****************************************************************************/
  536|       |
  537|       |template <typename ScalarX, typename ScalarY,
  538|       |          bool IsInteger = NumTraits<ScalarX>::IsInteger && NumTraits<ScalarY>::IsInteger>
  539|       |struct pow_impl {
  540|       |  // typedef Scalar retval;
  541|       |  typedef typename ScalarBinaryOpTraits<ScalarX, ScalarY, internal::scalar_pow_op<ScalarX, ScalarY>>::ReturnType
  542|       |      result_type;
  543|       |  static EIGEN_DEVICE_FUNC inline result_type run(const ScalarX& x, const ScalarY& y) {
  544|       |    EIGEN_USING_STD(pow);
  545|       |    return pow(x, y);
  546|       |  }
  547|       |};
  548|       |
  549|       |template <typename ScalarX, typename ScalarY>
  550|       |struct pow_impl<ScalarX, ScalarY, true> {
  551|       |  typedef ScalarX result_type;
  552|       |  static EIGEN_DEVICE_FUNC inline ScalarX run(ScalarX x, ScalarY y) {
  553|       |    ScalarX res(1);
  554|       |    eigen_assert(!NumTraits<ScalarY>::IsSigned || y >= 0);
  555|       |    if (y & 1) res *= x;
  556|       |    y >>= 1;
  557|       |    while (y) {
  558|       |      x *= x;
  559|       |      if (y & 1) res *= x;
  560|       |      y >>= 1;
  561|       |    }
  562|       |    return res;
  563|       |  }
  564|       |};
  565|       |
  566|       |enum { meta_floor_log2_terminate, meta_floor_log2_move_up, meta_floor_log2_move_down, meta_floor_log2_bogus };
  567|       |
  568|       |template <unsigned int n, int lower, int upper>
  569|       |struct meta_floor_log2_selector {
  570|       |  enum {
  571|       |    middle = (lower + upper) / 2,
  572|       |    value = (upper <= lower + 1)  ? int(meta_floor_log2_terminate)
  573|       |            : (n < (1 << middle)) ? int(meta_floor_log2_move_down)
  574|       |            : (n == 0)            ? int(meta_floor_log2_bogus)
  575|       |                                  : int(meta_floor_log2_move_up)
  576|       |  };
  577|       |};
  578|       |
  579|       |template <unsigned int n, int lower = 0, int upper = sizeof(unsigned int) * CHAR_BIT - 1,
  580|       |          int selector = meta_floor_log2_selector<n, lower, upper>::value>
  581|       |struct meta_floor_log2 {};
  582|       |
  583|       |template <unsigned int n, int lower, int upper>
  584|       |struct meta_floor_log2<n, lower, upper, meta_floor_log2_move_down> {
  585|       |  enum { value = meta_floor_log2<n, lower, meta_floor_log2_selector<n, lower, upper>::middle>::value };
  586|       |};
  587|       |
  588|       |template <unsigned int n, int lower, int upper>
  589|       |struct meta_floor_log2<n, lower, upper, meta_floor_log2_move_up> {
  590|       |  enum { value = meta_floor_log2<n, meta_floor_log2_selector<n, lower, upper>::middle, upper>::value };
  591|       |};
  592|       |
  593|       |template <unsigned int n, int lower, int upper>
  594|       |struct meta_floor_log2<n, lower, upper, meta_floor_log2_terminate> {
  595|       |  enum { value = (n >= ((unsigned int)(1) << (lower + 1))) ? lower + 1 : lower };
  596|       |};
  597|       |
  598|       |template <unsigned int n, int lower, int upper>
  599|       |struct meta_floor_log2<n, lower, upper, meta_floor_log2_bogus> {
  600|       |  // no value, error at compile time
  601|       |};
  602|       |
  603|       |template <typename BitsType, typename EnableIf = void>
  604|       |struct count_bits_impl {
  605|       |  static_assert(std::is_integral<BitsType>::value && std::is_unsigned<BitsType>::value,
  606|       |                "BitsType must be an unsigned integer");
  607|       |  static EIGEN_DEVICE_FUNC inline int clz(BitsType bits) {
  608|       |    int n = CHAR_BIT * sizeof(BitsType);
  609|       |    int shift = n / 2;
  610|       |    while (bits > 0 && shift > 0) {
  611|       |      BitsType y = bits >> shift;
  612|       |      if (y > 0) {
  613|       |        n -= shift;
  614|       |        bits = y;
  615|       |      }
  616|       |      shift /= 2;
  617|       |    }
  618|       |    if (shift == 0) {
  619|       |      --n;
  620|       |    }
  621|       |    return n;
  622|       |  }
  623|       |
  624|       |  static EIGEN_DEVICE_FUNC inline int ctz(BitsType bits) {
  625|       |    int n = CHAR_BIT * sizeof(BitsType);
  626|       |    int shift = n / 2;
  627|       |    while (bits > 0 && shift > 0) {
  628|       |      BitsType y = bits << shift;
  629|       |      if (y > 0) {
  630|       |        n -= shift;
  631|       |        bits = y;
  632|       |      }
  633|       |      shift /= 2;
  634|       |    }
  635|       |    if (shift == 0) {
  636|       |      --n;
  637|       |    }
  638|       |    return n;
  639|       |  }
  640|       |};
  641|       |
  642|       |// Count leading zeros.
  643|       |template <typename BitsType>
  644|       |EIGEN_DEVICE_FUNC inline int clz(BitsType bits) {
  645|       |  return count_bits_impl<BitsType>::clz(bits);
  646|       |}
  647|       |
  648|       |// Count trailing zeros.
  649|       |template <typename BitsType>
  650|       |EIGEN_DEVICE_FUNC inline int ctz(BitsType bits) {
  651|       |  return count_bits_impl<BitsType>::ctz(bits);
  652|       |}
  653|       |
  654|       |#if EIGEN_COMP_GNUC || EIGEN_COMP_CLANG
  655|       |
  656|       |template <typename BitsType>
  657|       |struct count_bits_impl<
  658|       |    BitsType, std::enable_if_t<std::is_integral<BitsType>::value && sizeof(BitsType) <= sizeof(unsigned int)>> {
  659|       |  static constexpr int kNumBits = static_cast<int>(sizeof(BitsType) * CHAR_BIT);
  660|       |  static EIGEN_DEVICE_FUNC inline int clz(BitsType bits) {
  661|       |    static constexpr int kLeadingBitsOffset = (sizeof(unsigned int) - sizeof(BitsType)) * CHAR_BIT;
  662|       |    return bits == 0 ? kNumBits : __builtin_clz(static_cast<unsigned int>(bits)) - kLeadingBitsOffset;
  663|       |  }
  664|       |
  665|       |  static EIGEN_DEVICE_FUNC inline int ctz(BitsType bits) {
  666|       |    return bits == 0 ? kNumBits : __builtin_ctz(static_cast<unsigned int>(bits));
  667|       |  }
  668|       |};
  669|       |
  670|       |template <typename BitsType>
  671|       |struct count_bits_impl<BitsType,
  672|       |                       std::enable_if_t<std::is_integral<BitsType>::value && sizeof(unsigned int) < sizeof(BitsType) &&
  673|       |                                        sizeof(BitsType) <= sizeof(unsigned long)>> {
  674|       |  static constexpr int kNumBits = static_cast<int>(sizeof(BitsType) * CHAR_BIT);
  675|       |  static EIGEN_DEVICE_FUNC inline int clz(BitsType bits) {
  676|       |    static constexpr int kLeadingBitsOffset = (sizeof(unsigned long) - sizeof(BitsType)) * CHAR_BIT;
  677|       |    return bits == 0 ? kNumBits : __builtin_clzl(static_cast<unsigned long>(bits)) - kLeadingBitsOffset;
  678|       |  }
  679|       |
  680|       |  static EIGEN_DEVICE_FUNC inline int ctz(BitsType bits) {
  681|       |    return bits == 0 ? kNumBits : __builtin_ctzl(static_cast<unsigned long>(bits));
  682|       |  }
  683|       |};
  684|       |
  685|       |template <typename BitsType>
  686|       |struct count_bits_impl<BitsType,
  687|       |                       std::enable_if_t<std::is_integral<BitsType>::value && sizeof(unsigned long) < sizeof(BitsType) &&
  688|       |                                        sizeof(BitsType) <= sizeof(unsigned long long)>> {
  689|       |  static constexpr int kNumBits = static_cast<int>(sizeof(BitsType) * CHAR_BIT);
  690|       |  static EIGEN_DEVICE_FUNC inline int clz(BitsType bits) {
  691|       |    static constexpr int kLeadingBitsOffset = (sizeof(unsigned long long) - sizeof(BitsType)) * CHAR_BIT;
  692|       |    return bits == 0 ? kNumBits : __builtin_clzll(static_cast<unsigned long long>(bits)) - kLeadingBitsOffset;
  693|       |  }
  694|       |
  695|       |  static EIGEN_DEVICE_FUNC inline int ctz(BitsType bits) {
  696|       |    return bits == 0 ? kNumBits : __builtin_ctzll(static_cast<unsigned long long>(bits));
  697|       |  }
  698|       |};
  699|       |
  700|       |#elif EIGEN_COMP_MSVC
  701|       |
  702|       |template <typename BitsType>
  703|       |struct count_bits_impl<
  704|       |    BitsType, std::enable_if_t<std::is_integral<BitsType>::value && sizeof(BitsType) <= sizeof(unsigned long)>> {
  705|       |  static constexpr int kNumBits = static_cast<int>(sizeof(BitsType) * CHAR_BIT);
  706|       |  static EIGEN_DEVICE_FUNC inline int clz(BitsType bits) {
  707|       |    unsigned long out;
  708|       |    _BitScanReverse(&out, static_cast<unsigned long>(bits));
  709|       |    return bits == 0 ? kNumBits : (kNumBits - 1) - static_cast<int>(out);
  710|       |  }
  711|       |
  712|       |  static EIGEN_DEVICE_FUNC inline int ctz(BitsType bits) {
  713|       |    unsigned long out;
  714|       |    _BitScanForward(&out, static_cast<unsigned long>(bits));
  715|       |    return bits == 0 ? kNumBits : static_cast<int>(out);
  716|       |  }
  717|       |};
  718|       |
  719|       |#ifdef _WIN64
  720|       |
  721|       |template <typename BitsType>
  722|       |struct count_bits_impl<BitsType,
  723|       |                       std::enable_if_t<std::is_integral<BitsType>::value && sizeof(unsigned long) < sizeof(BitsType) &&
  724|       |                                        sizeof(BitsType) <= sizeof(__int64)>> {
  725|       |  static constexpr int kNumBits = static_cast<int>(sizeof(BitsType) * CHAR_BIT);
  726|       |  static EIGEN_DEVICE_FUNC inline int clz(BitsType bits) {
  727|       |    unsigned long out;
  728|       |    _BitScanReverse64(&out, static_cast<unsigned __int64>(bits));
  729|       |    return bits == 0 ? kNumBits : (kNumBits - 1) - static_cast<int>(out);
  730|       |  }
  731|       |
  732|       |  static EIGEN_DEVICE_FUNC inline int ctz(BitsType bits) {
  733|       |    unsigned long out;
  734|       |    _BitScanForward64(&out, static_cast<unsigned __int64>(bits));
  735|       |    return bits == 0 ? kNumBits : static_cast<int>(out);
  736|       |  }
  737|       |};
  738|       |
  739|       |#endif  // _WIN64
  740|       |
  741|       |#endif  // EIGEN_COMP_GNUC || EIGEN_COMP_CLANG
  742|       |
  743|       |template <typename BitsType>
  744|       |struct log_2_impl {
  745|       |  static constexpr int kTotalBits = sizeof(BitsType) * CHAR_BIT;
  746|       |  static EIGEN_DEVICE_FUNC inline int run_ceil(const BitsType& x) {
  747|       |    const int n = kTotalBits - clz(x);
  748|       |    bool power_of_two = (x & (x - 1)) == 0;
  749|       |    return x == 0 ? 0 : power_of_two ? (n - 1) : n;
  750|       |  }
  751|       |  static EIGEN_DEVICE_FUNC inline int run_floor(const BitsType& x) {
  752|       |    const int n = kTotalBits - clz(x);
  753|       |    return x == 0 ? 0 : n - 1;
  754|       |  }
  755|       |};
  756|       |
  757|       |template <typename BitsType>
  758|       |int log2_ceil(const BitsType& x) {
  759|       |  return log_2_impl<BitsType>::run_ceil(x);
  760|       |}
  761|       |
  762|       |template <typename BitsType>
  763|       |int log2_floor(const BitsType& x) {
  764|       |  return log_2_impl<BitsType>::run_floor(x);
  765|       |}
  766|       |
  767|       |// Implementation of is* functions
  768|       |
  769|       |template <typename T>
  770|       |EIGEN_DEVICE_FUNC std::enable_if_t<!(std::numeric_limits<T>::has_infinity || std::numeric_limits<T>::has_quiet_NaN ||
  771|       |                                     std::numeric_limits<T>::has_signaling_NaN),
  772|       |                                   bool>
  773|       |isfinite_impl(const T&) {
  774|       |  return true;
  775|       |}
  776|       |
  777|       |template <typename T>
  778|       |EIGEN_DEVICE_FUNC std::enable_if_t<(std::numeric_limits<T>::has_infinity || std::numeric_limits<T>::has_quiet_NaN ||
  779|       |                                    std::numeric_limits<T>::has_signaling_NaN) &&
  780|       |                                       (!NumTraits<T>::IsComplex),
  781|       |                                   bool>
  782|      0|isfinite_impl(const T& x) {
  783|      0|  EIGEN_USING_STD(isfinite);
  784|      0|  return isfinite EIGEN_NOT_A_MACRO(x);
  785|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13isfinite_implIdEENSt9enable_ifIXaaoooosr3std14numeric_limitsIT_EE12has_infinitysr3std14numeric_limitsIS3_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNntsr9NumTraitsIS3_EE9IsComplexEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13isfinite_implIfEENSt9enable_ifIXaaoooosr3std14numeric_limitsIT_EE12has_infinitysr3std14numeric_limitsIS3_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNntsr9NumTraitsIS3_EE9IsComplexEbE4typeERKS3_
  ------------------
  786|       |
  787|       |template <typename T>
  788|       |EIGEN_DEVICE_FUNC std::enable_if_t<!std::numeric_limits<T>::has_infinity, bool> isinf_impl(const T&) {
  789|       |  return false;
  790|       |}
  791|       |
  792|       |template <typename T>
  793|       |EIGEN_DEVICE_FUNC std::enable_if_t<(std::numeric_limits<T>::has_infinity && !NumTraits<T>::IsComplex), bool> isinf_impl(
  794|       |    const T& x) {
  795|       |  EIGEN_USING_STD(isinf);
  796|       |  return isinf EIGEN_NOT_A_MACRO(x);
  797|       |}
  798|       |
  799|       |template <typename T>
  800|       |EIGEN_DEVICE_FUNC
  801|       |    std::enable_if_t<!(std::numeric_limits<T>::has_quiet_NaN || std::numeric_limits<T>::has_signaling_NaN), bool>
  802|      0|    isnan_impl(const T&) {
  803|      0|  return false;
  804|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIsEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implItEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIiEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIjEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIlEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implImEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIxEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIyEENSt9enable_ifIXntoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNEbE4typeERKS3_
  ------------------
  805|       |
  806|       |template <typename T>
  807|       |EIGEN_DEVICE_FUNC std::enable_if_t<
  808|       |    (std::numeric_limits<T>::has_quiet_NaN || std::numeric_limits<T>::has_signaling_NaN) && (!NumTraits<T>::IsComplex),
  809|       |    bool>
  810|      0|isnan_impl(const T& x) {
  811|      0|  EIGEN_USING_STD(isnan);
  812|      0|  return isnan EIGEN_NOT_A_MACRO(x);
  813|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIfEENSt9enable_ifIXaaoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNntsr9NumTraitsIS3_EE9IsComplexEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implIdEENSt9enable_ifIXaaoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS3_EE17has_signaling_NaNntsr9NumTraitsIS3_EE9IsComplexEbE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal10isnan_implINS_4halfEEENSt9enable_ifIXaaoosr3std14numeric_limitsIT_EE13has_quiet_NaNsr3std14numeric_limitsIS4_EE17has_signaling_NaNntsr9NumTraitsIS4_EE9IsComplexEbE4typeERKS4_
  ------------------
  814|       |
  815|       |// The following overload are defined at the end of this file
  816|       |template <typename T>
  817|       |EIGEN_DEVICE_FUNC bool isfinite_impl(const std::complex<T>& x);
  818|       |template <typename T>
  819|       |EIGEN_DEVICE_FUNC bool isnan_impl(const std::complex<T>& x);
  820|       |template <typename T>
  821|       |EIGEN_DEVICE_FUNC bool isinf_impl(const std::complex<T>& x);
  822|       |template <typename T>
  823|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS T ptanh_float(const T& a_x);
  824|       |
  825|       |/****************************************************************************
  826|       | * Implementation of sign                                                 *
  827|       | ****************************************************************************/
  828|       |template <typename Scalar, bool IsComplex = (NumTraits<Scalar>::IsComplex != 0),
  829|       |          bool IsInteger = (NumTraits<Scalar>::IsInteger != 0)>
  830|       |struct sign_impl {
  831|       |  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& a) { return Scalar((a > Scalar(0)) - (a < Scalar(0))); }
  832|       |};
  833|       |
  834|       |template <typename Scalar>
  835|       |struct sign_impl<Scalar, false, false> {
  836|       |  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& a) {
  837|       |    return (isnan_impl<Scalar>)(a) ? a : Scalar((a > Scalar(0)) - (a < Scalar(0)));
  838|       |  }
  839|       |};
  840|       |
  841|       |template <typename Scalar, bool IsInteger>
  842|       |struct sign_impl<Scalar, true, IsInteger> {
  843|       |  EIGEN_DEVICE_FUNC static inline Scalar run(const Scalar& a) {
  844|       |    using real_type = typename NumTraits<Scalar>::Real;
  845|       |    EIGEN_USING_STD(abs);
  846|       |    real_type aa = abs(a);
  847|       |    if (aa == real_type(0)) return Scalar(0);
  848|       |    aa = real_type(1) / aa;
  849|       |    return Scalar(a.real() * aa, a.imag() * aa);
  850|       |  }
  851|       |};
  852|       |
  853|       |// The sign function for bool is the identity.
  854|       |template <>
  855|       |struct sign_impl<bool, false, true> {
  856|      0|  EIGEN_DEVICE_FUNC static inline bool run(const bool& a) { return a; }
  857|       |};
  858|       |
  859|       |template <typename Scalar>
  860|       |struct sign_retval {
  861|       |  typedef Scalar type;
  862|       |};
  863|       |
  864|       |// suppress "unary minus operator applied to unsigned type, result still unsigned" warnings on MSVC
  865|       |// note: `0 - a` is distinct from `-a` when Scalar is a floating point type and `a` is zero
  866|       |
  867|       |template <typename Scalar, bool IsInteger = NumTraits<Scalar>::IsInteger>
  868|       |struct negate_impl {
  869|       |  static EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Scalar run(const Scalar& a) { return -a; }
  870|       |};
  871|       |
  872|       |template <typename Scalar>
  873|       |struct negate_impl<Scalar, true> {
  874|       |  EIGEN_STATIC_ASSERT((!is_same<Scalar, bool>::value), NEGATE IS NOT DEFINED FOR BOOLEAN TYPES)
  875|       |  static EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE Scalar run(const Scalar& a) { return Scalar(0) - a; }
  876|       |};
  877|       |
  878|       |template <typename Scalar>
  879|       |struct negate_retval {
  880|       |  typedef Scalar type;
  881|       |};
  882|       |
  883|       |template <typename Scalar, bool IsInteger = NumTraits<typename unpacket_traits<Scalar>::type>::IsInteger>
  884|       |struct nearest_integer_impl {
  885|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_floor(const Scalar& x) {
  886|       |    EIGEN_USING_STD(floor) return floor(x);
  887|       |  }
  888|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_ceil(const Scalar& x) {
  889|       |    EIGEN_USING_STD(ceil) return ceil(x);
  890|       |  }
  891|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_rint(const Scalar& x) {
  892|       |    EIGEN_USING_STD(rint) return rint(x);
  893|       |  }
  894|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_round(const Scalar& x) {
  895|       |    EIGEN_USING_STD(round) return round(x);
  896|       |  }
  897|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_trunc(const Scalar& x) {
  898|       |    EIGEN_USING_STD(trunc) return trunc(x);
  899|       |  }
  900|       |};
  901|       |template <typename Scalar>
  902|       |struct nearest_integer_impl<Scalar, true> {
  903|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_floor(const Scalar& x) { return x; }
  904|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_ceil(const Scalar& x) { return x; }
  905|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_rint(const Scalar& x) { return x; }
  906|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_round(const Scalar& x) { return x; }
  907|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_trunc(const Scalar& x) { return x; }
  908|       |};
  909|       |
  910|       |}  // end namespace internal
  911|       |
  912|       |/****************************************************************************
  913|       | * Generic math functions                                                    *
  914|       | ****************************************************************************/
  915|       |
  916|       |namespace numext {
  917|       |
  918|       |#if (!defined(EIGEN_GPUCC) || defined(EIGEN_CONSTEXPR_ARE_DEVICE_FUNC))
  919|       |template <typename T>
  920|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T mini(const T& x, const T& y) {
  921|      0|  EIGEN_USING_STD(min)
  922|      0|  return min EIGEN_NOT_A_MACRO(x, y);
  923|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4miniIfEET_RKS2_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4miniIdEET_RKS2_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4miniINS_4halfEEET_RKS3_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4miniINS_8bfloat16EEET_RKS3_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4miniIeEET_RKS2_S4_
  ------------------
  924|       |
  925|       |template <typename T>
  926|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T maxi(const T& x, const T& y) {
  927|      0|  EIGEN_USING_STD(max)
  928|      0|  return max EIGEN_NOT_A_MACRO(x, y);
  929|      0|}
  930|       |#else
  931|       |template <typename T>
  932|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T mini(const T& x, const T& y) {
  933|       |  return y < x ? y : x;
  934|       |}
  935|       |template <>
  936|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float mini(const float& x, const float& y) {
  937|       |  return fminf(x, y);
  938|       |}
  939|       |template <>
  940|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double mini(const double& x, const double& y) {
  941|       |  return fmin(x, y);
  942|       |}
  943|       |
  944|       |#ifndef EIGEN_GPU_COMPILE_PHASE
  945|       |template <>
  946|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE long double mini(const long double& x, const long double& y) {
  947|       |#if defined(EIGEN_HIPCC)
  948|       |  // no "fminl" on HIP yet
  949|       |  return (x < y) ? x : y;
  950|       |#else
  951|       |  return fminl(x, y);
  952|       |#endif
  953|       |}
  954|       |#endif
  955|       |
  956|       |template <typename T>
  957|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T maxi(const T& x, const T& y) {
  958|       |  return x < y ? y : x;
  959|       |}
  960|       |template <>
  961|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float maxi(const float& x, const float& y) {
  962|       |  return fmaxf(x, y);
  963|       |}
  964|       |template <>
  965|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double maxi(const double& x, const double& y) {
  966|       |  return fmax(x, y);
  967|       |}
  968|       |#ifndef EIGEN_GPU_COMPILE_PHASE
  969|       |template <>
  970|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE long double maxi(const long double& x, const long double& y) {
  971|       |#if defined(EIGEN_HIPCC)
  972|       |  // no "fmaxl" on HIP yet
  973|       |  return (x > y) ? x : y;
  974|       |#else
  975|       |  return fmaxl(x, y);
  976|       |#endif
  977|       |}
  978|       |#endif
  979|       |#endif
  980|       |
  981|       |#if defined(SYCL_DEVICE_ONLY)
  982|       |
  983|       |#define SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_BINARY(NAME, FUNC) \
  984|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_char)    \
  985|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_short)   \
  986|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_int)     \
  987|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_long)
  988|       |#define SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_UNARY(NAME, FUNC) \
  989|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_char)    \
  990|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_short)   \
  991|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_int)     \
  992|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_long)
  993|       |#define SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_BINARY(NAME, FUNC) \
  994|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_uchar)     \
  995|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_ushort)    \
  996|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_uint)      \
  997|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_ulong)
  998|       |#define SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_UNARY(NAME, FUNC) \
  999|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_uchar)     \
 1000|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_ushort)    \
 1001|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_uint)      \
 1002|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_ulong)
 1003|       |#define SYCL_SPECIALIZE_INTEGER_TYPES_BINARY(NAME, FUNC)  \
 1004|       |  SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_BINARY(NAME, FUNC) \
 1005|       |  SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_BINARY(NAME, FUNC)
 1006|       |#define SYCL_SPECIALIZE_INTEGER_TYPES_UNARY(NAME, FUNC)  \
 1007|       |  SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_UNARY(NAME, FUNC) \
 1008|       |  SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_UNARY(NAME, FUNC)
 1009|       |#define SYCL_SPECIALIZE_FLOATING_TYPES_BINARY(NAME, FUNC)     \
 1010|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_float) \
 1011|       |  SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, cl::sycl::cl_double)
 1012|       |#define SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(NAME, FUNC)     \
 1013|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_float) \
 1014|       |  SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, cl::sycl::cl_double)
 1015|       |#define SYCL_SPECIALIZE_FLOATING_TYPES_UNARY_FUNC_RET_TYPE(NAME, FUNC, RET_TYPE) \
 1016|       |  SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, cl::sycl::cl_float)       \
 1017|       |  SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, cl::sycl::cl_double)
 1018|       |
 1019|       |#define SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, RET_TYPE, ARG_TYPE)     \
 1020|       |  template <>                                                              \
 1021|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE RET_TYPE NAME(const ARG_TYPE& x) { \
 1022|       |    return cl::sycl::FUNC(x);                                              \
 1023|       |  }
 1024|       |
 1025|       |#define SYCL_SPECIALIZE_UNARY_FUNC(NAME, FUNC, TYPE) SYCL_SPECIALIZE_GEN_UNARY_FUNC(NAME, FUNC, TYPE, TYPE)
 1026|       |
 1027|       |#define SYCL_SPECIALIZE_GEN1_BINARY_FUNC(NAME, FUNC, RET_TYPE, ARG_TYPE1, ARG_TYPE2)            \
 1028|       |  template <>                                                                                   \
 1029|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE RET_TYPE NAME(const ARG_TYPE1& x, const ARG_TYPE2& y) { \
 1030|       |    return cl::sycl::FUNC(x, y);                                                                \
 1031|       |  }
 1032|       |
 1033|       |#define SYCL_SPECIALIZE_GEN2_BINARY_FUNC(NAME, FUNC, RET_TYPE, ARG_TYPE) \
 1034|       |  SYCL_SPECIALIZE_GEN1_BINARY_FUNC(NAME, FUNC, RET_TYPE, ARG_TYPE, ARG_TYPE)
 1035|       |
 1036|       |#define SYCL_SPECIALIZE_BINARY_FUNC(NAME, FUNC, TYPE) SYCL_SPECIALIZE_GEN2_BINARY_FUNC(NAME, FUNC, TYPE, TYPE)
 1037|       |
 1038|       |SYCL_SPECIALIZE_INTEGER_TYPES_BINARY(mini, min)
 1039|       |SYCL_SPECIALIZE_FLOATING_TYPES_BINARY(mini, fmin)
 1040|       |SYCL_SPECIALIZE_INTEGER_TYPES_BINARY(maxi, max)
 1041|       |SYCL_SPECIALIZE_FLOATING_TYPES_BINARY(maxi, fmax)
 1042|       |
 1043|       |#endif
 1044|       |
 1045|       |template <typename Scalar>
 1046|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(real, Scalar) real(const Scalar& x) {
 1047|       |  return EIGEN_MATHFUNC_IMPL(real, Scalar)::run(x);
 1048|       |}
 1049|       |
 1050|       |template <typename Scalar>
 1051|       |EIGEN_DEVICE_FUNC inline internal::add_const_on_value_type_t<EIGEN_MATHFUNC_RETVAL(real_ref, Scalar)> real_ref(
 1052|      0|    const Scalar& x) {
 1053|      0|  return internal::real_ref_impl<Scalar>::run(x);
 1054|      0|}
 1055|       |
 1056|       |template <typename Scalar>
 1057|      0|EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(real_ref, Scalar) real_ref(Scalar& x) {
 1058|      0|  return EIGEN_MATHFUNC_IMPL(real_ref, Scalar)::run(x);
 1059|      0|}
 1060|       |
 1061|       |template <typename Scalar>
 1062|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(imag, Scalar) imag(const Scalar& x) {
 1063|       |  return EIGEN_MATHFUNC_IMPL(imag, Scalar)::run(x);
 1064|       |}
 1065|       |
 1066|       |template <typename Scalar>
 1067|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(arg, Scalar) arg(const Scalar& x) {
 1068|       |  return EIGEN_MATHFUNC_IMPL(arg, Scalar)::run(x);
 1069|       |}
 1070|       |
 1071|       |template <typename Scalar>
 1072|       |EIGEN_DEVICE_FUNC inline internal::add_const_on_value_type_t<EIGEN_MATHFUNC_RETVAL(imag_ref, Scalar)> imag_ref(
 1073|       |    const Scalar& x) {
 1074|       |  return internal::imag_ref_impl<Scalar>::run(x);
 1075|       |}
 1076|       |
 1077|       |template <typename Scalar>
 1078|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(imag_ref, Scalar) imag_ref(Scalar& x) {
 1079|       |  return EIGEN_MATHFUNC_IMPL(imag_ref, Scalar)::run(x);
 1080|       |}
 1081|       |
 1082|       |template <typename Scalar>
 1083|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(conj, Scalar) conj(const Scalar& x) {
 1084|       |  return EIGEN_MATHFUNC_IMPL(conj, Scalar)::run(x);
 1085|       |}
 1086|       |
 1087|       |template <typename Scalar>
 1088|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(sign, Scalar) sign(const Scalar& x) {
 1089|       |  return EIGEN_MATHFUNC_IMPL(sign, Scalar)::run(x);
 1090|       |}
 1091|       |
 1092|       |template <typename Scalar>
 1093|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(negate, Scalar) negate(const Scalar& x) {
 1094|       |  return EIGEN_MATHFUNC_IMPL(negate, Scalar)::run(x);
 1095|       |}
 1096|       |
 1097|       |template <typename Scalar>
 1098|      0|EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(abs2, Scalar) abs2(const Scalar& x) {
 1099|      0|  return EIGEN_MATHFUNC_IMPL(abs2, Scalar)::run(x);
 1100|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4abs2IfEENS_8internal11abs2_retvalINS2_36global_math_functions_filtering_baseIT_vE4typeEE4typeERKS5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4abs2ISt7complexIfEEENS_8internal11abs2_retvalINS4_36global_math_functions_filtering_baseIT_vE4typeEE4typeERKS7_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4abs2ISt7complexIdEEENS_8internal11abs2_retvalINS4_36global_math_functions_filtering_baseIT_vE4typeEE4typeERKS7_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext4abs2ISt7complexIeEEENS_8internal11abs2_retvalINS4_36global_math_functions_filtering_baseIT_vE4typeEE4typeERKS7_
  ------------------
 1101|       |
 1102|      0|EIGEN_DEVICE_FUNC inline bool abs2(bool x) { return x; }
 1103|       |
 1104|       |template <typename T>
 1105|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T absdiff(const T& x, const T& y) {
 1106|       |  return x > y ? x - y : y - x;
 1107|       |}
 1108|       |template <>
 1109|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float absdiff(const float& x, const float& y) {
 1110|      0|  return fabsf(x - y);
 1111|      0|}
 1112|       |template <>
 1113|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double absdiff(const double& x, const double& y) {
 1114|      0|  return fabs(x - y);
 1115|      0|}
 1116|       |
 1117|       |// HIP and CUDA do not support long double.
 1118|       |#ifndef EIGEN_GPU_COMPILE_PHASE
 1119|       |template <>
 1120|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE long double absdiff(const long double& x, const long double& y) {
 1121|      0|  return fabsl(x - y);
 1122|      0|}
 1123|       |#endif
 1124|       |
 1125|       |template <typename Scalar>
 1126|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(norm1, Scalar) norm1(const Scalar& x) {
 1127|       |  return EIGEN_MATHFUNC_IMPL(norm1, Scalar)::run(x);
 1128|       |}
 1129|       |
 1130|       |template <typename Scalar>
 1131|       |EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(hypot, Scalar) hypot(const Scalar& x, const Scalar& y) {
 1132|       |  return EIGEN_MATHFUNC_IMPL(hypot, Scalar)::run(x, y);
 1133|       |}
 1134|       |
 1135|       |#if defined(SYCL_DEVICE_ONLY)
 1136|       |SYCL_SPECIALIZE_FLOATING_TYPES_BINARY(hypot, hypot)
 1137|       |#endif
 1138|       |
 1139|       |template <typename Scalar>
 1140|      0|EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(log1p, Scalar) log1p(const Scalar& x) {
 1141|      0|  return EIGEN_MATHFUNC_IMPL(log1p, Scalar)::run(x);
 1142|      0|}
 1143|       |
 1144|       |#if defined(SYCL_DEVICE_ONLY)
 1145|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(log1p, log1p)
 1146|       |#endif
 1147|       |
 1148|       |#if defined(EIGEN_GPUCC)
 1149|       |template <>
 1150|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float log1p(const float& x) {
 1151|       |  return ::log1pf(x);
 1152|       |}
 1153|       |
 1154|       |template <>
 1155|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double log1p(const double& x) {
 1156|       |  return ::log1p(x);
 1157|       |}
 1158|       |#endif
 1159|       |
 1160|       |template <typename ScalarX, typename ScalarY>
 1161|       |EIGEN_DEVICE_FUNC inline typename internal::pow_impl<ScalarX, ScalarY>::result_type pow(const ScalarX& x,
 1162|       |                                                                                        const ScalarY& y) {
 1163|       |  return internal::pow_impl<ScalarX, ScalarY>::run(x, y);
 1164|       |}
 1165|       |
 1166|       |#if defined(SYCL_DEVICE_ONLY)
 1167|       |SYCL_SPECIALIZE_FLOATING_TYPES_BINARY(pow, pow)
 1168|       |#endif
 1169|       |
 1170|       |template <typename T>
 1171|      0|EIGEN_DEVICE_FUNC bool(isnan)(const T& x) {
 1172|      0|  return internal::isnan_impl(x);
 1173|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIfEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIsEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanItEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIiEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIjEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIlEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanImEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIxEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIyEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanIdEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext5isnanINS_4halfEEEbRKT_
  ------------------
 1174|       |template <typename T>
 1175|       |EIGEN_DEVICE_FUNC bool(isinf)(const T& x) {
 1176|       |  return internal::isinf_impl(x);
 1177|       |}
 1178|       |template <typename T>
 1179|      0|EIGEN_DEVICE_FUNC bool(isfinite)(const T& x) {
 1180|      0|  return internal::isfinite_impl(x);
 1181|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8isfiniteIdEEbRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8isfiniteIfEEbRKT_
  ------------------
 1182|       |
 1183|       |#if defined(SYCL_DEVICE_ONLY)
 1184|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY_FUNC_RET_TYPE(isnan, isnan, bool)
 1185|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY_FUNC_RET_TYPE(isinf, isinf, bool)
 1186|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY_FUNC_RET_TYPE(isfinite, isfinite, bool)
 1187|       |#endif
 1188|       |
 1189|       |template <typename Scalar>
 1190|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar rint(const Scalar& x) {
 1191|       |  return internal::nearest_integer_impl<Scalar>::run_rint(x);
 1192|       |}
 1193|       |
 1194|       |template <typename Scalar>
 1195|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar round(const Scalar& x) {
 1196|       |  return internal::nearest_integer_impl<Scalar>::run_round(x);
 1197|       |}
 1198|       |
 1199|       |template <typename Scalar>
 1200|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar(floor)(const Scalar& x) {
 1201|       |  return internal::nearest_integer_impl<Scalar>::run_floor(x);
 1202|       |}
 1203|       |
 1204|       |template <typename Scalar>
 1205|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar(ceil)(const Scalar& x) {
 1206|       |  return internal::nearest_integer_impl<Scalar>::run_ceil(x);
 1207|       |}
 1208|       |
 1209|       |template <typename Scalar>
 1210|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar(trunc)(const Scalar& x) {
 1211|       |  return internal::nearest_integer_impl<Scalar>::run_trunc(x);
 1212|       |}
 1213|       |
 1214|       |#if defined(SYCL_DEVICE_ONLY)
 1215|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(round, round)
 1216|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(floor, floor)
 1217|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(ceil, ceil)
 1218|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(trunc, trunc)
 1219|       |#endif
 1220|       |
 1221|       |#if defined(EIGEN_GPUCC)
 1222|       |template <>
 1223|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float floor(const float& x) {
 1224|       |  return ::floorf(x);
 1225|       |}
 1226|       |template <>
 1227|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double floor(const double& x) {
 1228|       |  return ::floor(x);
 1229|       |}
 1230|       |template <>
 1231|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float ceil(const float& x) {
 1232|       |  return ::ceilf(x);
 1233|       |}
 1234|       |template <>
 1235|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double ceil(const double& x) {
 1236|       |  return ::ceil(x);
 1237|       |}
 1238|       |template <>
 1239|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float trunc(const float& x) {
 1240|       |  return ::truncf(x);
 1241|       |}
 1242|       |template <>
 1243|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double trunc(const double& x) {
 1244|       |  return ::trunc(x);
 1245|       |}
 1246|       |#endif
 1247|       |
 1248|       |// Integer division with rounding up.
 1249|       |// T is assumed to be an integer type with a>=0, and b>0
 1250|       |template <typename T>
 1251|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE EIGEN_CONSTEXPR T div_ceil(T a, T b) {
 1252|       |  using UnsignedT = typename internal::make_unsigned<T>::type;
 1253|       |  EIGEN_STATIC_ASSERT((NumTraits<T>::IsInteger), THIS FUNCTION IS FOR INTEGER TYPES)
 1254|       |  eigen_assert(a >= 0);
 1255|       |  eigen_assert(b > 0);
 1256|       |  // Note: explicitly declaring a and b as non-negative values allows the compiler to use better optimizations
 1257|       |  const UnsignedT ua = UnsignedT(a);
 1258|       |  const UnsignedT ub = UnsignedT(b);
 1259|       |  // Note: This form is used because it cannot overflow.
 1260|       |  return ua == 0 ? 0 : (ua - 1) / ub + 1;
 1261|       |}
 1262|       |
 1263|       |// Integer round down to nearest power of b
 1264|       |// T is assumed to be an integer type with a>=0, and b>0
 1265|       |template <typename T, typename U>
 1266|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE EIGEN_CONSTEXPR T round_down(T a, U b) {
 1267|      0|  using UnsignedT = typename internal::make_unsigned<T>::type;
 1268|      0|  using UnsignedU = typename internal::make_unsigned<U>::type;
 1269|      0|  EIGEN_STATIC_ASSERT((NumTraits<T>::IsInteger), THIS FUNCTION IS FOR INTEGER TYPES)
 1270|      0|  EIGEN_STATIC_ASSERT((NumTraits<U>::IsInteger), THIS FUNCTION IS FOR INTEGER TYPES)
 1271|      0|  eigen_assert(a >= 0);
 1272|      0|  eigen_assert(b > 0);
 1273|      0|  // Note: explicitly declaring a and b as non-negative values allows the compiler to use better optimizations
 1274|      0|  const UnsignedT ua = UnsignedT(a);
 1275|      0|  const UnsignedU ub = UnsignedU(b);
 1276|      0|  return ub * (ua / ub);
 1277|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext10round_downImiEET_S2_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext10round_downIllEET_S2_T0_
  ------------------
 1278|       |
 1279|       |/** Log base 2 for 32 bits positive integers.
 1280|       | * Conveniently returns 0 for x==0. */
 1281|      0|EIGEN_CONSTEXPR inline int log2(int x) {
 1282|      0|  eigen_assert(x >= 0);
 1283|      0|  unsigned int v(x);
 1284|      0|  constexpr int table[32] = {0, 9,  1,  10, 13, 21, 2,  29, 11, 14, 16, 18, 22, 25, 3, 30,
 1285|      0|                             8, 12, 20, 28, 15, 17, 24, 7,  19, 27, 23, 6,  26, 5,  4, 31};
 1286|      0|  v |= v >> 1;
 1287|      0|  v |= v >> 2;
 1288|      0|  v |= v >> 4;
 1289|      0|  v |= v >> 8;
 1290|      0|  v |= v >> 16;
 1291|      0|  return table[(v * 0x07C4ACDDU) >> 27];
 1292|      0|}
 1293|       |
 1294|       |/** \returns the square root of \a x.
 1295|       | *
 1296|       | * It is essentially equivalent to
 1297|       | * \code using std::sqrt; return sqrt(x); \endcode
 1298|       | * but slightly faster for float/double and some compilers (e.g., gcc), thanks to
 1299|       | * specializations when SSE is enabled.
 1300|       | *
 1301|       | * It's usage is justified in performance critical functions, like norm/normalize.
 1302|       | */
 1303|       |template <typename Scalar>
 1304|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE EIGEN_MATHFUNC_RETVAL(sqrt, Scalar) sqrt(const Scalar& x) {
 1305|       |  return EIGEN_MATHFUNC_IMPL(sqrt, Scalar)::run(x);
 1306|       |}
 1307|       |
 1308|       |// Boolean specialization, avoids implicit float to bool conversion (-Wimplicit-conversion-floating-point-to-bool).
 1309|       |template <>
 1310|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS EIGEN_DEVICE_FUNC bool sqrt<bool>(const bool& x) {
 1311|      0|  return x;
 1312|      0|}
 1313|       |
 1314|       |#if defined(SYCL_DEVICE_ONLY)
 1315|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(sqrt, sqrt)
 1316|       |#endif
 1317|       |
 1318|       |/** \returns the cube root of \a x. **/
 1319|       |template <typename T>
 1320|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T cbrt(const T& x) {
 1321|       |  EIGEN_USING_STD(cbrt);
 1322|       |  return static_cast<T>(cbrt(x));
 1323|       |}
 1324|       |
 1325|       |/** \returns the reciprocal square root of \a x. **/
 1326|       |template <typename T>
 1327|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T rsqrt(const T& x) {
 1328|       |  return internal::rsqrt_impl<T>::run(x);
 1329|       |}
 1330|       |
 1331|       |template <typename T>
 1332|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T log(const T& x) {
 1333|       |  return internal::log_impl<T>::run(x);
 1334|       |}
 1335|       |
 1336|       |#if defined(SYCL_DEVICE_ONLY)
 1337|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(log, log)
 1338|       |#endif
 1339|       |
 1340|       |#if defined(EIGEN_GPUCC)
 1341|       |template <>
 1342|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float log(const float& x) {
 1343|       |  return ::logf(x);
 1344|       |}
 1345|       |
 1346|       |template <>
 1347|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double log(const double& x) {
 1348|       |  return ::log(x);
 1349|       |}
 1350|       |#endif
 1351|       |
 1352|       |template <typename T>
 1353|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
 1354|       |    std::enable_if_t<NumTraits<T>::IsSigned || NumTraits<T>::IsComplex, typename NumTraits<T>::Real>
 1355|      0|    abs(const T& x) {
 1356|      0|  EIGEN_USING_STD(abs);
 1357|      0|  return abs(x);
 1358|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext3absIfEENSt9enable_ifIXoosr9NumTraitsIT_EE8IsSignedsr9NumTraitsIS3_EE9IsComplexENS_9NumTraitsIS3_E4RealEE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext3absIdEENSt9enable_ifIXoosr9NumTraitsIT_EE8IsSignedsr9NumTraitsIS3_EE9IsComplexENS_9NumTraitsIS3_E4RealEE4typeERKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext3absINS_4halfEEENSt9enable_ifIXoosr9NumTraitsIT_EE8IsSignedsr9NumTraitsIS4_EE9IsComplexENS_9NumTraitsIS4_E4RealEE4typeERKS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext3absINS_8bfloat16EEENSt9enable_ifIXoosr9NumTraitsIT_EE8IsSignedsr9NumTraitsIS4_EE9IsComplexENS_9NumTraitsIS4_E4RealEE4typeERKS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext3absIeEENSt9enable_ifIXoosr9NumTraitsIT_EE8IsSignedsr9NumTraitsIS3_EE9IsComplexENS_9NumTraitsIS3_E4RealEE4typeERKS3_
  ------------------
 1359|       |
 1360|       |template <typename T>
 1361|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE
 1362|       |    std::enable_if_t<!(NumTraits<T>::IsSigned || NumTraits<T>::IsComplex), typename NumTraits<T>::Real>
 1363|       |    abs(const T& x) {
 1364|       |  return x;
 1365|       |}
 1366|       |
 1367|       |#if defined(SYCL_DEVICE_ONLY)
 1368|       |SYCL_SPECIALIZE_INTEGER_TYPES_UNARY(abs, abs)
 1369|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(abs, fabs)
 1370|       |#endif
 1371|       |
 1372|       |#if defined(EIGEN_GPUCC)
 1373|       |template <>
 1374|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float abs(const float& x) {
 1375|       |  return ::fabsf(x);
 1376|       |}
 1377|       |
 1378|       |template <>
 1379|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double abs(const double& x) {
 1380|       |  return ::fabs(x);
 1381|       |}
 1382|       |
 1383|       |template <>
 1384|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float abs(const std::complex<float>& x) {
 1385|       |  return ::hypotf(x.real(), x.imag());
 1386|       |}
 1387|       |
 1388|       |template <>
 1389|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double abs(const std::complex<double>& x) {
 1390|       |  return ::hypot(x.real(), x.imag());
 1391|       |}
 1392|       |#endif
 1393|       |
 1394|       |template <typename Scalar, bool IsInteger = NumTraits<Scalar>::IsInteger, bool IsSigned = NumTraits<Scalar>::IsSigned>
 1395|       |struct signbit_impl;
 1396|       |template <typename Scalar>
 1397|       |struct signbit_impl<Scalar, false, true> {
 1398|       |  static constexpr size_t Size = sizeof(Scalar);
 1399|       |  static constexpr size_t Shift = (CHAR_BIT * Size) - 1;
 1400|       |  using intSize_t = typename get_integer_by_size<Size>::signed_type;
 1401|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE static Scalar run(const Scalar& x) {
 1402|       |    intSize_t a = bit_cast<intSize_t, Scalar>(x);
 1403|       |    a = a >> Shift;
 1404|       |    Scalar result = bit_cast<Scalar, intSize_t>(a);
 1405|       |    return result;
 1406|       |  }
 1407|       |};
 1408|       |template <typename Scalar>
 1409|       |struct signbit_impl<Scalar, true, true> {
 1410|       |  static constexpr size_t Size = sizeof(Scalar);
 1411|       |  static constexpr size_t Shift = (CHAR_BIT * Size) - 1;
 1412|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE static constexpr Scalar run(const Scalar& x) { return x >> Shift; }
 1413|       |};
 1414|       |template <typename Scalar>
 1415|       |struct signbit_impl<Scalar, true, false> {
 1416|       |  EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE static constexpr Scalar run(const Scalar&) { return Scalar(0); }
 1417|       |};
 1418|       |template <typename Scalar>
 1419|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE static constexpr Scalar signbit(const Scalar& x) {
 1420|       |  return signbit_impl<Scalar>::run(x);
 1421|       |}
 1422|       |
 1423|       |template <typename T>
 1424|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T exp(const T& x) {
 1425|      0|  EIGEN_USING_STD(exp);
 1426|      0|  return exp(x);
 1427|      0|}
 1428|       |
 1429|       |// MSVC screws up some edge-cases for std::exp(complex).
 1430|       |#ifdef EIGEN_COMP_MSVC
 1431|       |template <typename RealScalar>
 1432|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE std::complex<RealScalar> exp(const std::complex<RealScalar>& x) {
 1433|       |  EIGEN_USING_STD(exp);
 1434|       |  // If z is (x,±∞) (for any finite x), the result is (NaN,NaN) and FE_INVALID is raised.
 1435|       |  // If z is (x,NaN) (for any finite x), the result is (NaN,NaN) and FE_INVALID may be raised.
 1436|       |  if ((isfinite)(real_ref(x)) && !(isfinite)(imag_ref(x))) {
 1437|       |    return std::complex<RealScalar>(NumTraits<RealScalar>::quiet_NaN(), NumTraits<RealScalar>::quiet_NaN());
 1438|       |  }
 1439|       |  // If z is (+∞,±∞), the result is (±∞,NaN) and FE_INVALID is raised (the sign of the real part is unspecified)
 1440|       |  // If z is (+∞,NaN), the result is (±∞,NaN) (the sign of the real part is unspecified)
 1441|       |  if ((real_ref(x) == NumTraits<RealScalar>::infinity() && !(isfinite)(imag_ref(x)))) {
 1442|       |    return std::complex<RealScalar>(NumTraits<RealScalar>::infinity(), NumTraits<RealScalar>::quiet_NaN());
 1443|       |  }
 1444|       |  return exp(x);
 1445|       |}
 1446|       |#endif
 1447|       |
 1448|       |#if defined(SYCL_DEVICE_ONLY)
 1449|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(exp, exp)
 1450|       |#endif
 1451|       |
 1452|       |#if defined(EIGEN_GPUCC)
 1453|       |template <>
 1454|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float exp(const float& x) {
 1455|       |  return ::expf(x);
 1456|       |}
 1457|       |
 1458|       |template <>
 1459|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double exp(const double& x) {
 1460|       |  return ::exp(x);
 1461|       |}
 1462|       |
 1463|       |template <>
 1464|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE std::complex<float> exp(const std::complex<float>& x) {
 1465|       |  float com = ::expf(x.real());
 1466|       |  float res_real = com * ::cosf(x.imag());
 1467|       |  float res_imag = com * ::sinf(x.imag());
 1468|       |  return std::complex<float>(res_real, res_imag);
 1469|       |}
 1470|       |
 1471|       |template <>
 1472|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE std::complex<double> exp(const std::complex<double>& x) {
 1473|       |  double com = ::exp(x.real());
 1474|       |  double res_real = com * ::cos(x.imag());
 1475|       |  double res_imag = com * ::sin(x.imag());
 1476|       |  return std::complex<double>(res_real, res_imag);
 1477|       |}
 1478|       |#endif
 1479|       |
 1480|       |template <typename T>
 1481|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T exp2(const T& x) {
 1482|       |  EIGEN_USING_STD(exp2);
 1483|       |  return exp2(x);
 1484|       |}
 1485|       |
 1486|       |// MSVC screws up some edge-cases for std::exp2(complex).
 1487|       |#ifdef EIGEN_COMP_MSVC
 1488|       |template <typename RealScalar>
 1489|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE std::complex<RealScalar> exp2(const std::complex<RealScalar>& x) {
 1490|       |  EIGEN_USING_STD(exp);
 1491|       |  // If z is (x,±∞) (for any finite x), the result is (NaN,NaN) and FE_INVALID is raised.
 1492|       |  // If z is (x,NaN) (for any finite x), the result is (NaN,NaN) and FE_INVALID may be raised.
 1493|       |  if ((isfinite)(real_ref(x)) && !(isfinite)(imag_ref(x))) {
 1494|       |    return std::complex<RealScalar>(NumTraits<RealScalar>::quiet_NaN(), NumTraits<RealScalar>::quiet_NaN());
 1495|       |  }
 1496|       |  // If z is (+∞,±∞), the result is (±∞,NaN) and FE_INVALID is raised (the sign of the real part is unspecified)
 1497|       |  // If z is (+∞,NaN), the result is (±∞,NaN) (the sign of the real part is unspecified)
 1498|       |  if ((real_ref(x) == NumTraits<RealScalar>::infinity() && !(isfinite)(imag_ref(x)))) {
 1499|       |    return std::complex<RealScalar>(NumTraits<RealScalar>::infinity(), NumTraits<RealScalar>::quiet_NaN());
 1500|       |  }
 1501|       |  return exp2(x);
 1502|       |}
 1503|       |#endif
 1504|       |
 1505|       |#if defined(SYCL_DEVICE_ONLY)
 1506|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(exp2, exp2)
 1507|       |#endif
 1508|       |
 1509|       |#if defined(EIGEN_GPUCC)
 1510|       |template <>
 1511|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float exp2(const float& x) {
 1512|       |  return ::exp2f(x);
 1513|       |}
 1514|       |
 1515|       |template <>
 1516|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double exp2(const double& x) {
 1517|       |  return ::exp2(x);
 1518|       |}
 1519|       |
 1520|       |template <>
 1521|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE std::complex<float> exp2(const std::complex<float>& x) {
 1522|       |  float com = ::exp2f(x.real());
 1523|       |  float res_real = com * ::cosf(static_cast<float>(EIGEN_LN2) * x.imag());
 1524|       |  float res_imag = com * ::sinf(static_cast<float>(EIGEN_LN2) * x.imag());
 1525|       |  return std::complex<float>(res_real, res_imag);
 1526|       |}
 1527|       |
 1528|       |template <>
 1529|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE std::complex<double> exp2(const std::complex<double>& x) {
 1530|       |  double com = ::exp2(x.real());
 1531|       |  double res_real = com * ::cos(static_cast<double>(EIGEN_LN2) * x.imag());
 1532|       |  double res_imag = com * ::sin(static_cast<double>(EIGEN_LN2) * x.imag());
 1533|       |  return std::complex<double>(res_real, res_imag);
 1534|       |}
 1535|       |#endif
 1536|       |
 1537|       |template <typename Scalar>
 1538|      0|EIGEN_DEVICE_FUNC inline EIGEN_MATHFUNC_RETVAL(expm1, Scalar) expm1(const Scalar& x) {
 1539|      0|  return EIGEN_MATHFUNC_IMPL(expm1, Scalar)::run(x);
 1540|      0|}
 1541|       |
 1542|       |#if defined(SYCL_DEVICE_ONLY)
 1543|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(expm1, expm1)
 1544|       |#endif
 1545|       |
 1546|       |#if defined(EIGEN_GPUCC)
 1547|       |template <>
 1548|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float expm1(const float& x) {
 1549|       |  return ::expm1f(x);
 1550|       |}
 1551|       |
 1552|       |template <>
 1553|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double expm1(const double& x) {
 1554|       |  return ::expm1(x);
 1555|       |}
 1556|       |#endif
 1557|       |
 1558|       |template <typename T>
 1559|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T cos(const T& x) {
 1560|       |  EIGEN_USING_STD(cos);
 1561|       |  return cos(x);
 1562|       |}
 1563|       |
 1564|       |#if defined(SYCL_DEVICE_ONLY)
 1565|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(cos, cos)
 1566|       |#endif
 1567|       |
 1568|       |#if defined(EIGEN_GPUCC)
 1569|       |template <>
 1570|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float cos(const float& x) {
 1571|       |  return ::cosf(x);
 1572|       |}
 1573|       |
 1574|       |template <>
 1575|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double cos(const double& x) {
 1576|       |  return ::cos(x);
 1577|       |}
 1578|       |#endif
 1579|       |
 1580|       |template <typename T>
 1581|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T sin(const T& x) {
 1582|       |  EIGEN_USING_STD(sin);
 1583|       |  return sin(x);
 1584|       |}
 1585|       |
 1586|       |#if defined(SYCL_DEVICE_ONLY)
 1587|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(sin, sin)
 1588|       |#endif
 1589|       |
 1590|       |#if defined(EIGEN_GPUCC)
 1591|       |template <>
 1592|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float sin(const float& x) {
 1593|       |  return ::sinf(x);
 1594|       |}
 1595|       |
 1596|       |template <>
 1597|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double sin(const double& x) {
 1598|       |  return ::sin(x);
 1599|       |}
 1600|       |#endif
 1601|       |
 1602|       |template <typename T>
 1603|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T tan(const T& x) {
 1604|       |  EIGEN_USING_STD(tan);
 1605|       |  return tan(x);
 1606|       |}
 1607|       |
 1608|       |#if defined(SYCL_DEVICE_ONLY)
 1609|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(tan, tan)
 1610|       |#endif
 1611|       |
 1612|       |#if defined(EIGEN_GPUCC)
 1613|       |template <>
 1614|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float tan(const float& x) {
 1615|       |  return ::tanf(x);
 1616|       |}
 1617|       |
 1618|       |template <>
 1619|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double tan(const double& x) {
 1620|       |  return ::tan(x);
 1621|       |}
 1622|       |#endif
 1623|       |
 1624|       |template <typename T>
 1625|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T acos(const T& x) {
 1626|       |  EIGEN_USING_STD(acos);
 1627|       |  return acos(x);
 1628|       |}
 1629|       |
 1630|       |template <typename T>
 1631|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T acosh(const T& x) {
 1632|       |  EIGEN_USING_STD(acosh);
 1633|       |  return static_cast<T>(acosh(x));
 1634|       |}
 1635|       |
 1636|       |#if defined(SYCL_DEVICE_ONLY)
 1637|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(acos, acos)
 1638|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(acosh, acosh)
 1639|       |#endif
 1640|       |
 1641|       |#if defined(EIGEN_GPUCC)
 1642|       |template <>
 1643|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float acos(const float& x) {
 1644|       |  return ::acosf(x);
 1645|       |}
 1646|       |
 1647|       |template <>
 1648|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double acos(const double& x) {
 1649|       |  return ::acos(x);
 1650|       |}
 1651|       |#endif
 1652|       |
 1653|       |template <typename T>
 1654|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T asin(const T& x) {
 1655|       |  EIGEN_USING_STD(asin);
 1656|       |  return asin(x);
 1657|       |}
 1658|       |
 1659|       |template <typename T>
 1660|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T asinh(const T& x) {
 1661|       |  EIGEN_USING_STD(asinh);
 1662|       |  return static_cast<T>(asinh(x));
 1663|       |}
 1664|       |
 1665|       |#if defined(SYCL_DEVICE_ONLY)
 1666|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(asin, asin)
 1667|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(asinh, asinh)
 1668|       |#endif
 1669|       |
 1670|       |#if defined(EIGEN_GPUCC)
 1671|       |template <>
 1672|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float asin(const float& x) {
 1673|       |  return ::asinf(x);
 1674|       |}
 1675|       |
 1676|       |template <>
 1677|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double asin(const double& x) {
 1678|       |  return ::asin(x);
 1679|       |}
 1680|       |#endif
 1681|       |
 1682|       |template <typename T>
 1683|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T atan(const T& x) {
 1684|       |  EIGEN_USING_STD(atan);
 1685|       |  return static_cast<T>(atan(x));
 1686|       |}
 1687|       |
 1688|       |template <typename T, std::enable_if_t<!NumTraits<T>::IsComplex, int> = 0>
 1689|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T atan2(const T& y, const T& x) {
 1690|       |  EIGEN_USING_STD(atan2);
 1691|       |  return static_cast<T>(atan2(y, x));
 1692|       |}
 1693|       |
 1694|       |template <typename T>
 1695|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T atanh(const T& x) {
 1696|       |  EIGEN_USING_STD(atanh);
 1697|       |  return static_cast<T>(atanh(x));
 1698|       |}
 1699|       |
 1700|       |#if defined(SYCL_DEVICE_ONLY)
 1701|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(atan, atan)
 1702|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(atanh, atanh)
 1703|       |#endif
 1704|       |
 1705|       |#if defined(EIGEN_GPUCC)
 1706|       |template <>
 1707|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float atan(const float& x) {
 1708|       |  return ::atanf(x);
 1709|       |}
 1710|       |
 1711|       |template <>
 1712|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double atan(const double& x) {
 1713|       |  return ::atan(x);
 1714|       |}
 1715|       |#endif
 1716|       |
 1717|       |template <typename T>
 1718|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T cosh(const T& x) {
 1719|       |  EIGEN_USING_STD(cosh);
 1720|       |  return static_cast<T>(cosh(x));
 1721|       |}
 1722|       |
 1723|       |#if defined(SYCL_DEVICE_ONLY)
 1724|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(cosh, cosh)
 1725|       |#endif
 1726|       |
 1727|       |#if defined(EIGEN_GPUCC)
 1728|       |template <>
 1729|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float cosh(const float& x) {
 1730|       |  return ::coshf(x);
 1731|       |}
 1732|       |
 1733|       |template <>
 1734|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double cosh(const double& x) {
 1735|       |  return ::cosh(x);
 1736|       |}
 1737|       |#endif
 1738|       |
 1739|       |template <typename T>
 1740|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T sinh(const T& x) {
 1741|       |  EIGEN_USING_STD(sinh);
 1742|       |  return static_cast<T>(sinh(x));
 1743|       |}
 1744|       |
 1745|       |#if defined(SYCL_DEVICE_ONLY)
 1746|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(sinh, sinh)
 1747|       |#endif
 1748|       |
 1749|       |#if defined(EIGEN_GPUCC)
 1750|       |template <>
 1751|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float sinh(const float& x) {
 1752|       |  return ::sinhf(x);
 1753|       |}
 1754|       |
 1755|       |template <>
 1756|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double sinh(const double& x) {
 1757|       |  return ::sinh(x);
 1758|       |}
 1759|       |#endif
 1760|       |
 1761|       |template <typename T>
 1762|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T tanh(const T& x) {
 1763|       |  EIGEN_USING_STD(tanh);
 1764|       |  return tanh(x);
 1765|       |}
 1766|       |
 1767|       |#if (!defined(EIGEN_GPUCC)) && EIGEN_FAST_MATH && !defined(SYCL_DEVICE_ONLY)
 1768|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float tanh(float x) { return internal::ptanh_float(x); }
 1769|       |#endif
 1770|       |
 1771|       |#if defined(SYCL_DEVICE_ONLY)
 1772|       |SYCL_SPECIALIZE_FLOATING_TYPES_UNARY(tanh, tanh)
 1773|       |#endif
 1774|       |
 1775|       |#if defined(EIGEN_GPUCC)
 1776|       |template <>
 1777|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float tanh(const float& x) {
 1778|       |  return ::tanhf(x);
 1779|       |}
 1780|       |
 1781|       |template <>
 1782|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double tanh(const double& x) {
 1783|       |  return ::tanh(x);
 1784|       |}
 1785|       |#endif
 1786|       |
 1787|       |template <typename T>
 1788|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE T fmod(const T& a, const T& b) {
 1789|       |  EIGEN_USING_STD(fmod);
 1790|       |  return fmod(a, b);
 1791|       |}
 1792|       |
 1793|       |#if defined(SYCL_DEVICE_ONLY)
 1794|       |SYCL_SPECIALIZE_FLOATING_TYPES_BINARY(fmod, fmod)
 1795|       |#endif
 1796|       |
 1797|       |#if defined(EIGEN_GPUCC)
 1798|       |template <>
 1799|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float fmod(const float& a, const float& b) {
 1800|       |  return ::fmodf(a, b);
 1801|       |}
 1802|       |
 1803|       |template <>
 1804|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double fmod(const double& a, const double& b) {
 1805|       |  return ::fmod(a, b);
 1806|       |}
 1807|       |#endif
 1808|       |
 1809|       |#if defined(SYCL_DEVICE_ONLY)
 1810|       |#undef SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_BINARY
 1811|       |#undef SYCL_SPECIALIZE_SIGNED_INTEGER_TYPES_UNARY
 1812|       |#undef SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_BINARY
 1813|       |#undef SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_UNARY
 1814|       |#undef SYCL_SPECIALIZE_INTEGER_TYPES_BINARY
 1815|       |#undef SYCL_SPECIALIZE_UNSIGNED_INTEGER_TYPES_UNARY
 1816|       |#undef SYCL_SPECIALIZE_FLOATING_TYPES_BINARY
 1817|       |#undef SYCL_SPECIALIZE_FLOATING_TYPES_UNARY
 1818|       |#undef SYCL_SPECIALIZE_FLOATING_TYPES_UNARY_FUNC_RET_TYPE
 1819|       |#undef SYCL_SPECIALIZE_GEN_UNARY_FUNC
 1820|       |#undef SYCL_SPECIALIZE_UNARY_FUNC
 1821|       |#undef SYCL_SPECIALIZE_GEN1_BINARY_FUNC
 1822|       |#undef SYCL_SPECIALIZE_GEN2_BINARY_FUNC
 1823|       |#undef SYCL_SPECIALIZE_BINARY_FUNC
 1824|       |#endif
 1825|       |
 1826|       |template <typename Scalar, typename Enable = std::enable_if_t<std::is_integral<Scalar>::value>>
 1827|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar logical_shift_left(const Scalar& a, int n) {
 1828|       |  return a << n;
 1829|       |}
 1830|       |
 1831|       |template <typename Scalar, typename Enable = std::enable_if_t<std::is_integral<Scalar>::value>>
 1832|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar logical_shift_right(const Scalar& a, int n) {
 1833|       |  using UnsignedScalar = typename numext::get_integer_by_size<sizeof(Scalar)>::unsigned_type;
 1834|       |  return bit_cast<Scalar, UnsignedScalar>(bit_cast<UnsignedScalar, Scalar>(a) >> n);
 1835|       |}
 1836|       |
 1837|       |template <typename Scalar, typename Enable = std::enable_if_t<std::is_integral<Scalar>::value>>
 1838|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar arithmetic_shift_right(const Scalar& a, int n) {
 1839|       |  using SignedScalar = typename numext::get_integer_by_size<sizeof(Scalar)>::signed_type;
 1840|       |  return bit_cast<Scalar, SignedScalar>(bit_cast<SignedScalar, Scalar>(a) >> n);
 1841|       |}
 1842|       |
 1843|       |}  // end namespace numext
 1844|       |
 1845|       |namespace internal {
 1846|       |
 1847|       |template <typename T>
 1848|       |EIGEN_DEVICE_FUNC bool isfinite_impl(const std::complex<T>& x) {
 1849|       |  return (numext::isfinite)(numext::real(x)) && (numext::isfinite)(numext::imag(x));
 1850|       |}
 1851|       |
 1852|       |template <typename T>
 1853|       |EIGEN_DEVICE_FUNC bool isnan_impl(const std::complex<T>& x) {
 1854|       |  return (numext::isnan)(numext::real(x)) || (numext::isnan)(numext::imag(x));
 1855|       |}
 1856|       |
 1857|       |template <typename T>
 1858|       |EIGEN_DEVICE_FUNC bool isinf_impl(const std::complex<T>& x) {
 1859|       |  return ((numext::isinf)(numext::real(x)) || (numext::isinf)(numext::imag(x))) && (!(numext::isnan)(x));
 1860|       |}
 1861|       |
 1862|       |/****************************************************************************
 1863|       | * Implementation of fuzzy comparisons                                       *
 1864|       | ****************************************************************************/
 1865|       |
 1866|       |template <typename Scalar, bool IsComplex, bool IsInteger>
 1867|       |struct scalar_fuzzy_default_impl {};
 1868|       |
 1869|       |template <typename Scalar>
 1870|       |struct scalar_fuzzy_default_impl<Scalar, false, false> {
 1871|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
 1872|       |  template <typename OtherScalar>
 1873|       |  EIGEN_DEVICE_FUNC static inline bool isMuchSmallerThan(const Scalar& x, const OtherScalar& y,
 1874|      0|                                                         const RealScalar& prec) {
 1875|      0|    return numext::abs(x) <= numext::abs(y) * prec;
 1876|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIfLb0ELb0EE17isMuchSmallerThanIfEEbRKfRKT_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIdLb0ELb0EE17isMuchSmallerThanIdEEbRKdRKT_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implINS_4halfELb0ELb0EE17isMuchSmallerThanIS2_EEbRKS2_RKT_S6_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implINS_8bfloat16ELb0ELb0EE17isMuchSmallerThanIS2_EEbRKS2_RKT_S6_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIeLb0ELb0EE17isMuchSmallerThanIeEEbRKeRKT_S5_
  ------------------
 1877|      0|  EIGEN_DEVICE_FUNC static inline bool isApprox(const Scalar& x, const Scalar& y, const RealScalar& prec) {
 1878|      0|    return numext::abs(x - y) <= numext::mini(numext::abs(x), numext::abs(y)) * prec;
 1879|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIfLb0ELb0EE8isApproxERKfS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIdLb0ELb0EE8isApproxERKdS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implINS_4halfELb0ELb0EE8isApproxERKS2_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implINS_8bfloat16ELb0ELb0EE8isApproxERKS2_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIeLb0ELb0EE8isApproxERKeS4_S4_
  ------------------
 1880|      0|  EIGEN_DEVICE_FUNC static inline bool isApproxOrLessThan(const Scalar& x, const Scalar& y, const RealScalar& prec) {
 1881|      0|    return x <= y || isApprox(x, y, prec);
 1882|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIfLb0ELb0EE18isApproxOrLessThanERKfS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIdLb0ELb0EE18isApproxOrLessThanERKdS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implINS_4halfELb0ELb0EE18isApproxOrLessThanERKS2_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implINS_8bfloat16ELb0ELb0EE18isApproxOrLessThanERKS2_S5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIeLb0ELb0EE18isApproxOrLessThanERKeS4_S4_
  ------------------
 1883|       |};
 1884|       |
 1885|       |template <typename Scalar>
 1886|       |struct scalar_fuzzy_default_impl<Scalar, false, true> {
 1887|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
 1888|       |  template <typename OtherScalar>
 1889|      0|  EIGEN_DEVICE_FUNC static inline bool isMuchSmallerThan(const Scalar& x, const Scalar&, const RealScalar&) {
 1890|      0|    return x == Scalar(0);
 1891|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIsLb0ELb1EE17isMuchSmallerThanIsEEbRKsS5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implItLb0ELb1EE17isMuchSmallerThanItEEbRKtS5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIiLb0ELb1EE17isMuchSmallerThanIiEEbRKiS5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIjLb0ELb1EE17isMuchSmallerThanIjEEbRKjS5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIlLb0ELb1EE17isMuchSmallerThanIlEEbRKlS5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implImLb0ELb1EE17isMuchSmallerThanImEEbRKmS5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIxLb0ELb1EE17isMuchSmallerThanIxEEbRKxS5_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIyLb0ELb1EE17isMuchSmallerThanIyEEbRKyS5_S5_
  ------------------
 1892|      0|  EIGEN_DEVICE_FUNC static inline bool isApprox(const Scalar& x, const Scalar& y, const RealScalar&) { return x == y; }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIsLb0ELb1EE8isApproxERKsS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implItLb0ELb1EE8isApproxERKtS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIiLb0ELb1EE8isApproxERKiS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIjLb0ELb1EE8isApproxERKjS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIlLb0ELb1EE8isApproxERKlS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implImLb0ELb1EE8isApproxERKmS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIxLb0ELb1EE8isApproxERKxS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIyLb0ELb1EE8isApproxERKyS4_S4_
  ------------------
 1893|      0|  EIGEN_DEVICE_FUNC static inline bool isApproxOrLessThan(const Scalar& x, const Scalar& y, const RealScalar&) {
 1894|      0|    return x <= y;
 1895|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIsLb0ELb1EE18isApproxOrLessThanERKsS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implItLb0ELb1EE18isApproxOrLessThanERKtS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIiLb0ELb1EE18isApproxOrLessThanERKiS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIjLb0ELb1EE18isApproxOrLessThanERKjS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIlLb0ELb1EE18isApproxOrLessThanERKlS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implImLb0ELb1EE18isApproxOrLessThanERKmS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIxLb0ELb1EE18isApproxOrLessThanERKxS4_S4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implIyLb0ELb1EE18isApproxOrLessThanERKyS4_S4_
  ------------------
 1896|       |};
 1897|       |
 1898|       |template <typename Scalar>
 1899|       |struct scalar_fuzzy_default_impl<Scalar, true, false> {
 1900|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
 1901|       |  template <typename OtherScalar>
 1902|       |  EIGEN_DEVICE_FUNC static inline bool isMuchSmallerThan(const Scalar& x, const OtherScalar& y,
 1903|      0|                                                         const RealScalar& prec) {
 1904|      0|    return numext::abs2(x) <= numext::abs2(y) * prec * prec;
 1905|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implISt7complexIfELb1ELb0EE17isMuchSmallerThanIS3_EEbRKS3_RKT_RKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implISt7complexIdELb1ELb0EE17isMuchSmallerThanIS3_EEbRKS3_RKT_RKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implISt7complexIeELb1ELb0EE17isMuchSmallerThanIS3_EEbRKS3_RKT_RKe
  ------------------
 1906|      0|  EIGEN_DEVICE_FUNC static inline bool isApprox(const Scalar& x, const Scalar& y, const RealScalar& prec) {
 1907|      0|    return numext::abs2(x - y) <= numext::mini(numext::abs2(x), numext::abs2(y)) * prec * prec;
 1908|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implISt7complexIfELb1ELb0EE8isApproxERKS3_S6_RKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implISt7complexIdELb1ELb0EE8isApproxERKS3_S6_RKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25scalar_fuzzy_default_implISt7complexIeELb1ELb0EE8isApproxERKS3_S6_RKe
  ------------------
 1909|       |};
 1910|       |
 1911|       |template <typename Scalar>
 1912|       |struct scalar_fuzzy_impl
 1913|       |    : scalar_fuzzy_default_impl<Scalar, NumTraits<Scalar>::IsComplex, NumTraits<Scalar>::IsInteger> {};
 1914|       |
 1915|       |template <typename Scalar, typename OtherScalar>
 1916|       |EIGEN_DEVICE_FUNC inline bool isMuchSmallerThan(
 1917|       |    const Scalar& x, const OtherScalar& y,
 1918|      0|    const typename NumTraits<Scalar>::Real& precision = NumTraits<Scalar>::dummy_precision()) {
 1919|      0|  return scalar_fuzzy_impl<Scalar>::template isMuchSmallerThan<OtherScalar>(x, y, precision);
 1920|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIssEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIttEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIiiEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIjjEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIllEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanImmEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIxxEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIyyEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIffEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIddEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanINS_4halfES2_EEbRKT_RKT0_RKNS_9NumTraitsIS3_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanINS_8bfloat16ES2_EEbRKT_RKT0_RKNS_9NumTraitsIS3_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanISt7complexIfES3_EEbRKT_RKT0_RKNS_9NumTraitsIS4_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanISt7complexIdES3_EEbRKT_RKT0_RKNS_9NumTraitsIS4_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanISt7complexIeES3_EEbRKT_RKT0_RKNS_9NumTraitsIS4_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17isMuchSmallerThanIeeEEbRKT_RKT0_RKNS_9NumTraitsIS2_E4RealE
  ------------------
 1921|       |
 1922|       |template <typename Scalar>
 1923|       |EIGEN_DEVICE_FUNC inline bool isApprox(
 1924|       |    const Scalar& x, const Scalar& y,
 1925|      0|    const typename NumTraits<Scalar>::Real& precision = NumTraits<Scalar>::dummy_precision()) {
 1926|      0|  return scalar_fuzzy_impl<Scalar>::isApprox(x, y, precision);
 1927|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIsEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxItEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIiEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIjEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIlEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxImEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIxEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIyEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIfEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIdEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxINS_4halfEEEbRKT_S5_RKNS_9NumTraitsIS3_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxINS_8bfloat16EEEbRKT_S5_RKNS_9NumTraitsIS3_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxISt7complexIfEEEbRKT_S6_RKNS_9NumTraitsIS4_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxISt7complexIdEEEbRKT_S6_RKNS_9NumTraitsIS4_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxISt7complexIeEEEbRKT_S6_RKNS_9NumTraitsIS4_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal8isApproxIeEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
 1928|       |
 1929|       |template <typename Scalar>
 1930|       |EIGEN_DEVICE_FUNC inline bool isApproxOrLessThan(
 1931|       |    const Scalar& x, const Scalar& y,
 1932|      0|    const typename NumTraits<Scalar>::Real& precision = NumTraits<Scalar>::dummy_precision()) {
 1933|      0|  return scalar_fuzzy_impl<Scalar>::isApproxOrLessThan(x, y, precision);
 1934|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIsEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanItEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIiEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIjEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIlEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanImEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIxEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIyEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIfEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIdEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanINS_4halfEEEbRKT_S5_RKNS_9NumTraitsIS3_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanINS_8bfloat16EEEbRKT_S5_RKNS_9NumTraitsIS3_E4RealE
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18isApproxOrLessThanIeEEbRKT_S4_RKNS_9NumTraitsIS2_E4RealE
  ------------------
 1935|       |
 1936|       |/******************************************
 1937|       |***  The special case of the  bool type ***
 1938|       |******************************************/
 1939|       |
 1940|       |template <>
 1941|       |struct scalar_fuzzy_impl<bool> {
 1942|       |  typedef bool RealScalar;
 1943|       |
 1944|       |  template <typename OtherScalar>
 1945|       |  EIGEN_DEVICE_FUNC static inline bool isMuchSmallerThan(const bool& x, const bool&, const bool&) {
 1946|       |    return !x;
 1947|       |  }
 1948|       |
 1949|      0|  EIGEN_DEVICE_FUNC static inline bool isApprox(bool x, bool y, bool) { return x == y; }
 1950|       |
 1951|      0|  EIGEN_DEVICE_FUNC static inline bool isApproxOrLessThan(const bool& x, const bool& y, const bool&) {
 1952|      0|    return (!x) || y;
 1953|      0|  }
 1954|       |};
 1955|       |
 1956|       |}  // end namespace internal
 1957|       |
 1958|       |// Default implementations that rely on other numext implementations
 1959|       |namespace internal {
 1960|       |
 1961|       |// Specialization for complex types that are not supported by std::expm1.
 1962|       |template <typename RealScalar>
 1963|       |struct expm1_impl<std::complex<RealScalar>> {
 1964|       |  EIGEN_STATIC_ASSERT_NON_INTEGER(RealScalar)
 1965|       |
 1966|       |  EIGEN_DEVICE_FUNC static inline std::complex<RealScalar> run(const std::complex<RealScalar>& x) {
 1967|       |    RealScalar xr = x.real();
 1968|       |    RealScalar xi = x.imag();
 1969|       |    // expm1(z) = exp(z) - 1
 1970|       |    //          = exp(x +  i * y) - 1
 1971|       |    //          = exp(x) * (cos(y) + i * sin(y)) - 1
 1972|       |    //          = exp(x) * cos(y) - 1 + i * exp(x) * sin(y)
 1973|       |    // Imag(expm1(z)) = exp(x) * sin(y)
 1974|       |    // Real(expm1(z)) = exp(x) * cos(y) - 1
 1975|       |    //          = exp(x) * cos(y) - 1.
 1976|       |    //          = expm1(x) + exp(x) * (cos(y) - 1)
 1977|       |    //          = expm1(x) + exp(x) * (2 * sin(y / 2) ** 2)
 1978|       |    RealScalar erm1 = numext::expm1<RealScalar>(xr);
 1979|       |    RealScalar er = erm1 + RealScalar(1.);
 1980|       |    RealScalar sin2 = numext::sin(xi / RealScalar(2.));
 1981|       |    sin2 = sin2 * sin2;
 1982|       |    RealScalar s = numext::sin(xi);
 1983|       |    RealScalar real_part = erm1 - RealScalar(2.) * er * sin2;
 1984|       |    return std::complex<RealScalar>(real_part, er * s);
 1985|       |  }
 1986|       |};
 1987|       |
 1988|       |template <typename T>
 1989|       |struct rsqrt_impl {
 1990|       |  EIGEN_DEVICE_FUNC static EIGEN_ALWAYS_INLINE T run(const T& x) { return T(1) / numext::sqrt(x); }
 1991|       |};
 1992|       |
 1993|       |#if defined(EIGEN_GPU_COMPILE_PHASE)
 1994|       |template <typename T>
 1995|       |struct conj_impl<std::complex<T>, true> {
 1996|       |  EIGEN_DEVICE_FUNC static inline std::complex<T> run(const std::complex<T>& x) {
 1997|       |    return std::complex<T>(numext::real(x), -numext::imag(x));
 1998|       |  }
 1999|       |};
 2000|       |#endif
 2001|       |
 2002|       |}  // end namespace internal
 2003|       |
 2004|       |}  // end namespace Eigen
 2005|       |
 2006|       |#endif  // EIGEN_MATHFUNCTIONS_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/MathFunctionsImpl.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2014 Pedro Gonnet (pedro.gonnet@gmail.com)
    5|       |// Copyright (C) 2016 Gael Guennebaud <gael.guennebaud@inria.fr>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_MATHFUNCTIONSIMPL_H
   12|       |#define EIGEN_MATHFUNCTIONSIMPL_H
   13|       |
   14|       |// IWYU pragma: private
   15|       |#include "./InternalHeaderCheck.h"
   16|       |
   17|       |namespace Eigen {
   18|       |
   19|       |namespace internal {
   20|       |
   21|       |/** \internal Fast reciprocal using Newton-Raphson's method.
   22|       |
   23|       | Preconditions:
   24|       |   1. The starting guess provided in approx_a_recip must have at least half
   25|       |      the leading mantissa bits in the correct result, such that a single
   26|       |      Newton-Raphson step is sufficient to get within 1-2 ulps of the correct
   27|       |      result.
   28|       |   2. If a is zero, approx_a_recip must be infinite with the same sign as a.
   29|       |   3. If a is infinite, approx_a_recip must be zero with the same sign as a.
   30|       |
   31|       |   If the preconditions are satisfied, which they are for for the _*_rcp_ps
   32|       |   instructions on x86, the result has a maximum relative error of 2 ulps,
   33|       |   and correctly handles reciprocals of zero, infinity, and NaN.
   34|       |*/
   35|       |template <typename Packet, int Steps>
   36|       |struct generic_reciprocal_newton_step {
   37|       |  static_assert(Steps > 0, "Steps must be at least 1.");
   38|       |  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Packet run(const Packet& a, const Packet& approx_a_recip) {
   39|       |    using Scalar = typename unpacket_traits<Packet>::type;
   40|       |    const Packet two = pset1<Packet>(Scalar(2));
   41|       |    // Refine the approximation using one Newton-Raphson step:
   42|       |    //   x_{i} = x_{i-1} * (2 - a * x_{i-1})
   43|       |    const Packet x = generic_reciprocal_newton_step<Packet, Steps - 1>::run(a, approx_a_recip);
   44|       |    const Packet tmp = pnmadd(a, x, two);
   45|       |    // If tmp is NaN, it means that a is either +/-0 or +/-Inf.
   46|       |    // In this case return the approximation directly.
   47|       |    const Packet is_not_nan = pcmp_eq(tmp, tmp);
   48|       |    return pselect(is_not_nan, pmul(x, tmp), x);
   49|       |  }
   50|       |};
   51|       |
   52|       |template <typename Packet>
   53|       |struct generic_reciprocal_newton_step<Packet, 0> {
   54|       |  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Packet run(const Packet& /*unused*/, const Packet& approx_rsqrt) {
   55|       |    return approx_rsqrt;
   56|       |  }
   57|       |};
   58|       |
   59|       |/** \internal Fast reciprocal sqrt using Newton-Raphson's method.
   60|       |
   61|       | Preconditions:
   62|       |   1. The starting guess provided in approx_a_recip must have at least half
   63|       |      the leading mantissa bits in the correct result, such that a single
   64|       |      Newton-Raphson step is sufficient to get within 1-2 ulps of the correct
   65|       |      result.
   66|       |   2. If a is zero, approx_a_recip must be infinite with the same sign as a.
   67|       |   3. If a is infinite, approx_a_recip must be zero with the same sign as a.
   68|       |
   69|       |   If the preconditions are satisfied, which they are for for the _*_rcp_ps
   70|       |   instructions on x86, the result has a maximum relative error of 2 ulps,
   71|       |   and correctly handles zero, infinity, and NaN. Positive denormals are
   72|       |   treated as zero.
   73|       |*/
   74|       |template <typename Packet, int Steps>
   75|       |struct generic_rsqrt_newton_step {
   76|       |  static_assert(Steps > 0, "Steps must be at least 1.");
   77|       |  using Scalar = typename unpacket_traits<Packet>::type;
   78|      0|  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Packet run(const Packet& a, const Packet& approx_rsqrt) {
   79|      0|    constexpr Scalar kMinusHalf = Scalar(-1) / Scalar(2);
   80|      0|    const Packet cst_minus_half = pset1<Packet>(kMinusHalf);
   81|      0|    const Packet cst_minus_one = pset1<Packet>(Scalar(-1));
   82|      0|
   83|      0|    Packet inv_sqrt = approx_rsqrt;
   84|      0|    for (int step = 0; step < Steps; ++step) {
   85|      0|      // Refine the approximation using one Newton-Raphson step:
   86|      0|      // h_n = (x * inv_sqrt) * inv_sqrt - 1 (so that h_n is nearly 0).
   87|      0|      // inv_sqrt = inv_sqrt - 0.5 * inv_sqrt * h_n
   88|      0|      Packet r2 = pmul(a, inv_sqrt);
   89|      0|      Packet half_r = pmul(inv_sqrt, cst_minus_half);
   90|      0|      Packet h_n = pmadd(r2, inv_sqrt, cst_minus_one);
   91|      0|      inv_sqrt = pmadd(half_r, h_n, inv_sqrt);
   92|      0|    }
   93|      0|
   94|      0|    // If x is NaN, then either:
   95|      0|    // 1) the input is NaN
   96|      0|    // 2) zero and infinity were multiplied
   97|      0|    // In either of these cases, return approx_rsqrt
   98|      0|    return pselect(pisnan(inv_sqrt), approx_rsqrt, inv_sqrt);
   99|      0|  }
  100|       |};
  101|       |
  102|       |template <typename Packet>
  103|       |struct generic_rsqrt_newton_step<Packet, 0> {
  104|       |  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Packet run(const Packet& /*unused*/, const Packet& approx_rsqrt) {
  105|       |    return approx_rsqrt;
  106|       |  }
  107|       |};
  108|       |
  109|       |/** \internal Fast sqrt using Newton-Raphson's method.
  110|       |
  111|       | Preconditions:
  112|       |   1. The starting guess for the reciprocal sqrt provided in approx_rsqrt must
  113|       |      have at least half the leading mantissa bits in the correct result, such
  114|       |      that a single Newton-Raphson step is sufficient to get within 1-2 ulps of
  115|       |      the correct result.
  116|       |   2. If a is zero, approx_rsqrt must be infinite.
  117|       |   3. If a is infinite, approx_rsqrt must be zero.
  118|       |
  119|       |   If the preconditions are satisfied, which they are for for the _*_rsqrt_ps
  120|       |   instructions on x86, the result has a maximum relative error of 2 ulps,
  121|       |   and correctly handles zero and infinity, and NaN. Positive denormal inputs
  122|       |   are treated as zero.
  123|       |*/
  124|       |template <typename Packet, int Steps = 1>
  125|       |struct generic_sqrt_newton_step {
  126|       |  static_assert(Steps > 0, "Steps must be at least 1.");
  127|       |
  128|       |  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Packet run(const Packet& a, const Packet& approx_rsqrt) {
  129|       |    using Scalar = typename unpacket_traits<Packet>::type;
  130|       |    const Packet one_point_five = pset1<Packet>(Scalar(1.5));
  131|       |    const Packet minus_half = pset1<Packet>(Scalar(-0.5));
  132|       |    // If a is inf or zero, return a directly.
  133|       |    const Packet inf_mask = pcmp_eq(a, pset1<Packet>(NumTraits<Scalar>::infinity()));
  134|       |    const Packet return_a = por(pcmp_eq(a, pzero(a)), inf_mask);
  135|       |    // Do a single step of Newton's iteration for reciprocal square root:
  136|       |    //   x_{n+1} = x_n * (1.5 + (-0.5 * x_n) * (a * x_n))).
  137|       |    // The Newton's step is computed this way to avoid over/under-flows.
  138|       |    Packet rsqrt = pmul(approx_rsqrt, pmadd(pmul(minus_half, approx_rsqrt), pmul(a, approx_rsqrt), one_point_five));
  139|       |    for (int step = 1; step < Steps; ++step) {
  140|       |      rsqrt = pmul(rsqrt, pmadd(pmul(minus_half, rsqrt), pmul(a, rsqrt), one_point_five));
  141|       |    }
  142|       |
  143|       |    // Return sqrt(x) = x * rsqrt(x) for non-zero finite positive arguments.
  144|       |    // Return a itself for 0 or +inf, NaN for negative arguments.
  145|       |    return pselect(return_a, a, pmul(a, rsqrt));
  146|       |  }
  147|       |};
  148|       |
  149|       |template <typename RealScalar>
  150|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE RealScalar positive_real_hypot(const RealScalar& x, const RealScalar& y) {
  151|       |  // IEEE IEC 6059 special cases.
  152|       |  if ((numext::isinf)(x) || (numext::isinf)(y)) return NumTraits<RealScalar>::infinity();
  153|       |  if ((numext::isnan)(x) || (numext::isnan)(y)) return NumTraits<RealScalar>::quiet_NaN();
  154|       |
  155|       |  EIGEN_USING_STD(sqrt);
  156|       |  RealScalar p, qp;
  157|       |  p = numext::maxi(x, y);
  158|       |  if (numext::is_exactly_zero(p)) return RealScalar(0);
  159|       |  qp = numext::mini(y, x) / p;
  160|       |  return p * sqrt(RealScalar(1) + qp * qp);
  161|       |}
  162|       |
  163|       |template <typename Scalar>
  164|       |struct hypot_impl {
  165|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  166|       |  static EIGEN_DEVICE_FUNC inline RealScalar run(const Scalar& x, const Scalar& y) {
  167|       |    EIGEN_USING_STD(abs);
  168|       |    return positive_real_hypot<RealScalar>(abs(x), abs(y));
  169|       |  }
  170|       |};
  171|       |
  172|       |// Generic complex sqrt implementation that correctly handles corner cases
  173|       |// according to https://en.cppreference.com/w/cpp/numeric/complex/sqrt
  174|       |template <typename T>
  175|       |EIGEN_DEVICE_FUNC std::complex<T> complex_sqrt(const std::complex<T>& z) {
  176|       |  // Computes the principal sqrt of the input.
  177|       |  //
  178|       |  // For a complex square root of the number x + i*y. We want to find real
  179|       |  // numbers u and v such that
  180|       |  //    (u + i*v)^2 = x + i*y  <=>
  181|       |  //    u^2 - v^2 + i*2*u*v = x + i*v.
  182|       |  // By equating the real and imaginary parts we get:
  183|       |  //    u^2 - v^2 = x
  184|       |  //    2*u*v = y.
  185|       |  //
  186|       |  // For x >= 0, this has the numerically stable solution
  187|       |  //    u = sqrt(0.5 * (x + sqrt(x^2 + y^2)))
  188|       |  //    v = y / (2 * u)
  189|       |  // and for x < 0,
  190|       |  //    v = sign(y) * sqrt(0.5 * (-x + sqrt(x^2 + y^2)))
  191|       |  //    u = y / (2 * v)
  192|       |  //
  193|       |  // Letting w = sqrt(0.5 * (|x| + |z|)),
  194|       |  //   if x == 0: u = w, v = sign(y) * w
  195|       |  //   if x > 0:  u = w, v = y / (2 * w)
  196|       |  //   if x < 0:  u = |y| / (2 * w), v = sign(y) * w
  197|       |
  198|       |  const T x = numext::real(z);
  199|       |  const T y = numext::imag(z);
  200|       |  const T zero = T(0);
  201|       |  const T w = numext::sqrt(T(0.5) * (numext::abs(x) + numext::hypot(x, y)));
  202|       |
  203|       |  return (numext::isinf)(y)           ? std::complex<T>(NumTraits<T>::infinity(), y)
  204|       |         : numext::is_exactly_zero(x) ? std::complex<T>(w, y < zero ? -w : w)
  205|       |         : x > zero                   ? std::complex<T>(w, y / (2 * w))
  206|       |                                      : std::complex<T>(numext::abs(y) / (2 * w), y < zero ? -w : w);
  207|       |}
  208|       |
  209|       |// Generic complex rsqrt implementation.
  210|       |template <typename T>
  211|       |EIGEN_DEVICE_FUNC std::complex<T> complex_rsqrt(const std::complex<T>& z) {
  212|       |  // Computes the principal reciprocal sqrt of the input.
  213|       |  //
  214|       |  // For a complex reciprocal square root of the number z = x + i*y. We want to
  215|       |  // find real numbers u and v such that
  216|       |  //    (u + i*v)^2 = 1 / (x + i*y)  <=>
  217|       |  //    u^2 - v^2 + i*2*u*v = x/|z|^2 - i*v/|z|^2.
  218|       |  // By equating the real and imaginary parts we get:
  219|       |  //    u^2 - v^2 = x/|z|^2
  220|       |  //    2*u*v = y/|z|^2.
  221|       |  //
  222|       |  // For x >= 0, this has the numerically stable solution
  223|       |  //    u = sqrt(0.5 * (x + |z|)) / |z|
  224|       |  //    v = -y / (2 * u * |z|)
  225|       |  // and for x < 0,
  226|       |  //    v = -sign(y) * sqrt(0.5 * (-x + |z|)) / |z|
  227|       |  //    u = -y / (2 * v * |z|)
  228|       |  //
  229|       |  // Letting w = sqrt(0.5 * (|x| + |z|)),
  230|       |  //   if x == 0: u = w / |z|, v = -sign(y) * w / |z|
  231|       |  //   if x > 0:  u = w / |z|, v = -y / (2 * w * |z|)
  232|       |  //   if x < 0:  u = |y| / (2 * w * |z|), v = -sign(y) * w / |z|
  233|       |
  234|       |  const T x = numext::real(z);
  235|       |  const T y = numext::imag(z);
  236|       |  const T zero = T(0);
  237|       |
  238|       |  const T abs_z = numext::hypot(x, y);
  239|       |  const T w = numext::sqrt(T(0.5) * (numext::abs(x) + abs_z));
  240|       |  const T woz = w / abs_z;
  241|       |  // Corner cases consistent with 1/sqrt(z) on gcc/clang.
  242|       |  return numext::is_exactly_zero(abs_z) ? std::complex<T>(NumTraits<T>::infinity(), NumTraits<T>::quiet_NaN())
  243|       |         : ((numext::isinf)(x) || (numext::isinf)(y)) ? std::complex<T>(zero, zero)
  244|       |         : numext::is_exactly_zero(x)                 ? std::complex<T>(woz, y < zero ? woz : -woz)
  245|       |         : x > zero                                   ? std::complex<T>(woz, -y / (2 * w * abs_z))
  246|       |                    : std::complex<T>(numext::abs(y) / (2 * w * abs_z), y < zero ? woz : -woz);
  247|       |}
  248|       |
  249|       |template <typename T>
  250|       |EIGEN_DEVICE_FUNC std::complex<T> complex_log(const std::complex<T>& z) {
  251|       |  // Computes complex log.
  252|       |  T a = numext::abs(z);
  253|       |  EIGEN_USING_STD(atan2);
  254|       |  T b = atan2(z.imag(), z.real());
  255|       |  return std::complex<T>(numext::log(a), b);
  256|       |}
  257|       |
  258|       |}  // end namespace internal
  259|       |
  260|       |}  // end namespace Eigen
  261|       |
  262|       |#endif  // EIGEN_MATHFUNCTIONSIMPL_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/NumTraits.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2006-2010 Benoit Jacob <jacob.benoit.1@gmail.com>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_NUMTRAITS_H
   11|       |#define EIGEN_NUMTRAITS_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "./InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |// default implementation of digits(), based on numeric_limits if specialized,
   21|       |// 0 for integer types, and log2(epsilon()) otherwise.
   22|       |template <typename T, bool use_numeric_limits = std::numeric_limits<T>::is_specialized,
   23|       |          bool is_integer = NumTraits<T>::IsInteger>
   24|       |struct default_digits_impl {
   25|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() { return std::numeric_limits<T>::digits; }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal19default_digits_implIdLb1ELb0EE3runEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal19default_digits_implIeLb1ELb0EE3runEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal19default_digits_implIfLb1ELb0EE3runEv
  ------------------
   26|       |};
   27|       |
   28|       |template <typename T>
   29|       |struct default_digits_impl<T, false, false>  // Floating point
   30|       |{
   31|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() {
   32|       |    using std::ceil;
   33|       |    using std::log2;
   34|       |    typedef typename NumTraits<T>::Real Real;
   35|       |    return int(ceil(-log2(NumTraits<Real>::epsilon())));
   36|       |  }
   37|       |};
   38|       |
   39|       |template <typename T>
   40|       |struct default_digits_impl<T, false, true>  // Integer
   41|       |{
   42|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() { return 0; }
   43|       |};
   44|       |
   45|       |// default implementation of digits10(), based on numeric_limits if specialized,
   46|       |// 0 for integer types, and floor((digits()-1)*log10(2)) otherwise.
   47|       |template <typename T, bool use_numeric_limits = std::numeric_limits<T>::is_specialized,
   48|       |          bool is_integer = NumTraits<T>::IsInteger>
   49|       |struct default_digits10_impl {
   50|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() { return std::numeric_limits<T>::digits10; }
   51|       |};
   52|       |
   53|       |template <typename T>
   54|       |struct default_digits10_impl<T, false, false>  // Floating point
   55|       |{
   56|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() {
   57|       |    using std::floor;
   58|       |    using std::log10;
   59|       |    typedef typename NumTraits<T>::Real Real;
   60|       |    return int(floor((internal::default_digits_impl<Real>::run() - 1) * log10(2)));
   61|       |  }
   62|       |};
   63|       |
   64|       |template <typename T>
   65|       |struct default_digits10_impl<T, false, true>  // Integer
   66|       |{
   67|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() { return 0; }
   68|       |};
   69|       |
   70|       |// default implementation of max_digits10(), based on numeric_limits if specialized,
   71|       |// 0 for integer types, and log10(2) * digits() + 1 otherwise.
   72|       |template <typename T, bool use_numeric_limits = std::numeric_limits<T>::is_specialized,
   73|       |          bool is_integer = NumTraits<T>::IsInteger>
   74|       |struct default_max_digits10_impl {
   75|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() { return std::numeric_limits<T>::max_digits10; }
   76|       |};
   77|       |
   78|       |template <typename T>
   79|       |struct default_max_digits10_impl<T, false, false>  // Floating point
   80|       |{
   81|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() {
   82|       |    using std::ceil;
   83|       |    using std::log10;
   84|       |    typedef typename NumTraits<T>::Real Real;
   85|       |    return int(ceil(internal::default_digits_impl<Real>::run() * log10(2) + 1));
   86|       |  }
   87|       |};
   88|       |
   89|       |template <typename T>
   90|       |struct default_max_digits10_impl<T, false, true>  // Integer
   91|       |{
   92|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static int run() { return 0; }
   93|       |};
   94|       |
   95|       |}  // end namespace internal
   96|       |
   97|       |namespace numext {
   98|       |/** \internal bit-wise cast without changing the underlying bit representation. */
   99|       |
  100|       |// TODO: Replace by std::bit_cast (available in C++20)
  101|       |template <typename Tgt, typename Src>
  102|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Tgt bit_cast(const Src& src) {
  103|      0|  // The behaviour of memcpy is not specified for non-trivially copyable types
  104|      0|  EIGEN_STATIC_ASSERT(std::is_trivially_copyable<Src>::value, THIS_TYPE_IS_NOT_SUPPORTED)
  105|      0|  EIGEN_STATIC_ASSERT(std::is_trivially_copyable<Tgt>::value && std::is_default_constructible<Tgt>::value,
  106|      0|                      THIS_TYPE_IS_NOT_SUPPORTED)
  107|      0|  EIGEN_STATIC_ASSERT(sizeof(Src) == sizeof(Tgt), THIS_TYPE_IS_NOT_SUPPORTED)
  108|      0|
  109|      0|  Tgt tgt;
  110|      0|  // Load src into registers first. This allows the memcpy to be elided by CUDA.
  111|      0|  const Src staged = src;
  112|      0|  EIGEN_USING_STD(memcpy)
  113|      0|  memcpy(static_cast<void*>(&tgt), static_cast<const void*>(&staged), sizeof(Tgt));
  114|      0|  return tgt;
  115|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8bit_castImdEET_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8bit_castIdmEET_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8bit_castIjfEET_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8bit_castIfjEET_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8bit_castIijEET_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext8bit_castIjiEET_RKT0_
  ------------------
  116|       |}  // namespace numext
  117|       |
  118|       |/** \class NumTraits
  119|       | * \ingroup Core_Module
  120|       | *
  121|       | * \brief Holds information about the various numeric (i.e. scalar) types allowed by Eigen.
  122|       | *
  123|       | * \tparam T the numeric type at hand
  124|       | *
  125|       | * This class stores enums, typedefs and static methods giving information about a numeric type.
  126|       | *
  127|       | * The provided data consists of:
  128|       | * \li A typedef \c Real, giving the "real part" type of \a T. If \a T is already real,
  129|       | *     then \c Real is just a typedef to \a T. If \a T is \c std::complex<U> then \c Real
  130|       | *     is a typedef to \a U.
  131|       | * \li A typedef \c NonInteger, giving the type that should be used for operations producing non-integral values,
  132|       | *     such as quotients, square roots, etc. If \a T is a floating-point type, then this typedef just gives
  133|       | *     \a T again. Note however that many Eigen functions such as internal::sqrt simply refuse to
  134|       | *     take integers. Outside of a few cases, Eigen doesn't do automatic type promotion. Thus, this typedef is
  135|       | *     only intended as a helper for code that needs to explicitly promote types.
  136|       | * \li A typedef \c Literal giving the type to use for numeric literals such as "2" or "0.5". For instance, for \c
  137|       | * std::complex<U>, Literal is defined as \c U. Of course, this type must be fully compatible with \a T. In doubt, just
  138|       | * use \a T here. \li A typedef \a Nested giving the type to use to nest a value inside of the expression tree. If you
  139|       | * don't know what this means, just use \a T here. \li An enum value \a IsComplex. It is equal to 1 if \a T is a \c
  140|       | * std::complex type, and to 0 otherwise. \li An enum value \a IsInteger. It is equal to \c 1 if \a T is an integer type
  141|       | * such as \c int, and to \c 0 otherwise. \li Enum values ReadCost, AddCost and MulCost representing a rough estimate of
  142|       | * the number of CPU cycles needed to by move / add / mul instructions respectively, assuming the data is already stored
  143|       | * in CPU registers. Stay vague here. No need to do architecture-specific stuff. If you don't know what this means, just
  144|       | * use \c Eigen::HugeCost. \li An enum value \a IsSigned. It is equal to \c 1 if \a T is a signed type and to 0 if \a T
  145|       | * is unsigned. \li An enum value \a RequireInitialization. It is equal to \c 1 if the constructor of the numeric type
  146|       | * \a T must be called, and to 0 if it is safe not to call it. Default is 0 if \a T is an arithmetic type, and 1
  147|       | * otherwise. \li An epsilon() function which, unlike <a
  148|       | * href="http://en.cppreference.com/w/cpp/types/numeric_limits/epsilon">std::numeric_limits::epsilon()</a>, it returns a
  149|       | * \a Real instead of a \a T. \li A dummy_precision() function returning a weak epsilon value. It is mainly used as a
  150|       | * default value by the fuzzy comparison operators. \li highest() and lowest() functions returning the highest and
  151|       | * lowest possible values respectively. \li digits() function returning the number of radix digits (non-sign digits for
  152|       | * integers, mantissa for floating-point). This is the analogue of <a
  153|       | * href="http://en.cppreference.com/w/cpp/types/numeric_limits/digits">std::numeric_limits<T>::digits</a> which is used
  154|       | * as the default implementation if specialized. \li digits10() function returning the number of decimal digits that can
  155|       | * be represented without change. This is the analogue of <a
  156|       | * href="http://en.cppreference.com/w/cpp/types/numeric_limits/digits10">std::numeric_limits<T>::digits10</a> which is
  157|       | * used as the default implementation if specialized. \li max_digits10() function returning the number of decimal digits
  158|       | * required to uniquely represent all distinct values of the type. This is the analogue of <a
  159|       | * href="http://en.cppreference.com/w/cpp/types/numeric_limits/max_digits10">std::numeric_limits<T>::max_digits10</a>
  160|       | *     which is used as the default implementation if specialized.
  161|       | * \li min_exponent() and max_exponent() functions returning the highest and lowest possible values, respectively,
  162|       | *     such that the radix raised to the power exponent-1 is a normalized floating-point number.  These are equivalent
  163|       | * to <a
  164|       | * href="http://en.cppreference.com/w/cpp/types/numeric_limits/min_exponent">std::numeric_limits<T>::min_exponent</a>/
  165|       | *     <a
  166|       | * href="http://en.cppreference.com/w/cpp/types/numeric_limits/max_exponent">std::numeric_limits<T>::max_exponent</a>.
  167|       | * \li infinity() function returning a representation of positive infinity, if available.
  168|       | * \li quiet_NaN function returning a non-signaling "not-a-number", if available.
  169|       | */
  170|       |
  171|       |template <typename T>
  172|       |struct GenericNumTraits {
  173|       |  enum {
  174|       |    IsInteger = std::numeric_limits<T>::is_integer,
  175|       |    IsSigned = std::numeric_limits<T>::is_signed,
  176|       |    IsComplex = 0,
  177|       |    RequireInitialization = internal::is_arithmetic<T>::value ? 0 : 1,
  178|       |    ReadCost = 1,
  179|       |    AddCost = 1,
  180|       |    MulCost = 1
  181|       |  };
  182|       |
  183|       |  typedef T Real;
  184|       |  typedef std::conditional_t<IsInteger, std::conditional_t<sizeof(T) <= 2, float, double>, T> NonInteger;
  185|       |  typedef T Nested;
  186|       |  typedef T Literal;
  187|       |
  188|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline Real epsilon() { return numext::numeric_limits<T>::epsilon(); }
  189|       |
  190|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline int digits10() { return internal::default_digits10_impl<T>::run(); }
  191|       |
  192|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline int max_digits10() {
  193|       |    return internal::default_max_digits10_impl<T>::run();
  194|       |  }
  195|       |
  196|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline int digits() { return internal::default_digits_impl<T>::run(); }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIdE6digitsEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIeE6digitsEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIfE6digitsEv
  ------------------
  197|       |
  198|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline int min_exponent() { return numext::numeric_limits<T>::min_exponent; }
  199|       |
  200|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline int max_exponent() { return numext::numeric_limits<T>::max_exponent; }
  201|       |
  202|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline Real dummy_precision() {
  203|      0|    // make sure to override this for floating-point types
  204|      0|    return Real(0);
  205|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIsE15dummy_precisionEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsItE15dummy_precisionEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIiE15dummy_precisionEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIjE15dummy_precisionEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIlE15dummy_precisionEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsImE15dummy_precisionEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIxE15dummy_precisionEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIyE15dummy_precisionEv
  ------------------
  206|       |
  207|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline T highest() { return (numext::numeric_limits<T>::max)(); }
  208|       |
  209|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline T lowest() { return (numext::numeric_limits<T>::lowest)(); }
  210|       |
  211|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline T infinity() { return numext::numeric_limits<T>::infinity(); }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIdE8infinityEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen16GenericNumTraitsIfE8infinityEv
  ------------------
  212|       |
  213|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline T quiet_NaN() { return numext::numeric_limits<T>::quiet_NaN(); }
  214|       |};
  215|       |
  216|       |template <typename T>
  217|       |struct NumTraits : GenericNumTraits<T> {};
  218|       |
  219|       |template <>
  220|       |struct NumTraits<float> : GenericNumTraits<float> {
  221|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline float dummy_precision() { return 1e-5f; }
  222|       |};
  223|       |
  224|       |template <>
  225|       |struct NumTraits<double> : GenericNumTraits<double> {
  226|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline double dummy_precision() { return 1e-12; }
  227|       |};
  228|       |
  229|       |// GPU devices treat `long double` as `double`.
  230|       |#ifndef EIGEN_GPU_COMPILE_PHASE
  231|       |template <>
  232|       |struct NumTraits<long double> : GenericNumTraits<long double> {
  233|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline long double dummy_precision() {
  234|      0|    return static_cast<long double>(1e-15l);
  235|      0|  }
  236|       |
  237|       |#if defined(EIGEN_ARCH_PPC) && (__LDBL_MANT_DIG__ == 106)
  238|       |  // PowerPC double double causes issues with some values
  239|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline long double epsilon() {
  240|       |    // 2^(-(__LDBL_MANT_DIG__)+1)
  241|       |    return static_cast<long double>(2.4651903288156618919116517665087e-32l);
  242|       |  }
  243|       |#endif
  244|       |};
  245|       |#endif
  246|       |
  247|       |template <typename Real_>
  248|       |struct NumTraits<std::complex<Real_> > : GenericNumTraits<std::complex<Real_> > {
  249|       |  typedef Real_ Real;
  250|       |  typedef typename NumTraits<Real_>::Literal Literal;
  251|       |  enum {
  252|       |    IsComplex = 1,
  253|       |    IsSigned = NumTraits<Real_>::IsSigned,
  254|       |    RequireInitialization = NumTraits<Real_>::RequireInitialization,
  255|       |    ReadCost = 2 * NumTraits<Real_>::ReadCost,
  256|       |    AddCost = 2 * NumTraits<Real>::AddCost,
  257|       |    MulCost = 4 * NumTraits<Real>::MulCost + 2 * NumTraits<Real>::AddCost
  258|       |  };
  259|       |
  260|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline Real epsilon() { return NumTraits<Real>::epsilon(); }
  261|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline Real dummy_precision() { return NumTraits<Real>::dummy_precision(); }
  262|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline int digits10() { return NumTraits<Real>::digits10(); }
  263|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline int max_digits10() { return NumTraits<Real>::max_digits10(); }
  264|       |};
  265|       |
  266|       |template <typename Scalar, int Rows, int Cols, int Options, int MaxRows, int MaxCols>
  267|       |struct NumTraits<Array<Scalar, Rows, Cols, Options, MaxRows, MaxCols> > {
  268|       |  typedef Array<Scalar, Rows, Cols, Options, MaxRows, MaxCols> ArrayType;
  269|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  270|       |  typedef Array<RealScalar, Rows, Cols, Options, MaxRows, MaxCols> Real;
  271|       |  typedef typename NumTraits<Scalar>::NonInteger NonIntegerScalar;
  272|       |  typedef Array<NonIntegerScalar, Rows, Cols, Options, MaxRows, MaxCols> NonInteger;
  273|       |  typedef ArrayType& Nested;
  274|       |  typedef typename NumTraits<Scalar>::Literal Literal;
  275|       |
  276|       |  enum {
  277|       |    IsComplex = NumTraits<Scalar>::IsComplex,
  278|       |    IsInteger = NumTraits<Scalar>::IsInteger,
  279|       |    IsSigned = NumTraits<Scalar>::IsSigned,
  280|       |    RequireInitialization = 1,
  281|       |    ReadCost = ArrayType::SizeAtCompileTime == Dynamic
  282|       |                   ? HugeCost
  283|       |                   : ArrayType::SizeAtCompileTime * int(NumTraits<Scalar>::ReadCost),
  284|       |    AddCost = ArrayType::SizeAtCompileTime == Dynamic ? HugeCost
  285|       |                                                      : ArrayType::SizeAtCompileTime * int(NumTraits<Scalar>::AddCost),
  286|       |    MulCost = ArrayType::SizeAtCompileTime == Dynamic ? HugeCost
  287|       |                                                      : ArrayType::SizeAtCompileTime * int(NumTraits<Scalar>::MulCost)
  288|       |  };
  289|       |
  290|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline RealScalar epsilon() { return NumTraits<RealScalar>::epsilon(); }
  291|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static inline RealScalar dummy_precision() {
  292|       |    return NumTraits<RealScalar>::dummy_precision();
  293|       |  }
  294|       |
  295|       |  EIGEN_CONSTEXPR
  296|       |  static inline int digits10() { return NumTraits<Scalar>::digits10(); }
  297|       |  EIGEN_CONSTEXPR
  298|       |  static inline int max_digits10() { return NumTraits<Scalar>::max_digits10(); }
  299|       |};
  300|       |
  301|       |template <>
  302|       |struct NumTraits<std::string> : GenericNumTraits<std::string> {
  303|       |  enum { RequireInitialization = 1, ReadCost = HugeCost, AddCost = HugeCost, MulCost = HugeCost };
  304|       |
  305|       |  EIGEN_CONSTEXPR
  306|      0|  static inline int digits10() { return 0; }
  307|       |  EIGEN_CONSTEXPR
  308|      0|  static inline int max_digits10() { return 0; }
  309|       |
  310|       | private:
  311|       |  static inline std::string epsilon();
  312|       |  static inline std::string dummy_precision();
  313|       |  static inline std::string lowest();
  314|       |  static inline std::string highest();
  315|       |  static inline std::string infinity();
  316|       |  static inline std::string quiet_NaN();
  317|       |};
  318|       |
  319|       |// Empty specialization for void to allow template specialization based on NumTraits<T>::Real with T==void and SFINAE.
  320|       |template <>
  321|       |struct NumTraits<void> {};
  322|       |
  323|       |template <>
  324|       |struct NumTraits<bool> : GenericNumTraits<bool> {};
  325|       |
  326|       |}  // end namespace Eigen
  327|       |
  328|       |#endif  // EIGEN_NUMTRAITS_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/RandomImpl.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2024 Charles Schlosser <cs.schlosser@gmail.com>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_RANDOM_IMPL_H
   11|       |#define EIGEN_RANDOM_IMPL_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "./InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |/****************************************************************************
   21|       | * Implementation of random                                               *
   22|       | ****************************************************************************/
   23|       |
   24|       |template <typename Scalar, bool IsComplex, bool IsInteger>
   25|       |struct random_default_impl {};
   26|       |
   27|       |template <typename Scalar>
   28|       |struct random_impl : random_default_impl<Scalar, NumTraits<Scalar>::IsComplex, NumTraits<Scalar>::IsInteger> {};
   29|       |
   30|       |template <typename Scalar>
   31|       |struct random_retval {
   32|       |  typedef Scalar type;
   33|       |};
   34|       |
   35|       |template <typename Scalar>
   36|       |inline EIGEN_MATHFUNC_RETVAL(random, Scalar) random(const Scalar& x, const Scalar& y) {
   37|       |  return EIGEN_MATHFUNC_IMPL(random, Scalar)::run(x, y);
   38|       |}
   39|       |
   40|       |template <typename Scalar>
   41|      0|inline EIGEN_MATHFUNC_RETVAL(random, Scalar) random() {
   42|      0|  return EIGEN_MATHFUNC_IMPL(random, Scalar)::run();
   43|      0|}
   44|       |
   45|       |// TODO: replace or provide alternatives to this, e.g. std::random_device
   46|       |struct eigen_random_device {
   47|       |  using ReturnType = int;
   48|       |  static constexpr int Entropy = meta_floor_log2<(unsigned int)(RAND_MAX) + 1>::value;
   49|       |  static constexpr ReturnType Highest = RAND_MAX;
   50|      0|  static EIGEN_DEVICE_FUNC inline ReturnType run() { return std::rand(); }
   51|       |};
   52|       |
   53|       |// Fill a built-in unsigned integer with numRandomBits beginning with the least significant bit
   54|       |template <typename Scalar>
   55|       |struct random_bits_impl {
   56|       |  EIGEN_STATIC_ASSERT(std::is_unsigned<Scalar>::value, SCALAR MUST BE A BUILT - IN UNSIGNED INTEGER)
   57|       |  using RandomDevice = eigen_random_device;
   58|       |  using RandomReturnType = typename RandomDevice::ReturnType;
   59|       |  static constexpr int kEntropy = RandomDevice::Entropy;
   60|       |  static constexpr int kTotalBits = sizeof(Scalar) * CHAR_BIT;
   61|       |  // return a Scalar filled with numRandomBits beginning from the least significant bit
   62|      0|  static EIGEN_DEVICE_FUNC inline Scalar run(int numRandomBits) {
   63|      0|    eigen_assert((numRandomBits >= 0) && (numRandomBits <= kTotalBits));
   64|      0|    const Scalar mask = Scalar(-1) >> ((kTotalBits - numRandomBits) & (kTotalBits - 1));
   65|      0|    Scalar randomBits = 0;
   66|      0|    for (int shift = 0; shift < numRandomBits; shift += kEntropy) {
   67|      0|      RandomReturnType r = RandomDevice::run();
   68|      0|      randomBits |= static_cast<Scalar>(r) << shift;
   69|      0|    }
   70|       |    // clear the excess bits
   71|      0|    randomBits &= mask;
   72|      0|    return randomBits;
   73|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal16random_bits_implIjE3runEi
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal16random_bits_implImE3runEi
  ------------------
   74|       |};
   75|       |
   76|       |template <typename BitsType>
   77|      0|EIGEN_DEVICE_FUNC inline BitsType getRandomBits(int numRandomBits) {
   78|      0|  return random_bits_impl<BitsType>::run(numRandomBits);
   79|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13getRandomBitsIjEET_i
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13getRandomBitsImEET_i
  ------------------
   80|       |
   81|       |// random implementation for a built-in floating point type
   82|       |template <typename Scalar, bool BuiltIn = std::is_floating_point<Scalar>::value>
   83|       |struct random_float_impl {
   84|       |  using BitsType = typename numext::get_integer_by_size<sizeof(Scalar)>::unsigned_type;
   85|      0|  static constexpr EIGEN_DEVICE_FUNC inline int mantissaBits() {
   86|      0|    const int digits = NumTraits<Scalar>::digits();
   87|      0|    return digits - 1;
   88|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17random_float_implIdLb1EE12mantissaBitsEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17random_float_implIfLb1EE12mantissaBitsEv
  ------------------
   89|      0|  static EIGEN_DEVICE_FUNC inline Scalar run(int numRandomBits) {
   90|      0|    eigen_assert(numRandomBits >= 0 && numRandomBits <= mantissaBits());
   91|      0|    BitsType randomBits = getRandomBits<BitsType>(numRandomBits);
   92|      0|    // if fewer than MantissaBits is requested, shift them to the left
   93|      0|    randomBits <<= (mantissaBits() - numRandomBits);
   94|      0|    // randomBits is in the half-open interval [2,4)
   95|      0|    randomBits |= numext::bit_cast<BitsType>(Scalar(2));
   96|      0|    // result is in the half-open interval [-1,1)
   97|      0|    Scalar result = numext::bit_cast<Scalar>(randomBits) - Scalar(3);
   98|      0|    return result;
   99|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17random_float_implIdLb1EE3runEi
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal17random_float_implIfLb1EE3runEi
  ------------------
  100|       |};
  101|       |// random implementation for a custom floating point type
  102|       |// uses double as the implementation with a mantissa with a size equal to either the target scalar's mantissa or that of
  103|       |// double, whichever is smaller
  104|       |template <typename Scalar>
  105|       |struct random_float_impl<Scalar, false> {
  106|       |  static EIGEN_DEVICE_FUNC inline int mantissaBits() {
  107|       |    const int digits = NumTraits<Scalar>::digits();
  108|       |    constexpr int kDoubleDigits = NumTraits<double>::digits();
  109|       |    return numext::mini(digits, kDoubleDigits) - 1;
  110|       |  }
  111|       |  static EIGEN_DEVICE_FUNC inline Scalar run(int numRandomBits) {
  112|       |    eigen_assert(numRandomBits >= 0 && numRandomBits <= mantissaBits());
  113|       |    Scalar result = static_cast<Scalar>(random_float_impl<double>::run(numRandomBits));
  114|       |    return result;
  115|       |  }
  116|       |};
  117|       |
  118|       |#if !EIGEN_COMP_NVCC
  119|       |// random implementation for long double
  120|       |// this specialization is not compatible with double-double scalars
  121|       |template <bool Specialize = (sizeof(long double) == 2 * sizeof(uint64_t)) &&
  122|       |                            ((std::numeric_limits<long double>::digits != (2 * std::numeric_limits<double>::digits)))>
  123|       |struct random_longdouble_impl {
  124|       |  static constexpr int Size = sizeof(long double);
  125|       |  static constexpr EIGEN_DEVICE_FUNC inline int mantissaBits() { return NumTraits<long double>::digits() - 1; }
  126|       |  static EIGEN_DEVICE_FUNC inline long double run(int numRandomBits) {
  127|       |    eigen_assert(numRandomBits >= 0 && numRandomBits <= mantissaBits());
  128|       |    EIGEN_USING_STD(memcpy);
  129|       |    int numLowBits = numext::mini(numRandomBits, 64);
  130|       |    int numHighBits = numext::maxi(numRandomBits - 64, 0);
  131|       |    uint64_t randomBits[2];
  132|       |    long double result = 2.0L;
  133|       |    memcpy(&randomBits, &result, Size);
  134|       |    randomBits[0] |= getRandomBits<uint64_t>(numLowBits);
  135|       |    randomBits[1] |= getRandomBits<uint64_t>(numHighBits);
  136|       |    memcpy(&result, &randomBits, Size);
  137|       |    result -= 3.0L;
  138|       |    return result;
  139|       |  }
  140|       |};
  141|       |template <>
  142|       |struct random_longdouble_impl<false> {
  143|      0|  static constexpr EIGEN_DEVICE_FUNC inline int mantissaBits() { return NumTraits<double>::digits() - 1; }
  144|      0|  static EIGEN_DEVICE_FUNC inline long double run(int numRandomBits) {
  145|      0|    return static_cast<long double>(random_float_impl<double>::run(numRandomBits));
  146|      0|  }
  147|       |};
  148|       |template <>
  149|       |struct random_float_impl<long double> : random_longdouble_impl<> {};
  150|       |#endif
  151|       |
  152|       |template <typename Scalar>
  153|       |struct random_default_impl<Scalar, false, false> {
  154|       |  using Impl = random_float_impl<Scalar>;
  155|      0|  static EIGEN_DEVICE_FUNC inline Scalar run(const Scalar& x, const Scalar& y, int numRandomBits) {
  156|      0|    Scalar half_x = Scalar(0.5) * x;
  157|      0|    Scalar half_y = Scalar(0.5) * y;
  158|      0|    Scalar result = (half_x + half_y) + (half_y - half_x) * run(numRandomBits);
  159|      0|    // result is in the half-open interval [x, y) -- provided that x < y
  160|      0|    return result;
  161|      0|  }
  162|       |  static EIGEN_DEVICE_FUNC inline Scalar run(const Scalar& x, const Scalar& y) {
  163|       |    return run(x, y, Impl::mantissaBits());
  164|       |  }
  165|      0|  static EIGEN_DEVICE_FUNC inline Scalar run(int numRandomBits) { return Impl::run(numRandomBits); }
  166|       |  static EIGEN_DEVICE_FUNC inline Scalar run() { return run(Impl::mantissaBits()); }
  167|       |};
  168|       |
  169|       |template <typename Scalar, bool IsSigned = NumTraits<Scalar>::IsSigned, bool BuiltIn = std::is_integral<Scalar>::value>
  170|       |struct random_int_impl;
  171|       |
  172|       |// random implementation for a built-in unsigned integer type
  173|       |template <typename Scalar>
  174|       |struct random_int_impl<Scalar, false, true> {
  175|       |  static constexpr int kTotalBits = sizeof(Scalar) * CHAR_BIT;
  176|       |  static EIGEN_DEVICE_FUNC inline Scalar run(const Scalar& x, const Scalar& y) {
  177|       |    if (y <= x) return x;
  178|       |    Scalar range = y - x;
  179|       |    // handle edge case where [x,y] spans the entire range of Scalar
  180|       |    if (range == NumTraits<Scalar>::highest()) return run();
  181|       |    Scalar count = range + 1;
  182|       |    // calculate the number of random bits needed to fill range
  183|       |    int numRandomBits = log2_ceil(count);
  184|       |    Scalar randomBits;
  185|       |    do {
  186|       |      randomBits = getRandomBits<Scalar>(numRandomBits);
  187|       |      // if the random draw is outside [0, range), try again (rejection sampling)
  188|       |      // in the worst-case scenario, the probability of rejection is: 1/2 - 1/2^numRandomBits < 50%
  189|       |    } while (randomBits >= count);
  190|       |    Scalar result = x + randomBits;
  191|       |    return result;
  192|       |  }
  193|       |  static EIGEN_DEVICE_FUNC inline Scalar run() { return getRandomBits<Scalar>(kTotalBits); }
  194|       |};
  195|       |
  196|       |// random implementation for a built-in signed integer type
  197|       |template <typename Scalar>
  198|       |struct random_int_impl<Scalar, true, true> {
  199|       |  static constexpr int kTotalBits = sizeof(Scalar) * CHAR_BIT;
  200|       |  using BitsType = typename make_unsigned<Scalar>::type;
  201|       |  static EIGEN_DEVICE_FUNC inline Scalar run(const Scalar& x, const Scalar& y) {
  202|       |    if (y <= x) return x;
  203|       |    // Avoid overflow by representing `range` as an unsigned type
  204|       |    BitsType range = static_cast<BitsType>(y) - static_cast<BitsType>(x);
  205|       |    BitsType randomBits = random_int_impl<BitsType>::run(0, range);
  206|       |    // Avoid overflow in the case where `x` is negative and there is a large range so
  207|       |    // `randomBits` would also be negative if cast to `Scalar` first.
  208|       |    Scalar result = static_cast<Scalar>(static_cast<BitsType>(x) + randomBits);
  209|       |    return result;
  210|       |  }
  211|       |  static EIGEN_DEVICE_FUNC inline Scalar run() { return static_cast<Scalar>(getRandomBits<BitsType>(kTotalBits)); }
  212|       |};
  213|       |
  214|       |// todo: custom integers
  215|       |template <typename Scalar, bool IsSigned>
  216|       |struct random_int_impl<Scalar, IsSigned, false> {
  217|       |  static EIGEN_DEVICE_FUNC inline Scalar run(const Scalar&, const Scalar&) { return run(); }
  218|       |  static EIGEN_DEVICE_FUNC inline Scalar run() {
  219|       |    eigen_assert(std::false_type::value && "RANDOM FOR CUSTOM INTEGERS NOT YET SUPPORTED");
  220|       |    return Scalar(0);
  221|       |  }
  222|       |};
  223|       |
  224|       |template <typename Scalar>
  225|       |struct random_default_impl<Scalar, false, true> : random_int_impl<Scalar> {};
  226|       |
  227|       |template <>
  228|       |struct random_impl<bool> {
  229|      0|  static EIGEN_DEVICE_FUNC inline bool run(const bool& x, const bool& y) {
  230|      0|    if (y <= x) return x;
  231|      0|    return run();
  232|      0|  }
  233|      0|  static EIGEN_DEVICE_FUNC inline bool run() { return getRandomBits<unsigned>(1) ? true : false; }
  234|       |};
  235|       |
  236|       |template <typename Scalar>
  237|       |struct random_default_impl<Scalar, true, false> {
  238|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  239|       |  using Impl = random_impl<RealScalar>;
  240|       |  static EIGEN_DEVICE_FUNC inline Scalar run(const Scalar& x, const Scalar& y, int numRandomBits) {
  241|       |    return Scalar(Impl::run(x.real(), y.real(), numRandomBits), Impl::run(x.imag(), y.imag(), numRandomBits));
  242|       |  }
  243|       |  static EIGEN_DEVICE_FUNC inline Scalar run(const Scalar& x, const Scalar& y) {
  244|       |    return Scalar(Impl::run(x.real(), y.real()), Impl::run(x.imag(), y.imag()));
  245|       |  }
  246|       |  static EIGEN_DEVICE_FUNC inline Scalar run(int numRandomBits) {
  247|       |    return Scalar(Impl::run(numRandomBits), Impl::run(numRandomBits));
  248|       |  }
  249|       |  static EIGEN_DEVICE_FUNC inline Scalar run() { return Scalar(Impl::run(), Impl::run()); }
  250|       |};
  251|       |
  252|       |}  // namespace internal
  253|       |}  // namespace Eigen
  254|       |
  255|       |#endif  // EIGEN_RANDOM_IMPL_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/Default/BFloat16.h:
    1|       |/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
    2|       |
    3|       |Licensed under the Apache License, Version 2.0 (the "License");
    4|       |you may not use this file except in compliance with the License.
    5|       |You may obtain a copy of the License at
    6|       |
    7|       |    http://www.apache.org/licenses/LICENSE-2.0
    8|       |
    9|       |Unless required by applicable law or agreed to in writing, software
   10|       |distributed under the License is distributed on an "AS IS" BASIS,
   11|       |WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   12|       |See the License for the specific language governing permissions and
   13|       |limitations under the License.
   14|       |==============================================================================*/
   15|       |
   16|       |#ifndef EIGEN_BFLOAT16_H
   17|       |#define EIGEN_BFLOAT16_H
   18|       |
   19|       |// IWYU pragma: private
   20|       |#include "../../InternalHeaderCheck.h"
   21|       |
   22|       |#if defined(EIGEN_HAS_HIP_BF16)
   23|       |// When compiling with GPU support, the "hip_bfloat16" base class as well as
   24|       |// some other routines are defined in the GPU compiler header files
   25|       |// (hip_bfloat16.h), and they are not tagged constexpr
   26|       |// As a consequence, we get compile failures when compiling Eigen with
   27|       |// GPU support. Hence the need to disable EIGEN_CONSTEXPR when building
   28|       |// Eigen with GPU support
   29|       |#pragma push_macro("EIGEN_CONSTEXPR")
   30|       |#undef EIGEN_CONSTEXPR
   31|       |#define EIGEN_CONSTEXPR
   32|       |#endif
   33|       |
   34|       |#define BF16_PACKET_FUNCTION(PACKET_F, PACKET_BF16, METHOD)                                         \
   35|       |  template <>                                                                                       \
   36|       |  EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS EIGEN_UNUSED PACKET_BF16 METHOD<PACKET_BF16>( \
   37|       |      const PACKET_BF16& _x) {                                                                      \
   38|       |    return F32ToBf16(METHOD<PACKET_F>(Bf16ToF32(_x)));                                              \
   39|       |  }
   40|       |
   41|       |// Only use HIP GPU bf16 in kernels
   42|       |#if defined(EIGEN_HAS_HIP_BF16) && defined(EIGEN_GPU_COMPILE_PHASE)
   43|       |#define EIGEN_USE_HIP_BF16
   44|       |#endif
   45|       |
   46|       |namespace Eigen {
   47|       |
   48|       |struct bfloat16;
   49|       |
   50|       |namespace numext {
   51|       |template <>
   52|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::bfloat16 bit_cast<Eigen::bfloat16, uint16_t>(const uint16_t& src);
   53|       |
   54|       |template <>
   55|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC uint16_t bit_cast<uint16_t, Eigen::bfloat16>(const Eigen::bfloat16& src);
   56|       |}  // namespace numext
   57|       |namespace bfloat16_impl {
   58|       |
   59|       |#if defined(EIGEN_USE_HIP_BF16)
   60|       |
   61|       |struct __bfloat16_raw : public hip_bfloat16 {
   62|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw() {}
   63|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw(hip_bfloat16 hb) : hip_bfloat16(hb) {}
   64|       |  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw(unsigned short raw) : hip_bfloat16(raw) {}
   65|       |};
   66|       |
   67|       |#else
   68|       |
   69|       |// Make our own __bfloat16_raw definition.
   70|       |struct __bfloat16_raw {
   71|       |#if defined(EIGEN_HAS_HIP_BF16) && !defined(EIGEN_GPU_COMPILE_PHASE)
   72|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw() {}
   73|       |#else
   74|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw() : value(0) {}
   75|       |#endif
   76|      0|  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw(unsigned short raw) : value(raw) {}
   77|       |  unsigned short value;
   78|       |};
   79|       |
   80|       |#endif  // defined(EIGEN_USE_HIP_BF16)
   81|       |
   82|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw raw_uint16_to_bfloat16(unsigned short value);
   83|       |template <bool AssumeArgumentIsNormalOrInfinityOrZero>
   84|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne(float ff);
   85|       |// Forward declarations of template specializations, to avoid Visual C++ 2019 errors, saying:
   86|       |// > error C2908: explicit specialization; 'float_to_bfloat16_rtne' has already been instantiated
   87|       |template <>
   88|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<false>(float ff);
   89|       |template <>
   90|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<true>(float ff);
   91|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float bfloat16_to_float(__bfloat16_raw h);
   92|       |
   93|       |struct bfloat16_base : public __bfloat16_raw {
   94|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR bfloat16_base() {}
   95|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR bfloat16_base(const __bfloat16_raw& h) : __bfloat16_raw(h) {}
   96|       |};
   97|       |
   98|       |}  // namespace bfloat16_impl
   99|       |
  100|       |// Class definition.
  101|       |struct bfloat16 : public bfloat16_impl::bfloat16_base {
  102|       |  typedef bfloat16_impl::__bfloat16_raw __bfloat16_raw;
  103|       |
  104|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR bfloat16() {}
  105|       |
  106|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR bfloat16(const __bfloat16_raw& h) : bfloat16_impl::bfloat16_base(h) {}
  107|       |
  108|       |  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR bfloat16(bool b)
  109|      0|      : bfloat16_impl::bfloat16_base(bfloat16_impl::raw_uint16_to_bfloat16(b ? 0x3f80 : 0)) {}
  110|       |
  111|       |  template <class T>
  112|       |  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR bfloat16(T val)
  113|       |      : bfloat16_impl::bfloat16_base(
  114|       |            bfloat16_impl::float_to_bfloat16_rtne<internal::is_integral<T>::value>(static_cast<float>(val))) {}
  115|       |
  116|       |  explicit EIGEN_DEVICE_FUNC bfloat16(float f)
  117|      0|      : bfloat16_impl::bfloat16_base(bfloat16_impl::float_to_bfloat16_rtne<false>(f)) {}
  118|       |
  119|       |  // Following the convention of numpy, converting between complex and
  120|       |  // float will lead to loss of imag value.
  121|       |  template <typename RealScalar>
  122|       |  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR bfloat16(const std::complex<RealScalar>& val)
  123|       |      : bfloat16_impl::bfloat16_base(bfloat16_impl::float_to_bfloat16_rtne<false>(static_cast<float>(val.real()))) {}
  124|       |
  125|      0|  EIGEN_DEVICE_FUNC operator float() const {  // NOLINT: Allow implicit conversion to float, because it is lossless.
  126|      0|    return bfloat16_impl::bfloat16_to_float(*this);
  127|      0|  }
  128|       |};
  129|       |
  130|       |// TODO(majnemer): Get rid of this once we can rely on C++17 inline variables do
  131|       |// solve the ODR issue.
  132|       |namespace bfloat16_impl {
  133|       |template <typename = void>
  134|       |struct numeric_limits_bfloat16_impl {
  135|       |  static EIGEN_CONSTEXPR const bool is_specialized = true;
  136|       |  static EIGEN_CONSTEXPR const bool is_signed = true;
  137|       |  static EIGEN_CONSTEXPR const bool is_integer = false;
  138|       |  static EIGEN_CONSTEXPR const bool is_exact = false;
  139|       |  static EIGEN_CONSTEXPR const bool has_infinity = true;
  140|       |  static EIGEN_CONSTEXPR const bool has_quiet_NaN = true;
  141|       |  static EIGEN_CONSTEXPR const bool has_signaling_NaN = true;
  142|       |  EIGEN_DIAGNOSTICS(push)
  143|       |  EIGEN_DISABLE_DEPRECATED_WARNING
  144|       |  static EIGEN_CONSTEXPR const std::float_denorm_style has_denorm = std::denorm_present;
  145|       |  static EIGEN_CONSTEXPR const bool has_denorm_loss = false;
  146|       |  EIGEN_DIAGNOSTICS(pop)
  147|       |  static EIGEN_CONSTEXPR const std::float_round_style round_style = std::numeric_limits<float>::round_style;
  148|       |  static EIGEN_CONSTEXPR const bool is_iec559 = true;
  149|       |  // The C++ standard defines this as "true if the set of values representable
  150|       |  // by the type is finite." BFloat16 has finite precision.
  151|       |  static EIGEN_CONSTEXPR const bool is_bounded = true;
  152|       |  static EIGEN_CONSTEXPR const bool is_modulo = false;
  153|       |  static EIGEN_CONSTEXPR const int digits = 8;
  154|       |  static EIGEN_CONSTEXPR const int digits10 = 2;
  155|       |  static EIGEN_CONSTEXPR const int max_digits10 = 4;
  156|       |  static EIGEN_CONSTEXPR const int radix = std::numeric_limits<float>::radix;
  157|       |  static EIGEN_CONSTEXPR const int min_exponent = std::numeric_limits<float>::min_exponent;
  158|       |  static EIGEN_CONSTEXPR const int min_exponent10 = std::numeric_limits<float>::min_exponent10;
  159|       |  static EIGEN_CONSTEXPR const int max_exponent = std::numeric_limits<float>::max_exponent;
  160|       |  static EIGEN_CONSTEXPR const int max_exponent10 = std::numeric_limits<float>::max_exponent10;
  161|       |  static EIGEN_CONSTEXPR const bool traps = std::numeric_limits<float>::traps;
  162|       |  // IEEE754: "The implementer shall choose how tininess is detected, but shall
  163|       |  // detect tininess in the same way for all operations in radix two"
  164|       |  static EIGEN_CONSTEXPR const bool tinyness_before = std::numeric_limits<float>::tinyness_before;
  165|       |
  166|       |  static EIGEN_CONSTEXPR Eigen::bfloat16(min)() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x0080); }
  167|       |  static EIGEN_CONSTEXPR Eigen::bfloat16 lowest() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0xff7f); }
  168|       |  static EIGEN_CONSTEXPR Eigen::bfloat16(max)() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7f7f); }
  169|       |  static EIGEN_CONSTEXPR Eigen::bfloat16 epsilon() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x3c00); }
  170|       |  static EIGEN_CONSTEXPR Eigen::bfloat16 round_error() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x3f00); }
  171|       |  static EIGEN_CONSTEXPR Eigen::bfloat16 infinity() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7f80); }
  172|       |  static EIGEN_CONSTEXPR Eigen::bfloat16 quiet_NaN() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7fc0); }
  173|       |  static EIGEN_CONSTEXPR Eigen::bfloat16 signaling_NaN() {
  174|       |    return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x7fa0);
  175|       |  }
  176|       |  static EIGEN_CONSTEXPR Eigen::bfloat16 denorm_min() { return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(0x0001); }
  177|       |};
  178|       |
  179|       |template <typename T>
  180|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::is_specialized;
  181|       |template <typename T>
  182|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::is_signed;
  183|       |template <typename T>
  184|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::is_integer;
  185|       |template <typename T>
  186|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::is_exact;
  187|       |template <typename T>
  188|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::has_infinity;
  189|       |template <typename T>
  190|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::has_quiet_NaN;
  191|       |template <typename T>
  192|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::has_signaling_NaN;
  193|       |EIGEN_DIAGNOSTICS(push)
  194|       |EIGEN_DISABLE_DEPRECATED_WARNING
  195|       |template <typename T>
  196|       |EIGEN_CONSTEXPR const std::float_denorm_style numeric_limits_bfloat16_impl<T>::has_denorm;
  197|       |template <typename T>
  198|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::has_denorm_loss;
  199|       |EIGEN_DIAGNOSTICS(pop)
  200|       |template <typename T>
  201|       |EIGEN_CONSTEXPR const std::float_round_style numeric_limits_bfloat16_impl<T>::round_style;
  202|       |template <typename T>
  203|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::is_iec559;
  204|       |template <typename T>
  205|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::is_bounded;
  206|       |template <typename T>
  207|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::is_modulo;
  208|       |template <typename T>
  209|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::digits;
  210|       |template <typename T>
  211|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::digits10;
  212|       |template <typename T>
  213|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::max_digits10;
  214|       |template <typename T>
  215|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::radix;
  216|       |template <typename T>
  217|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::min_exponent;
  218|       |template <typename T>
  219|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::min_exponent10;
  220|       |template <typename T>
  221|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::max_exponent;
  222|       |template <typename T>
  223|       |EIGEN_CONSTEXPR const int numeric_limits_bfloat16_impl<T>::max_exponent10;
  224|       |template <typename T>
  225|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::traps;
  226|       |template <typename T>
  227|       |EIGEN_CONSTEXPR const bool numeric_limits_bfloat16_impl<T>::tinyness_before;
  228|       |}  // end namespace bfloat16_impl
  229|       |}  // end namespace Eigen
  230|       |
  231|       |namespace std {
  232|       |// If std::numeric_limits<T> is specialized, should also specialize
  233|       |// std::numeric_limits<const T>, std::numeric_limits<volatile T>, and
  234|       |// std::numeric_limits<const volatile T>
  235|       |// https://stackoverflow.com/a/16519653/
  236|       |template <>
  237|       |class numeric_limits<Eigen::bfloat16> : public Eigen::bfloat16_impl::numeric_limits_bfloat16_impl<> {};
  238|       |template <>
  239|       |class numeric_limits<const Eigen::bfloat16> : public numeric_limits<Eigen::bfloat16> {};
  240|       |template <>
  241|       |class numeric_limits<volatile Eigen::bfloat16> : public numeric_limits<Eigen::bfloat16> {};
  242|       |template <>
  243|       |class numeric_limits<const volatile Eigen::bfloat16> : public numeric_limits<Eigen::bfloat16> {};
  244|       |}  // end namespace std
  245|       |
  246|       |namespace Eigen {
  247|       |
  248|       |namespace bfloat16_impl {
  249|       |
  250|       |// We need to distinguish ‘clang as the CUDA compiler’ from ‘clang as the host compiler,
  251|       |// invoked by NVCC’ (e.g. on MacOS). The former needs to see both host and device implementation
  252|       |// of the functions, while the latter can only deal with one of them.
  253|       |#if !defined(EIGEN_HAS_NATIVE_BF16) || (EIGEN_COMP_CLANG && !EIGEN_COMP_NVCC)  // Emulate support for bfloat16 floats
  254|       |
  255|       |#if EIGEN_COMP_CLANG && defined(EIGEN_CUDACC)
  256|       |// We need to provide emulated *host-side* BF16 operators for clang.
  257|       |#pragma push_macro("EIGEN_DEVICE_FUNC")
  258|       |#undef EIGEN_DEVICE_FUNC
  259|       |#if (defined(EIGEN_HAS_GPU_BF16) && defined(EIGEN_HAS_NATIVE_BF16))
  260|       |#define EIGEN_DEVICE_FUNC __host__
  261|       |#else  // both host and device need emulated ops.
  262|       |#define EIGEN_DEVICE_FUNC __host__ __device__
  263|       |#endif
  264|       |#endif
  265|       |
  266|       |// Definitions for CPUs, mostly working through conversion
  267|       |// to/from fp32.
  268|       |
  269|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator+(const bfloat16& a, const bfloat16& b) {
  270|      0|  return bfloat16(float(a) + float(b));
  271|      0|}
  272|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator+(const bfloat16& a, const int& b) {
  273|      0|  return bfloat16(float(a) + static_cast<float>(b));
  274|      0|}
  275|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator+(const int& a, const bfloat16& b) {
  276|      0|  return bfloat16(static_cast<float>(a) + float(b));
  277|      0|}
  278|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator*(const bfloat16& a, const bfloat16& b) {
  279|      0|  return bfloat16(float(a) * float(b));
  280|      0|}
  281|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator-(const bfloat16& a, const bfloat16& b) {
  282|      0|  return bfloat16(float(a) - float(b));
  283|      0|}
  284|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator/(const bfloat16& a, const bfloat16& b) {
  285|      0|  return bfloat16(float(a) / float(b));
  286|      0|}
  287|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator-(const bfloat16& a) {
  288|      0|  numext::uint16_t x = numext::bit_cast<uint16_t>(a) ^ 0x8000;
  289|      0|  return numext::bit_cast<bfloat16>(x);
  290|      0|}
  291|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator+=(bfloat16& a, const bfloat16& b) {
  292|      0|  a = bfloat16(float(a) + float(b));
  293|      0|  return a;
  294|      0|}
  295|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator*=(bfloat16& a, const bfloat16& b) {
  296|      0|  a = bfloat16(float(a) * float(b));
  297|      0|  return a;
  298|      0|}
  299|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator-=(bfloat16& a, const bfloat16& b) {
  300|      0|  a = bfloat16(float(a) - float(b));
  301|      0|  return a;
  302|      0|}
  303|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16& operator/=(bfloat16& a, const bfloat16& b) {
  304|      0|  a = bfloat16(float(a) / float(b));
  305|      0|  return a;
  306|      0|}
  307|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator++(bfloat16& a) {
  308|      0|  a += bfloat16(1);
  309|      0|  return a;
  310|      0|}
  311|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator--(bfloat16& a) {
  312|      0|  a -= bfloat16(1);
  313|      0|  return a;
  314|      0|}
  315|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator++(bfloat16& a, int) {
  316|      0|  bfloat16 original_value = a;
  317|      0|  ++a;
  318|      0|  return original_value;
  319|      0|}
  320|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator--(bfloat16& a, int) {
  321|      0|  bfloat16 original_value = a;
  322|      0|  --a;
  323|      0|  return original_value;
  324|      0|}
  325|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator==(const bfloat16& a, const bfloat16& b) {
  326|      0|  return numext::equal_strict(float(a), float(b));
  327|      0|}
  328|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator!=(const bfloat16& a, const bfloat16& b) {
  329|      0|  return numext::not_equal_strict(float(a), float(b));
  330|      0|}
  331|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator<(const bfloat16& a, const bfloat16& b) {
  332|      0|  return float(a) < float(b);
  333|      0|}
  334|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator<=(const bfloat16& a, const bfloat16& b) {
  335|      0|  return float(a) <= float(b);
  336|      0|}
  337|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator>(const bfloat16& a, const bfloat16& b) {
  338|      0|  return float(a) > float(b);
  339|      0|}
  340|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator>=(const bfloat16& a, const bfloat16& b) {
  341|      0|  return float(a) >= float(b);
  342|      0|}
  343|       |
  344|       |#if EIGEN_COMP_CLANG && defined(EIGEN_CUDACC)
  345|       |#pragma pop_macro("EIGEN_DEVICE_FUNC")
  346|       |#endif
  347|       |#endif  // Emulate support for bfloat16 floats
  348|       |
  349|       |// Division by an index. Do it in full float precision to avoid accuracy
  350|       |// issues in converting the denominator to bfloat16.
  351|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 operator/(const bfloat16& a, Index b) {
  352|      0|  return bfloat16(static_cast<float>(a) / static_cast<float>(b));
  353|      0|}
  354|       |
  355|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw truncate_to_bfloat16(const float v) {
  356|      0|#if defined(EIGEN_USE_HIP_BF16)
  357|      0|  return __bfloat16_raw(__bfloat16_raw::round_to_bfloat16(v, __bfloat16_raw::truncate));
  358|      0|#else
  359|      0|  __bfloat16_raw output;
  360|      0|  if (numext::isnan EIGEN_NOT_A_MACRO(v)) {
  361|      0|    output.value = std::signbit(v) ? 0xFFC0 : 0x7FC0;
  362|      0|    return output;
  363|      0|  }
  364|      0|  output.value = static_cast<numext::uint16_t>(numext::bit_cast<numext::uint32_t>(v) >> 16);
  365|      0|  return output;
  366|      0|#endif
  367|      0|}
  368|       |
  369|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __bfloat16_raw raw_uint16_to_bfloat16(numext::uint16_t value) {
  370|      0|#if defined(EIGEN_USE_HIP_BF16)
  371|      0|  __bfloat16_raw bf;
  372|      0|  bf.data = value;
  373|      0|  return bf;
  374|      0|#else
  375|      0|  return __bfloat16_raw(value);
  376|      0|#endif
  377|      0|}
  378|       |
  379|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR numext::uint16_t raw_bfloat16_as_uint16(
  380|      0|    const __bfloat16_raw& bf) {
  381|      0|#if defined(EIGEN_USE_HIP_BF16)
  382|      0|  return bf.data;
  383|      0|#else
  384|      0|  return bf.value;
  385|      0|#endif
  386|      0|}
  387|       |
  388|       |// float_to_bfloat16_rtne template specialization that does not make any
  389|       |// assumption about the value of its function argument (ff).
  390|       |template <>
  391|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<false>(float ff) {
  392|      0|#if defined(EIGEN_USE_HIP_BF16)
  393|      0|  return __bfloat16_raw(__bfloat16_raw::round_to_bfloat16(ff));
  394|      0|#else
  395|      0|  __bfloat16_raw output;
  396|      0|
  397|      0|  if (numext::isnan EIGEN_NOT_A_MACRO(ff)) {
  398|      0|    // If the value is a NaN, squash it to a qNaN with msb of fraction set,
  399|      0|    // this makes sure after truncation we don't end up with an inf.
  400|      0|    //
  401|      0|    // qNaN magic: All exponent bits set + most significant bit of fraction
  402|      0|    // set.
  403|      0|    output.value = std::signbit(ff) ? 0xFFC0 : 0x7FC0;
  404|      0|  } else {
  405|      0|    // Fast rounding algorithm that rounds a half value to nearest even. This
  406|      0|    // reduces expected error when we convert a large number of floats. Here
  407|      0|    // is how it works:
  408|      0|    //
  409|      0|    // Definitions:
  410|      0|    // To convert a float 32 to bfloat16, a float 32 can be viewed as 32 bits
  411|      0|    // with the following tags:
  412|      0|    //
  413|      0|    // Sign |  Exp (8 bits) | Frac (23 bits)
  414|      0|    //  S     EEEEEEEE         FFFFFFLRTTTTTTTTTTTTTTT
  415|      0|    //
  416|      0|    //  S: Sign bit.
  417|      0|    //  E: Exponent bits.
  418|      0|    //  F: First 6 bits of fraction.
  419|      0|    //  L: Least significant bit of resulting bfloat16 if we truncate away the
  420|      0|    //  rest of the float32. This is also the 7th bit of fraction
  421|      0|    //  R: Rounding bit, 8th bit of fraction.
  422|      0|    //  T: Sticky bits, rest of fraction, 15 bits.
  423|      0|    //
  424|      0|    // To round half to nearest even, there are 3 cases where we want to round
  425|      0|    // down (simply truncate the result of the bits away, which consists of
  426|      0|    // rounding bit and sticky bits) and two cases where we want to round up
  427|      0|    // (truncate then add one to the result).
  428|      0|    //
  429|      0|    // The fast converting algorithm simply adds lsb (L) to 0x7fff (15 bits of
  430|      0|    // 1s) as the rounding bias, adds the rounding bias to the input, then
  431|      0|    // truncates the last 16 bits away.
  432|      0|    //
  433|      0|    // To understand how it works, we can analyze this algorithm case by case:
  434|      0|    //
  435|      0|    // 1. L = 0, R = 0:
  436|      0|    //   Expect: round down, this is less than half value.
  437|      0|    //
  438|      0|    //   Algorithm:
  439|      0|    //   - Rounding bias: 0x7fff + 0 = 0x7fff
  440|      0|    //   - Adding rounding bias to input may create any carry, depending on
  441|      0|    //   whether there is any value set to 1 in T bits.
  442|      0|    //   - R may be set to 1 if there is a carry.
  443|      0|    //   - L remains 0.
  444|      0|    //   - Note that this case also handles Inf and -Inf, where all fraction
  445|      0|    //   bits, including L, R and Ts are all 0. The output remains Inf after
  446|      0|    //   this algorithm.
  447|      0|    //
  448|      0|    // 2. L = 1, R = 0:
  449|      0|    //   Expect: round down, this is less than half value.
  450|      0|    //
  451|      0|    //   Algorithm:
  452|      0|    //   - Rounding bias: 0x7fff + 1 = 0x8000
  453|      0|    //   - Adding rounding bias to input doesn't change sticky bits but
  454|      0|    //   adds 1 to rounding bit.
  455|      0|    //   - L remains 1.
  456|      0|    //
  457|      0|    // 3. L = 0, R = 1, all of T are 0:
  458|      0|    //   Expect: round down, this is exactly at half, the result is already
  459|      0|    //   even (L=0).
  460|      0|    //
  461|      0|    //   Algorithm:
  462|      0|    //   - Rounding bias: 0x7fff + 0 = 0x7fff
  463|      0|    //   - Adding rounding bias to input sets all sticky bits to 1, but
  464|      0|    //   doesn't create a carry.
  465|      0|    //   - R remains 1.
  466|      0|    //   - L remains 0.
  467|      0|    //
  468|      0|    // 4. L = 1, R = 1:
  469|      0|    //   Expect: round up, this is exactly at half, the result needs to be
  470|      0|    //   round to the next even number.
  471|      0|    //
  472|      0|    //   Algorithm:
  473|      0|    //   - Rounding bias: 0x7fff + 1 = 0x8000
  474|      0|    //   - Adding rounding bias to input doesn't change sticky bits, but
  475|      0|    //   creates a carry from rounding bit.
  476|      0|    //   - The carry sets L to 0, creates another carry bit and propagate
  477|      0|    //   forward to F bits.
  478|      0|    //   - If all the F bits are 1, a carry then propagates to the exponent
  479|      0|    //   bits, which then creates the minimum value with the next exponent
  480|      0|    //   value. Note that we won't have the case where exponents are all 1,
  481|      0|    //   since that's either a NaN (handled in the other if condition) or inf
  482|      0|    //   (handled in case 1).
  483|      0|    //
  484|      0|    // 5. L = 0, R = 1, any of T is 1:
  485|      0|    //   Expect: round up, this is greater than half.
  486|      0|    //
  487|      0|    //   Algorithm:
  488|      0|    //   - Rounding bias: 0x7fff + 0 = 0x7fff
  489|      0|    //   - Adding rounding bias to input creates a carry from sticky bits,
  490|      0|    //   sets rounding bit to 0, then create another carry.
  491|      0|    //   - The second carry sets L to 1.
  492|      0|    //
  493|      0|    // Examples:
  494|      0|    //
  495|      0|    //  Exact half value that is already even:
  496|      0|    //    Input:
  497|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
  498|      0|    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
  499|      0|    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 1 0     1000000000000000
  500|      0|    //
  501|      0|    //     This falls into case 3. We truncate the rest of 16 bits and no
  502|      0|    //     carry is created into F and L:
  503|      0|    //
  504|      0|    //    Output:
  505|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
  506|      0|    //     S     E E E E E E E E      F F F F F F L
  507|      0|    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 1 0
  508|      0|    //
  509|      0|    //  Exact half value, round to next even number:
  510|      0|    //    Input:
  511|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
  512|      0|    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
  513|      0|    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 0 1     1000000000000000
  514|      0|    //
  515|      0|    //     This falls into case 4. We create a carry from R and T,
  516|      0|    //     which then propagates into L and F:
  517|      0|    //
  518|      0|    //    Output:
  519|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
  520|      0|    //     S     E E E E E E E E      F F F F F F L
  521|      0|    //     0     0 0 0 0 0 0 0 0      0 0 0 0 0 1 0
  522|      0|    //
  523|      0|    //
  524|      0|    //  Max denormal value round to min normal value:
  525|      0|    //    Input:
  526|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
  527|      0|    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
  528|      0|    //     0     0 0 0 0 0 0 0 0      1 1 1 1 1 1 1     1111111111111111
  529|      0|    //
  530|      0|    //     This falls into case 4. We create a carry from R and T,
  531|      0|    //     propagate into L and F, which then propagates into exponent
  532|      0|    //     bits:
  533|      0|    //
  534|      0|    //    Output:
  535|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
  536|      0|    //     S     E E E E E E E E      F F F F F F L
  537|      0|    //     0     0 0 0 0 0 0 0 1      0 0 0 0 0 0 0
  538|      0|    //
  539|      0|    //  Max normal value round to Inf:
  540|      0|    //    Input:
  541|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit) | Frac (last 16 bit)
  542|      0|    //     S     E E E E E E E E      F F F F F F L     RTTTTTTTTTTTTTTT
  543|      0|    //     0     1 1 1 1 1 1 1 0      1 1 1 1 1 1 1     1111111111111111
  544|      0|    //
  545|      0|    //     This falls into case 4. We create a carry from R and T,
  546|      0|    //     propagate into L and F, which then propagates into exponent
  547|      0|    //     bits:
  548|      0|    //
  549|      0|    //    Sign |  Exp (8 bit)     | Frac (first 7 bit)
  550|      0|    //     S     E E E E E E E E      F F F F F F L
  551|      0|    //     0     1 1 1 1 1 1 1 1      0 0 0 0 0 0 0
  552|      0|
  553|      0|    // At this point, ff must be either a normal float, or +/-infinity.
  554|      0|    output = float_to_bfloat16_rtne<true>(ff);
  555|      0|  }
  556|      0|  return output;
  557|      0|#endif
  558|      0|}
  559|       |
  560|       |// float_to_bfloat16_rtne template specialization that assumes that its function
  561|       |// argument (ff) is either a normal floating point number, or +/-infinity, or
  562|       |// zero. Used to improve the runtime performance of conversion from an integer
  563|       |// type to bfloat16.
  564|       |template <>
  565|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __bfloat16_raw float_to_bfloat16_rtne<true>(float ff) {
  566|      0|#if defined(EIGEN_USE_HIP_BF16)
  567|      0|  return __bfloat16_raw(__bfloat16_raw::round_to_bfloat16(ff));
  568|      0|#else
  569|      0|  numext::uint32_t input = numext::bit_cast<numext::uint32_t>(ff);
  570|      0|  __bfloat16_raw output;
  571|      0|
  572|      0|  // Least significant bit of resulting bfloat.
  573|      0|  numext::uint32_t lsb = (input >> 16) & 1;
  574|      0|  numext::uint32_t rounding_bias = 0x7fff + lsb;
  575|      0|  input += rounding_bias;
  576|      0|  output.value = static_cast<numext::uint16_t>(input >> 16);
  577|      0|  return output;
  578|      0|#endif
  579|      0|}
  580|       |
  581|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float bfloat16_to_float(__bfloat16_raw h) {
  582|      0|#if defined(EIGEN_USE_HIP_BF16)
  583|      0|  return static_cast<float>(h);
  584|      0|#else
  585|      0|  return numext::bit_cast<float>(static_cast<numext::uint32_t>(h.value) << 16);
  586|      0|#endif
  587|      0|}
  588|       |
  589|       |// --- standard functions ---
  590|       |
  591|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool(isinf)(const bfloat16& a) {
  592|      0|  EIGEN_USING_STD(isinf);
  593|      0|#if defined(EIGEN_USE_HIP_BF16)
  594|      0|  return (isinf)(a);  // Uses HIP hip_bfloat16 isinf operator
  595|      0|#else
  596|      0|  return (isinf)(float(a));
  597|      0|#endif
  598|      0|}
  599|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool(isnan)(const bfloat16& a) {
  600|      0|  EIGEN_USING_STD(isnan);
  601|      0|#if defined(EIGEN_USE_HIP_BF16)
  602|      0|  return (isnan)(a);  // Uses HIP hip_bfloat16 isnan operator
  603|      0|#else
  604|      0|  return (isnan)(float(a));
  605|      0|#endif
  606|      0|}
  607|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool(isfinite)(const bfloat16& a) {
  608|      0|  return !(isinf EIGEN_NOT_A_MACRO(a)) && !(isnan EIGEN_NOT_A_MACRO(a));
  609|      0|}
  610|       |
  611|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 abs(const bfloat16& a) {
  612|      0|  numext::uint16_t x = numext::bit_cast<numext::uint16_t>(a) & 0x7FFF;
  613|      0|  return numext::bit_cast<bfloat16>(x);
  614|      0|}
  615|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 exp(const bfloat16& a) { return bfloat16(::expf(float(a))); }
  616|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 exp2(const bfloat16& a) { return bfloat16(::exp2f(float(a))); }
  617|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 expm1(const bfloat16& a) { return bfloat16(numext::expm1(float(a))); }
  618|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 log(const bfloat16& a) { return bfloat16(::logf(float(a))); }
  619|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 log1p(const bfloat16& a) { return bfloat16(numext::log1p(float(a))); }
  620|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 log10(const bfloat16& a) { return bfloat16(::log10f(float(a))); }
  621|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 log2(const bfloat16& a) {
  622|      0|  return bfloat16(static_cast<float>(EIGEN_LOG2E) * ::logf(float(a)));
  623|      0|}
  624|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 sqrt(const bfloat16& a) { return bfloat16(::sqrtf(float(a))); }
  625|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 pow(const bfloat16& a, const bfloat16& b) {
  626|      0|  return bfloat16(::powf(float(a), float(b)));
  627|      0|}
  628|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 atan2(const bfloat16& a, const bfloat16& b) {
  629|      0|  return bfloat16(::atan2f(float(a), float(b)));
  630|      0|}
  631|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 sin(const bfloat16& a) { return bfloat16(::sinf(float(a))); }
  632|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 cos(const bfloat16& a) { return bfloat16(::cosf(float(a))); }
  633|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 tan(const bfloat16& a) { return bfloat16(::tanf(float(a))); }
  634|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 asin(const bfloat16& a) { return bfloat16(::asinf(float(a))); }
  635|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 acos(const bfloat16& a) { return bfloat16(::acosf(float(a))); }
  636|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 atan(const bfloat16& a) { return bfloat16(::atanf(float(a))); }
  637|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 sinh(const bfloat16& a) { return bfloat16(::sinhf(float(a))); }
  638|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 cosh(const bfloat16& a) { return bfloat16(::coshf(float(a))); }
  639|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 tanh(const bfloat16& a) { return bfloat16(::tanhf(float(a))); }
  640|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 asinh(const bfloat16& a) { return bfloat16(::asinhf(float(a))); }
  641|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 acosh(const bfloat16& a) { return bfloat16(::acoshf(float(a))); }
  642|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 atanh(const bfloat16& a) { return bfloat16(::atanhf(float(a))); }
  643|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 floor(const bfloat16& a) { return bfloat16(::floorf(float(a))); }
  644|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 ceil(const bfloat16& a) { return bfloat16(::ceilf(float(a))); }
  645|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 rint(const bfloat16& a) { return bfloat16(::rintf(float(a))); }
  646|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 round(const bfloat16& a) { return bfloat16(::roundf(float(a))); }
  647|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 trunc(const bfloat16& a) { return bfloat16(::truncf(float(a))); }
  648|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 fmod(const bfloat16& a, const bfloat16& b) {
  649|      0|  return bfloat16(::fmodf(float(a), float(b)));
  650|      0|}
  651|       |
  652|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16(min)(const bfloat16& a, const bfloat16& b) {
  653|      0|  const float f1 = static_cast<float>(a);
  654|      0|  const float f2 = static_cast<float>(b);
  655|      0|  return f2 < f1 ? b : a;
  656|      0|}
  657|       |
  658|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16(max)(const bfloat16& a, const bfloat16& b) {
  659|      0|  const float f1 = static_cast<float>(a);
  660|      0|  const float f2 = static_cast<float>(b);
  661|      0|  return f1 < f2 ? b : a;
  662|      0|}
  663|       |
  664|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 fmin(const bfloat16& a, const bfloat16& b) {
  665|      0|  const float f1 = static_cast<float>(a);
  666|      0|  const float f2 = static_cast<float>(b);
  667|      0|  return bfloat16(::fminf(f1, f2));
  668|      0|}
  669|       |
  670|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 fmax(const bfloat16& a, const bfloat16& b) {
  671|      0|  const float f1 = static_cast<float>(a);
  672|      0|  const float f2 = static_cast<float>(b);
  673|      0|  return bfloat16(::fmaxf(f1, f2));
  674|      0|}
  675|       |
  676|       |#ifndef EIGEN_NO_IO
  677|      0|EIGEN_ALWAYS_INLINE std::ostream& operator<<(std::ostream& os, const bfloat16& v) {
  678|      0|  os << static_cast<float>(v);
  679|      0|  return os;
  680|      0|}
  681|       |#endif
  682|       |
  683|       |}  // namespace bfloat16_impl
  684|       |
  685|       |namespace internal {
  686|       |
  687|       |template <>
  688|       |struct is_arithmetic<bfloat16> {
  689|       |  enum { value = true };
  690|       |};
  691|       |
  692|       |template <>
  693|       |struct random_impl<bfloat16> {
  694|       |  enum : int { MantissaBits = 7 };
  695|       |  using Impl = random_impl<float>;
  696|      0|  static EIGEN_DEVICE_FUNC inline bfloat16 run(const bfloat16& x, const bfloat16& y) {
  697|      0|    float result = Impl::run(x, y, MantissaBits);
  698|      0|    return bfloat16(result);
  699|      0|  }
  700|      0|  static EIGEN_DEVICE_FUNC inline bfloat16 run() {
  701|      0|    float result = Impl::run(MantissaBits);
  702|      0|    return bfloat16(result);
  703|      0|  }
  704|       |};
  705|       |
  706|       |}  // namespace internal
  707|       |
  708|       |template <>
  709|       |struct NumTraits<Eigen::bfloat16> : GenericNumTraits<Eigen::bfloat16> {
  710|       |  enum { IsSigned = true, IsInteger = false, IsComplex = false, RequireInitialization = false };
  711|       |
  712|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::bfloat16 epsilon() {
  713|      0|    return bfloat16_impl::raw_uint16_to_bfloat16(0x3c00);
  714|      0|  }
  715|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::bfloat16 dummy_precision() {
  716|      0|    return bfloat16_impl::raw_uint16_to_bfloat16(0x3D4D);  // bfloat16(5e-2f);
  717|      0|  }
  718|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::bfloat16 highest() {
  719|      0|    return bfloat16_impl::raw_uint16_to_bfloat16(0x7F7F);
  720|      0|  }
  721|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::bfloat16 lowest() {
  722|      0|    return bfloat16_impl::raw_uint16_to_bfloat16(0xFF7F);
  723|      0|  }
  724|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::bfloat16 infinity() {
  725|      0|    return bfloat16_impl::raw_uint16_to_bfloat16(0x7f80);
  726|      0|  }
  727|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::bfloat16 quiet_NaN() {
  728|      0|    return bfloat16_impl::raw_uint16_to_bfloat16(0x7fc0);
  729|      0|  }
  730|       |};
  731|       |
  732|       |}  // namespace Eigen
  733|       |
  734|       |#if defined(EIGEN_HAS_HIP_BF16)
  735|       |#pragma pop_macro("EIGEN_CONSTEXPR")
  736|       |#endif
  737|       |
  738|       |namespace Eigen {
  739|       |namespace numext {
  740|       |
  741|       |template <>
  742|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE bool(isnan)(const Eigen::bfloat16& h) {
  743|      0|  return (bfloat16_impl::isnan)(h);
  744|      0|}
  745|       |
  746|       |template <>
  747|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE bool(isinf)(const Eigen::bfloat16& h) {
  748|      0|  return (bfloat16_impl::isinf)(h);
  749|      0|}
  750|       |
  751|       |template <>
  752|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE bool(isfinite)(const Eigen::bfloat16& h) {
  753|      0|  return (bfloat16_impl::isfinite)(h);
  754|      0|}
  755|       |
  756|       |template <>
  757|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::bfloat16 bit_cast<Eigen::bfloat16, uint16_t>(const uint16_t& src) {
  758|      0|  return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(src);
  759|      0|}
  760|       |
  761|       |template <>
  762|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC uint16_t bit_cast<uint16_t, Eigen::bfloat16>(const Eigen::bfloat16& src) {
  763|      0|  return Eigen::bfloat16_impl::raw_bfloat16_as_uint16(src);
  764|      0|}
  765|       |
  766|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bfloat16 nextafter(const bfloat16& from, const bfloat16& to) {
  767|      0|  if (numext::isnan EIGEN_NOT_A_MACRO(from)) {
  768|      0|    return from;
  769|      0|  }
  770|      0|  if (numext::isnan EIGEN_NOT_A_MACRO(to)) {
  771|      0|    return to;
  772|      0|  }
  773|      0|  if (from == to) {
  774|      0|    return to;
  775|      0|  }
  776|      0|  uint16_t from_bits = numext::bit_cast<uint16_t>(from);
  777|      0|  bool from_sign = from_bits >> 15;
  778|      0|  // Whether we are adjusting toward the infinity with the same sign as from.
  779|      0|  bool toward_inf = (to > from) == !from_sign;
  780|      0|  if (toward_inf) {
  781|      0|    ++from_bits;
  782|      0|  } else if ((from_bits & 0x7fff) == 0) {
  783|      0|    // Adjusting away from inf, but from is zero, so just toggle the sign.
  784|      0|    from_bits ^= 0x8000;
  785|      0|  } else {
  786|      0|    --from_bits;
  787|      0|  }
  788|      0|  return numext::bit_cast<bfloat16>(from_bits);
  789|      0|}
  790|       |
  791|       |}  // namespace numext
  792|       |}  // namespace Eigen
  793|       |
  794|       |#if EIGEN_HAS_STD_HASH
  795|       |namespace std {
  796|       |template <>
  797|       |struct hash<Eigen::bfloat16> {
  798|      0|  EIGEN_STRONG_INLINE std::size_t operator()(const Eigen::bfloat16& a) const {
  799|      0|    return static_cast<std::size_t>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(a));
  800|      0|  }
  801|       |};
  802|       |}  // namespace std
  803|       |#endif
  804|       |
  805|       |// Add the missing shfl* intrinsics.
  806|       |// The __shfl* functions are only valid on HIP or _CUDA_ARCH_ >= 300.
  807|       |//   CUDA defines them for (__CUDA_ARCH__ >= 300 || !defined(__CUDA_ARCH__))
  808|       |//
  809|       |// HIP and CUDA prior to SDK 9.0 define
  810|       |//    __shfl, __shfl_up, __shfl_down, __shfl_xor for int and float
  811|       |// CUDA since 9.0 deprecates those and instead defines
  812|       |//    __shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync,
  813|       |//    with native support for __half and __nv_bfloat16
  814|       |//
  815|       |// Note that the following are __device__ - only functions.
  816|       |#if defined(EIGEN_HIPCC)
  817|       |
  818|       |#if defined(EIGEN_HAS_HIP_BF16)
  819|       |
  820|       |__device__ EIGEN_STRONG_INLINE Eigen::bfloat16 __shfl(Eigen::bfloat16 var, int srcLane, int width = warpSize) {
  821|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  822|       |  return Eigen::numext::bit_cast<Eigen::bfloat16>(static_cast<Eigen::numext::uint16_t>(__shfl(ivar, srcLane, width)));
  823|       |}
  824|       |
  825|       |__device__ EIGEN_STRONG_INLINE Eigen::bfloat16 __shfl_up(Eigen::bfloat16 var, unsigned int delta,
  826|       |                                                         int width = warpSize) {
  827|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  828|       |  return Eigen::numext::bit_cast<Eigen::bfloat16>(static_cast<Eigen::numext::uint16_t>(__shfl_up(ivar, delta, width)));
  829|       |}
  830|       |
  831|       |__device__ EIGEN_STRONG_INLINE Eigen::bfloat16 __shfl_down(Eigen::bfloat16 var, unsigned int delta,
  832|       |                                                           int width = warpSize) {
  833|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  834|       |  return Eigen::numext::bit_cast<Eigen::bfloat16>(
  835|       |      static_cast<Eigen::numext::uint16_t>(__shfl_down(ivar, delta, width)));
  836|       |}
  837|       |
  838|       |__device__ EIGEN_STRONG_INLINE Eigen::bfloat16 __shfl_xor(Eigen::bfloat16 var, int laneMask, int width = warpSize) {
  839|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  840|       |  return Eigen::numext::bit_cast<Eigen::bfloat16>(
  841|       |      static_cast<Eigen::numext::uint16_t>(__shfl_xor(ivar, laneMask, width)));
  842|       |}
  843|       |
  844|       |#endif  // HIP
  845|       |
  846|       |#endif  // __shfl*
  847|       |
  848|       |#if defined(EIGEN_HIPCC)
  849|       |EIGEN_STRONG_INLINE __device__ Eigen::bfloat16 __ldg(const Eigen::bfloat16* ptr) {
  850|       |  return Eigen::bfloat16_impl::raw_uint16_to_bfloat16(
  851|       |      __ldg(Eigen::numext::bit_cast<const Eigen::numext::uint16_t*>(ptr)));
  852|       |}
  853|       |#endif  // __ldg
  854|       |
  855|       |#endif  // EIGEN_BFLOAT16_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/Default/ConjHelper.h:
    1|       |
    2|       |// This file is part of Eigen, a lightweight C++ template library
    3|       |// for linear algebra.
    4|       |//
    5|       |// Copyright (C) 2017 Gael Guennebaud <gael.guennebaud@inria.fr>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_ARCH_CONJ_HELPER_H
   12|       |#define EIGEN_ARCH_CONJ_HELPER_H
   13|       |
   14|       |#define EIGEN_MAKE_CONJ_HELPER_CPLX_REAL(PACKET_CPLX, PACKET_REAL)                                                  \
   15|       |  template <>                                                                                                       \
   16|       |  struct conj_helper<PACKET_REAL, PACKET_CPLX, false, false> {                                                      \
   17|      0|    EIGEN_STRONG_INLINE PACKET_CPLX pmadd(const PACKET_REAL& x, const PACKET_CPLX& y, const PACKET_CPLX& c) const { \
   18|      0|      return padd(c, this->pmul(x, y));                                                                             \
   19|      0|    }                                                                                                               \
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperIDv4_fNS0_9Packet2cfELb0ELb0EE5pmaddERKS2_RKS3_S8_
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperIDv2_dNS0_9Packet1cdELb0ELb0EE5pmaddERKS2_RKS3_S8_
  ------------------
   20|      0|    EIGEN_STRONG_INLINE PACKET_CPLX pmul(const PACKET_REAL& x, const PACKET_CPLX& y) const {                        \
   21|      0|      return PACKET_CPLX(Eigen::internal::pmul<PACKET_REAL>(x, y.v));                                               \
   22|      0|    }                                                                                                               \
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperIDv4_fNS0_9Packet2cfELb0ELb0EE4pmulERKS2_RKS3_
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperIDv2_dNS0_9Packet1cdELb0ELb0EE4pmulERKS2_RKS3_
  ------------------
   23|       |  };                                                                                                                \
   24|       |                                                                                                                    \
   25|       |  template <>                                                                                                       \
   26|       |  struct conj_helper<PACKET_CPLX, PACKET_REAL, false, false> {                                                      \
   27|      0|    EIGEN_STRONG_INLINE PACKET_CPLX pmadd(const PACKET_CPLX& x, const PACKET_REAL& y, const PACKET_CPLX& c) const { \
   28|      0|      return padd(c, this->pmul(x, y));                                                                             \
   29|      0|    }                                                                                                               \
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperINS0_9Packet2cfEDv4_fLb0ELb0EE5pmaddERKS2_RKS3_S6_
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperINS0_9Packet1cdEDv2_dLb0ELb0EE5pmaddERKS2_RKS3_S6_
  ------------------
   30|      0|    EIGEN_STRONG_INLINE PACKET_CPLX pmul(const PACKET_CPLX& x, const PACKET_REAL& y) const {                        \
   31|      0|      return PACKET_CPLX(Eigen::internal::pmul<PACKET_REAL>(x.v, y));                                               \
   32|      0|    }                                                                                                               \
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperINS0_9Packet2cfEDv4_fLb0ELb0EE4pmulERKS2_RKS3_
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal11conj_helperINS0_9Packet1cdEDv2_dLb0ELb0EE4pmulERKS2_RKS3_
  ------------------
   33|       |  };
   34|       |
   35|       |// IWYU pragma: private
   36|       |#include "../../InternalHeaderCheck.h"
   37|       |
   38|       |namespace Eigen {
   39|       |namespace internal {
   40|       |
   41|       |template <bool Conjugate>
   42|       |struct conj_if;
   43|       |
   44|       |template <>
   45|       |struct conj_if<true> {
   46|       |  template <typename T>
   47|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T operator()(const T& x) const {
   48|       |    return numext::conj(x);
   49|       |  }
   50|       |  template <typename T>
   51|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T pconj(const T& x) const {
   52|       |    return internal::pconj(x);
   53|       |  }
   54|       |};
   55|       |
   56|       |template <>
   57|       |struct conj_if<false> {
   58|       |  template <typename T>
   59|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const T& operator()(const T& x) const {
   60|       |    return x;
   61|       |  }
   62|       |  template <typename T>
   63|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const T& pconj(const T& x) const {
   64|       |    return x;
   65|       |  }
   66|       |};
   67|       |
   68|       |// Generic Implementation, assume scalars since the packet-version is
   69|       |// specialized below.
   70|       |template <typename LhsType, typename RhsType, bool ConjLhs, bool ConjRhs>
   71|       |struct conj_helper {
   72|       |  typedef typename ScalarBinaryOpTraits<LhsType, RhsType>::ReturnType ResultType;
   73|       |
   74|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ResultType pmadd(const LhsType& x, const RhsType& y,
   75|       |                                                         const ResultType& c) const {
   76|       |    return this->pmul(x, y) + c;
   77|       |  }
   78|       |
   79|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ResultType pmul(const LhsType& x, const RhsType& y) const {
   80|       |    return conj_if<ConjLhs>()(x) * conj_if<ConjRhs>()(y);
   81|       |  }
   82|       |};
   83|       |
   84|       |template <typename LhsScalar, typename RhsScalar>
   85|       |struct conj_helper<LhsScalar, RhsScalar, true, true> {
   86|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar>::ReturnType ResultType;
   87|       |
   88|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ResultType pmadd(const LhsScalar& x, const RhsScalar& y,
   89|       |                                                         const ResultType& c) const {
   90|       |    return this->pmul(x, y) + c;
   91|       |  }
   92|       |
   93|       |  // We save a conjuation by using the identity conj(a)*conj(b) = conj(a*b).
   94|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ResultType pmul(const LhsScalar& x, const RhsScalar& y) const {
   95|       |    return numext::conj(x * y);
   96|       |  }
   97|       |};
   98|       |
   99|       |// Implementation with equal type, use packet operations.
  100|       |template <typename Packet, bool ConjLhs, bool ConjRhs>
  101|       |struct conj_helper<Packet, Packet, ConjLhs, ConjRhs> {
  102|       |  typedef Packet ResultType;
  103|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet pmadd(const Packet& x, const Packet& y, const Packet& c) const {
  104|       |    return Eigen::internal::pmadd(conj_if<ConjLhs>().pconj(x), conj_if<ConjRhs>().pconj(y), c);
  105|       |  }
  106|       |
  107|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet pmul(const Packet& x, const Packet& y) const {
  108|       |    return Eigen::internal::pmul(conj_if<ConjLhs>().pconj(x), conj_if<ConjRhs>().pconj(y));
  109|       |  }
  110|       |};
  111|       |
  112|       |template <typename Packet>
  113|       |struct conj_helper<Packet, Packet, true, true> {
  114|       |  typedef Packet ResultType;
  115|       |
  116|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet pmadd(const Packet& x, const Packet& y, const Packet& c) const {
  117|       |    return Eigen::internal::pmadd(pconj(x), pconj(y), c);
  118|       |  }
  119|       |  // We save a conjuation by using the identity conj(a)*conj(b) = conj(a*b).
  120|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet pmul(const Packet& x, const Packet& y) const {
  121|       |    return pconj(Eigen::internal::pmul(x, y));
  122|       |  }
  123|       |};
  124|       |
  125|       |}  // namespace internal
  126|       |}  // namespace Eigen
  127|       |
  128|       |#endif  // EIGEN_ARCH_CONJ_HELPER_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/Default/GenericPacketMathFunctions.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2007 Julien Pommier
    5|       |// Copyright (C) 2014 Pedro Gonnet (pedro.gonnet@gmail.com)
    6|       |// Copyright (C) 2009-2019 Gael Guennebaud <gael.guennebaud@inria.fr>
    7|       |//
    8|       |// This Source Code Form is subject to the terms of the Mozilla
    9|       |// Public License v. 2.0. If a copy of the MPL was not distributed
   10|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   11|       |
   12|       |/* The exp and log functions of this file initially come from
   13|       | * Julien Pommier's sse math library: http://gruntthepeon.free.fr/ssemath/
   14|       | */
   15|       |
   16|       |#ifndef EIGEN_ARCH_GENERIC_PACKET_MATH_FUNCTIONS_H
   17|       |#define EIGEN_ARCH_GENERIC_PACKET_MATH_FUNCTIONS_H
   18|       |
   19|       |// IWYU pragma: private
   20|       |#include "../../InternalHeaderCheck.h"
   21|       |
   22|       |namespace Eigen {
   23|       |namespace internal {
   24|       |
   25|       |// Creates a Scalar integer type with same bit-width.
   26|       |template <typename T>
   27|       |struct make_integer;
   28|       |template <>
   29|       |struct make_integer<float> {
   30|       |  typedef numext::int32_t type;
   31|       |};
   32|       |template <>
   33|       |struct make_integer<double> {
   34|       |  typedef numext::int64_t type;
   35|       |};
   36|       |template <>
   37|       |struct make_integer<half> {
   38|       |  typedef numext::int16_t type;
   39|       |};
   40|       |template <>
   41|       |struct make_integer<bfloat16> {
   42|       |  typedef numext::int16_t type;
   43|       |};
   44|       |
   45|       |/* polevl (modified for Eigen)
   46|       | *
   47|       | *      Evaluate polynomial
   48|       | *
   49|       | *
   50|       | *
   51|       | * SYNOPSIS:
   52|       | *
   53|       | * int N;
   54|       | * Scalar x, y, coef[N+1];
   55|       | *
   56|       | * y = polevl<decltype(x), N>( x, coef);
   57|       | *
   58|       | *
   59|       | *
   60|       | * DESCRIPTION:
   61|       | *
   62|       | * Evaluates polynomial of degree N:
   63|       | *
   64|       | *                     2          N
   65|       | * y  =  C  + C x + C x  +...+ C x
   66|       | *        0    1     2          N
   67|       | *
   68|       | * Coefficients are stored in reverse order:
   69|       | *
   70|       | * coef[0] = C  , ..., coef[N] = C  .
   71|       | *            N                   0
   72|       | *
   73|       | *  The function p1evl() assumes that coef[N] = 1.0 and is
   74|       | * omitted from the array.  Its calling arguments are
   75|       | * otherwise the same as polevl().
   76|       | *
   77|       | *
   78|       | * The Eigen implementation is templatized.  For best speed, store
   79|       | * coef as a const array (constexpr), e.g.
   80|       | *
   81|       | * const double coef[] = {1.0, 2.0, 3.0, ...};
   82|       | *
   83|       | */
   84|       |template <typename Packet, int N>
   85|       |struct ppolevl {
   86|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x,
   87|      0|                                                          const typename unpacket_traits<Packet>::type coeff[]) {
   88|      0|    EIGEN_STATIC_ASSERT((N > 0), YOU_MADE_A_PROGRAMMING_MISTAKE);
   89|      0|    return pmadd(ppolevl<Packet, N - 1>::run(x, coeff), x, pset1<Packet>(coeff[N]));
   90|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIfLi3EE3runERKfPS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIfLi2EE3runERKfPS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIfLi1EE3runERKfPS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIfLi4EE3runERKfPS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv4_fLi4EE3runERKS2_PKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv4_fLi3EE3runERKS2_PKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv4_fLi2EE3runERKS2_PKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv4_fLi1EE3runERKS2_PKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi4EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi3EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi2EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi1EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi5EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi8EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi7EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi6EE3runERKS2_PKd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi9EE3runERKS2_PKd
  ------------------
   91|       |};
   92|       |
   93|       |template <typename Packet>
   94|       |struct ppolevl<Packet, 0> {
   95|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x,
   96|      0|                                                          const typename unpacket_traits<Packet>::type coeff[]) {
   97|      0|    EIGEN_UNUSED_VARIABLE(x);
   98|      0|    return pset1<Packet>(coeff[0]);
   99|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIfLi0EE3runERKfPS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv4_fLi0EE3runERKS2_PKf
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7ppolevlIDv2_dLi0EE3runERKS2_PKd
  ------------------
  100|       |};
  101|       |
  102|       |/* chbevl (modified for Eigen)
  103|       | *
  104|       | *     Evaluate Chebyshev series
  105|       | *
  106|       | *
  107|       | *
  108|       | * SYNOPSIS:
  109|       | *
  110|       | * int N;
  111|       | * Scalar x, y, coef[N], chebevl();
  112|       | *
  113|       | * y = chbevl( x, coef, N );
  114|       | *
  115|       | *
  116|       | *
  117|       | * DESCRIPTION:
  118|       | *
  119|       | * Evaluates the series
  120|       | *
  121|       | *        N-1
  122|       | *         - '
  123|       | *  y  =   >   coef[i] T (x/2)
  124|       | *         -            i
  125|       | *        i=0
  126|       | *
  127|       | * of Chebyshev polynomials Ti at argument x/2.
  128|       | *
  129|       | * Coefficients are stored in reverse order, i.e. the zero
  130|       | * order term is last in the array.  Note N is the number of
  131|       | * coefficients, not the order.
  132|       | *
  133|       | * If coefficients are for the interval a to b, x must
  134|       | * have been transformed to x -> 2(2x - b - a)/(b-a) before
  135|       | * entering the routine.  This maps x from (a, b) to (-1, 1),
  136|       | * over which the Chebyshev polynomials are defined.
  137|       | *
  138|       | * If the coefficients are for the inverted interval, in
  139|       | * which (a, b) is mapped to (1/b, 1/a), the transformation
  140|       | * required is x -> 2(2ab/x - b - a)/(b-a).  If b is infinity,
  141|       | * this becomes x -> 4a/x - 1.
  142|       | *
  143|       | *
  144|       | *
  145|       | * SPEED:
  146|       | *
  147|       | * Taking advantage of the recurrence properties of the
  148|       | * Chebyshev polynomials, the routine requires one more
  149|       | * addition per loop than evaluating a nested polynomial of
  150|       | * the same degree.
  151|       | *
  152|       | */
  153|       |
  154|       |template <typename Packet, int N>
  155|       |struct pchebevl {
  156|       |  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE Packet run(Packet x,
  157|       |                                                          const typename unpacket_traits<Packet>::type coef[]) {
  158|       |    typedef typename unpacket_traits<Packet>::type Scalar;
  159|       |    Packet b0 = pset1<Packet>(coef[0]);
  160|       |    Packet b1 = pset1<Packet>(static_cast<Scalar>(0.f));
  161|       |    Packet b2;
  162|       |
  163|       |    for (int i = 1; i < N; i++) {
  164|       |      b2 = b1;
  165|       |      b1 = b0;
  166|       |      b0 = psub(pmadd(x, b1, pset1<Packet>(coef[i])), b2);
  167|       |    }
  168|       |
  169|       |    return pmul(pset1<Packet>(static_cast<Scalar>(0.5f)), psub(b0, b2));
  170|       |  }
  171|       |};
  172|       |
  173|       |template <typename Packet>
  174|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pfrexp_generic_get_biased_exponent(const Packet& a) {
  175|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
  176|      0|  typedef typename unpacket_traits<Packet>::integer_packet PacketI;
  177|      0|  static constexpr int mantissa_bits = numext::numeric_limits<Scalar>::digits - 1;
  178|      0|  return pcast<PacketI, Packet>(plogical_shift_right<mantissa_bits>(preinterpret<PacketI>(pabs(a))));
  179|      0|}
  180|       |
  181|       |// Safely applies frexp, correctly handles denormals.
  182|       |// Assumes IEEE floating point format.
  183|       |template <typename Packet>
  184|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pfrexp_generic(const Packet& a, Packet& exponent) {
  185|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
  186|      0|  typedef typename make_unsigned<typename make_integer<Scalar>::type>::type ScalarUI;
  187|      0|  static constexpr int TotalBits = sizeof(Scalar) * CHAR_BIT, MantissaBits = numext::numeric_limits<Scalar>::digits - 1,
  188|      0|                       ExponentBits = TotalBits - MantissaBits - 1;
  189|      0|
  190|      0|  EIGEN_CONSTEXPR ScalarUI scalar_sign_mantissa_mask =
  191|      0|      ~(((ScalarUI(1) << ExponentBits) - ScalarUI(1)) << MantissaBits);  // ~0x7f800000
  192|      0|  const Packet sign_mantissa_mask = pset1frombits<Packet>(static_cast<ScalarUI>(scalar_sign_mantissa_mask));
  193|      0|  const Packet half = pset1<Packet>(Scalar(0.5));
  194|      0|  const Packet zero = pzero(a);
  195|      0|  const Packet normal_min = pset1<Packet>((numext::numeric_limits<Scalar>::min)());  // Minimum normal value, 2^-126
  196|      0|
  197|      0|  // To handle denormals, normalize by multiplying by 2^(int(MantissaBits)+1).
  198|      0|  const Packet is_denormal = pcmp_lt(pabs(a), normal_min);
  199|      0|  EIGEN_CONSTEXPR ScalarUI scalar_normalization_offset = ScalarUI(MantissaBits + 1);  // 24
  200|      0|  // The following cannot be constexpr because bfloat16(uint16_t) is not constexpr.
  201|      0|  const Scalar scalar_normalization_factor = Scalar(ScalarUI(1) << int(scalar_normalization_offset));  // 2^24
  202|      0|  const Packet normalization_factor = pset1<Packet>(scalar_normalization_factor);
  203|      0|  const Packet normalized_a = pselect(is_denormal, pmul(a, normalization_factor), a);
  204|      0|
  205|      0|  // Determine exponent offset: -126 if normal, -126-24 if denormal
  206|      0|  const Scalar scalar_exponent_offset = -Scalar((ScalarUI(1) << (ExponentBits - 1)) - ScalarUI(2));  // -126
  207|      0|  Packet exponent_offset = pset1<Packet>(scalar_exponent_offset);
  208|      0|  const Packet normalization_offset = pset1<Packet>(-Scalar(scalar_normalization_offset));  // -24
  209|      0|  exponent_offset = pselect(is_denormal, padd(exponent_offset, normalization_offset), exponent_offset);
  210|      0|
  211|      0|  // Determine exponent and mantissa from normalized_a.
  212|      0|  exponent = pfrexp_generic_get_biased_exponent(normalized_a);
  213|      0|  // Zero, Inf and NaN return 'a' unmodified, exponent is zero
  214|      0|  // (technically the exponent is unspecified for inf/NaN, but GCC/Clang set it to zero)
  215|      0|  const Scalar scalar_non_finite_exponent = Scalar((ScalarUI(1) << ExponentBits) - ScalarUI(1));  // 255
  216|      0|  const Packet non_finite_exponent = pset1<Packet>(scalar_non_finite_exponent);
  217|      0|  const Packet is_zero_or_not_finite = por(pcmp_eq(a, zero), pcmp_eq(exponent, non_finite_exponent));
  218|      0|  const Packet m = pselect(is_zero_or_not_finite, a, por(pand(normalized_a, sign_mantissa_mask), half));
  219|      0|  exponent = pselect(is_zero_or_not_finite, zero, padd(exponent, exponent_offset));
  220|      0|  return m;
  221|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14pfrexp_genericIDv4_fEET_RKS3_RS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14pfrexp_genericIDv2_dEET_RKS3_RS3_
  ------------------
  222|       |
  223|       |// Safely applies ldexp, correctly handles overflows, underflows and denormals.
  224|       |// Assumes IEEE floating point format.
  225|       |template <typename Packet>
  226|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pldexp_generic(const Packet& a, const Packet& exponent) {
  227|      0|  // We want to return a * 2^exponent, allowing for all possible integer
  228|      0|  // exponents without overflowing or underflowing in intermediate
  229|      0|  // computations.
  230|      0|  //
  231|      0|  // Since 'a' and the output can be denormal, the maximum range of 'exponent'
  232|      0|  // to consider for a float is:
  233|      0|  //   -255-23 -> 255+23
  234|      0|  // Below -278 any finite float 'a' will become zero, and above +278 any
  235|      0|  // finite float will become inf, including when 'a' is the smallest possible
  236|      0|  // denormal.
  237|      0|  //
  238|      0|  // Unfortunately, 2^(278) cannot be represented using either one or two
  239|      0|  // finite normal floats, so we must split the scale factor into at least
  240|      0|  // three parts. It turns out to be faster to split 'exponent' into four
  241|      0|  // factors, since [exponent>>2] is much faster to compute that [exponent/3].
  242|      0|  //
  243|      0|  // Set e = min(max(exponent, -278), 278);
  244|      0|  //     b = floor(e/4);
  245|      0|  //   out = ((((a * 2^(b)) * 2^(b)) * 2^(b)) * 2^(e-3*b))
  246|      0|  //
  247|      0|  // This will avoid any intermediate overflows and correctly handle 0, inf,
  248|      0|  // NaN cases.
  249|      0|  typedef typename unpacket_traits<Packet>::integer_packet PacketI;
  250|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
  251|      0|  typedef typename unpacket_traits<PacketI>::type ScalarI;
  252|      0|  static constexpr int TotalBits = sizeof(Scalar) * CHAR_BIT, MantissaBits = numext::numeric_limits<Scalar>::digits - 1,
  253|      0|                       ExponentBits = TotalBits - MantissaBits - 1;
  254|      0|
  255|      0|  const Packet max_exponent = pset1<Packet>(Scalar((ScalarI(1) << ExponentBits) + ScalarI(MantissaBits - 1)));  // 278
  256|      0|  const PacketI bias = pset1<PacketI>((ScalarI(1) << (ExponentBits - 1)) - ScalarI(1));                         // 127
  257|      0|  const PacketI e = pcast<Packet, PacketI>(pmin(pmax(exponent, pnegate(max_exponent)), max_exponent));
  258|      0|  PacketI b = parithmetic_shift_right<2>(e);                                          // floor(e/4);
  259|      0|  Packet c = preinterpret<Packet>(plogical_shift_left<MantissaBits>(padd(b, bias)));  // 2^b
  260|      0|  Packet out = pmul(pmul(pmul(a, c), c), c);                                          // a * 2^(3b)
  261|      0|  b = pnmadd(pset1<PacketI>(3), b, e);                                                // e - 3b
  262|      0|  c = preinterpret<Packet>(plogical_shift_left<MantissaBits>(padd(b, bias)));         // 2^(e-3*b)
  263|      0|  out = pmul(out, c);
  264|      0|  return out;
  265|      0|}
  266|       |
  267|       |// Explicitly multiplies
  268|       |//    a * (2^e)
  269|       |// clamping e to the range
  270|       |// [NumTraits<Scalar>::min_exponent()-2, NumTraits<Scalar>::max_exponent()]
  271|       |//
  272|       |// This is approx 7x faster than pldexp_impl, but will prematurely over/underflow
  273|       |// if 2^e doesn't fit into a normal floating-point Scalar.
  274|       |//
  275|       |// Assumes IEEE floating point format
  276|       |template <typename Packet>
  277|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pldexp_fast(const Packet& a, const Packet& exponent) {
  278|      0|  typedef typename unpacket_traits<Packet>::integer_packet PacketI;
  279|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
  280|      0|  typedef typename unpacket_traits<PacketI>::type ScalarI;
  281|      0|  static constexpr int TotalBits = sizeof(Scalar) * CHAR_BIT, MantissaBits = numext::numeric_limits<Scalar>::digits - 1,
  282|      0|                       ExponentBits = TotalBits - MantissaBits - 1;
  283|      0|
  284|      0|  const Packet bias = pset1<Packet>(Scalar((ScalarI(1) << (ExponentBits - 1)) - ScalarI(1)));  // 127
  285|      0|  const Packet limit = pset1<Packet>(Scalar((ScalarI(1) << ExponentBits) - ScalarI(1)));       // 255
  286|      0|  // restrict biased exponent between 0 and 255 for float.
  287|      0|  const PacketI e = pcast<Packet, PacketI>(pmin(pmax(padd(exponent, bias), pzero(limit)), limit));  // exponent + 127
  288|      0|  // return a * (2^e)
  289|      0|  return pmul(a, preinterpret<Packet>(plogical_shift_left<MantissaBits>(e)));
  290|      0|}
  291|       |
  292|       |// Natural or base 2 logarithm.
  293|       |// Computes log(x) as log(2^e * m) = C*e + log(m), where the constant C =log(2)
  294|       |// and m is in the range [sqrt(1/2),sqrt(2)). In this range, the logarithm can
  295|       |// be easily approximated by a polynomial centered on m=1 for stability.
  296|       |// TODO(gonnet): Further reduce the interval allowing for lower-degree
  297|       |//               polynomial interpolants -> ... -> profit!
  298|       |template <typename Packet, bool base2>
  299|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_impl_float(const Packet _x) {
  300|      0|  const Packet cst_1 = pset1<Packet>(1.0f);
  301|      0|  const Packet cst_minus_inf = pset1frombits<Packet>(static_cast<Eigen::numext::uint32_t>(0xff800000u));
  302|      0|  const Packet cst_pos_inf = pset1frombits<Packet>(static_cast<Eigen::numext::uint32_t>(0x7f800000u));
  303|      0|
  304|      0|  const Packet cst_cephes_SQRTHF = pset1<Packet>(0.707106781186547524f);
  305|      0|  Packet e, x;
  306|      0|  // extract significant in the range [0.5,1) and exponent
  307|      0|  x = pfrexp(_x, e);
  308|      0|
  309|      0|  // part2: Shift the inputs from the range [0.5,1) to [sqrt(1/2),sqrt(2))
  310|      0|  // and shift by -1. The values are then centered around 0, which improves
  311|      0|  // the stability of the polynomial evaluation.
  312|      0|  //   if( x < SQRTHF ) {
  313|      0|  //     e -= 1;
  314|      0|  //     x = x + x - 1.0;
  315|      0|  //   } else { x = x - 1.0; }
  316|      0|  Packet mask = pcmp_lt(x, cst_cephes_SQRTHF);
  317|      0|  Packet tmp = pand(x, mask);
  318|      0|  x = psub(x, cst_1);
  319|      0|  e = psub(e, pand(cst_1, mask));
  320|      0|  x = padd(x, tmp);
  321|      0|
  322|      0|  // Polynomial coefficients for rational r(x) = p(x)/q(x)
  323|      0|  // approximating log(1+x) on [sqrt(0.5)-1;sqrt(2)-1].
  324|      0|  constexpr float alpha[] = {0.18256296349849254f, 1.0000000190281063f, 1.0000000190281136f};
  325|      0|  constexpr float beta[] = {0.049616247954120038f, 0.59923249590823520f, 1.4999999999999927f, 1.0f};
  326|      0|
  327|      0|  Packet p = ppolevl<Packet, 2>::run(x, alpha);
  328|      0|  p = pmul(x, p);
  329|      0|  Packet q = ppolevl<Packet, 3>::run(x, beta);
  330|      0|  x = pdiv(p, q);
  331|      0|
  332|      0|  // Add the logarithm of the exponent back to the result of the interpolation.
  333|      0|  if (base2) {
  334|      0|    const Packet cst_log2e = pset1<Packet>(static_cast<float>(EIGEN_LOG2E));
  335|      0|    x = pmadd(x, cst_log2e, e);
  336|      0|  } else {
  337|      0|    const Packet cst_ln2 = pset1<Packet>(static_cast<float>(EIGEN_LN2));
  338|      0|    x = pmadd(e, cst_ln2, x);
  339|      0|  }
  340|      0|
  341|      0|  Packet invalid_mask = pcmp_lt_or_nan(_x, pzero(_x));
  342|      0|  Packet iszero_mask = pcmp_eq(_x, pzero(_x));
  343|      0|  Packet pos_inf_mask = pcmp_eq(_x, cst_pos_inf);
  344|      0|  // Filter out invalid inputs, i.e.:
  345|      0|  //  - negative arg will be NAN
  346|      0|  //  - 0 will be -INF
  347|      0|  //  - +INF will be +INF
  348|      0|  return pselect(iszero_mask, cst_minus_inf, por(pselect(pos_inf_mask, cst_pos_inf, x), invalid_mask));
  349|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal15plog_impl_floatIDv4_fLb0EEET_S3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal15plog_impl_floatIDv4_fLb1EEET_S3_
  ------------------
  350|       |
  351|       |template <typename Packet>
  352|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_float(const Packet _x) {
  353|      0|  return plog_impl_float<Packet, /* base2 */ false>(_x);
  354|      0|}
  355|       |
  356|       |template <typename Packet>
  357|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog2_float(const Packet _x) {
  358|      0|  return plog_impl_float<Packet, /* base2 */ true>(_x);
  359|      0|}
  360|       |
  361|       |/* Returns the base e (2.718...) or base 2 logarithm of x.
  362|       | * The argument is separated into its exponent and fractional parts.
  363|       | * The logarithm of the fraction in the interval [sqrt(1/2), sqrt(2)],
  364|       | * is approximated by
  365|       | *
  366|       | *     log(1+x) = x - 0.5 x**2 + x**3 P(x)/Q(x).
  367|       | *
  368|       | * for more detail see: http://www.netlib.org/cephes/
  369|       | */
  370|       |template <typename Packet, bool base2>
  371|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_impl_double(const Packet _x) {
  372|      0|  Packet x = _x;
  373|      0|
  374|      0|  const Packet cst_1 = pset1<Packet>(1.0);
  375|      0|  const Packet cst_neg_half = pset1<Packet>(-0.5);
  376|      0|  const Packet cst_minus_inf = pset1frombits<Packet>(static_cast<uint64_t>(0xfff0000000000000ull));
  377|      0|  const Packet cst_pos_inf = pset1frombits<Packet>(static_cast<uint64_t>(0x7ff0000000000000ull));
  378|      0|
  379|      0|  // Polynomial Coefficients for log(1+x) = x - x**2/2 + x**3 P(x)/Q(x)
  380|      0|  //                             1/sqrt(2) <= x < sqrt(2)
  381|      0|  const Packet cst_cephes_SQRTHF = pset1<Packet>(0.70710678118654752440E0);
  382|      0|  const Packet cst_cephes_log_p0 = pset1<Packet>(1.01875663804580931796E-4);
  383|      0|  const Packet cst_cephes_log_p1 = pset1<Packet>(4.97494994976747001425E-1);
  384|      0|  const Packet cst_cephes_log_p2 = pset1<Packet>(4.70579119878881725854E0);
  385|      0|  const Packet cst_cephes_log_p3 = pset1<Packet>(1.44989225341610930846E1);
  386|      0|  const Packet cst_cephes_log_p4 = pset1<Packet>(1.79368678507819816313E1);
  387|      0|  const Packet cst_cephes_log_p5 = pset1<Packet>(7.70838733755885391666E0);
  388|      0|
  389|      0|  const Packet cst_cephes_log_q0 = pset1<Packet>(1.0);
  390|      0|  const Packet cst_cephes_log_q1 = pset1<Packet>(1.12873587189167450590E1);
  391|      0|  const Packet cst_cephes_log_q2 = pset1<Packet>(4.52279145837532221105E1);
  392|      0|  const Packet cst_cephes_log_q3 = pset1<Packet>(8.29875266912776603211E1);
  393|      0|  const Packet cst_cephes_log_q4 = pset1<Packet>(7.11544750618563894466E1);
  394|      0|  const Packet cst_cephes_log_q5 = pset1<Packet>(2.31251620126765340583E1);
  395|      0|
  396|      0|  Packet e;
  397|      0|  // extract significant in the range [0.5,1) and exponent
  398|      0|  x = pfrexp(x, e);
  399|      0|
  400|      0|  // Shift the inputs from the range [0.5,1) to [sqrt(1/2),sqrt(2))
  401|      0|  // and shift by -1. The values are then centered around 0, which improves
  402|      0|  // the stability of the polynomial evaluation.
  403|      0|  //   if( x < SQRTHF ) {
  404|      0|  //     e -= 1;
  405|      0|  //     x = x + x - 1.0;
  406|      0|  //   } else { x = x - 1.0; }
  407|      0|  Packet mask = pcmp_lt(x, cst_cephes_SQRTHF);
  408|      0|  Packet tmp = pand(x, mask);
  409|      0|  x = psub(x, cst_1);
  410|      0|  e = psub(e, pand(cst_1, mask));
  411|      0|  x = padd(x, tmp);
  412|      0|
  413|      0|  Packet x2 = pmul(x, x);
  414|      0|  Packet x3 = pmul(x2, x);
  415|      0|
  416|      0|  // Evaluate the polynomial approximant , probably to improve instruction-level parallelism.
  417|      0|  // y = x - 0.5*x^2 + x^3 * polevl( x, P, 5 ) / p1evl( x, Q, 5 ) );
  418|      0|  Packet y, y1, y_;
  419|      0|  y = pmadd(cst_cephes_log_p0, x, cst_cephes_log_p1);
  420|      0|  y1 = pmadd(cst_cephes_log_p3, x, cst_cephes_log_p4);
  421|      0|  y = pmadd(y, x, cst_cephes_log_p2);
  422|      0|  y1 = pmadd(y1, x, cst_cephes_log_p5);
  423|      0|  y_ = pmadd(y, x3, y1);
  424|      0|
  425|      0|  y = pmadd(cst_cephes_log_q0, x, cst_cephes_log_q1);
  426|      0|  y1 = pmadd(cst_cephes_log_q3, x, cst_cephes_log_q4);
  427|      0|  y = pmadd(y, x, cst_cephes_log_q2);
  428|      0|  y1 = pmadd(y1, x, cst_cephes_log_q5);
  429|      0|  y = pmadd(y, x3, y1);
  430|      0|
  431|      0|  y_ = pmul(y_, x3);
  432|      0|  y = pdiv(y_, y);
  433|      0|
  434|      0|  y = pmadd(cst_neg_half, x2, y);
  435|      0|  x = padd(x, y);
  436|      0|
  437|      0|  // Add the logarithm of the exponent back to the result of the interpolation.
  438|      0|  if (base2) {
  439|      0|    const Packet cst_log2e = pset1<Packet>(static_cast<double>(EIGEN_LOG2E));
  440|      0|    x = pmadd(x, cst_log2e, e);
  441|      0|  } else {
  442|      0|    const Packet cst_ln2 = pset1<Packet>(static_cast<double>(EIGEN_LN2));
  443|      0|    x = pmadd(e, cst_ln2, x);
  444|      0|  }
  445|      0|
  446|      0|  Packet invalid_mask = pcmp_lt_or_nan(_x, pzero(_x));
  447|      0|  Packet iszero_mask = pcmp_eq(_x, pzero(_x));
  448|      0|  Packet pos_inf_mask = pcmp_eq(_x, cst_pos_inf);
  449|      0|  // Filter out invalid inputs, i.e.:
  450|      0|  //  - negative arg will be NAN
  451|      0|  //  - 0 will be -INF
  452|      0|  //  - +INF will be +INF
  453|      0|  return pselect(iszero_mask, cst_minus_inf, por(pselect(pos_inf_mask, cst_pos_inf, x), invalid_mask));
  454|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal16plog_impl_doubleIDv2_dLb0EEET_S3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal16plog_impl_doubleIDv2_dLb1EEET_S3_
  ------------------
  455|       |
  456|       |template <typename Packet>
  457|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_double(const Packet _x) {
  458|      0|  return plog_impl_double<Packet, /* base2 */ false>(_x);
  459|      0|}
  460|       |
  461|       |template <typename Packet>
  462|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog2_double(const Packet _x) {
  463|      0|  return plog_impl_double<Packet, /* base2 */ true>(_x);
  464|      0|}
  465|       |
  466|       |/** \internal \returns log(1 + x) computed using W. Kahan's formula.
  467|       |    See: http://www.plunk.org/~hatch/rightway.php
  468|       | */
  469|       |template <typename Packet>
  470|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_log1p(const Packet& x) {
  471|      0|  typedef typename unpacket_traits<Packet>::type ScalarType;
  472|      0|  const Packet one = pset1<Packet>(ScalarType(1));
  473|      0|  Packet xp1 = padd(x, one);
  474|      0|  Packet small_mask = pcmp_eq(xp1, one);
  475|      0|  Packet log1 = plog(xp1);
  476|      0|  Packet inf_mask = pcmp_eq(xp1, log1);
  477|      0|  Packet log_large = pmul(x, pdiv(log1, psub(xp1, one)));
  478|      0|  return pselect(por(small_mask, inf_mask), x, log_large);
  479|      0|}
  480|       |
  481|       |/** \internal \returns exp(x)-1 computed using W. Kahan's formula.
  482|       |    See: http://www.plunk.org/~hatch/rightway.php
  483|       | */
  484|       |template <typename Packet>
  485|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_expm1(const Packet& x) {
  486|      0|  typedef typename unpacket_traits<Packet>::type ScalarType;
  487|      0|  const Packet one = pset1<Packet>(ScalarType(1));
  488|      0|  const Packet neg_one = pset1<Packet>(ScalarType(-1));
  489|      0|  Packet u = pexp(x);
  490|      0|  Packet one_mask = pcmp_eq(u, one);
  491|      0|  Packet u_minus_one = psub(u, one);
  492|      0|  Packet neg_one_mask = pcmp_eq(u_minus_one, neg_one);
  493|      0|  Packet logu = plog(u);
  494|      0|  // The following comparison is to catch the case where
  495|      0|  // exp(x) = +inf. It is written in this way to avoid having
  496|      0|  // to form the constant +inf, which depends on the packet
  497|      0|  // type.
  498|      0|  Packet pos_inf_mask = pcmp_eq(logu, u);
  499|      0|  Packet expm1 = pmul(u_minus_one, pdiv(x, logu));
  500|      0|  expm1 = pselect(pos_inf_mask, u, expm1);
  501|      0|  return pselect(one_mask, x, pselect(neg_one_mask, neg_one, expm1));
  502|      0|}
  503|       |
  504|       |// Exponential function. Works by writing "x = m*log(2) + r" where
  505|       |// "m = floor(x/log(2)+1/2)" and "r" is the remainder. The result is then
  506|       |// "exp(x) = 2^m*exp(r)" where exp(r) is in the range [-1,1).
  507|       |// exp(r) is computed using a 6th order minimax polynomial approximation.
  508|       |template <typename Packet>
  509|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp_float(const Packet _x) {
  510|      0|  const Packet cst_zero = pset1<Packet>(0.0f);
  511|      0|  const Packet cst_one = pset1<Packet>(1.0f);
  512|      0|  const Packet cst_half = pset1<Packet>(0.5f);
  513|      0|  const Packet cst_exp_hi = pset1<Packet>(88.723f);
  514|      0|  const Packet cst_exp_lo = pset1<Packet>(-104.f);
  515|      0|  const Packet cst_pldexp_threshold = pset1<Packet>(87.0);
  516|      0|
  517|      0|  const Packet cst_cephes_LOG2EF = pset1<Packet>(1.44269504088896341f);
  518|      0|  const Packet cst_p2 = pset1<Packet>(0.49999988079071044921875f);
  519|      0|  const Packet cst_p3 = pset1<Packet>(0.16666518151760101318359375f);
  520|      0|  const Packet cst_p4 = pset1<Packet>(4.166965186595916748046875e-2f);
  521|      0|  const Packet cst_p5 = pset1<Packet>(8.36894474923610687255859375e-3f);
  522|      0|  const Packet cst_p6 = pset1<Packet>(1.37449637986719608306884765625e-3f);
  523|      0|
  524|      0|  // Clamp x.
  525|      0|  Packet zero_mask = pcmp_lt(_x, cst_exp_lo);
  526|      0|  Packet x = pmin(_x, cst_exp_hi);
  527|      0|
  528|      0|  // Express exp(x) as exp(m*ln(2) + r), start by extracting
  529|      0|  // m = floor(x/ln(2) + 0.5).
  530|      0|  Packet m = pfloor(pmadd(x, cst_cephes_LOG2EF, cst_half));
  531|      0|
  532|      0|  // Get r = x - m*ln(2). If no FMA instructions are available, m*ln(2) is
  533|      0|  // subtracted out in two parts, m*C1+m*C2 = m*ln(2), to avoid accumulating
  534|      0|  // truncation errors.
  535|      0|  const Packet cst_cephes_exp_C1 = pset1<Packet>(-0.693359375f);
  536|      0|  const Packet cst_cephes_exp_C2 = pset1<Packet>(2.12194440e-4f);
  537|      0|  Packet r = pmadd(m, cst_cephes_exp_C1, x);
  538|      0|  r = pmadd(m, cst_cephes_exp_C2, r);
  539|      0|
  540|      0|  // Evaluate the 6th order polynomial approximation to exp(r)
  541|      0|  // with r in the interval [-ln(2)/2;ln(2)/2].
  542|      0|  const Packet r2 = pmul(r, r);
  543|      0|  Packet p_even = pmadd(r2, cst_p6, cst_p4);
  544|      0|  const Packet p_odd = pmadd(r2, cst_p5, cst_p3);
  545|      0|  p_even = pmadd(r2, p_even, cst_p2);
  546|      0|  const Packet p_low = padd(r, cst_one);
  547|      0|  Packet y = pmadd(r, p_odd, p_even);
  548|      0|  y = pmadd(r2, y, p_low);
  549|      0|
  550|      0|  // Return 2^m * exp(r).
  551|      0|  const Packet fast_pldexp_unsafe = pcmp_lt(cst_pldexp_threshold, pabs(x));
  552|      0|  if (!predux_any(fast_pldexp_unsafe)) {
  553|      0|    // For |x| <= 87, we know the result is not zero or inf, and we can safely use
  554|      0|    // the fast version of pldexp.
  555|      0|    return pmax(pldexp_fast(y, m), _x);
  556|      0|  }
  557|      0|  return pselect(zero_mask, cst_zero, pmax(pldexp(y, m), _x));
  558|      0|}
  559|       |
  560|       |template <typename Packet>
  561|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp_double(const Packet _x) {
  562|      0|  Packet x = _x;
  563|      0|  const Packet cst_zero = pset1<Packet>(0.0);
  564|      0|  const Packet cst_1 = pset1<Packet>(1.0);
  565|      0|  const Packet cst_2 = pset1<Packet>(2.0);
  566|      0|  const Packet cst_half = pset1<Packet>(0.5);
  567|      0|
  568|      0|  const Packet cst_exp_hi = pset1<Packet>(709.784);
  569|      0|  const Packet cst_exp_lo = pset1<Packet>(-745.519);
  570|      0|  const Packet cst_pldexp_threshold = pset1<Packet>(708.0);
  571|      0|  const Packet cst_cephes_LOG2EF = pset1<Packet>(1.4426950408889634073599);
  572|      0|  const Packet cst_cephes_exp_p0 = pset1<Packet>(1.26177193074810590878e-4);
  573|      0|  const Packet cst_cephes_exp_p1 = pset1<Packet>(3.02994407707441961300e-2);
  574|      0|  const Packet cst_cephes_exp_p2 = pset1<Packet>(9.99999999999999999910e-1);
  575|      0|  const Packet cst_cephes_exp_q0 = pset1<Packet>(3.00198505138664455042e-6);
  576|      0|  const Packet cst_cephes_exp_q1 = pset1<Packet>(2.52448340349684104192e-3);
  577|      0|  const Packet cst_cephes_exp_q2 = pset1<Packet>(2.27265548208155028766e-1);
  578|      0|  const Packet cst_cephes_exp_q3 = pset1<Packet>(2.00000000000000000009e0);
  579|      0|  const Packet cst_cephes_exp_C1 = pset1<Packet>(0.693145751953125);
  580|      0|  const Packet cst_cephes_exp_C2 = pset1<Packet>(1.42860682030941723212e-6);
  581|      0|
  582|      0|  Packet tmp, fx;
  583|      0|
  584|      0|  // clamp x
  585|      0|  Packet zero_mask = pcmp_lt(_x, cst_exp_lo);
  586|      0|  x = pmin(x, cst_exp_hi);
  587|      0|  // Express exp(x) as exp(g + n*log(2)).
  588|      0|  fx = pmadd(cst_cephes_LOG2EF, x, cst_half);
  589|      0|
  590|      0|  // Get the integer modulus of log(2), i.e. the "n" described above.
  591|      0|  fx = pfloor(fx);
  592|      0|
  593|      0|  // Get the remainder modulo log(2), i.e. the "g" described above. Subtract
  594|      0|  // n*log(2) out in two steps, i.e. n*C1 + n*C2, C1+C2=log2 to get the last
  595|      0|  // digits right.
  596|      0|  tmp = pmul(fx, cst_cephes_exp_C1);
  597|      0|  Packet z = pmul(fx, cst_cephes_exp_C2);
  598|      0|  x = psub(x, tmp);
  599|      0|  x = psub(x, z);
  600|      0|
  601|      0|  Packet x2 = pmul(x, x);
  602|      0|
  603|      0|  // Evaluate the numerator polynomial of the rational interpolant.
  604|      0|  Packet px = cst_cephes_exp_p0;
  605|      0|  px = pmadd(px, x2, cst_cephes_exp_p1);
  606|      0|  px = pmadd(px, x2, cst_cephes_exp_p2);
  607|      0|  px = pmul(px, x);
  608|      0|
  609|      0|  // Evaluate the denominator polynomial of the rational interpolant.
  610|      0|  Packet qx = cst_cephes_exp_q0;
  611|      0|  qx = pmadd(qx, x2, cst_cephes_exp_q1);
  612|      0|  qx = pmadd(qx, x2, cst_cephes_exp_q2);
  613|      0|  qx = pmadd(qx, x2, cst_cephes_exp_q3);
  614|      0|
  615|      0|  // I don't really get this bit, copied from the SSE2 routines, so...
  616|      0|  // TODO(gonnet): Figure out what is going on here, perhaps find a better
  617|      0|  // rational interpolant?
  618|      0|  x = pdiv(px, psub(qx, px));
  619|      0|  x = pmadd(cst_2, x, cst_1);
  620|      0|
  621|      0|  // Construct the result 2^n * exp(g) = e * x. The max is used to catch
  622|      0|  // non-finite values in the input.
  623|      0|  const Packet fast_pldexp_unsafe = pcmp_lt(cst_pldexp_threshold, pabs(_x));
  624|      0|  if (!predux_any(fast_pldexp_unsafe)) {
  625|      0|    // For |x| <= 708, we know the result is not zero or inf, and we can safely use
  626|      0|    // the fast version of pldexp.
  627|      0|    return pmax(pldexp_fast(x, fx), _x);
  628|      0|  }
  629|      0|  return pselect(zero_mask, cst_zero, pmax(pldexp(x, fx), _x));
  630|      0|}
  631|       |
  632|       |// The following code is inspired by the following stack-overflow answer:
  633|       |//   https://stackoverflow.com/questions/30463616/payne-hanek-algorithm-implementation-in-c/30465751#30465751
  634|       |// It has been largely optimized:
  635|       |//  - By-pass calls to frexp.
  636|       |//  - Aligned loads of required 96 bits of 2/pi. This is accomplished by
  637|       |//    (1) balancing the mantissa and exponent to the required bits of 2/pi are
  638|       |//    aligned on 8-bits, and (2) replicating the storage of the bits of 2/pi.
  639|       |//  - Avoid a branch in rounding and extraction of the remaining fractional part.
  640|       |// Overall, I measured a speed up higher than x2 on x86-64.
  641|      0|inline float trig_reduce_huge(float xf, Eigen::numext::int32_t* quadrant) {
  642|      0|  using Eigen::numext::int32_t;
  643|      0|  using Eigen::numext::int64_t;
  644|      0|  using Eigen::numext::uint32_t;
  645|      0|  using Eigen::numext::uint64_t;
  646|      0|
  647|      0|  const double pio2_62 = 3.4061215800865545e-19;     // pi/2 * 2^-62
  648|      0|  const uint64_t zero_dot_five = uint64_t(1) << 61;  // 0.5 in 2.62-bit fixed-point format
  649|      0|
  650|      0|  // 192 bits of 2/pi for Payne-Hanek reduction
  651|      0|  // Bits are introduced by packet of 8 to enable aligned reads.
  652|      0|  static const uint32_t two_over_pi[] = {
  653|      0|      0x00000028, 0x000028be, 0x0028be60, 0x28be60db, 0xbe60db93, 0x60db9391, 0xdb939105, 0x9391054a, 0x91054a7f,
  654|      0|      0x054a7f09, 0x4a7f09d5, 0x7f09d5f4, 0x09d5f47d, 0xd5f47d4d, 0xf47d4d37, 0x7d4d3770, 0x4d377036, 0x377036d8,
  655|      0|      0x7036d8a5, 0x36d8a566, 0xd8a5664f, 0xa5664f10, 0x664f10e4, 0x4f10e410, 0x10e41000, 0xe4100000};
  656|      0|
  657|      0|  uint32_t xi = numext::bit_cast<uint32_t>(xf);
  658|      0|  // Below, -118 = -126 + 8.
  659|      0|  //   -126 is to get the exponent,
  660|      0|  //   +8 is to enable alignment of 2/pi's bits on 8 bits.
  661|      0|  // This is possible because the fractional part of x as only 24 meaningful bits.
  662|      0|  uint32_t e = (xi >> 23) - 118;
  663|      0|  // Extract the mantissa and shift it to align it wrt the exponent
  664|      0|  xi = ((xi & 0x007fffffu) | 0x00800000u) << (e & 0x7);
  665|      0|
  666|      0|  uint32_t i = e >> 3;
  667|      0|  uint32_t twoopi_1 = two_over_pi[i - 1];
  668|      0|  uint32_t twoopi_2 = two_over_pi[i + 3];
  669|      0|  uint32_t twoopi_3 = two_over_pi[i + 7];
  670|      0|
  671|      0|  // Compute x * 2/pi in 2.62-bit fixed-point format.
  672|      0|  uint64_t p;
  673|      0|  p = uint64_t(xi) * twoopi_3;
  674|      0|  p = uint64_t(xi) * twoopi_2 + (p >> 32);
  675|      0|  p = (uint64_t(xi * twoopi_1) << 32) + p;
  676|      0|
  677|      0|  // Round to nearest: add 0.5 and extract integral part.
  678|      0|  uint64_t q = (p + zero_dot_five) >> 62;
  679|      0|  *quadrant = int(q);
  680|      0|  // Now it remains to compute "r = x - q*pi/2" with high accuracy,
  681|      0|  // since we have p=x/(pi/2) with high accuracy, we can more efficiently compute r as:
  682|      0|  //   r = (p-q)*pi/2,
  683|      0|  // where the product can be be carried out with sufficient accuracy using double precision.
  684|      0|  p -= q << 62;
  685|      0|  return float(double(int64_t(p)) * pio2_62);
  686|      0|}
  687|       |
  688|       |template <bool ComputeSine, typename Packet, bool ComputeBoth = false>
  689|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS
  690|       |#if EIGEN_COMP_GNUC_STRICT
  691|       |    __attribute__((optimize("-fno-unsafe-math-optimizations")))
  692|       |#endif
  693|       |    Packet
  694|      0|    psincos_float(const Packet& _x) {
  695|      0|  typedef typename unpacket_traits<Packet>::integer_packet PacketI;
  696|      0|
  697|      0|  const Packet cst_2oPI = pset1<Packet>(0.636619746685028076171875f);  // 2/PI
  698|      0|  const Packet cst_rounding_magic = pset1<Packet>(12582912);           // 2^23 for rounding
  699|      0|  const PacketI csti_1 = pset1<PacketI>(1);
  700|      0|  const Packet cst_sign_mask = pset1frombits<Packet>(static_cast<Eigen::numext::uint32_t>(0x80000000u));
  701|      0|
  702|      0|  Packet x = pabs(_x);
  703|      0|
  704|      0|  // Scale x by 2/Pi to find x's octant.
  705|      0|  Packet y = pmul(x, cst_2oPI);
  706|      0|
  707|      0|  // Rounding trick to find nearest integer:
  708|      0|  Packet y_round = padd(y, cst_rounding_magic);
  709|      0|  EIGEN_OPTIMIZATION_BARRIER(y_round)
  710|      0|  PacketI y_int = preinterpret<PacketI>(y_round);  // last 23 digits represent integer (if abs(x)<2^24)
  711|      0|  y = psub(y_round, cst_rounding_magic);           // nearest integer to x * (2/pi)
  712|      0|
  713|      0|// Subtract y * Pi/2 to reduce x to the interval -Pi/4 <= x <= +Pi/4
  714|      0|// using "Extended precision modular arithmetic"
  715|      0|#if defined(EIGEN_VECTORIZE_FMA)
  716|      0|  // This version requires true FMA for high accuracy.
  717|      0|  // It provides a max error of 1ULP up to (with absolute_error < 5.9605e-08):
  718|      0|  const float huge_th = ComputeSine ? 117435.992f : 71476.0625f;
  719|      0|  x = pmadd(y, pset1<Packet>(-1.57079601287841796875f), x);
  720|      0|  x = pmadd(y, pset1<Packet>(-3.1391647326017846353352069854736328125e-07f), x);
  721|      0|  x = pmadd(y, pset1<Packet>(-5.390302529957764765544681040410068817436695098876953125e-15f), x);
  722|      0|#else
  723|      0|  // Without true FMA, the previous set of coefficients maintain 1ULP accuracy
  724|      0|  // up to x<15.7 (for sin), but accuracy is immediately lost for x>15.7.
  725|      0|  // We thus use one more iteration to maintain 2ULPs up to reasonably large inputs.
  726|      0|
  727|      0|  // The following set of coefficients maintain 1ULP up to 9.43 and 14.16 for sin and cos respectively.
  728|      0|  // and 2 ULP up to:
  729|      0|  const float huge_th = ComputeSine ? 25966.f : 18838.f;
  730|      0|  x = pmadd(y, pset1<Packet>(-1.5703125), x);  // = 0xbfc90000
  731|      0|  EIGEN_OPTIMIZATION_BARRIER(x)
  732|      0|  x = pmadd(y, pset1<Packet>(-0.000483989715576171875), x);  // = 0xb9fdc000
  733|      0|  EIGEN_OPTIMIZATION_BARRIER(x)
  734|      0|  x = pmadd(y, pset1<Packet>(1.62865035235881805419921875e-07), x);                      // = 0x342ee000
  735|      0|  x = pmadd(y, pset1<Packet>(5.5644315544167710640977020375430583953857421875e-11), x);  // = 0x2e74b9ee
  736|      0|
  737|      0|// For the record, the following set of coefficients maintain 2ULP up
  738|      0|// to a slightly larger range:
  739|      0|// const float huge_th = ComputeSine ? 51981.f : 39086.125f;
  740|      0|// but it slightly fails to maintain 1ULP for two values of sin below pi.
  741|      0|// x = pmadd(y, pset1<Packet>(-3.140625/2.), x);
  742|      0|// x = pmadd(y, pset1<Packet>(-0.00048351287841796875), x);
  743|      0|// x = pmadd(y, pset1<Packet>(-3.13855707645416259765625e-07), x);
  744|      0|// x = pmadd(y, pset1<Packet>(-6.0771006282767103812147979624569416046142578125e-11), x);
  745|      0|
  746|      0|// For the record, with only 3 iterations it is possible to maintain
  747|      0|// 1 ULP up to 3PI (maybe more) and 2ULP up to 255.
  748|      0|// The coefficients are: 0xbfc90f80, 0xb7354480, 0x2e74b9ee
  749|      0|#endif
  750|      0|
  751|      0|  if (predux_any(pcmp_le(pset1<Packet>(huge_th), pabs(_x)))) {
  752|      0|    const int PacketSize = unpacket_traits<Packet>::size;
  753|      0|    EIGEN_ALIGN_TO_BOUNDARY(sizeof(Packet)) float vals[PacketSize];
  754|      0|    EIGEN_ALIGN_TO_BOUNDARY(sizeof(Packet)) float x_cpy[PacketSize];
  755|      0|    EIGEN_ALIGN_TO_BOUNDARY(sizeof(Packet)) Eigen::numext::int32_t y_int2[PacketSize];
  756|      0|    pstoreu(vals, pabs(_x));
  757|      0|    pstoreu(x_cpy, x);
  758|      0|    pstoreu(y_int2, y_int);
  759|      0|    for (int k = 0; k < PacketSize; ++k) {
  760|      0|      float val = vals[k];
  761|      0|      if (val >= huge_th && (numext::isfinite)(val)) x_cpy[k] = trig_reduce_huge(val, &y_int2[k]);
  762|      0|    }
  763|      0|    x = ploadu<Packet>(x_cpy);
  764|      0|    y_int = ploadu<PacketI>(y_int2);
  765|      0|  }
  766|      0|
  767|      0|  // Compute the sign to apply to the polynomial.
  768|      0|  // sin: sign = second_bit(y_int) xor signbit(_x)
  769|      0|  // cos: sign = second_bit(y_int+1)
  770|      0|  Packet sign_bit = ComputeSine ? pxor(_x, preinterpret<Packet>(plogical_shift_left<30>(y_int)))
  771|      0|                                : preinterpret<Packet>(plogical_shift_left<30>(padd(y_int, csti_1)));
  772|      0|  sign_bit = pand(sign_bit, cst_sign_mask);  // clear all but left most bit
  773|      0|
  774|      0|  // Get the polynomial selection mask from the second bit of y_int
  775|      0|  // We'll calculate both (sin and cos) polynomials and then select from the two.
  776|      0|  Packet poly_mask = preinterpret<Packet>(pcmp_eq(pand(y_int, csti_1), pzero(y_int)));
  777|      0|
  778|      0|  Packet x2 = pmul(x, x);
  779|      0|
  780|      0|  // Evaluate the cos(x) polynomial. (-Pi/4 <= x <= Pi/4)
  781|      0|  Packet y1 = pset1<Packet>(2.4372266125283204019069671630859375e-05f);
  782|      0|  y1 = pmadd(y1, x2, pset1<Packet>(-0.00138865201734006404876708984375f));
  783|      0|  y1 = pmadd(y1, x2, pset1<Packet>(0.041666619479656219482421875f));
  784|      0|  y1 = pmadd(y1, x2, pset1<Packet>(-0.5f));
  785|      0|  y1 = pmadd(y1, x2, pset1<Packet>(1.f));
  786|      0|
  787|      0|  // Evaluate the sin(x) polynomial. (Pi/4 <= x <= Pi/4)
  788|      0|  // octave/matlab code to compute those coefficients:
  789|      0|  //    x = (0:0.0001:pi/4)';
  790|      0|  //    A = [x.^3 x.^5 x.^7];
  791|      0|  //    w = ((1.-(x/(pi/4)).^2).^5)*2000+1;         # weights trading relative accuracy
  792|      0|  //    c = (A'*diag(w)*A)\(A'*diag(w)*(sin(x)-x)); # weighted LS, linear coeff forced to 1
  793|      0|  //    printf('%.64f\n %.64f\n%.64f\n', c(3), c(2), c(1))
  794|      0|  //
  795|      0|  Packet y2 = pset1<Packet>(-0.0001959234114083702898469196984621021329076029360294342041015625f);
  796|      0|  y2 = pmadd(y2, x2, pset1<Packet>(0.0083326873655616851693794799871284340042620897293090820312500000f));
  797|      0|  y2 = pmadd(y2, x2, pset1<Packet>(-0.1666666203982298255503735617821803316473960876464843750000000000f));
  798|      0|  y2 = pmul(y2, x2);
  799|      0|  y2 = pmadd(y2, x, x);
  800|      0|
  801|      0|  // Select the correct result from the two polynomials.
  802|      0|  if (ComputeBoth) {
  803|      0|    Packet peven = peven_mask(x);
  804|      0|    Packet ysin = pselect(poly_mask, y2, y1);
  805|      0|    Packet ycos = pselect(poly_mask, y1, y2);
  806|      0|    Packet sign_bit_sin = pxor(_x, preinterpret<Packet>(plogical_shift_left<30>(y_int)));
  807|      0|    Packet sign_bit_cos = preinterpret<Packet>(plogical_shift_left<30>(padd(y_int, csti_1)));
  808|      0|    sign_bit_sin = pand(sign_bit_sin, cst_sign_mask);  // clear all but left most bit
  809|      0|    sign_bit_cos = pand(sign_bit_cos, cst_sign_mask);  // clear all but left most bit
  810|      0|    y = pselect(peven, pxor(ysin, sign_bit_sin), pxor(ycos, sign_bit_cos));
  811|      0|  } else {
  812|      0|    y = ComputeSine ? pselect(poly_mask, y2, y1) : pselect(poly_mask, y1, y2);
  813|      0|    y = pxor(y, sign_bit);
  814|      0|  }
  815|      0|  // Update the sign and filter huge inputs
  816|      0|  return y;
  817|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13psincos_floatILb1EDv4_fLb0EEET0_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13psincos_floatILb0EDv4_fLb0EEET0_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13psincos_floatILb0EDv4_fLb1EEET0_RKS3_
  ------------------
  818|       |
  819|       |template <typename Packet>
  820|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psin_float(const Packet& x) {
  821|      0|  return psincos_float<true>(x);
  822|      0|}
  823|       |
  824|       |template <typename Packet>
  825|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pcos_float(const Packet& x) {
  826|      0|  return psincos_float<false>(x);
  827|      0|}
  828|       |
  829|       |// Trigonometric argument reduction for double for inputs smaller than 15.
  830|       |// Reduces trigonometric arguments for double inputs where x < 15. Given an argument x and its corresponding quadrant
  831|       |// count n, the function computes and returns the reduced argument t such that x = n * pi/2 + t.
  832|       |template <typename Packet>
  833|      0|Packet trig_reduce_small_double(const Packet& x, const Packet& q) {
  834|      0|  // Pi/2 split into 2 values
  835|      0|  const Packet cst_pio2_a = pset1<Packet>(-1.570796325802803);
  836|      0|  const Packet cst_pio2_b = pset1<Packet>(-9.920935184482005e-10);
  837|      0|
  838|      0|  Packet t;
  839|      0|  t = pmadd(cst_pio2_a, q, x);
  840|      0|  t = pmadd(cst_pio2_b, q, t);
  841|      0|  return t;
  842|      0|}
  843|       |
  844|       |// Trigonometric argument reduction for double for inputs smaller than 1e14.
  845|       |// Reduces trigonometric arguments for double inputs where x < 1e14. Given an argument x and its corresponding quadrant
  846|       |// count n, the function computes and returns the reduced argument t such that x = n * pi/2 + t.
  847|       |template <typename Packet>
  848|      0|Packet trig_reduce_medium_double(const Packet& x, const Packet& q_high, const Packet& q_low) {
  849|      0|  // Pi/2 split into 4 values
  850|      0|  const Packet cst_pio2_a = pset1<Packet>(-1.570796325802803);
  851|      0|  const Packet cst_pio2_b = pset1<Packet>(-9.920935184482005e-10);
  852|      0|  const Packet cst_pio2_c = pset1<Packet>(-6.123234014771656e-17);
  853|      0|  const Packet cst_pio2_d = pset1<Packet>(1.903488962019325e-25);
  854|      0|
  855|      0|  Packet t;
  856|      0|  t = pmadd(cst_pio2_a, q_high, x);
  857|      0|  t = pmadd(cst_pio2_a, q_low, t);
  858|      0|  t = pmadd(cst_pio2_b, q_high, t);
  859|      0|  t = pmadd(cst_pio2_b, q_low, t);
  860|      0|  t = pmadd(cst_pio2_c, q_high, t);
  861|      0|  t = pmadd(cst_pio2_c, q_low, t);
  862|      0|  t = pmadd(cst_pio2_d, padd(q_low, q_high), t);
  863|      0|  return t;
  864|      0|}
  865|       |
  866|       |template <bool ComputeSine, typename Packet, bool ComputeBoth = false>
  867|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS
  868|       |#if EIGEN_COMP_GNUC_STRICT
  869|       |    __attribute__((optimize("-fno-unsafe-math-optimizations")))
  870|       |#endif
  871|       |    Packet
  872|      0|    psincos_double(const Packet& x) {
  873|      0|  typedef typename unpacket_traits<Packet>::integer_packet PacketI;
  874|      0|  typedef typename unpacket_traits<PacketI>::type ScalarI;
  875|      0|
  876|      0|  const Packet cst_sign_mask = pset1frombits<Packet>(static_cast<Eigen::numext::uint64_t>(0x8000000000000000u));
  877|      0|
  878|      0|  // If the argument is smaller than this value, use a simpler argument reduction
  879|      0|  const double small_th = 15;
  880|      0|  // If the argument is bigger than this value, use the non-vectorized std version
  881|      0|  const double huge_th = 1e14;
  882|      0|
  883|      0|  const Packet cst_2oPI = pset1<Packet>(0.63661977236758134307553505349006);  // 2/PI
  884|      0|  // Integer Packet constants
  885|      0|  const PacketI cst_one = pset1<PacketI>(ScalarI(1));
  886|      0|  // Constant for splitting
  887|      0|  const Packet cst_split = pset1<Packet>(1 << 24);
  888|      0|
  889|      0|  Packet x_abs = pabs(x);
  890|      0|
  891|      0|  // Scale x by 2/Pi
  892|      0|  PacketI q_int;
  893|      0|  Packet s;
  894|      0|
  895|      0|  // TODO Implement huge angle argument reduction
  896|      0|  if (EIGEN_PREDICT_FALSE(predux_any(pcmp_le(pset1<Packet>(small_th), x_abs)))) {
  897|      0|    Packet q_high = pmul(pfloor(pmul(x_abs, pdiv(cst_2oPI, cst_split))), cst_split);
  898|      0|    Packet q_low_noround = psub(pmul(x_abs, cst_2oPI), q_high);
  899|      0|    q_int = pcast<Packet, PacketI>(padd(q_low_noround, pset1<Packet>(0.5)));
  900|      0|    Packet q_low = pcast<PacketI, Packet>(q_int);
  901|      0|    s = trig_reduce_medium_double(x_abs, q_high, q_low);
  902|      0|  } else {
  903|      0|    Packet qval_noround = pmul(x_abs, cst_2oPI);
  904|      0|    q_int = pcast<Packet, PacketI>(padd(qval_noround, pset1<Packet>(0.5)));
  905|      0|    Packet q = pcast<PacketI, Packet>(q_int);
  906|      0|    s = trig_reduce_small_double(x_abs, q);
  907|      0|  }
  908|      0|
  909|      0|  // All the upcoming approximating polynomials have even exponents
  910|      0|  Packet ss = pmul(s, s);
  911|      0|
  912|      0|  // Padé approximant of cos(x)
  913|      0|  // Assuring < 1 ULP error on the interval [-pi/4, pi/4]
  914|      0|  // cos(x) ~= (80737373*x^8 - 13853547000*x^6 + 727718024880*x^4 - 11275015752000*x^2 + 23594700729600)/(147173*x^8 +
  915|      0|  // 39328920*x^6 + 5772800880*x^4 + 522334612800*x^2 + 23594700729600)
  916|      0|  // MATLAB code to compute those coefficients:
  917|      0|  //    syms x;
  918|      0|  //    cosf = @(x) cos(x);
  919|      0|  //    pade_cosf = pade(cosf(x), x, 0, 'Order', 8)
  920|      0|  Packet sc1_num = pmadd(ss, pset1<Packet>(80737373), pset1<Packet>(-13853547000));
  921|      0|  Packet sc2_num = pmadd(sc1_num, ss, pset1<Packet>(727718024880));
  922|      0|  Packet sc3_num = pmadd(sc2_num, ss, pset1<Packet>(-11275015752000));
  923|      0|  Packet sc4_num = pmadd(sc3_num, ss, pset1<Packet>(23594700729600));
  924|      0|  Packet sc1_denum = pmadd(ss, pset1<Packet>(147173), pset1<Packet>(39328920));
  925|      0|  Packet sc2_denum = pmadd(sc1_denum, ss, pset1<Packet>(5772800880));
  926|      0|  Packet sc3_denum = pmadd(sc2_denum, ss, pset1<Packet>(522334612800));
  927|      0|  Packet sc4_denum = pmadd(sc3_denum, ss, pset1<Packet>(23594700729600));
  928|      0|  Packet scos = pdiv(sc4_num, sc4_denum);
  929|      0|
  930|      0|  // Padé approximant of sin(x)
  931|      0|  // Assuring < 1 ULP error on the interval [-pi/4, pi/4]
  932|      0|  // sin(x) ~= (x*(4585922449*x^8 - 1066023933480*x^6 + 83284044283440*x^4 - 2303682236856000*x^2 +
  933|      0|  // 15605159573203200))/(45*(1029037*x^8 + 345207016*x^6 + 61570292784*x^4 + 6603948711360*x^2 + 346781323848960))
  934|      0|  // MATLAB code to compute those coefficients:
  935|      0|  //    syms x;
  936|      0|  //    sinf = @(x) sin(x);
  937|      0|  //    pade_sinf = pade(sinf(x), x, 0, 'Order', 8, 'OrderMode', 'relative')
  938|      0|  Packet ss1_num = pmadd(ss, pset1<Packet>(4585922449), pset1<Packet>(-1066023933480));
  939|      0|  Packet ss2_num = pmadd(ss1_num, ss, pset1<Packet>(83284044283440));
  940|      0|  Packet ss3_num = pmadd(ss2_num, ss, pset1<Packet>(-2303682236856000));
  941|      0|  Packet ss4_num = pmadd(ss3_num, ss, pset1<Packet>(15605159573203200));
  942|      0|  Packet ss1_denum = pmadd(ss, pset1<Packet>(1029037), pset1<Packet>(345207016));
  943|      0|  Packet ss2_denum = pmadd(ss1_denum, ss, pset1<Packet>(61570292784));
  944|      0|  Packet ss3_denum = pmadd(ss2_denum, ss, pset1<Packet>(6603948711360));
  945|      0|  Packet ss4_denum = pmadd(ss3_denum, ss, pset1<Packet>(346781323848960));
  946|      0|  Packet ssin = pdiv(pmul(s, ss4_num), pmul(pset1<Packet>(45), ss4_denum));
  947|      0|
  948|      0|  Packet poly_mask = preinterpret<Packet>(pcmp_eq(pand(q_int, cst_one), pzero(q_int)));
  949|      0|
  950|      0|  Packet sign_sin = pxor(x, preinterpret<Packet>(plogical_shift_left<62>(q_int)));
  951|      0|  Packet sign_cos = preinterpret<Packet>(plogical_shift_left<62>(padd(q_int, cst_one)));
  952|      0|  Packet sign_bit, sFinalRes;
  953|      0|  if (ComputeBoth) {
  954|      0|    Packet peven = peven_mask(x);
  955|      0|    sign_bit = pselect((s), sign_sin, sign_cos);
  956|      0|    sFinalRes = pselect(pxor(peven, poly_mask), ssin, scos);
  957|      0|  } else {
  958|      0|    sign_bit = ComputeSine ? sign_sin : sign_cos;
  959|      0|    sFinalRes = ComputeSine ? pselect(poly_mask, ssin, scos) : pselect(poly_mask, scos, ssin);
  960|      0|  }
  961|      0|  sign_bit = pand(sign_bit, cst_sign_mask);  // clear all but left most bit
  962|      0|  sFinalRes = pxor(sFinalRes, sign_bit);
  963|      0|
  964|      0|  // If the inputs values are higher than that a value that the argument reduction can currently address, compute them
  965|      0|  // using std::sin and std::cos
  966|      0|  // TODO Remove it when huge angle argument reduction is implemented
  967|      0|  if (EIGEN_PREDICT_FALSE(predux_any(pcmp_le(pset1<Packet>(huge_th), x_abs)))) {
  968|      0|    const int PacketSize = unpacket_traits<Packet>::size;
  969|      0|    EIGEN_ALIGN_TO_BOUNDARY(sizeof(Packet)) double sincos_vals[PacketSize];
  970|      0|    EIGEN_ALIGN_TO_BOUNDARY(sizeof(Packet)) double x_cpy[PacketSize];
  971|      0|    pstoreu(x_cpy, x);
  972|      0|    pstoreu(sincos_vals, sFinalRes);
  973|      0|    for (int k = 0; k < PacketSize; ++k) {
  974|      0|      double val = x_cpy[k];
  975|      0|      if (std::abs(val) > huge_th && (numext::isfinite)(val)) {
  976|      0|        if (ComputeBoth)
  977|      0|          sincos_vals[k] = k % 2 == 0 ? std::sin(val) : std::cos(val);
  978|      0|        else
  979|      0|          sincos_vals[k] = ComputeSine ? std::sin(val) : std::cos(val);
  980|      0|      }
  981|      0|    }
  982|      0|    sFinalRes = ploadu<Packet>(sincos_vals);
  983|      0|  }
  984|      0|  return sFinalRes;
  985|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14psincos_doubleILb1EDv2_dLb0EEET0_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14psincos_doubleILb0EDv2_dLb0EEET0_RKS3_
  ------------------
  986|       |
  987|       |template <typename Packet>
  988|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psin_double(const Packet& x) {
  989|      0|  return psincos_double<true>(x);
  990|      0|}
  991|       |
  992|       |template <typename Packet>
  993|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pcos_double(const Packet& x) {
  994|      0|  return psincos_double<false>(x);
  995|      0|}
  996|       |
  997|       |// Generic implementation of acos(x).
  998|       |template <typename Packet>
  999|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pacos_float(const Packet& x_in) {
 1000|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1001|      0|  static_assert(std::is_same<Scalar, float>::value, "Scalar type must be float");
 1002|      0|
 1003|      0|  const Packet cst_one = pset1<Packet>(Scalar(1));
 1004|      0|  const Packet cst_pi = pset1<Packet>(Scalar(EIGEN_PI));
 1005|      0|  const Packet p6 = pset1<Packet>(Scalar(2.36423197202384471893310546875e-3));
 1006|      0|  const Packet p5 = pset1<Packet>(Scalar(-1.1368644423782825469970703125e-2));
 1007|      0|  const Packet p4 = pset1<Packet>(Scalar(2.717843465507030487060546875e-2));
 1008|      0|  const Packet p3 = pset1<Packet>(Scalar(-4.8969544470310211181640625e-2));
 1009|      0|  const Packet p2 = pset1<Packet>(Scalar(8.8804088532924652099609375e-2));
 1010|      0|  const Packet p1 = pset1<Packet>(Scalar(-0.214591205120086669921875));
 1011|      0|  const Packet p0 = pset1<Packet>(Scalar(1.57079637050628662109375));
 1012|      0|
 1013|      0|  // For x in [0:1], we approximate acos(x)/sqrt(1-x), which is a smooth
 1014|      0|  // function, by a 6'th order polynomial.
 1015|      0|  // For x in [-1:0) we use that acos(-x) = pi - acos(x).
 1016|      0|  const Packet neg_mask = psignbit(x_in);
 1017|      0|  const Packet abs_x = pabs(x_in);
 1018|      0|
 1019|      0|  // Evaluate the polynomial using Horner's rule:
 1020|      0|  //   P(x) = p0 + x * (p1 +  x * (p2 + ... (p5 + x * p6)) ... ) .
 1021|      0|  // We evaluate even and odd terms independently to increase
 1022|      0|  // instruction level parallelism.
 1023|      0|  Packet x2 = pmul(x_in, x_in);
 1024|      0|  Packet p_even = pmadd(p6, x2, p4);
 1025|      0|  Packet p_odd = pmadd(p5, x2, p3);
 1026|      0|  p_even = pmadd(p_even, x2, p2);
 1027|      0|  p_odd = pmadd(p_odd, x2, p1);
 1028|      0|  p_even = pmadd(p_even, x2, p0);
 1029|      0|  Packet p = pmadd(p_odd, abs_x, p_even);
 1030|      0|
 1031|      0|  // The polynomial approximates acos(x)/sqrt(1-x), so
 1032|      0|  // multiply by sqrt(1-x) to get acos(x).
 1033|      0|  // Conveniently returns NaN for arguments outside [-1:1].
 1034|      0|  Packet denom = psqrt(psub(cst_one, abs_x));
 1035|      0|  Packet result = pmul(denom, p);
 1036|      0|  // Undo mapping for negative arguments.
 1037|      0|  return pselect(neg_mask, psub(cst_pi, result), result);
 1038|      0|}
 1039|       |
 1040|       |// Generic implementation of asin(x).
 1041|       |template <typename Packet>
 1042|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pasin_float(const Packet& x_in) {
 1043|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1044|      0|  static_assert(std::is_same<Scalar, float>::value, "Scalar type must be float");
 1045|      0|
 1046|      0|  constexpr float kPiOverTwo = static_cast<float>(EIGEN_PI / 2);
 1047|      0|
 1048|      0|  const Packet cst_half = pset1<Packet>(0.5f);
 1049|      0|  const Packet cst_one = pset1<Packet>(1.0f);
 1050|      0|  const Packet cst_two = pset1<Packet>(2.0f);
 1051|      0|  const Packet cst_pi_over_two = pset1<Packet>(kPiOverTwo);
 1052|      0|
 1053|      0|  const Packet abs_x = pabs(x_in);
 1054|      0|  const Packet sign_mask = pandnot(x_in, abs_x);
 1055|      0|  const Packet invalid_mask = pcmp_lt(cst_one, abs_x);
 1056|      0|
 1057|      0|  // For arguments |x| > 0.5, we map x back to [0:0.5] using
 1058|      0|  // the transformation x_large = sqrt(0.5*(1-x)), and use the
 1059|      0|  // identity
 1060|      0|  //   asin(x) = pi/2 - 2 * asin( sqrt( 0.5 * (1 - x)))
 1061|      0|
 1062|      0|  const Packet x_large = psqrt(pnmadd(cst_half, abs_x, cst_half));
 1063|      0|  const Packet large_mask = pcmp_lt(cst_half, abs_x);
 1064|      0|  const Packet x = pselect(large_mask, x_large, abs_x);
 1065|      0|  const Packet x2 = pmul(x, x);
 1066|      0|
 1067|      0|  // For |x| < 0.5 approximate asin(x)/x by an 8th order polynomial with
 1068|      0|  // even terms only.
 1069|      0|  constexpr float alpha[] = {5.08838854730129241943359375e-2f, 3.95139865577220916748046875e-2f,
 1070|      0|                             7.550220191478729248046875e-2f, 0.16664917767047882080078125f, 1.00000011920928955078125f};
 1071|      0|  Packet p = ppolevl<Packet, 4>::run(x2, alpha);
 1072|      0|  p = pmul(p, x);
 1073|      0|
 1074|      0|  const Packet p_large = pnmadd(cst_two, p, cst_pi_over_two);
 1075|      0|  p = pselect(large_mask, p_large, p);
 1076|      0|  // Flip the sign for negative arguments.
 1077|      0|  p = pxor(p, sign_mask);
 1078|      0|  // Return NaN for arguments outside [-1:1].
 1079|      0|  return por(invalid_mask, p);
 1080|      0|}
 1081|       |
 1082|       |template <typename Scalar>
 1083|       |struct patan_reduced {
 1084|       |  template <typename Packet>
 1085|       |  static EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet run(const Packet& x);
 1086|       |};
 1087|       |
 1088|       |template <>
 1089|       |template <typename Packet>
 1090|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patan_reduced<double>::run(const Packet& x) {
 1091|      0|  constexpr double alpha[] = {2.6667153866462208e-05, 3.0917513112462781e-03, 5.2574296781008604e-02,
 1092|      0|                              3.0409318473444424e-01, 7.5365702534987022e-01, 8.2704055405494614e-01,
 1093|      0|                              3.3004361289279920e-01};
 1094|      0|
 1095|      0|  constexpr double beta[] = {
 1096|      0|      2.7311202462436667e-04, 1.0899150928962708e-02, 1.1548932646420353e-01, 4.9716458728465573e-01, 1.0,
 1097|      0|      9.3705509168587852e-01, 3.3004361289279920e-01};
 1098|      0|
 1099|      0|  Packet x2 = pmul(x, x);
 1100|      0|  Packet p = ppolevl<Packet, 6>::run(x2, alpha);
 1101|      0|  Packet q = ppolevl<Packet, 6>::run(x2, beta);
 1102|      0|  return pmul(x, pdiv(p, q));
 1103|      0|}
 1104|       |
 1105|       |// Computes elementwise atan(x) for x in [-1:1] with 2 ulp accuracy.
 1106|       |template <>
 1107|       |template <typename Packet>
 1108|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patan_reduced<float>::run(const Packet& x) {
 1109|      0|  constexpr float alpha[] = {1.12026982009410858154296875e-01f, 7.296695709228515625e-01f, 8.109951019287109375e-01f};
 1110|      0|
 1111|      0|  constexpr float beta[] = {1.00917108356952667236328125e-02f, 2.8318560123443603515625e-01f, 1.0f,
 1112|      0|                            8.109951019287109375e-01f};
 1113|      0|
 1114|      0|  Packet x2 = pmul(x, x);
 1115|      0|  Packet p = ppolevl<Packet, 2>::run(x2, alpha);
 1116|      0|  Packet q = ppolevl<Packet, 3>::run(x2, beta);
 1117|      0|  return pmul(x, pdiv(p, q));
 1118|      0|}
 1119|       |
 1120|       |template <typename Packet>
 1121|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_atan(const Packet& x_in) {
 1122|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1123|      0|
 1124|      0|  constexpr Scalar kPiOverTwo = static_cast<Scalar>(EIGEN_PI / 2);
 1125|      0|
 1126|      0|  const Packet cst_signmask = pset1<Packet>(-Scalar(0));
 1127|      0|  const Packet cst_one = pset1<Packet>(Scalar(1));
 1128|      0|  const Packet cst_pi_over_two = pset1<Packet>(kPiOverTwo);
 1129|      0|
 1130|      0|  //   "Large": For |x| > 1, use atan(1/x) = sign(x)*pi/2 - atan(x).
 1131|      0|  //   "Small": For |x| <= 1, approximate atan(x) directly by a polynomial
 1132|      0|  //            calculated using Rminimax.
 1133|      0|
 1134|      0|  const Packet abs_x = pabs(x_in);
 1135|      0|  const Packet x_signmask = pand(x_in, cst_signmask);
 1136|      0|  const Packet large_mask = pcmp_lt(cst_one, abs_x);
 1137|      0|  const Packet x = pselect(large_mask, preciprocal(abs_x), abs_x);
 1138|      0|  const Packet p = patan_reduced<Scalar>::run(x);
 1139|      0|  // Apply transformations according to the range reduction masks.
 1140|      0|  Packet result = pselect(large_mask, psub(cst_pi_over_two, p), p);
 1141|      0|  // Return correct sign
 1142|      0|  return pxor(result, x_signmask);
 1143|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12generic_atanIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12generic_atanIDv2_dEET_RKS3_
  ------------------
 1144|       |
 1145|       |/** \internal \returns the hyperbolic tan of \a a (coeff-wise)
 1146|       |    Doesn't do anything fancy, just a 9/8-degree rational interpolant which
 1147|       |    is accurate up to a couple of ulps in the (approximate) range [-8, 8],
 1148|       |    outside of which tanh(x) = +/-1 in single precision. The input is clamped
 1149|       |    to the range [-c, c]. The value c is chosen as the smallest value where
 1150|       |    the approximation evaluates to exactly 1.
 1151|       |
 1152|       |    This implementation works on both scalars and packets.
 1153|       |*/
 1154|       |template <typename T>
 1155|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS T ptanh_float(const T& a_x) {
 1156|      0|  // Clamp the inputs to the range [-c, c] and set everything
 1157|      0|  // outside that range to 1.0. The value c is chosen as the smallest
 1158|      0|  // floating point argument such that the approximation is exactly 1.
 1159|      0|  // This saves clamping the value at the end.
 1160|      0|#ifdef EIGEN_VECTORIZE_FMA
 1161|      0|  const T plus_clamp = pset1<T>(8.01773357391357422f);
 1162|      0|  const T minus_clamp = pset1<T>(-8.01773357391357422f);
 1163|      0|#else
 1164|      0|  const T plus_clamp = pset1<T>(7.90738964080810547f);
 1165|      0|  const T minus_clamp = pset1<T>(-7.90738964080810547f);
 1166|      0|#endif
 1167|      0|  const T x = pmax(pmin(a_x, plus_clamp), minus_clamp);
 1168|      0|
 1169|      0|  // The following rational approximation was generated by rminimax
 1170|      0|  // (https://gitlab.inria.fr/sfilip/rminimax) using the following
 1171|      0|  // command:
 1172|      0|  // $ ratapprox --function="tanh(x)" --dom='[-8.67,8.67]' --num="odd"
 1173|      0|  //   --den="even" --type="[9,8]" --numF="[SG]" --denF="[SG]" --log
 1174|      0|  //   --output=tanhf.sollya --dispCoeff="dec"
 1175|      0|
 1176|      0|  // The monomial coefficients of the numerator polynomial (odd).
 1177|      0|  constexpr float alpha[] = {1.394553628e-8f, 2.102733560e-5f, 3.520756727e-3f, 1.340216100e-1f};
 1178|      0|
 1179|      0|  // The monomial coefficients of the denominator polynomial (even).
 1180|      0|  constexpr float beta[] = {8.015776984e-7f, 3.326951409e-4f, 2.597254514e-2f, 4.673548340e-1f, 1.0f};
 1181|      0|
 1182|      0|  // Since the polynomials are odd/even, we need x^2.
 1183|      0|  const T x2 = pmul(x, x);
 1184|      0|  const T x3 = pmul(x2, x);
 1185|      0|
 1186|      0|  T p = ppolevl<T, 3>::run(x2, alpha);
 1187|      0|  T q = ppolevl<T, 4>::run(x2, beta);
 1188|      0|  // Take advantage of the fact that the constant term in p is 1 to compute
 1189|      0|  // x*(x^2*p + 1) = x^3 * p + x.
 1190|      0|  p = pmadd(x3, p, x);
 1191|      0|
 1192|      0|  // Divide the numerator by the denominator.
 1193|      0|  return pdiv(p, q);
 1194|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal11ptanh_floatIfEET_RKS2_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal11ptanh_floatIDv4_fEET_RKS3_
  ------------------
 1195|       |
 1196|       |/** \internal \returns the hyperbolic tan of \a a (coeff-wise)
 1197|       |    This uses a 19/18-degree rational interpolant which
 1198|       |    is accurate up to a couple of ulps in the (approximate) range [-18.7, 18.7],
 1199|       |    outside of which tanh(x) = +/-1 in single precision. The input is clamped
 1200|       |    to the range [-c, c]. The value c is chosen as the smallest value where
 1201|       |    the approximation evaluates to exactly 1.
 1202|       |
 1203|       |    This implementation works on both scalars and packets.
 1204|       |*/
 1205|       |template <typename T>
 1206|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS T ptanh_double(const T& a_x) {
 1207|      0|  // Clamp the inputs to the range [-c, c] and set everything
 1208|      0|  // outside that range to 1.0. The value c is chosen as the smallest
 1209|      0|  // floating point argument such that the approximation is exactly 1.
 1210|      0|  // This saves clamping the value at the end.
 1211|      0|#ifdef EIGEN_VECTORIZE_FMA
 1212|      0|  const T plus_clamp = pset1<T>(17.6610191624600077);
 1213|      0|  const T minus_clamp = pset1<T>(-17.6610191624600077);
 1214|      0|#else
 1215|      0|  const T plus_clamp = pset1<T>(17.714196154005176);
 1216|      0|  const T minus_clamp = pset1<T>(-17.714196154005176);
 1217|      0|#endif
 1218|      0|  const T x = pmax(pmin(a_x, plus_clamp), minus_clamp);
 1219|      0|
 1220|      0|  // The following rational approximation was generated by rminimax
 1221|      0|  // (https://gitlab.inria.fr/sfilip/rminimax) using the following
 1222|      0|  // command:
 1223|      0|  // $ ./ratapprox --function="tanh(x)" --dom='[-18.72,18.72]'
 1224|      0|  //   --num="odd" --den="even" --type="[19,18]" --numF="[D]"
 1225|      0|  //   --denF="[D]" --log --output=tanh.sollya --dispCoeff="dec"
 1226|      0|
 1227|      0|  // The monomial coefficients of the numerator polynomial (odd).
 1228|      0|  constexpr double alpha[] = {2.6158007860482230e-23, 7.6534862268749319e-19, 3.1309488231386680e-15,
 1229|      0|                              4.2303918148209176e-12, 2.4618379131293676e-09, 6.8644367682497074e-07,
 1230|      0|                              9.3839087674268880e-05, 5.9809711724441161e-03, 1.5184719640284322e-01};
 1231|      0|
 1232|      0|  // The monomial coefficients of the denominator polynomial (even).
 1233|      0|  constexpr double beta[] = {6.463747022670968018e-21, 5.782506856739003571e-17,
 1234|      0|                             1.293019623712687916e-13, 1.123643448069621992e-10,
 1235|      0|                             4.492975677839633985e-08, 8.785185266237658698e-06,
 1236|      0|                             8.295161192716231542e-04, 3.437448108450402717e-02,
 1237|      0|                             4.851805297361760360e-01, 1.0};
 1238|      0|
 1239|      0|  // Since the polynomials are odd/even, we need x^2.
 1240|      0|  const T x2 = pmul(x, x);
 1241|      0|  const T x3 = pmul(x2, x);
 1242|      0|
 1243|      0|  // Interleave the evaluation of the numerator polynomial p and
 1244|      0|  // denominator polynomial q.
 1245|      0|  T p = ppolevl<T, 8>::run(x2, alpha);
 1246|      0|  T q = ppolevl<T, 9>::run(x2, beta);
 1247|      0|  // Take advantage of the fact that the constant term in p is 1 to compute
 1248|      0|  // x*(x^2*p + 1) = x^3 * p + x.
 1249|      0|  p = pmadd(x3, p, x);
 1250|      0|
 1251|      0|  // Divide the numerator by the denominator.
 1252|      0|  return pdiv(p, q);
 1253|      0|}
 1254|       |
 1255|       |template <typename Packet>
 1256|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patanh_float(const Packet& x) {
 1257|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1258|      0|  static_assert(std::is_same<Scalar, float>::value, "Scalar type must be float");
 1259|      0|
 1260|      0|  // For |x| in [0:0.5] we use a polynomial approximation of the form
 1261|      0|  // P(x) = x + x^3*(alpha[4] + x^2 * (alpha[3] + x^2 * (... x^2 * alpha[0]) ... )).
 1262|      0|  constexpr float alpha[] = {0.1819281280040740966796875f, 8.2311116158962249755859375e-2f,
 1263|      0|                             0.14672131836414337158203125f, 0.1997792422771453857421875f, 0.3333373963832855224609375f};
 1264|      0|  const Packet x2 = pmul(x, x);
 1265|      0|  const Packet x3 = pmul(x, x2);
 1266|      0|  Packet p = ppolevl<Packet, 4>::run(x2, alpha);
 1267|      0|  p = pmadd(x3, p, x);
 1268|      0|
 1269|      0|  // For |x| in ]0.5:1.0] we use atanh = 0.5*ln((1+x)/(1-x));
 1270|      0|  const Packet half = pset1<Packet>(0.5f);
 1271|      0|  const Packet one = pset1<Packet>(1.0f);
 1272|      0|  Packet r = pdiv(padd(one, x), psub(one, x));
 1273|      0|  r = pmul(half, plog(r));
 1274|      0|
 1275|      0|  const Packet x_gt_half = pcmp_le(half, pabs(x));
 1276|      0|  const Packet x_eq_one = pcmp_eq(one, pabs(x));
 1277|      0|  const Packet x_gt_one = pcmp_lt(one, pabs(x));
 1278|      0|  const Packet sign_mask = pset1<Packet>(-0.0f);
 1279|      0|  const Packet x_sign = pand(sign_mask, x);
 1280|      0|  const Packet inf = pset1<Packet>(std::numeric_limits<float>::infinity());
 1281|      0|  return por(x_gt_one, pselect(x_eq_one, por(x_sign, inf), pselect(x_gt_half, r, p)));
 1282|      0|}
 1283|       |
 1284|       |template <typename Packet>
 1285|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patanh_double(const Packet& x) {
 1286|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1287|      0|  static_assert(std::is_same<Scalar, double>::value, "Scalar type must be double");
 1288|      0|  // For x in [-0.5:0.5] we use a rational approximation of the form
 1289|      0|  // R(x) = x + x^3*P(x^2)/Q(x^2), where P is or order 4 and Q is of order 5.
 1290|      0|  constexpr double alpha[] = {3.3071338469301391e-03, -4.7129526768798737e-02, 1.8185306179826699e-01,
 1291|      0|                              -2.5949536095445679e-01, 1.2306328729812676e-01};
 1292|      0|
 1293|      0|  constexpr double beta[] = {-3.8679974580640881e-03, 7.6391885763341910e-02,  -4.2828141436397615e-01,
 1294|      0|                             9.8733495886883648e-01,  -1.0000000000000000e+00, 3.6918986189438030e-01};
 1295|      0|
 1296|      0|  const Packet x2 = pmul(x, x);
 1297|      0|  const Packet x3 = pmul(x, x2);
 1298|      0|  Packet p = ppolevl<Packet, 4>::run(x2, alpha);
 1299|      0|  Packet q = ppolevl<Packet, 5>::run(x2, beta);
 1300|      0|  Packet y_small = pmadd(x3, pdiv(p, q), x);
 1301|      0|
 1302|      0|  // For |x| in ]0.5:1.0] we use atanh = 0.5*ln((1+x)/(1-x));
 1303|      0|  const Packet half = pset1<Packet>(0.5);
 1304|      0|  const Packet one = pset1<Packet>(1.0);
 1305|      0|  Packet y_large = pdiv(padd(one, x), psub(one, x));
 1306|      0|  y_large = pmul(half, plog(y_large));
 1307|      0|
 1308|      0|  const Packet x_gt_half = pcmp_le(half, pabs(x));
 1309|      0|  const Packet x_eq_one = pcmp_eq(one, pabs(x));
 1310|      0|  const Packet x_gt_one = pcmp_lt(one, pabs(x));
 1311|      0|  const Packet sign_mask = pset1<Packet>(-0.0);
 1312|      0|  const Packet x_sign = pand(sign_mask, x);
 1313|      0|  const Packet inf = pset1<Packet>(std::numeric_limits<double>::infinity());
 1314|      0|  return por(x_gt_one, pselect(x_eq_one, por(x_sign, inf), pselect(x_gt_half, y_large, y_small)));
 1315|      0|}
 1316|       |
 1317|       |template <typename Packet>
 1318|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pdiv_complex(const Packet& x, const Packet& y) {
 1319|      0|  typedef typename unpacket_traits<Packet>::as_real RealPacket;
 1320|      0|  // In the following we annotate the code for the case where the inputs
 1321|      0|  // are a pair length-2 SIMD vectors representing a single pair of complex
 1322|      0|  // numbers x = a + i*b, y = c + i*d.
 1323|      0|  const RealPacket y_abs = pabs(y.v);                        // |c|, |d|
 1324|      0|  const RealPacket y_abs_flip = pcplxflip(Packet(y_abs)).v;  // |d|, |c|
 1325|      0|  const RealPacket y_max = pmax(y_abs, y_abs_flip);          // max(|c|, |d|), max(|c|, |d|)
 1326|      0|  const RealPacket y_scaled = pdiv(y.v, y_max);              // c / max(|c|, |d|), d / max(|c|, |d|)
 1327|      0|  // Compute scaled denominator.
 1328|      0|  const RealPacket y_scaled_sq = pmul(y_scaled, y_scaled);  // c'**2, d'**2
 1329|      0|  const RealPacket denom = padd(y_scaled_sq, pcplxflip(Packet(y_scaled_sq)).v);
 1330|      0|  Packet result_scaled = pmul(x, pconj(Packet(y_scaled)));  // a * c' + b * d', -a * d + b * c
 1331|      0|  // Divide elementwise by denom.
 1332|      0|  result_scaled = Packet(pdiv(result_scaled.v, denom));
 1333|      0|  // Rescale result
 1334|      0|  return Packet(pdiv(result_scaled.v, y_max));
 1335|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pdiv_complexINS0_9Packet2cfEEET_RKS3_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12pdiv_complexINS0_9Packet1cdEEET_RKS3_S5_
  ------------------
 1336|       |
 1337|       |template <typename Packet>
 1338|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_complex(const Packet& x) {
 1339|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1340|      0|  typedef typename Scalar::value_type RealScalar;
 1341|      0|  typedef typename unpacket_traits<Packet>::as_real RealPacket;
 1342|      0|
 1343|      0|  RealPacket real_mask_rp = peven_mask(x.v);
 1344|      0|  Packet real_mask(real_mask_rp);
 1345|      0|
 1346|      0|  // Real part
 1347|      0|  RealPacket x_flip = pcplxflip(x).v;  // b, a
 1348|      0|  Packet x_norm = phypot_complex(x);   // sqrt(a^2 + b^2), sqrt(a^2 + b^2)
 1349|      0|  RealPacket xlogr = plog(x_norm.v);   // log(sqrt(a^2 + b^2)), log(sqrt(a^2 + b^2))
 1350|      0|
 1351|      0|  // Imag part
 1352|      0|  RealPacket ximg = patan2(x.v, x_flip);  // atan2(a, b), atan2(b, a)
 1353|      0|
 1354|      0|  const RealPacket cst_pos_inf = pset1<RealPacket>(NumTraits<RealScalar>::infinity());
 1355|      0|  RealPacket x_abs = pabs(x.v);
 1356|      0|  RealPacket is_x_pos_inf = pcmp_eq(x_abs, cst_pos_inf);
 1357|      0|  RealPacket is_y_pos_inf = pcplxflip(Packet(is_x_pos_inf)).v;
 1358|      0|  RealPacket is_any_inf = por(is_x_pos_inf, is_y_pos_inf);
 1359|      0|  RealPacket xreal = pselect(is_any_inf, cst_pos_inf, xlogr);
 1360|      0|
 1361|      0|  Packet xres = pselect(real_mask, Packet(xreal), Packet(ximg));  // log(sqrt(a^2 + b^2)), atan2(b, a)
 1362|      0|  return xres;
 1363|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12plog_complexINS0_9Packet1cdEEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12plog_complexINS0_9Packet2cfEEET_RKS3_
  ------------------
 1364|       |
 1365|       |template <typename Packet>
 1366|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp_complex(const Packet& a) {
 1367|      0|  typedef typename unpacket_traits<Packet>::as_real RealPacket;
 1368|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1369|      0|  typedef typename Scalar::value_type RealScalar;
 1370|      0|  const RealPacket even_mask = peven_mask(a.v);
 1371|      0|  const RealPacket odd_mask = pcplxflip(Packet(even_mask)).v;
 1372|      0|
 1373|      0|  // Let a = x + iy.
 1374|      0|  // exp(a) = exp(x) * cis(y), plus some special edge-case handling.
 1375|      0|
 1376|      0|  // exp(x):
 1377|      0|  RealPacket x = pand(a.v, even_mask);
 1378|      0|  x = por(x, pcplxflip(Packet(x)).v);
 1379|      0|  RealPacket expx = pexp(x);  // exp(x);
 1380|      0|
 1381|      0|  // cis(y):
 1382|      0|  RealPacket y = pand(odd_mask, a.v);
 1383|      0|  y = por(y, pcplxflip(Packet(y)).v);
 1384|      0|  RealPacket cisy = psincos_float<false, RealPacket, true>(y);
 1385|      0|  cisy = pcplxflip(Packet(cisy)).v;  // cos(y) + i * sin(y)
 1386|      0|
 1387|      0|  const RealPacket cst_pos_inf = pset1<RealPacket>(NumTraits<RealScalar>::infinity());
 1388|      0|  const RealPacket cst_neg_inf = pset1<RealPacket>(-NumTraits<RealScalar>::infinity());
 1389|      0|
 1390|      0|  // If x is -inf, we know that cossin(y) is bounded,
 1391|      0|  //   so the result is (0, +/-0), where the sign of the imaginary part comes
 1392|      0|  //   from the sign of cossin(y).
 1393|      0|  RealPacket cisy_sign = por(pandnot(cisy, pabs(cisy)), pset1<RealPacket>(RealScalar(1)));
 1394|      0|  cisy = pselect(pcmp_eq(x, cst_neg_inf), cisy_sign, cisy);
 1395|      0|
 1396|      0|  // If x is inf, and cos(y) has unknown sign (y is inf or NaN), the result
 1397|      0|  // is (+/-inf, NaN), where the signs are undetermined (take the sign of y).
 1398|      0|  RealPacket y_sign = por(pandnot(y, pabs(y)), pset1<RealPacket>(RealScalar(1)));
 1399|      0|  cisy = pselect(pand(pcmp_eq(x, cst_pos_inf), pisnan(cisy)), pand(y_sign, even_mask), cisy);
 1400|      0|  Packet result = Packet(pmul(expx, cisy));
 1401|      0|
 1402|      0|  // If y is +/- 0, the input is real, so take the real result for consistency.
 1403|      0|  result = pselect(Packet(pcmp_eq(y, pzero(y))), Packet(por(pand(expx, even_mask), pand(y, odd_mask))), result);
 1404|      0|
 1405|      0|  return result;
 1406|      0|}
 1407|       |
 1408|       |template <typename Packet>
 1409|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psqrt_complex(const Packet& a) {
 1410|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1411|      0|  typedef typename Scalar::value_type RealScalar;
 1412|      0|  typedef typename unpacket_traits<Packet>::as_real RealPacket;
 1413|      0|
 1414|      0|  // Computes the principal sqrt of the complex numbers in the input.
 1415|      0|  //
 1416|      0|  // For example, for packets containing 2 complex numbers stored in interleaved format
 1417|      0|  //    a = [a0, a1] = [x0, y0, x1, y1],
 1418|      0|  // where x0 = real(a0), y0 = imag(a0) etc., this function returns
 1419|      0|  //    b = [b0, b1] = [u0, v0, u1, v1],
 1420|      0|  // such that b0^2 = a0, b1^2 = a1.
 1421|      0|  //
 1422|      0|  // To derive the formula for the complex square roots, let's consider the equation for
 1423|      0|  // a single complex square root of the number x + i*y. We want to find real numbers
 1424|      0|  // u and v such that
 1425|      0|  //    (u + i*v)^2 = x + i*y  <=>
 1426|      0|  //    u^2 - v^2 + i*2*u*v = x + i*v.
 1427|      0|  // By equating the real and imaginary parts we get:
 1428|      0|  //    u^2 - v^2 = x
 1429|      0|  //    2*u*v = y.
 1430|      0|  //
 1431|      0|  // For x >= 0, this has the numerically stable solution
 1432|      0|  //    u = sqrt(0.5 * (x + sqrt(x^2 + y^2)))
 1433|      0|  //    v = 0.5 * (y / u)
 1434|      0|  // and for x < 0,
 1435|      0|  //    v = sign(y) * sqrt(0.5 * (-x + sqrt(x^2 + y^2)))
 1436|      0|  //    u = 0.5 * (y / v)
 1437|      0|  //
 1438|      0|  //  To avoid unnecessary over- and underflow, we compute sqrt(x^2 + y^2) as
 1439|      0|  //     l = max(|x|, |y|) * sqrt(1 + (min(|x|, |y|) / max(|x|, |y|))^2) ,
 1440|      0|
 1441|      0|  // In the following, without lack of generality, we have annotated the code, assuming
 1442|      0|  // that the input is a packet of 2 complex numbers.
 1443|      0|  //
 1444|      0|  // Step 1. Compute l = [l0, l0, l1, l1], where
 1445|      0|  //    l0 = sqrt(x0^2 + y0^2),  l1 = sqrt(x1^2 + y1^2)
 1446|      0|  // To avoid over- and underflow, we use the stable formula for each hypotenuse
 1447|      0|  //    l0 = (min0 == 0 ? max0 : max0 * sqrt(1 + (min0/max0)**2)),
 1448|      0|  // where max0 = max(|x0|, |y0|), min0 = min(|x0|, |y0|), and similarly for l1.
 1449|      0|
 1450|      0|  RealPacket a_abs = pabs(a.v);                        // [|x0|, |y0|, |x1|, |y1|]
 1451|      0|  RealPacket a_abs_flip = pcplxflip(Packet(a_abs)).v;  // [|y0|, |x0|, |y1|, |x1|]
 1452|      0|  RealPacket a_max = pmax(a_abs, a_abs_flip);
 1453|      0|  RealPacket a_min = pmin(a_abs, a_abs_flip);
 1454|      0|  RealPacket a_min_zero_mask = pcmp_eq(a_min, pzero(a_min));
 1455|      0|  RealPacket a_max_zero_mask = pcmp_eq(a_max, pzero(a_max));
 1456|      0|  RealPacket r = pdiv(a_min, a_max);
 1457|      0|  const RealPacket cst_one = pset1<RealPacket>(RealScalar(1));
 1458|      0|  RealPacket l = pmul(a_max, psqrt(padd(cst_one, pmul(r, r))));  // [l0, l0, l1, l1]
 1459|      0|  // Set l to a_max if a_min is zero.
 1460|      0|  l = pselect(a_min_zero_mask, a_max, l);
 1461|      0|
 1462|      0|  // Step 2. Compute [rho0, *, rho1, *], where
 1463|      0|  // rho0 = sqrt(0.5 * (l0 + |x0|)), rho1 =  sqrt(0.5 * (l1 + |x1|))
 1464|      0|  // We don't care about the imaginary parts computed here. They will be overwritten later.
 1465|      0|  const RealPacket cst_half = pset1<RealPacket>(RealScalar(0.5));
 1466|      0|  Packet rho;
 1467|      0|  rho.v = psqrt(pmul(cst_half, padd(a_abs, l)));
 1468|      0|
 1469|      0|  // Step 3. Compute [rho0, eta0, rho1, eta1], where
 1470|      0|  // eta0 = (y0 / l0) / 2, and eta1 = (y1 / l1) / 2.
 1471|      0|  // set eta = 0 of input is 0 + i0.
 1472|      0|  RealPacket eta = pandnot(pmul(cst_half, pdiv(a.v, pcplxflip(rho).v)), a_max_zero_mask);
 1473|      0|  RealPacket real_mask = peven_mask(a.v);
 1474|      0|  Packet positive_real_result;
 1475|      0|  // Compute result for inputs with positive real part.
 1476|      0|  positive_real_result.v = pselect(real_mask, rho.v, eta);
 1477|      0|
 1478|      0|  // Step 4. Compute solution for inputs with negative real part:
 1479|      0|  //         [|eta0|, sign(y0)*rho0, |eta1|, sign(y1)*rho1]
 1480|      0|  const RealPacket cst_imag_sign_mask = pset1<Packet>(Scalar(RealScalar(0.0), RealScalar(-0.0))).v;
 1481|      0|  RealPacket imag_signs = pand(a.v, cst_imag_sign_mask);
 1482|      0|  Packet negative_real_result;
 1483|      0|  // Notice that rho is positive, so taking it's absolute value is a noop.
 1484|      0|  negative_real_result.v = por(pabs(pcplxflip(positive_real_result).v), imag_signs);
 1485|      0|
 1486|      0|  // Step 5. Select solution branch based on the sign of the real parts.
 1487|      0|  Packet negative_real_mask;
 1488|      0|  negative_real_mask.v = pcmp_lt(pand(real_mask, a.v), pzero(a.v));
 1489|      0|  negative_real_mask.v = por(negative_real_mask.v, pcplxflip(negative_real_mask).v);
 1490|      0|  Packet result = pselect(negative_real_mask, negative_real_result, positive_real_result);
 1491|      0|
 1492|      0|  // Step 6. Handle special cases for infinities:
 1493|      0|  // * If z is (x,+∞), the result is (+∞,+∞) even if x is NaN
 1494|      0|  // * If z is (x,-∞), the result is (+∞,-∞) even if x is NaN
 1495|      0|  // * If z is (-∞,y), the result is (0*|y|,+∞) for finite or NaN y
 1496|      0|  // * If z is (+∞,y), the result is (+∞,0*|y|) for finite or NaN y
 1497|      0|  const RealPacket cst_pos_inf = pset1<RealPacket>(NumTraits<RealScalar>::infinity());
 1498|      0|  Packet is_inf;
 1499|      0|  is_inf.v = pcmp_eq(a_abs, cst_pos_inf);
 1500|      0|  Packet is_real_inf;
 1501|      0|  is_real_inf.v = pand(is_inf.v, real_mask);
 1502|      0|  is_real_inf = por(is_real_inf, pcplxflip(is_real_inf));
 1503|      0|  // prepare packet of (+∞,0*|y|) or (0*|y|,+∞), depending on the sign of the infinite real part.
 1504|      0|  Packet real_inf_result;
 1505|      0|  real_inf_result.v = pmul(a_abs, pset1<Packet>(Scalar(RealScalar(1.0), RealScalar(0.0))).v);
 1506|      0|  real_inf_result.v = pselect(negative_real_mask.v, pcplxflip(real_inf_result).v, real_inf_result.v);
 1507|      0|  // prepare packet of (+∞,+∞) or (+∞,-∞), depending on the sign of the infinite imaginary part.
 1508|      0|  Packet is_imag_inf;
 1509|      0|  is_imag_inf.v = pandnot(is_inf.v, real_mask);
 1510|      0|  is_imag_inf = por(is_imag_inf, pcplxflip(is_imag_inf));
 1511|      0|  Packet imag_inf_result;
 1512|      0|  imag_inf_result.v = por(pand(cst_pos_inf, real_mask), pandnot(a.v, real_mask));
 1513|      0|  // unless otherwise specified, if either the real or imaginary component is nan, the entire result is nan
 1514|      0|  Packet result_is_nan = pisnan(result);
 1515|      0|  result = por(result_is_nan, result);
 1516|      0|
 1517|      0|  return pselect(is_imag_inf, imag_inf_result, pselect(is_real_inf, real_inf_result, result));
 1518|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13psqrt_complexINS0_9Packet1cdEEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13psqrt_complexINS0_9Packet2cfEEET_RKS3_
  ------------------
 1519|       |
 1520|       |// \internal \returns the norm of a complex number z = x + i*y, defined as sqrt(x^2 + y^2).
 1521|       |// Implemented using the hypot(a,b) algorithm from https://doi.org/10.48550/arXiv.1904.09481
 1522|       |template <typename Packet>
 1523|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet phypot_complex(const Packet& a) {
 1524|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1525|      0|  typedef typename Scalar::value_type RealScalar;
 1526|      0|  typedef typename unpacket_traits<Packet>::as_real RealPacket;
 1527|      0|
 1528|      0|  const RealPacket cst_zero_rp = pset1<RealPacket>(static_cast<RealScalar>(0.0));
 1529|      0|  const RealPacket cst_minus_one_rp = pset1<RealPacket>(static_cast<RealScalar>(-1.0));
 1530|      0|  const RealPacket cst_two_rp = pset1<RealPacket>(static_cast<RealScalar>(2.0));
 1531|      0|  const RealPacket evenmask = peven_mask(a.v);
 1532|      0|
 1533|      0|  RealPacket a_abs = pabs(a.v);
 1534|      0|  RealPacket a_flip = pcplxflip(Packet(a_abs)).v;       // |b|, |a|
 1535|      0|  RealPacket a_all = pselect(evenmask, a_abs, a_flip);  // |a|, |a|
 1536|      0|  RealPacket b_all = pselect(evenmask, a_flip, a_abs);  // |b|, |b|
 1537|      0|
 1538|      0|  RealPacket a2 = pmul(a.v, a.v);                    // |a^2, b^2|
 1539|      0|  RealPacket a2_flip = pcplxflip(Packet(a2)).v;      // |b^2, a^2|
 1540|      0|  RealPacket h = psqrt(padd(a2, a2_flip));           // |sqrt(a^2 + b^2), sqrt(a^2 + b^2)|
 1541|      0|  RealPacket h_sq = pmul(h, h);                      // |a^2 + b^2, a^2 + b^2|
 1542|      0|  RealPacket a_sq = pselect(evenmask, a2, a2_flip);  // |a^2, a^2|
 1543|      0|  RealPacket m_h_sq = pmul(h_sq, cst_minus_one_rp);
 1544|      0|  RealPacket m_a_sq = pmul(a_sq, cst_minus_one_rp);
 1545|      0|  RealPacket x = psub(psub(pmadd(h, h, m_h_sq), pmadd(b_all, b_all, psub(a_sq, h_sq))), pmadd(a_all, a_all, m_a_sq));
 1546|      0|  h = psub(h, pdiv(x, pmul(cst_two_rp, h)));  // |h - x/(2*h), h - x/(2*h)|
 1547|      0|
 1548|      0|  // handle zero-case
 1549|      0|  RealPacket iszero = pcmp_eq(por(a_abs, a_flip), cst_zero_rp);
 1550|      0|
 1551|      0|  h = pandnot(h, iszero);  // |sqrt(a^2+b^2), sqrt(a^2+b^2)|
 1552|      0|  return Packet(h);        // |sqrt(a^2+b^2), sqrt(a^2+b^2)|
 1553|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14phypot_complexINS0_9Packet1cdEEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14phypot_complexINS0_9Packet2cfEEET_RKS3_
  ------------------
 1554|       |
 1555|       |template <typename Packet>
 1556|       |struct psign_impl<Packet, std::enable_if_t<!NumTraits<typename unpacket_traits<Packet>::type>::IsComplex &&
 1557|       |                                           !NumTraits<typename unpacket_traits<Packet>::type>::IsInteger>> {
 1558|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a) {
 1559|       |    using Scalar = typename unpacket_traits<Packet>::type;
 1560|       |    const Packet cst_one = pset1<Packet>(Scalar(1));
 1561|       |    const Packet cst_zero = pzero(a);
 1562|       |
 1563|       |    const Packet abs_a = pabs(a);
 1564|       |    const Packet sign_mask = pandnot(a, abs_a);
 1565|       |    const Packet nonzero_mask = pcmp_lt(cst_zero, abs_a);
 1566|       |
 1567|       |    return pselect(nonzero_mask, por(sign_mask, cst_one), abs_a);
 1568|       |  }
 1569|       |};
 1570|       |
 1571|       |template <typename Packet>
 1572|       |struct psign_impl<Packet, std::enable_if_t<!NumTraits<typename unpacket_traits<Packet>::type>::IsComplex &&
 1573|       |                                           NumTraits<typename unpacket_traits<Packet>::type>::IsSigned &&
 1574|       |                                           NumTraits<typename unpacket_traits<Packet>::type>::IsInteger>> {
 1575|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a) {
 1576|       |    using Scalar = typename unpacket_traits<Packet>::type;
 1577|       |    const Packet cst_one = pset1<Packet>(Scalar(1));
 1578|       |    const Packet cst_minus_one = pset1<Packet>(Scalar(-1));
 1579|       |    const Packet cst_zero = pzero(a);
 1580|       |
 1581|       |    const Packet positive_mask = pcmp_lt(cst_zero, a);
 1582|       |    const Packet positive = pand(positive_mask, cst_one);
 1583|       |    const Packet negative_mask = pcmp_lt(a, cst_zero);
 1584|       |    const Packet negative = pand(negative_mask, cst_minus_one);
 1585|       |
 1586|       |    return por(positive, negative);
 1587|       |  }
 1588|       |};
 1589|       |
 1590|       |template <typename Packet>
 1591|       |struct psign_impl<Packet, std::enable_if_t<!NumTraits<typename unpacket_traits<Packet>::type>::IsComplex &&
 1592|       |                                           !NumTraits<typename unpacket_traits<Packet>::type>::IsSigned &&
 1593|       |                                           NumTraits<typename unpacket_traits<Packet>::type>::IsInteger>> {
 1594|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a) {
 1595|       |    using Scalar = typename unpacket_traits<Packet>::type;
 1596|       |    const Packet cst_one = pset1<Packet>(Scalar(1));
 1597|       |    const Packet cst_zero = pzero(a);
 1598|       |
 1599|       |    const Packet zero_mask = pcmp_eq(cst_zero, a);
 1600|       |    return pandnot(cst_one, zero_mask);
 1601|       |  }
 1602|       |};
 1603|       |
 1604|       |// \internal \returns the the sign of a complex number z, defined as z / abs(z).
 1605|       |template <typename Packet>
 1606|       |struct psign_impl<Packet, std::enable_if_t<NumTraits<typename unpacket_traits<Packet>::type>::IsComplex &&
 1607|       |                                           unpacket_traits<Packet>::vectorizable>> {
 1608|       |  static EIGEN_DEVICE_FUNC inline Packet run(const Packet& a) {
 1609|       |    typedef typename unpacket_traits<Packet>::type Scalar;
 1610|       |    typedef typename Scalar::value_type RealScalar;
 1611|       |    typedef typename unpacket_traits<Packet>::as_real RealPacket;
 1612|       |
 1613|       |    // Step 1. Compute (for each element z = x + i*y in a)
 1614|       |    //     l = abs(z) = sqrt(x^2 + y^2).
 1615|       |    // To avoid over- and underflow, we use the stable formula for each hypotenuse
 1616|       |    //    l = (zmin == 0 ? zmax : zmax * sqrt(1 + (zmin/zmax)**2)),
 1617|       |    // where zmax = max(|x|, |y|), zmin = min(|x|, |y|),
 1618|       |    RealPacket a_abs = pabs(a.v);
 1619|       |    RealPacket a_abs_flip = pcplxflip(Packet(a_abs)).v;
 1620|       |    RealPacket a_max = pmax(a_abs, a_abs_flip);
 1621|       |    RealPacket a_min = pmin(a_abs, a_abs_flip);
 1622|       |    RealPacket a_min_zero_mask = pcmp_eq(a_min, pzero(a_min));
 1623|       |    RealPacket a_max_zero_mask = pcmp_eq(a_max, pzero(a_max));
 1624|       |    RealPacket r = pdiv(a_min, a_max);
 1625|       |    const RealPacket cst_one = pset1<RealPacket>(RealScalar(1));
 1626|       |    RealPacket l = pmul(a_max, psqrt(padd(cst_one, pmul(r, r))));  // [l0, l0, l1, l1]
 1627|       |    // Set l to a_max if a_min is zero, since the roundtrip sqrt(a_max^2) may be
 1628|       |    // lossy.
 1629|       |    l = pselect(a_min_zero_mask, a_max, l);
 1630|       |    // Step 2 compute a / abs(a).
 1631|       |    RealPacket sign_as_real = pandnot(pdiv(a.v, l), a_max_zero_mask);
 1632|       |    Packet sign;
 1633|       |    sign.v = sign_as_real;
 1634|       |    return sign;
 1635|       |  }
 1636|       |};
 1637|       |
 1638|       |// TODO(rmlarsen): The following set of utilities for double word arithmetic
 1639|       |// should perhaps be refactored as a separate file, since it would be generally
 1640|       |// useful for special function implementation etc. Writing the algorithms in
 1641|       |// terms if a double word type would also make the code more readable.
 1642|       |
 1643|       |// This function splits x into the nearest integer n and fractional part r,
 1644|       |// such that x = n + r holds exactly.
 1645|       |template <typename Packet>
 1646|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void absolute_split(const Packet& x, Packet& n, Packet& r) {
 1647|       |  n = pround(x);
 1648|       |  r = psub(x, n);
 1649|       |}
 1650|       |
 1651|       |// This function computes the sum {s, r}, such that x + y = s_hi + s_lo
 1652|       |// holds exactly, and s_hi = fl(x+y), if |x| >= |y|.
 1653|       |template <typename Packet>
 1654|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void fast_twosum(const Packet& x, const Packet& y, Packet& s_hi, Packet& s_lo) {
 1655|       |  s_hi = padd(x, y);
 1656|       |  const Packet t = psub(s_hi, x);
 1657|       |  s_lo = psub(y, t);
 1658|       |}
 1659|       |
 1660|       |#ifdef EIGEN_VECTORIZE_FMA
 1661|       |// This function implements the extended precision product of
 1662|       |// a pair of floating point numbers. Given {x, y}, it computes the pair
 1663|       |// {p_hi, p_lo} such that x * y = p_hi + p_lo holds exactly and
 1664|       |// p_hi = fl(x * y).
 1665|       |template <typename Packet>
 1666|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void twoprod(const Packet& x, const Packet& y, Packet& p_hi, Packet& p_lo) {
 1667|       |  p_hi = pmul(x, y);
 1668|       |  p_lo = pmsub(x, y, p_hi);
 1669|       |}
 1670|       |
 1671|       |// A version of twoprod that takes x, y, and fl(x*y) as input and returns the p_lo such that
 1672|       |// x * y = xy + p_lo holds exactly.
 1673|       |template <typename Packet>
 1674|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet twoprod_low(const Packet& x, const Packet& y, const Packet& xy) {
 1675|       |  return pmsub(x, y, xy);
 1676|       |}
 1677|       |
 1678|       |#else
 1679|       |
 1680|       |// This function implements the Veltkamp splitting. Given a floating point
 1681|       |// number x it returns the pair {x_hi, x_lo} such that x_hi + x_lo = x holds
 1682|       |// exactly and that half of the significant of x fits in x_hi.
 1683|       |// This is Algorithm 3 from Jean-Michel Muller, "Elementary Functions",
 1684|       |// 3rd edition, Birkh\"auser, 2016.
 1685|       |template <typename Packet>
 1686|      0|EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void veltkamp_splitting(const Packet& x, Packet& x_hi, Packet& x_lo) {
 1687|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 1688|      0|  EIGEN_CONSTEXPR int shift = (NumTraits<Scalar>::digits() + 1) / 2;
 1689|      0|  const Scalar shift_scale = Scalar(uint64_t(1) << shift);  // Scalar constructor not necessarily constexpr.
 1690|      0|  const Packet gamma = pmul(pset1<Packet>(shift_scale + Scalar(1)), x);
 1691|      0|  Packet rho = psub(x, gamma);
 1692|      0|  x_hi = padd(rho, gamma);
 1693|      0|  x_lo = psub(x, x_hi);
 1694|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18veltkamp_splittingIDv4_fEEvRKT_RS3_S6_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18veltkamp_splittingIDv2_dEEvRKT_RS3_S6_
  ------------------
 1695|       |
 1696|       |// This function implements Dekker's algorithm for products x * y.
 1697|       |// Given floating point numbers {x, y} computes the pair
 1698|       |// {p_hi, p_lo} such that x * y = p_hi + p_lo holds exactly and
 1699|       |// p_hi = fl(x * y).
 1700|       |template <typename Packet>
 1701|      0|EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void twoprod(const Packet& x, const Packet& y, Packet& p_hi, Packet& p_lo) {
 1702|      0|  Packet x_hi, x_lo, y_hi, y_lo;
 1703|      0|  veltkamp_splitting(x, x_hi, x_lo);
 1704|      0|  veltkamp_splitting(y, y_hi, y_lo);
 1705|      0|
 1706|      0|  p_hi = pmul(x, y);
 1707|      0|  p_lo = pmadd(x_hi, y_hi, pnegate(p_hi));
 1708|      0|  p_lo = pmadd(x_hi, y_lo, p_lo);
 1709|      0|  p_lo = pmadd(x_lo, y_hi, p_lo);
 1710|      0|  p_lo = pmadd(x_lo, y_lo, p_lo);
 1711|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7twoprodIDv4_fEEvRKT_S5_RS3_S6_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal7twoprodIDv2_dEEvRKT_S5_RS3_S6_
  ------------------
 1712|       |
 1713|       |// A version of twoprod that takes x, y, and fl(x*y) as input and returns the p_lo such that
 1714|       |// x * y = xy + p_lo holds exactly.
 1715|       |template <typename Packet>
 1716|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet twoprod_low(const Packet& x, const Packet& y, const Packet& xy) {
 1717|       |  Packet x_hi, x_lo, y_hi, y_lo;
 1718|       |  veltkamp_splitting(x, x_hi, x_lo);
 1719|       |  veltkamp_splitting(y, y_hi, y_lo);
 1720|       |
 1721|       |  Packet p_lo = pmadd(x_hi, y_hi, pnegate(xy));
 1722|       |  p_lo = pmadd(x_hi, y_lo, p_lo);
 1723|       |  p_lo = pmadd(x_lo, y_hi, p_lo);
 1724|       |  p_lo = pmadd(x_lo, y_lo, p_lo);
 1725|       |  return p_lo;
 1726|       |}
 1727|       |
 1728|       |#endif  // EIGEN_VECTORIZE_FMA
 1729|       |
 1730|       |// This function implements Dekker's algorithm for the addition
 1731|       |// of two double word numbers represented by {x_hi, x_lo} and {y_hi, y_lo}.
 1732|       |// It returns the result as a pair {s_hi, s_lo} such that
 1733|       |// x_hi + x_lo + y_hi + y_lo = s_hi + s_lo holds exactly.
 1734|       |// This is Algorithm 5 from Jean-Michel Muller, "Elementary Functions",
 1735|       |// 3rd edition, Birkh\"auser, 2016.
 1736|       |template <typename Packet>
 1737|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void twosum(const Packet& x_hi, const Packet& x_lo, const Packet& y_hi,
 1738|       |                                                  const Packet& y_lo, Packet& s_hi, Packet& s_lo) {
 1739|       |  const Packet x_greater_mask = pcmp_lt(pabs(y_hi), pabs(x_hi));
 1740|       |  Packet r_hi_1, r_lo_1;
 1741|       |  fast_twosum(x_hi, y_hi, r_hi_1, r_lo_1);
 1742|       |  Packet r_hi_2, r_lo_2;
 1743|       |  fast_twosum(y_hi, x_hi, r_hi_2, r_lo_2);
 1744|       |  const Packet r_hi = pselect(x_greater_mask, r_hi_1, r_hi_2);
 1745|       |
 1746|       |  const Packet s1 = padd(padd(y_lo, r_lo_1), x_lo);
 1747|       |  const Packet s2 = padd(padd(x_lo, r_lo_2), y_lo);
 1748|       |  const Packet s = pselect(x_greater_mask, s1, s2);
 1749|       |
 1750|       |  fast_twosum(r_hi, s, s_hi, s_lo);
 1751|       |}
 1752|       |
 1753|       |// This is a version of twosum for double word numbers,
 1754|       |// which assumes that |x_hi| >= |y_hi|.
 1755|       |template <typename Packet>
 1756|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void fast_twosum(const Packet& x_hi, const Packet& x_lo, const Packet& y_hi,
 1757|       |                                                       const Packet& y_lo, Packet& s_hi, Packet& s_lo) {
 1758|       |  Packet r_hi, r_lo;
 1759|       |  fast_twosum(x_hi, y_hi, r_hi, r_lo);
 1760|       |  const Packet s = padd(padd(y_lo, r_lo), x_lo);
 1761|       |  fast_twosum(r_hi, s, s_hi, s_lo);
 1762|       |}
 1763|       |
 1764|       |// This is a version of twosum for adding a floating point number x to
 1765|       |// double word number {y_hi, y_lo} number, with the assumption
 1766|       |// that |x| >= |y_hi|.
 1767|       |template <typename Packet>
 1768|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void fast_twosum(const Packet& x, const Packet& y_hi, const Packet& y_lo,
 1769|       |                                                       Packet& s_hi, Packet& s_lo) {
 1770|       |  Packet r_hi, r_lo;
 1771|       |  fast_twosum(x, y_hi, r_hi, r_lo);
 1772|       |  const Packet s = padd(y_lo, r_lo);
 1773|       |  fast_twosum(r_hi, s, s_hi, s_lo);
 1774|       |}
 1775|       |
 1776|       |// This function implements the multiplication of a double word
 1777|       |// number represented by {x_hi, x_lo} by a floating point number y.
 1778|       |// It returns the result as a pair {p_hi, p_lo} such that
 1779|       |// (x_hi + x_lo) * y = p_hi + p_lo hold with a relative error
 1780|       |// of less than 2*2^{-2p}, where p is the number of significand bit
 1781|       |// in the floating point type.
 1782|       |// This is Algorithm 7 from Jean-Michel Muller, "Elementary Functions",
 1783|       |// 3rd edition, Birkh\"auser, 2016.
 1784|       |template <typename Packet>
 1785|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void twoprod(const Packet& x_hi, const Packet& x_lo, const Packet& y,
 1786|       |                                                   Packet& p_hi, Packet& p_lo) {
 1787|       |  Packet c_hi, c_lo1;
 1788|       |  twoprod(x_hi, y, c_hi, c_lo1);
 1789|       |  const Packet c_lo2 = pmul(x_lo, y);
 1790|       |  Packet t_hi, t_lo1;
 1791|       |  fast_twosum(c_hi, c_lo2, t_hi, t_lo1);
 1792|       |  const Packet t_lo2 = padd(t_lo1, c_lo1);
 1793|       |  fast_twosum(t_hi, t_lo2, p_hi, p_lo);
 1794|       |}
 1795|       |
 1796|       |// This function implements the multiplication of two double word
 1797|       |// numbers represented by {x_hi, x_lo} and {y_hi, y_lo}.
 1798|       |// It returns the result as a pair {p_hi, p_lo} such that
 1799|       |// (x_hi + x_lo) * (y_hi + y_lo) = p_hi + p_lo holds with a relative error
 1800|       |// of less than 2*2^{-2p}, where p is the number of significand bit
 1801|       |// in the floating point type.
 1802|       |template <typename Packet>
 1803|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void twoprod(const Packet& x_hi, const Packet& x_lo, const Packet& y_hi,
 1804|       |                                                   const Packet& y_lo, Packet& p_hi, Packet& p_lo) {
 1805|       |  Packet p_hi_hi, p_hi_lo;
 1806|       |  twoprod(x_hi, x_lo, y_hi, p_hi_hi, p_hi_lo);
 1807|       |  Packet p_lo_hi, p_lo_lo;
 1808|       |  twoprod(x_hi, x_lo, y_lo, p_lo_hi, p_lo_lo);
 1809|       |  fast_twosum(p_hi_hi, p_hi_lo, p_lo_hi, p_lo_lo, p_hi, p_lo);
 1810|       |}
 1811|       |
 1812|       |// This function implements the division of double word {x_hi, x_lo}
 1813|       |// by float y. This is Algorithm 15 from "Tight and rigorous error bounds
 1814|       |// for basic building blocks of double-word arithmetic", Joldes, Muller, & Popescu,
 1815|       |// 2017. https://hal.archives-ouvertes.fr/hal-01351529
 1816|       |template <typename Packet>
 1817|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void doubleword_div_fp(const Packet& x_hi, const Packet& x_lo, const Packet& y,
 1818|       |                                                             Packet& z_hi, Packet& z_lo) {
 1819|       |  const Packet t_hi = pdiv(x_hi, y);
 1820|       |  Packet pi_hi, pi_lo;
 1821|       |  twoprod(t_hi, y, pi_hi, pi_lo);
 1822|       |  const Packet delta_hi = psub(x_hi, pi_hi);
 1823|       |  const Packet delta_t = psub(delta_hi, pi_lo);
 1824|       |  const Packet delta = padd(delta_t, x_lo);
 1825|       |  const Packet t_lo = pdiv(delta, y);
 1826|       |  fast_twosum(t_hi, t_lo, z_hi, z_lo);
 1827|       |}
 1828|       |
 1829|       |// This function computes log2(x) and returns the result as a double word.
 1830|       |template <typename Scalar>
 1831|       |struct accurate_log2 {
 1832|       |  template <typename Packet>
 1833|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(const Packet& x, Packet& log2_x_hi, Packet& log2_x_lo) {
 1834|       |    log2_x_hi = plog2(x);
 1835|       |    log2_x_lo = pzero(x);
 1836|       |  }
 1837|       |};
 1838|       |
 1839|       |// This specialization uses a more accurate algorithm to compute log2(x) for
 1840|       |// floats in [1/sqrt(2);sqrt(2)] with a relative accuracy of ~6.56508e-10.
 1841|       |// This additional accuracy is needed to counter the error-magnification
 1842|       |// inherent in multiplying by a potentially large exponent in pow(x,y).
 1843|       |// The minimax polynomial used was calculated using the Rminimax tool,
 1844|       |// see https://gitlab.inria.fr/sfilip/rminimax.
 1845|       |// Command line:
 1846|       |//   $ ratapprox --function="log2(1+x)/x"  --dom='[-0.2929,0.41422]'
 1847|       |//   --type=[10,0]
 1848|       |//       --numF="[D,D,SG]" --denF="[SG]" --log --dispCoeff="dec"
 1849|       |//
 1850|       |// The resulting implementation of pow(x,y) is accurate to 3 ulps.
 1851|       |template <>
 1852|       |struct accurate_log2<float> {
 1853|       |  template <typename Packet>
 1854|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(const Packet& z, Packet& log2_x_hi, Packet& log2_x_lo) {
 1855|       |    // Split the two lowest order constant coefficient into double-word representation.
 1856|       |    constexpr double kC0 = 1.442695041742110273474963832995854318141937255859375e+00;
 1857|       |    constexpr float kC0_hi = static_cast<float>(kC0);
 1858|       |    constexpr float kC0_lo = static_cast<float>(kC0 - static_cast<double>(kC0_hi));
 1859|       |    const Packet c0_hi = pset1<Packet>(kC0_hi);
 1860|       |    const Packet c0_lo = pset1<Packet>(kC0_lo);
 1861|       |
 1862|       |    constexpr double kC1 = -7.2134751588268664068692714863573201000690460205078125e-01;
 1863|       |    constexpr float kC1_hi = static_cast<float>(kC1);
 1864|       |    constexpr float kC1_lo = static_cast<float>(kC1 - static_cast<double>(kC1_hi));
 1865|       |    const Packet c1_hi = pset1<Packet>(kC1_hi);
 1866|       |    const Packet c1_lo = pset1<Packet>(kC1_lo);
 1867|       |
 1868|       |    constexpr float c[] = {
 1869|       |        9.7010828554630279541015625e-02,  -1.6896486282348632812500000e-01, 1.7200836539268493652343750e-01,
 1870|       |        -1.7892272770404815673828125e-01, 2.0505344867706298828125000e-01,  -2.4046677350997924804687500e-01,
 1871|       |        2.8857553005218505859375000e-01,  -3.6067414283752441406250000e-01, 4.8089790344238281250000000e-01};
 1872|       |
 1873|       |    // Evaluate the higher order terms in the polynomial using
 1874|       |    // standard arithmetic.
 1875|       |    const Packet one = pset1<Packet>(1.0f);
 1876|       |    const Packet x = psub(z, one);
 1877|       |    Packet p = ppolevl<Packet, 8>::run(x, c);
 1878|       |    // Evaluate the final two step in Horner's rule using double-word
 1879|       |    // arithmetic.
 1880|       |    Packet p_hi, p_lo;
 1881|       |    twoprod(x, p, p_hi, p_lo);
 1882|       |    fast_twosum(c1_hi, c1_lo, p_hi, p_lo, p_hi, p_lo);
 1883|       |    twoprod(p_hi, p_lo, x, p_hi, p_lo);
 1884|       |    fast_twosum(c0_hi, c0_lo, p_hi, p_lo, p_hi, p_lo);
 1885|       |    // Multiply by x to recover log2(z).
 1886|       |    twoprod(p_hi, p_lo, x, log2_x_hi, log2_x_lo);
 1887|       |  }
 1888|       |};
 1889|       |
 1890|       |// This specialization uses a more accurate algorithm to compute log2(x) for
 1891|       |// floats in [1/sqrt(2);sqrt(2)] with a relative accuracy of ~1.27e-18.
 1892|       |// This additional accuracy is needed to counter the error-magnification
 1893|       |// inherent in multiplying by a potentially large exponent in pow(x,y).
 1894|       |// The minimax polynomial used was calculated using the Sollya tool.
 1895|       |// See sollya.org.
 1896|       |
 1897|       |template <>
 1898|       |struct accurate_log2<double> {
 1899|       |  template <typename Packet>
 1900|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void operator()(const Packet& x, Packet& log2_x_hi, Packet& log2_x_lo) {
 1901|       |    // We use a transformation of variables:
 1902|       |    //    r = c * (x-1) / (x+1),
 1903|       |    // such that
 1904|       |    //    log2(x) = log2((1 + r/c) / (1 - r/c)) = f(r).
 1905|       |    // The function f(r) can be approximated well using an odd polynomial
 1906|       |    // of the form
 1907|       |    //   P(r) = ((Q(r^2) * r^2 + C) * r^2 + 1) * r,
 1908|       |    // For the implementation of log2<double> here, Q is of degree 6 with
 1909|       |    // coefficient represented in working precision (double), while C is a
 1910|       |    // constant represented in extra precision as a double word to achieve
 1911|       |    // full accuracy.
 1912|       |    //
 1913|       |    // The polynomial coefficients were computed by the Sollya script:
 1914|       |    //
 1915|       |    // c = 2 / log(2);
 1916|       |    // trans = c * (x-1)/(x+1);
 1917|       |    // itrans = (1+x/c)/(1-x/c);
 1918|       |    // interval=[trans(sqrt(0.5)); trans(sqrt(2))];
 1919|       |    // print(interval);
 1920|       |    // f = log2(itrans(x));
 1921|       |    // p=fpminimax(f,[|1,3,5,7,9,11,13,15,17|],[|1,DD,double...|],interval,relative,floating);
 1922|       |    const Packet q12 = pset1<Packet>(2.87074255468000586e-9);
 1923|       |    const Packet q10 = pset1<Packet>(2.38957980901884082e-8);
 1924|       |    const Packet q8 = pset1<Packet>(2.31032094540014656e-7);
 1925|       |    const Packet q6 = pset1<Packet>(2.27279857398537278e-6);
 1926|       |    const Packet q4 = pset1<Packet>(2.31271023278625638e-5);
 1927|       |    const Packet q2 = pset1<Packet>(2.47556738444535513e-4);
 1928|       |    const Packet q0 = pset1<Packet>(2.88543873228900172e-3);
 1929|       |    const Packet C_hi = pset1<Packet>(0.0400377511598501157);
 1930|       |    const Packet C_lo = pset1<Packet>(-4.77726582251425391e-19);
 1931|       |    const Packet one = pset1<Packet>(1.0);
 1932|       |
 1933|       |    const Packet cst_2_log2e_hi = pset1<Packet>(2.88539008177792677);
 1934|       |    const Packet cst_2_log2e_lo = pset1<Packet>(4.07660016854549667e-17);
 1935|       |    // c * (x - 1)
 1936|       |    Packet t_hi, t_lo;
 1937|       |    // t = c * (x-1)
 1938|       |    twoprod(cst_2_log2e_hi, cst_2_log2e_lo, psub(x, one), t_hi, t_lo);
 1939|       |    // r = c * (x-1) / (x+1),
 1940|       |    Packet r_hi, r_lo;
 1941|       |    doubleword_div_fp(t_hi, t_lo, padd(x, one), r_hi, r_lo);
 1942|       |
 1943|       |    // r2 = r * r
 1944|       |    Packet r2_hi, r2_lo;
 1945|       |    twoprod(r_hi, r_lo, r_hi, r_lo, r2_hi, r2_lo);
 1946|       |    // r4 = r2 * r2
 1947|       |    Packet r4_hi, r4_lo;
 1948|       |    twoprod(r2_hi, r2_lo, r2_hi, r2_lo, r4_hi, r4_lo);
 1949|       |
 1950|       |    // Evaluate Q(r^2) in working precision. We evaluate it in two parts
 1951|       |    // (even and odd in r^2) to improve instruction level parallelism.
 1952|       |    Packet q_even = pmadd(q12, r4_hi, q8);
 1953|       |    Packet q_odd = pmadd(q10, r4_hi, q6);
 1954|       |    q_even = pmadd(q_even, r4_hi, q4);
 1955|       |    q_odd = pmadd(q_odd, r4_hi, q2);
 1956|       |    q_even = pmadd(q_even, r4_hi, q0);
 1957|       |    Packet q = pmadd(q_odd, r2_hi, q_even);
 1958|       |
 1959|       |    // Now evaluate the low order terms of P(x) in double word precision.
 1960|       |    // In the following, due to the increasing magnitude of the coefficients
 1961|       |    // and r being constrained to [-0.5, 0.5] we can use fast_twosum instead
 1962|       |    // of the slower twosum.
 1963|       |    // Q(r^2) * r^2
 1964|       |    Packet p_hi, p_lo;
 1965|       |    twoprod(r2_hi, r2_lo, q, p_hi, p_lo);
 1966|       |    // Q(r^2) * r^2 + C
 1967|       |    Packet p1_hi, p1_lo;
 1968|       |    fast_twosum(C_hi, C_lo, p_hi, p_lo, p1_hi, p1_lo);
 1969|       |    // (Q(r^2) * r^2 + C) * r^2
 1970|       |    Packet p2_hi, p2_lo;
 1971|       |    twoprod(r2_hi, r2_lo, p1_hi, p1_lo, p2_hi, p2_lo);
 1972|       |    // ((Q(r^2) * r^2 + C) * r^2 + 1)
 1973|       |    Packet p3_hi, p3_lo;
 1974|       |    fast_twosum(one, p2_hi, p2_lo, p3_hi, p3_lo);
 1975|       |
 1976|       |    // log(z) ~= ((Q(r^2) * r^2 + C) * r^2 + 1) * r
 1977|       |    twoprod(p3_hi, p3_lo, r_hi, r_lo, log2_x_hi, log2_x_lo);
 1978|       |  }
 1979|       |};
 1980|       |
 1981|       |// This function implements the non-trivial case of pow(x,y) where x is
 1982|       |// positive and y is (possibly) non-integer.
 1983|       |// Formally, pow(x,y) = exp2(y * log2(x)), where exp2(x) is shorthand for 2^x.
 1984|       |// TODO(rmlarsen): We should probably add this as a packet up 'ppow', to make it
 1985|       |// easier to specialize or turn off for specific types and/or backends.x
 1986|       |template <typename Packet>
 1987|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_pow_impl(const Packet& x, const Packet& y) {
 1988|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 1989|       |  // Split x into exponent e_x and mantissa m_x.
 1990|       |  Packet e_x;
 1991|       |  Packet m_x = pfrexp(x, e_x);
 1992|       |
 1993|       |  // Adjust m_x to lie in [1/sqrt(2):sqrt(2)] to minimize absolute error in log2(m_x).
 1994|       |  EIGEN_CONSTEXPR Scalar sqrt_half = Scalar(0.70710678118654752440);
 1995|       |  const Packet m_x_scale_mask = pcmp_lt(m_x, pset1<Packet>(sqrt_half));
 1996|       |  m_x = pselect(m_x_scale_mask, pmul(pset1<Packet>(Scalar(2)), m_x), m_x);
 1997|       |  e_x = pselect(m_x_scale_mask, psub(e_x, pset1<Packet>(Scalar(1))), e_x);
 1998|       |
 1999|       |  // Compute log2(m_x) with 6 extra bits of accuracy.
 2000|       |  Packet rx_hi, rx_lo;
 2001|       |  accurate_log2<Scalar>()(m_x, rx_hi, rx_lo);
 2002|       |
 2003|       |  // Compute the two terms {y * e_x, y * r_x} in f = y * log2(x) with doubled
 2004|       |  // precision using double word arithmetic.
 2005|       |  Packet f1_hi, f1_lo, f2_hi, f2_lo;
 2006|       |  twoprod(e_x, y, f1_hi, f1_lo);
 2007|       |  twoprod(rx_hi, rx_lo, y, f2_hi, f2_lo);
 2008|       |  // Sum the two terms in f using double word arithmetic. We know
 2009|       |  // that |e_x| > |log2(m_x)|, except for the case where e_x==0.
 2010|       |  // This means that we can use fast_twosum(f1,f2).
 2011|       |  // In the case e_x == 0, e_x * y = f1 = 0, so we don't lose any
 2012|       |  // accuracy by violating the assumption of fast_twosum, because
 2013|       |  // it's a no-op.
 2014|       |  Packet f_hi, f_lo;
 2015|       |  fast_twosum(f1_hi, f1_lo, f2_hi, f2_lo, f_hi, f_lo);
 2016|       |
 2017|       |  // Split f into integer and fractional parts.
 2018|       |  Packet n_z, r_z;
 2019|       |  absolute_split(f_hi, n_z, r_z);
 2020|       |  r_z = padd(r_z, f_lo);
 2021|       |  Packet n_r;
 2022|       |  absolute_split(r_z, n_r, r_z);
 2023|       |  n_z = padd(n_z, n_r);
 2024|       |
 2025|       |  // We now have an accurate split of f = n_z + r_z and can compute
 2026|       |  //   x^y = 2**{n_z + r_z) = exp2(r_z) * 2**{n_z}.
 2027|       |  // Multiplication by the second factor can be done exactly using pldexp(), since
 2028|       |  // it is an integer power of 2.
 2029|       |  const Packet e_r = generic_exp2(r_z);
 2030|       |
 2031|       |  // Since we know that e_r is in [1/sqrt(2); sqrt(2)], we can use the fast version
 2032|       |  // of pldexp to multiply by 2**{n_z} when |n_z| is sufficiently small.
 2033|       |  constexpr Scalar kPldExpThresh = std::numeric_limits<Scalar>::max_exponent - 2;
 2034|       |  const Packet pldexp_fast_unsafe = pcmp_lt(pset1<Packet>(kPldExpThresh), pabs(n_z));
 2035|       |  if (predux_any(pldexp_fast_unsafe)) {
 2036|       |    return pldexp(e_r, n_z);
 2037|       |  }
 2038|       |  return pldexp_fast(e_r, n_z);
 2039|       |}
 2040|       |
 2041|       |// Generic implementation of pow(x,y).
 2042|       |template <typename Packet>
 2043|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_pow(const Packet& x, const Packet& y) {
 2044|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 2045|       |
 2046|       |  const Packet cst_inf = pset1<Packet>(NumTraits<Scalar>::infinity());
 2047|       |  const Packet cst_zero = pset1<Packet>(Scalar(0));
 2048|       |  const Packet cst_one = pset1<Packet>(Scalar(1));
 2049|       |  const Packet cst_nan = pset1<Packet>(NumTraits<Scalar>::quiet_NaN());
 2050|       |
 2051|       |  const Packet x_abs = pabs(x);
 2052|       |  Packet pow = generic_pow_impl(x_abs, y);
 2053|       |
 2054|       |  // In the following we enforce the special case handling prescribed in
 2055|       |  // https://en.cppreference.com/w/cpp/numeric/math/pow.
 2056|       |
 2057|       |  // Predicates for sign and magnitude of x.
 2058|       |  const Packet x_is_negative = pcmp_lt(x, cst_zero);
 2059|       |  const Packet x_is_zero = pcmp_eq(x, cst_zero);
 2060|       |  const Packet x_is_one = pcmp_eq(x, cst_one);
 2061|       |  const Packet x_has_signbit = psignbit(x);
 2062|       |  const Packet x_abs_gt_one = pcmp_lt(cst_one, x_abs);
 2063|       |  const Packet x_abs_is_inf = pcmp_eq(x_abs, cst_inf);
 2064|       |
 2065|       |  // Predicates for sign and magnitude of y.
 2066|       |  const Packet y_abs = pabs(y);
 2067|       |  const Packet y_abs_is_inf = pcmp_eq(y_abs, cst_inf);
 2068|       |  const Packet y_is_negative = pcmp_lt(y, cst_zero);
 2069|       |  const Packet y_is_zero = pcmp_eq(y, cst_zero);
 2070|       |  const Packet y_is_one = pcmp_eq(y, cst_one);
 2071|       |  // Predicates for whether y is integer and odd/even.
 2072|       |  const Packet y_is_int = pandnot(pcmp_eq(pfloor(y), y), y_abs_is_inf);
 2073|       |  const Packet y_div_2 = pmul(y, pset1<Packet>(Scalar(0.5)));
 2074|       |  const Packet y_is_even = pcmp_eq(pround(y_div_2), y_div_2);
 2075|       |  const Packet y_is_odd_int = pandnot(y_is_int, y_is_even);
 2076|       |  // Smallest exponent for which (1 + epsilon) overflows to infinity.
 2077|       |  EIGEN_CONSTEXPR Scalar huge_exponent =
 2078|       |      (NumTraits<Scalar>::max_exponent() * Scalar(EIGEN_LN2)) / NumTraits<Scalar>::epsilon();
 2079|       |  const Packet y_abs_is_huge = pcmp_le(pset1<Packet>(huge_exponent), y_abs);
 2080|       |
 2081|       |  // *  pow(base, exp) returns NaN if base is finite and negative
 2082|       |  //    and exp is finite and non-integer.
 2083|       |  pow = pselect(pandnot(x_is_negative, y_is_int), cst_nan, pow);
 2084|       |
 2085|       |  // * pow(±0, exp), where exp is negative, finite, and is an even integer or
 2086|       |  // a non-integer, returns +∞
 2087|       |  // * pow(±0, exp), where exp is positive non-integer or a positive even
 2088|       |  // integer, returns +0
 2089|       |  // * pow(+0, exp), where exp is a negative odd integer, returns +∞
 2090|       |  // * pow(-0, exp), where exp is a negative odd integer, returns -∞
 2091|       |  // * pow(+0, exp), where exp is a positive odd integer, returns +0
 2092|       |  // * pow(-0, exp), where exp is a positive odd integer, returns -0
 2093|       |  // Sign is flipped by the rule below.
 2094|       |  pow = pselect(x_is_zero, pselect(y_is_negative, cst_inf, cst_zero), pow);
 2095|       |
 2096|       |  // pow(base, exp) returns -pow(abs(base), exp) if base has the sign bit set,
 2097|       |  // and exp is an odd integer exponent.
 2098|       |  pow = pselect(pand(x_has_signbit, y_is_odd_int), pnegate(pow), pow);
 2099|       |
 2100|       |  // * pow(base, -∞) returns +∞ for any |base|<1
 2101|       |  // * pow(base, -∞) returns +0 for any |base|>1
 2102|       |  // * pow(base, +∞) returns +0 for any |base|<1
 2103|       |  // * pow(base, +∞) returns +∞ for any |base|>1
 2104|       |  // * pow(±0, -∞) returns +∞
 2105|       |  // * pow(-1, +-∞) = 1
 2106|       |  Packet inf_y_val = pselect(por(pand(y_is_negative, x_is_zero), pxor(y_is_negative, x_abs_gt_one)), cst_inf, cst_zero);
 2107|       |  inf_y_val = pselect(pcmp_eq(x, pset1<Packet>(Scalar(-1.0))), cst_one, inf_y_val);
 2108|       |  pow = pselect(y_abs_is_huge, inf_y_val, pow);
 2109|       |
 2110|       |  // * pow(+∞, exp) returns +0 for any negative exp
 2111|       |  // * pow(+∞, exp) returns +∞ for any positive exp
 2112|       |  // * pow(-∞, exp) returns -0 if exp is a negative odd integer.
 2113|       |  // * pow(-∞, exp) returns +0 if exp is a negative non-integer or negative
 2114|       |  //     even integer.
 2115|       |  // * pow(-∞, exp) returns -∞ if exp is a positive odd integer.
 2116|       |  // * pow(-∞, exp) returns +∞ if exp is a positive non-integer or positive
 2117|       |  //     even integer.
 2118|       |  auto x_pos_inf_value = pselect(y_is_negative, cst_zero, cst_inf);
 2119|       |  auto x_neg_inf_value = pselect(y_is_odd_int, pnegate(x_pos_inf_value), x_pos_inf_value);
 2120|       |  pow = pselect(x_abs_is_inf, pselect(x_is_negative, x_neg_inf_value, x_pos_inf_value), pow);
 2121|       |
 2122|       |  // All cases of NaN inputs return NaN, except the two below.
 2123|       |  pow = pselect(por(pisnan(x), pisnan(y)), cst_nan, pow);
 2124|       |
 2125|       |  // * pow(base, 1) returns base.
 2126|       |  // * pow(base, +/-0) returns 1, regardless of base, even NaN.
 2127|       |  // * pow(+1, exp) returns 1, regardless of exponent, even NaN.
 2128|       |  pow = pselect(y_is_one, x, pselect(por(x_is_one, y_is_zero), cst_one, pow));
 2129|       |
 2130|       |  return pow;
 2131|       |}
 2132|       |
 2133|       |namespace unary_pow {
 2134|       |
 2135|       |template <typename ScalarExponent, bool IsInteger = NumTraits<ScalarExponent>::IsInteger>
 2136|       |struct exponent_helper {
 2137|       |  using safe_abs_type = ScalarExponent;
 2138|       |  static constexpr ScalarExponent one_half = ScalarExponent(0.5);
 2139|       |  // these routines assume that exp is an integer stored as a floating point type
 2140|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ScalarExponent safe_abs(const ScalarExponent& exp) {
 2141|       |    return numext::abs(exp);
 2142|       |  }
 2143|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool is_odd(const ScalarExponent& exp) {
 2144|       |    eigen_assert(((numext::isfinite)(exp) && exp == numext::floor(exp)) && "exp must be an integer");
 2145|       |    ScalarExponent exp_div_2 = exp * one_half;
 2146|       |    ScalarExponent floor_exp_div_2 = numext::floor(exp_div_2);
 2147|       |    return exp_div_2 != floor_exp_div_2;
 2148|       |  }
 2149|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE ScalarExponent floor_div_two(const ScalarExponent& exp) {
 2150|       |    ScalarExponent exp_div_2 = exp * one_half;
 2151|       |    return numext::floor(exp_div_2);
 2152|       |  }
 2153|       |};
 2154|       |
 2155|       |template <typename ScalarExponent>
 2156|       |struct exponent_helper<ScalarExponent, true> {
 2157|       |  // if `exp` is a signed integer type, cast it to its unsigned counterpart to safely store its absolute value
 2158|       |  // consider the (rare) case where `exp` is an int32_t: abs(-2147483648) != 2147483648
 2159|       |  using safe_abs_type = typename numext::get_integer_by_size<sizeof(ScalarExponent)>::unsigned_type;
 2160|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE safe_abs_type safe_abs(const ScalarExponent& exp) {
 2161|       |    ScalarExponent mask = numext::signbit(exp);
 2162|       |    safe_abs_type result = safe_abs_type(exp ^ mask);
 2163|       |    return result + safe_abs_type(ScalarExponent(1) & mask);
 2164|       |  }
 2165|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool is_odd(const safe_abs_type& exp) {
 2166|       |    return exp % safe_abs_type(2) != safe_abs_type(0);
 2167|       |  }
 2168|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE safe_abs_type floor_div_two(const safe_abs_type& exp) {
 2169|       |    return exp >> safe_abs_type(1);
 2170|       |  }
 2171|       |};
 2172|       |
 2173|       |template <typename Packet, typename ScalarExponent,
 2174|       |          bool ReciprocateIfExponentIsNegative =
 2175|       |              !NumTraits<typename unpacket_traits<Packet>::type>::IsInteger && NumTraits<ScalarExponent>::IsSigned>
 2176|       |struct reciprocate {
 2177|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x, const ScalarExponent& exponent) {
 2178|       |    using Scalar = typename unpacket_traits<Packet>::type;
 2179|       |    const Packet cst_pos_one = pset1<Packet>(Scalar(1));
 2180|       |    return exponent < 0 ? pdiv(cst_pos_one, x) : x;
 2181|       |  }
 2182|       |};
 2183|       |
 2184|       |template <typename Packet, typename ScalarExponent>
 2185|       |struct reciprocate<Packet, ScalarExponent, false> {
 2186|       |  // pdiv not defined, nor necessary for integer base types
 2187|       |  // if the exponent is unsigned, then the exponent cannot be negative
 2188|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x, const ScalarExponent&) { return x; }
 2189|       |};
 2190|       |
 2191|       |template <typename Packet, typename ScalarExponent>
 2192|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet int_pow(const Packet& x, const ScalarExponent& exponent) {
 2193|       |  using Scalar = typename unpacket_traits<Packet>::type;
 2194|       |  using ExponentHelper = exponent_helper<ScalarExponent>;
 2195|       |  using AbsExponentType = typename ExponentHelper::safe_abs_type;
 2196|       |  const Packet cst_pos_one = pset1<Packet>(Scalar(1));
 2197|       |  if (exponent == ScalarExponent(0)) return cst_pos_one;
 2198|       |
 2199|       |  Packet result = reciprocate<Packet, ScalarExponent>::run(x, exponent);
 2200|       |  Packet y = cst_pos_one;
 2201|       |  AbsExponentType m = ExponentHelper::safe_abs(exponent);
 2202|       |
 2203|       |  while (m > 1) {
 2204|       |    bool odd = ExponentHelper::is_odd(m);
 2205|       |    if (odd) y = pmul(y, result);
 2206|       |    result = pmul(result, result);
 2207|       |    m = ExponentHelper::floor_div_two(m);
 2208|       |  }
 2209|       |
 2210|       |  return pmul(y, result);
 2211|       |}
 2212|       |
 2213|       |template <typename Packet>
 2214|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet gen_pow(const Packet& x,
 2215|       |                                                     const typename unpacket_traits<Packet>::type& exponent) {
 2216|       |  const Packet exponent_packet = pset1<Packet>(exponent);
 2217|       |  return generic_pow_impl(x, exponent_packet);
 2218|       |}
 2219|       |
 2220|       |template <typename Packet, typename ScalarExponent>
 2221|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet handle_nonint_nonint_errors(const Packet& x, const Packet& powx,
 2222|       |                                                                         const ScalarExponent& exponent) {
 2223|       |  using Scalar = typename unpacket_traits<Packet>::type;
 2224|       |
 2225|       |  // non-integer base and exponent case
 2226|       |
 2227|       |  const Scalar pos_zero = Scalar(0);
 2228|       |  const Scalar all_ones = ptrue<Scalar>(Scalar());
 2229|       |  const Scalar pos_one = Scalar(1);
 2230|       |  const Scalar pos_inf = NumTraits<Scalar>::infinity();
 2231|       |
 2232|       |  const Packet cst_pos_zero = pzero(x);
 2233|       |  const Packet cst_pos_one = pset1<Packet>(pos_one);
 2234|       |  const Packet cst_pos_inf = pset1<Packet>(pos_inf);
 2235|       |
 2236|       |  const bool exponent_is_not_fin = !(numext::isfinite)(exponent);
 2237|       |  const bool exponent_is_neg = exponent < ScalarExponent(0);
 2238|       |  const bool exponent_is_pos = exponent > ScalarExponent(0);
 2239|       |
 2240|       |  const Packet exp_is_not_fin = pset1<Packet>(exponent_is_not_fin ? all_ones : pos_zero);
 2241|       |  const Packet exp_is_neg = pset1<Packet>(exponent_is_neg ? all_ones : pos_zero);
 2242|       |  const Packet exp_is_pos = pset1<Packet>(exponent_is_pos ? all_ones : pos_zero);
 2243|       |  const Packet exp_is_inf = pand(exp_is_not_fin, por(exp_is_neg, exp_is_pos));
 2244|       |  const Packet exp_is_nan = pandnot(exp_is_not_fin, por(exp_is_neg, exp_is_pos));
 2245|       |
 2246|       |  const Packet x_is_le_zero = pcmp_le(x, cst_pos_zero);
 2247|       |  const Packet x_is_ge_zero = pcmp_le(cst_pos_zero, x);
 2248|       |  const Packet x_is_zero = pand(x_is_le_zero, x_is_ge_zero);
 2249|       |
 2250|       |  const Packet abs_x = pabs(x);
 2251|       |  const Packet abs_x_is_le_one = pcmp_le(abs_x, cst_pos_one);
 2252|       |  const Packet abs_x_is_ge_one = pcmp_le(cst_pos_one, abs_x);
 2253|       |  const Packet abs_x_is_inf = pcmp_eq(abs_x, cst_pos_inf);
 2254|       |  const Packet abs_x_is_one = pand(abs_x_is_le_one, abs_x_is_ge_one);
 2255|       |
 2256|       |  Packet pow_is_inf_if_exp_is_neg = por(x_is_zero, pand(abs_x_is_le_one, exp_is_inf));
 2257|       |  Packet pow_is_inf_if_exp_is_pos = por(abs_x_is_inf, pand(abs_x_is_ge_one, exp_is_inf));
 2258|       |  Packet pow_is_one = pand(abs_x_is_one, por(exp_is_inf, x_is_ge_zero));
 2259|       |
 2260|       |  Packet result = powx;
 2261|       |  result = por(x_is_le_zero, result);
 2262|       |  result = pselect(pow_is_inf_if_exp_is_neg, pand(cst_pos_inf, exp_is_neg), result);
 2263|       |  result = pselect(pow_is_inf_if_exp_is_pos, pand(cst_pos_inf, exp_is_pos), result);
 2264|       |  result = por(exp_is_nan, result);
 2265|       |  result = pselect(pow_is_one, cst_pos_one, result);
 2266|       |  return result;
 2267|       |}
 2268|       |
 2269|       |template <typename Packet, typename ScalarExponent,
 2270|       |          std::enable_if_t<NumTraits<typename unpacket_traits<Packet>::type>::IsSigned, bool> = true>
 2271|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet handle_negative_exponent(const Packet& x, const ScalarExponent& exponent) {
 2272|       |  using Scalar = typename unpacket_traits<Packet>::type;
 2273|       |
 2274|       |  // signed integer base, signed integer exponent case
 2275|       |
 2276|       |  // This routine handles negative exponents.
 2277|       |  // The return value is either 0, 1, or -1.
 2278|       |
 2279|       |  const Scalar pos_zero = Scalar(0);
 2280|       |  const Scalar all_ones = ptrue<Scalar>(Scalar());
 2281|       |  const Scalar pos_one = Scalar(1);
 2282|       |
 2283|       |  const Packet cst_pos_one = pset1<Packet>(pos_one);
 2284|       |
 2285|       |  const bool exponent_is_odd = exponent % ScalarExponent(2) != ScalarExponent(0);
 2286|       |
 2287|       |  const Packet exp_is_odd = pset1<Packet>(exponent_is_odd ? all_ones : pos_zero);
 2288|       |
 2289|       |  const Packet abs_x = pabs(x);
 2290|       |  const Packet abs_x_is_one = pcmp_eq(abs_x, cst_pos_one);
 2291|       |
 2292|       |  Packet result = pselect(exp_is_odd, x, abs_x);
 2293|       |  result = pand(abs_x_is_one, result);
 2294|       |  return result;
 2295|       |}
 2296|       |
 2297|       |template <typename Packet, typename ScalarExponent,
 2298|       |          std::enable_if_t<!NumTraits<typename unpacket_traits<Packet>::type>::IsSigned, bool> = true>
 2299|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet handle_negative_exponent(const Packet& x, const ScalarExponent&) {
 2300|       |  using Scalar = typename unpacket_traits<Packet>::type;
 2301|       |
 2302|       |  // unsigned integer base, signed integer exponent case
 2303|       |
 2304|       |  // This routine handles negative exponents.
 2305|       |  // The return value is either 0 or 1
 2306|       |
 2307|       |  const Scalar pos_one = Scalar(1);
 2308|       |
 2309|       |  const Packet cst_pos_one = pset1<Packet>(pos_one);
 2310|       |
 2311|       |  const Packet x_is_one = pcmp_eq(x, cst_pos_one);
 2312|       |
 2313|       |  return pand(x_is_one, x);
 2314|       |}
 2315|       |
 2316|       |}  // end namespace unary_pow
 2317|       |
 2318|       |template <typename Packet, typename ScalarExponent,
 2319|       |          bool BaseIsIntegerType = NumTraits<typename unpacket_traits<Packet>::type>::IsInteger,
 2320|       |          bool ExponentIsIntegerType = NumTraits<ScalarExponent>::IsInteger,
 2321|       |          bool ExponentIsSigned = NumTraits<ScalarExponent>::IsSigned>
 2322|       |struct unary_pow_impl;
 2323|       |
 2324|       |template <typename Packet, typename ScalarExponent, bool ExponentIsSigned>
 2325|       |struct unary_pow_impl<Packet, ScalarExponent, false, false, ExponentIsSigned> {
 2326|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 2327|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x, const ScalarExponent& exponent) {
 2328|       |    const bool exponent_is_integer = (numext::isfinite)(exponent) && numext::round(exponent) == exponent;
 2329|       |    if (exponent_is_integer) {
 2330|       |      // The simple recursive doubling implementation is only accurate to 3 ulps
 2331|       |      // for integer exponents in [-3:7]. Since this is a common case, we
 2332|       |      // specialize it here.
 2333|       |      bool use_repeated_squaring =
 2334|       |          (exponent <= ScalarExponent(7) && (!ExponentIsSigned || exponent >= ScalarExponent(-3)));
 2335|       |      return use_repeated_squaring ? unary_pow::int_pow(x, exponent) : generic_pow(x, pset1<Packet>(exponent));
 2336|       |    } else {
 2337|       |      Packet result = unary_pow::gen_pow(x, exponent);
 2338|       |      result = unary_pow::handle_nonint_nonint_errors(x, result, exponent);
 2339|       |      return result;
 2340|       |    }
 2341|       |  }
 2342|       |};
 2343|       |
 2344|       |template <typename Packet, typename ScalarExponent, bool ExponentIsSigned>
 2345|       |struct unary_pow_impl<Packet, ScalarExponent, false, true, ExponentIsSigned> {
 2346|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 2347|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x, const ScalarExponent& exponent) {
 2348|       |    return unary_pow::int_pow(x, exponent);
 2349|       |  }
 2350|       |};
 2351|       |
 2352|       |template <typename Packet, typename ScalarExponent>
 2353|       |struct unary_pow_impl<Packet, ScalarExponent, true, true, true> {
 2354|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 2355|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x, const ScalarExponent& exponent) {
 2356|       |    if (exponent < ScalarExponent(0)) {
 2357|       |      return unary_pow::handle_negative_exponent(x, exponent);
 2358|       |    } else {
 2359|       |      return unary_pow::int_pow(x, exponent);
 2360|       |    }
 2361|       |  }
 2362|       |};
 2363|       |
 2364|       |template <typename Packet, typename ScalarExponent>
 2365|       |struct unary_pow_impl<Packet, ScalarExponent, true, true, false> {
 2366|       |  typedef typename unpacket_traits<Packet>::type Scalar;
 2367|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run(const Packet& x, const ScalarExponent& exponent) {
 2368|       |    return unary_pow::int_pow(x, exponent);
 2369|       |  }
 2370|       |};
 2371|       |
 2372|       |// This function computes exp2(x) = exp(ln(2) * x).
 2373|       |// To improve accuracy, the product ln(2)*x is computed using the twoprod
 2374|       |// algorithm, such that ln(2) * x = p_hi + p_lo holds exactly. Then exp2(x) is
 2375|       |// computed as exp2(x) = exp(p_hi) * exp(p_lo) ~= exp(p_hi) * (1 + p_lo). This
 2376|       |// correction step this reduces the maximum absolute error as follows:
 2377|       |//
 2378|       |// type   | max error (simple product) | max error (twoprod) |
 2379|       |// -----------------------------------------------------------
 2380|       |// float  |       35 ulps              |       4 ulps        |
 2381|       |// double |      363 ulps              |     110 ulps        |
 2382|       |//
 2383|       |template <typename Packet>
 2384|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_exp2(const Packet& _x) {
 2385|      0|  typedef typename unpacket_traits<Packet>::type Scalar;
 2386|      0|  constexpr int max_exponent = std::numeric_limits<Scalar>::max_exponent;
 2387|      0|  constexpr int digits = std::numeric_limits<Scalar>::digits;
 2388|      0|  constexpr Scalar max_cap = Scalar(max_exponent + 1);
 2389|      0|  constexpr Scalar min_cap = -Scalar(max_exponent + digits - 1);
 2390|      0|  Packet x = pmax(pmin(_x, pset1<Packet>(max_cap)), pset1<Packet>(min_cap));
 2391|      0|  Packet p_hi, p_lo;
 2392|      0|  twoprod(pset1<Packet>(Scalar(EIGEN_LN2)), x, p_hi, p_lo);
 2393|      0|  Packet exp2_hi = pexp(p_hi);
 2394|      0|  Packet exp2_lo = padd(pset1<Packet>(Scalar(1)), p_lo);
 2395|      0|  return pmul(exp2_hi, exp2_lo);
 2396|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12generic_exp2IDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12generic_exp2IDv2_dEET_RKS3_
  ------------------
 2397|       |
 2398|       |template <typename Packet>
 2399|      0|EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_rint(const Packet& a) {
 2400|      0|  using Scalar = typename unpacket_traits<Packet>::type;
 2401|      0|  using IntType = typename numext::get_integer_by_size<sizeof(Scalar)>::signed_type;
 2402|      0|  // Adds and subtracts signum(a) * 2^kMantissaBits to force rounding.
 2403|      0|  const IntType kLimit = IntType(1) << (NumTraits<Scalar>::digits() - 1);
 2404|      0|  const Packet cst_limit = pset1<Packet>(static_cast<Scalar>(kLimit));
 2405|      0|  Packet abs_a = pabs(a);
 2406|      0|  Packet sign_a = pandnot(a, abs_a);
 2407|      0|  Packet rint_a = padd(abs_a, cst_limit);
 2408|      0|  // Don't compile-away addition and subtraction.
 2409|      0|  EIGEN_OPTIMIZATION_BARRIER(rint_a);
 2410|      0|  rint_a = psub(rint_a, cst_limit);
 2411|      0|  rint_a = por(rint_a, sign_a);
 2412|      0|  // If greater than limit (or NaN), simply return a.
 2413|      0|  Packet mask = pcmp_lt(abs_a, cst_limit);
 2414|      0|  Packet result = pselect(mask, rint_a, a);
 2415|      0|  return result;
 2416|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12generic_rintIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal12generic_rintIDv2_dEET_RKS3_
  ------------------
 2417|       |
 2418|       |template <typename Packet>
 2419|      0|EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_floor(const Packet& a) {
 2420|      0|  using Scalar = typename unpacket_traits<Packet>::type;
 2421|      0|  const Packet cst_1 = pset1<Packet>(Scalar(1));
 2422|      0|  Packet rint_a = generic_rint(a);
 2423|      0|  // if a < rint(a), then rint(a) == ceil(a)
 2424|      0|  Packet mask = pcmp_lt(a, rint_a);
 2425|      0|  Packet offset = pand(cst_1, mask);
 2426|      0|  Packet result = psub(rint_a, offset);
 2427|      0|  return result;
 2428|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13generic_floorIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal13generic_floorIDv2_dEET_RKS3_
  ------------------
 2429|       |
 2430|       |template <typename Packet>
 2431|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_ceil(const Packet& a) {
 2432|       |  using Scalar = typename unpacket_traits<Packet>::type;
 2433|       |  const Packet cst_1 = pset1<Packet>(Scalar(1));
 2434|       |  const Packet sign_mask = pset1<Packet>(static_cast<Scalar>(-0.0));
 2435|       |  Packet rint_a = generic_rint(a);
 2436|       |  // if rint(a) < a, then rint(a) == floor(a)
 2437|       |  Packet mask = pcmp_lt(rint_a, a);
 2438|       |  Packet offset = pand(cst_1, mask);
 2439|       |  Packet result = padd(rint_a, offset);
 2440|       |  // Signed zero must remain signed (e.g. ceil(-0.02) == -0).
 2441|       |  result = por(result, pand(sign_mask, a));
 2442|       |  return result;
 2443|       |}
 2444|       |
 2445|       |template <typename Packet>
 2446|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_trunc(const Packet& a) {
 2447|       |  Packet abs_a = pabs(a);
 2448|       |  Packet sign_a = pandnot(a, abs_a);
 2449|       |  Packet floor_abs_a = generic_floor(abs_a);
 2450|       |  Packet result = por(floor_abs_a, sign_a);
 2451|       |  return result;
 2452|       |}
 2453|       |
 2454|       |template <typename Packet>
 2455|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_round(const Packet& a) {
 2456|       |  using Scalar = typename unpacket_traits<Packet>::type;
 2457|       |  const Packet cst_half = pset1<Packet>(Scalar(0.5));
 2458|       |  const Packet cst_1 = pset1<Packet>(Scalar(1));
 2459|       |  Packet abs_a = pabs(a);
 2460|       |  Packet sign_a = pandnot(a, abs_a);
 2461|       |  Packet floor_abs_a = generic_floor(abs_a);
 2462|       |  Packet diff = psub(abs_a, floor_abs_a);
 2463|       |  Packet mask = pcmp_le(cst_half, diff);
 2464|       |  Packet offset = pand(cst_1, mask);
 2465|       |  Packet result = padd(floor_abs_a, offset);
 2466|       |  result = por(result, sign_a);
 2467|       |  return result;
 2468|       |}
 2469|       |
 2470|       |template <typename Packet>
 2471|       |struct nearest_integer_packetop_impl<Packet, /*IsScalar*/ false, /*IsInteger*/ false> {
 2472|       |  using Scalar = typename unpacket_traits<Packet>::type;
 2473|       |  static_assert(packet_traits<Scalar>::HasRound, "Generic nearest integer functions are disabled for this type.");
 2474|      0|  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_floor(const Packet& x) { return generic_floor(x); }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal29nearest_integer_packetop_implIDv4_fLb0ELb0EE9run_floorERKS2_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal29nearest_integer_packetop_implIDv2_dLb0ELb0EE9run_floorERKS2_
  ------------------
 2475|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_ceil(const Packet& x) { return generic_ceil(x); }
 2476|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_rint(const Packet& x) { return generic_rint(x); }
 2477|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_round(const Packet& x) { return generic_round(x); }
 2478|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_trunc(const Packet& x) { return generic_trunc(x); }
 2479|       |};
 2480|       |
 2481|       |template <typename Packet>
 2482|       |struct nearest_integer_packetop_impl<Packet, /*IsScalar*/ false, /*IsInteger*/ true> {
 2483|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_floor(const Packet& x) { return x; }
 2484|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_ceil(const Packet& x) { return x; }
 2485|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_rint(const Packet& x) { return x; }
 2486|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_round(const Packet& x) { return x; }
 2487|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet run_trunc(const Packet& x) { return x; }
 2488|       |};
 2489|       |
 2490|       |}  // end namespace internal
 2491|       |}  // end namespace Eigen
 2492|       |
 2493|       |#endif  // EIGEN_ARCH_GENERIC_PACKET_MATH_FUNCTIONS_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/Default/GenericPacketMathFunctionsFwd.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2019 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_ARCH_GENERIC_PACKET_MATH_FUNCTIONS_FWD_H
   11|       |#define EIGEN_ARCH_GENERIC_PACKET_MATH_FUNCTIONS_FWD_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |namespace internal {
   18|       |
   19|       |// Forward declarations of the generic math functions
   20|       |// implemented in GenericPacketMathFunctions.h
   21|       |// This is needed to workaround a circular dependency.
   22|       |
   23|       |/***************************************************************************
   24|       | * Some generic implementations to be used by implementers
   25|       | ***************************************************************************/
   26|       |
   27|       |/** Default implementation of pfrexp.
   28|       | * It is expected to be called by implementers of template<> pfrexp.
   29|       | */
   30|       |template <typename Packet>
   31|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pfrexp_generic(const Packet& a, Packet& exponent);
   32|       |
   33|       |// Extracts the biased exponent value from Packet p, and casts the results to
   34|       |// a floating-point Packet type. Used by pfrexp_generic. Override this if
   35|       |// there is no unpacket_traits<Packet>::integer_packet.
   36|       |template <typename Packet>
   37|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pfrexp_generic_get_biased_exponent(const Packet& p);
   38|       |
   39|       |/** Default implementation of pldexp.
   40|       | * It is expected to be called by implementers of template<> pldexp.
   41|       | */
   42|       |template <typename Packet>
   43|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pldexp_generic(const Packet& a, const Packet& exponent);
   44|       |
   45|       |// Explicitly multiplies
   46|       |//    a * (2^e)
   47|       |// clamping e to the range
   48|       |// [NumTraits<Scalar>::min_exponent()-2, NumTraits<Scalar>::max_exponent()]
   49|       |//
   50|       |// This is approx 7x faster than pldexp_impl, but will prematurely over/underflow
   51|       |// if 2^e doesn't fit into a normal floating-point Scalar.
   52|       |//
   53|       |// Assumes IEEE floating point format
   54|       |template <typename Packet>
   55|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Packet pldexp_fast(const Packet& a, const Packet& exponent);
   56|       |
   57|       |/** \internal \returns log(x) for single precision float */
   58|       |template <typename Packet>
   59|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_float(const Packet _x);
   60|       |
   61|       |/** \internal \returns log2(x) for single precision float */
   62|       |template <typename Packet>
   63|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog2_float(const Packet _x);
   64|       |
   65|       |/** \internal \returns log(x) for single precision float */
   66|       |template <typename Packet>
   67|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_double(const Packet _x);
   68|       |
   69|       |/** \internal \returns log2(x) for single precision float */
   70|       |template <typename Packet>
   71|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog2_double(const Packet _x);
   72|       |
   73|       |/** \internal \returns log(1 + x) */
   74|       |template <typename Packet>
   75|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_log1p(const Packet& x);
   76|       |
   77|       |/** \internal \returns exp(x)-1 */
   78|       |template <typename Packet>
   79|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_expm1(const Packet& x);
   80|       |
   81|       |/** \internal \returns atan(x) */
   82|       |template <typename Packet>
   83|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_atan(const Packet& x);
   84|       |
   85|       |/** \internal \returns exp2(x) */
   86|       |template <typename Packet>
   87|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet generic_exp2(const Packet& x);
   88|       |
   89|       |/** \internal \returns exp(x) for single precision float */
   90|       |template <typename Packet>
   91|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp_float(const Packet _x);
   92|       |
   93|       |/** \internal \returns exp(x) for double precision real numbers */
   94|       |template <typename Packet>
   95|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp_double(const Packet _x);
   96|       |
   97|       |/** \internal \returns sin(x) for single precision float */
   98|       |template <typename Packet>
   99|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psin_float(const Packet& x);
  100|       |
  101|       |/** \internal \returns cos(x) for single precision float */
  102|       |template <typename Packet>
  103|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pcos_float(const Packet& x);
  104|       |
  105|       |/** \internal \returns sin(x) for double precision float */
  106|       |template <typename Packet>
  107|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psin_double(const Packet& x);
  108|       |
  109|       |/** \internal \returns cos(x) for double precision float */
  110|       |template <typename Packet>
  111|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pcos_double(const Packet& x);
  112|       |
  113|       |/** \internal \returns asin(x) for single precision float */
  114|       |template <typename Packet>
  115|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pasin_float(const Packet& x);
  116|       |
  117|       |/** \internal \returns acos(x) for single precision float */
  118|       |template <typename Packet>
  119|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pacos_float(const Packet& x);
  120|       |
  121|       |/** \internal \returns tanh(x) for single precision float */
  122|       |template <typename Packet>
  123|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet ptanh_float(const Packet& x);
  124|       |
  125|       |/** \internal \returns tanh(x) for double precision float */
  126|       |template <typename Packet>
  127|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet ptanh_double(const Packet& x);
  128|       |
  129|       |/** \internal \returns atanh(x) for single precision float */
  130|       |template <typename Packet>
  131|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patanh_float(const Packet& x);
  132|       |
  133|       |/** \internal \returns atanh(x) for double precision float */
  134|       |template <typename Packet>
  135|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet patanh_double(const Packet& x);
  136|       |
  137|       |/** \internal \returns sqrt(x) for complex types */
  138|       |template <typename Packet>
  139|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet psqrt_complex(const Packet& a);
  140|       |
  141|       |/** \internal \returns x / y for complex types */
  142|       |template <typename Packet>
  143|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pdiv_complex(const Packet& x, const Packet& y);
  144|       |
  145|       |template <typename Packet, int N>
  146|       |struct ppolevl;
  147|       |
  148|       |/** \internal \returns log(x) for complex types */
  149|       |template <typename Packet>
  150|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet plog_complex(const Packet& x);
  151|       |
  152|       |/** \internal \returns exp(x) for complex types */
  153|       |template <typename Packet>
  154|       |EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet pexp_complex(const Packet& x);
  155|       |
  156|       |template <typename Packet>
  157|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_rint(const Packet& a);
  158|       |
  159|       |template <typename Packet>
  160|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_floor(const Packet& a);
  161|       |
  162|       |template <typename Packet>
  163|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_ceil(const Packet& a);
  164|       |
  165|       |template <typename Packet>
  166|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_trunc(const Packet& a);
  167|       |
  168|       |template <typename Packet>
  169|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet generic_round(const Packet& a);
  170|       |
  171|       |// Macros for instantiating these generic functions for different backends.
  172|       |#define EIGEN_PACKET_FUNCTION(METHOD, SCALAR, PACKET)                                             \
  173|       |  template <>                                                                                     \
  174|      0|  EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_UNUSED PACKET p##METHOD<PACKET>(const PACKET& _x) { \
  175|      0|    return p##METHOD##_##SCALAR(_x);                                                              \
  176|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4psinIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4pcosIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5pasinIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5pacosIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5ptanhIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6patanhIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4plogIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5plog2IDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4pexpIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6patanhIDv2_dEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4plogIDv2_dEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4psinIDv2_dEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4pcosIDv2_dEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5plog2IDv2_dEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal4pexpIDv2_dEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5ptanhIDv2_dEET_RKS3_
  ------------------
  177|       |
  178|       |// Macros for instantiating these generic functions for different backends.
  179|       |#define EIGEN_GENERIC_PACKET_FUNCTION(METHOD, PACKET)                                             \
  180|       |  template <>                                                                                     \
  181|      0|  EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_UNUSED PACKET p##METHOD<PACKET>(const PACKET& _x) { \
  182|      0|    return generic_##METHOD(_x);                                                                  \
  183|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6pexpm1IDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5pexp2IDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal6plog1pIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5patanIDv4_fEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5patanIDv2_dEET_RKS3_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal5pexp2IDv2_dEET_RKS3_
  ------------------
  184|       |
  185|       |#define EIGEN_FLOAT_PACKET_FUNCTION(METHOD, PACKET) EIGEN_PACKET_FUNCTION(METHOD, float, PACKET)
  186|       |#define EIGEN_DOUBLE_PACKET_FUNCTION(METHOD, PACKET) EIGEN_PACKET_FUNCTION(METHOD, double, PACKET)
  187|       |
  188|       |#define EIGEN_INSTANTIATE_GENERIC_MATH_FUNCS_FLOAT(PACKET) \
  189|       |  EIGEN_FLOAT_PACKET_FUNCTION(sin, PACKET)                 \
  190|       |  EIGEN_FLOAT_PACKET_FUNCTION(cos, PACKET)                 \
  191|       |  EIGEN_FLOAT_PACKET_FUNCTION(asin, PACKET)                \
  192|       |  EIGEN_FLOAT_PACKET_FUNCTION(acos, PACKET)                \
  193|       |  EIGEN_FLOAT_PACKET_FUNCTION(tanh, PACKET)                \
  194|       |  EIGEN_FLOAT_PACKET_FUNCTION(atanh, PACKET)               \
  195|       |  EIGEN_FLOAT_PACKET_FUNCTION(log, PACKET)                 \
  196|       |  EIGEN_FLOAT_PACKET_FUNCTION(log2, PACKET)                \
  197|       |  EIGEN_FLOAT_PACKET_FUNCTION(exp, PACKET)                 \
  198|       |  EIGEN_GENERIC_PACKET_FUNCTION(expm1, PACKET)             \
  199|       |  EIGEN_GENERIC_PACKET_FUNCTION(exp2, PACKET)              \
  200|       |  EIGEN_GENERIC_PACKET_FUNCTION(log1p, PACKET)             \
  201|       |  EIGEN_GENERIC_PACKET_FUNCTION(atan, PACKET)
  202|       |
  203|       |#define EIGEN_INSTANTIATE_GENERIC_MATH_FUNCS_DOUBLE(PACKET) \
  204|       |  EIGEN_DOUBLE_PACKET_FUNCTION(atanh, PACKET)               \
  205|       |  EIGEN_DOUBLE_PACKET_FUNCTION(log, PACKET)                 \
  206|       |  EIGEN_DOUBLE_PACKET_FUNCTION(sin, PACKET)                 \
  207|       |  EIGEN_DOUBLE_PACKET_FUNCTION(cos, PACKET)                 \
  208|       |  EIGEN_DOUBLE_PACKET_FUNCTION(log2, PACKET)                \
  209|       |  EIGEN_DOUBLE_PACKET_FUNCTION(exp, PACKET)                 \
  210|       |  EIGEN_DOUBLE_PACKET_FUNCTION(tanh, PACKET)                \
  211|       |  EIGEN_GENERIC_PACKET_FUNCTION(atan, PACKET)               \
  212|       |  EIGEN_GENERIC_PACKET_FUNCTION(exp2, PACKET)
  213|       |
  214|       |}  // end namespace internal
  215|       |}  // end namespace Eigen
  216|       |
  217|       |#endif  // EIGEN_ARCH_GENERIC_PACKET_MATH_FUNCTIONS_FWD_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/Default/Half.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// This Source Code Form is subject to the terms of the Mozilla
    5|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    6|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    7|       |//
    8|       |// The conversion routines are Copyright (c) Fabian Giesen, 2016.
    9|       |// The original license follows:
   10|       |//
   11|       |// Copyright (c) Fabian Giesen, 2016
   12|       |// All rights reserved.
   13|       |// Redistribution and use in source and binary forms, with or without
   14|       |// modification, are permitted.
   15|       |// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
   16|       |// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
   17|       |// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
   18|       |// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
   19|       |// HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
   20|       |// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
   21|       |// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
   22|       |// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
   23|       |// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
   24|       |// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
   25|       |// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
   26|       |
   27|       |// Standard 16-bit float type, mostly useful for GPUs. Defines a new
   28|       |// type Eigen::half (inheriting either from CUDA's or HIP's __half struct) with
   29|       |// operator overloads such that it behaves basically as an arithmetic
   30|       |// type. It will be quite slow on CPUs (so it is recommended to stay
   31|       |// in fp32 for CPUs, except for simple parameter conversions, I/O
   32|       |// to disk and the likes), but fast on GPUs.
   33|       |
   34|       |#ifndef EIGEN_HALF_H
   35|       |#define EIGEN_HALF_H
   36|       |
   37|       |// IWYU pragma: private
   38|       |#include "../../InternalHeaderCheck.h"
   39|       |
   40|       |#if defined(EIGEN_HAS_GPU_FP16) || defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
   41|       |// When compiling with GPU support, the "__half_raw" base class as well as
   42|       |// some other routines are defined in the GPU compiler header files
   43|       |// (cuda_fp16.h, hip_fp16.h), and they are not tagged constexpr
   44|       |// As a consequence, we get compile failures when compiling Eigen with
   45|       |// GPU support. Hence the need to disable EIGEN_CONSTEXPR when building
   46|       |// Eigen with GPU support
   47|       |#pragma push_macro("EIGEN_CONSTEXPR")
   48|       |#undef EIGEN_CONSTEXPR
   49|       |#define EIGEN_CONSTEXPR
   50|       |#endif
   51|       |
   52|       |#define F16_PACKET_FUNCTION(PACKET_F, PACKET_F16, METHOD)                                                  \
   53|       |  template <>                                                                                              \
   54|       |  EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_UNUSED PACKET_F16 METHOD<PACKET_F16>(const PACKET_F16& _x) { \
   55|       |    return float2half(METHOD<PACKET_F>(half2float(_x)));                                                   \
   56|       |  }
   57|       |
   58|       |namespace Eigen {
   59|       |
   60|       |struct half;
   61|       |
   62|       |namespace half_impl {
   63|       |
   64|       |// We want to use the __half_raw struct from the HIP header file only during the device compile phase.
   65|       |// This is required because of a quirk in the way TensorFlow GPU builds are done.
   66|       |// When compiling TensorFlow source code with GPU support, files that
   67|       |//  * contain GPU kernels (i.e. *.cu.cc files) are compiled via hipcc
   68|       |//  * do not contain GPU kernels ( i.e. *.cc files) are compiled via gcc (typically)
   69|       |//
   70|       |// Tensorflow uses the Eigen::half type as its FP16 type, and there are functions that
   71|       |//  * are defined in a file that gets compiled via hipcc AND
   72|       |//  * have Eigen::half as a pass-by-value argument AND
   73|       |//  * are called in a file that gets compiled via gcc
   74|       |//
   75|       |// In the scenario described above the caller and callee will see different versions
   76|       |// of the Eigen::half base class __half_raw, and they will be compiled by different compilers
   77|       |//
   78|       |// There appears to be an ABI mismatch between gcc and clang (which is called by hipcc) that results in
   79|       |// the callee getting corrupted values for the Eigen::half argument.
   80|       |//
   81|       |// Making the host side compile phase of hipcc use the same Eigen::half impl, as the gcc compile, resolves
   82|       |// this error, and hence the following convoluted #if condition
   83|       |#if !defined(EIGEN_HAS_GPU_FP16) || !defined(EIGEN_GPU_COMPILE_PHASE)
   84|       |// Make our own __half_raw definition that is similar to CUDA's.
   85|       |struct __half_raw {
   86|       |#if (defined(EIGEN_HAS_GPU_FP16) && !defined(EIGEN_GPU_COMPILE_PHASE))
   87|       |  // Eigen::half can be used as the datatype for shared memory declarations (in Eigen and TF)
   88|       |  // The element type for shared memory cannot have non-trivial constructors
   89|       |  // and hence the following special casing (which skips the zero-initilization).
   90|       |  // Note that this check gets done even in the host compilation phase, and
   91|       |  // hence the need for this
   92|       |  EIGEN_DEVICE_FUNC __half_raw() {}
   93|       |#else
   94|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __half_raw() : x(0) {}
   95|       |#endif
   96|       |#if defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
   97|       |  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __half_raw(numext::uint16_t raw) : x(numext::bit_cast<__fp16>(raw)) {}
   98|       |  __fp16 x;
   99|       |#else
  100|      0|  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __half_raw(numext::uint16_t raw) : x(raw) {}
  101|       |  numext::uint16_t x;
  102|       |#endif
  103|       |};
  104|       |
  105|       |#elif defined(EIGEN_HAS_HIP_FP16)
  106|       |// Nothing to do here
  107|       |// HIP fp16 header file has a definition for __half_raw
  108|       |#elif defined(EIGEN_HAS_CUDA_FP16)
  109|       |#if EIGEN_CUDA_SDK_VER < 90000
  110|       |// In CUDA < 9.0, __half is the equivalent of CUDA 9's __half_raw
  111|       |typedef __half __half_raw;
  112|       |#endif  // defined(EIGEN_HAS_CUDA_FP16)
  113|       |#elif defined(SYCL_DEVICE_ONLY)
  114|       |typedef cl::sycl::half __half_raw;
  115|       |#endif
  116|       |
  117|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __half_raw raw_uint16_to_half(numext::uint16_t x);
  118|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __half_raw float_to_half_rtne(float ff);
  119|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float half_to_float(__half_raw h);
  120|       |
  121|       |struct half_base : public __half_raw {
  122|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half_base() {}
  123|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half_base(const __half_raw& h) : __half_raw(h) {}
  124|       |
  125|       |#if defined(EIGEN_HAS_GPU_FP16)
  126|       |#if defined(EIGEN_HAS_HIP_FP16)
  127|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half_base(const __half& h) { x = __half_as_ushort(h); }
  128|       |#elif defined(EIGEN_HAS_CUDA_FP16)
  129|       |#if EIGEN_CUDA_SDK_VER >= 90000
  130|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half_base(const __half& h) : __half_raw(*(__half_raw*)&h) {}
  131|       |#endif
  132|       |#endif
  133|       |#endif
  134|       |};
  135|       |
  136|       |}  // namespace half_impl
  137|       |
  138|       |// Class definition.
  139|       |struct half : public half_impl::half_base {
  140|       |  // Writing this out as separate #if-else blocks to make the code easier to follow
  141|       |  // The same applies to most #if-else blocks in this file
  142|       |#if !defined(EIGEN_HAS_GPU_FP16) || !defined(EIGEN_GPU_COMPILE_PHASE)
  143|       |  // Use the same base class for the following two scenarios
  144|       |  // * when compiling without GPU support enabled
  145|       |  // * during host compile phase when compiling with GPU support enabled
  146|       |  typedef half_impl::__half_raw __half_raw;
  147|       |#elif defined(EIGEN_HAS_HIP_FP16)
  148|       |  // Nothing to do here
  149|       |  // HIP fp16 header file has a definition for __half_raw
  150|       |#elif defined(EIGEN_HAS_CUDA_FP16)
  151|       |// Note that EIGEN_CUDA_SDK_VER is set to 0 even when compiling with HIP, so
  152|       |// (EIGEN_CUDA_SDK_VER < 90000) is true even for HIP!  So keeping this within
  153|       |// #if defined(EIGEN_HAS_CUDA_FP16) is needed
  154|       |#if defined(EIGEN_CUDA_SDK_VER) && EIGEN_CUDA_SDK_VER < 90000
  155|       |  typedef half_impl::__half_raw __half_raw;
  156|       |#endif
  157|       |#endif
  158|       |
  159|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half() {}
  160|       |
  161|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half(const __half_raw& h) : half_impl::half_base(h) {}
  162|       |
  163|       |#if defined(EIGEN_HAS_GPU_FP16)
  164|       |#if defined(EIGEN_HAS_HIP_FP16)
  165|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half(const __half& h) : half_impl::half_base(h) {}
  166|       |#elif defined(EIGEN_HAS_CUDA_FP16)
  167|       |#if defined(EIGEN_CUDA_SDK_VER) && EIGEN_CUDA_SDK_VER >= 90000
  168|       |  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half(const __half& h) : half_impl::half_base(h) {}
  169|       |#endif
  170|       |#endif
  171|       |#endif
  172|       |
  173|       |  explicit EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR half(bool b)
  174|      0|      : half_impl::half_base(half_impl::raw_uint16_to_half(b ? 0x3c00 : 0)) {}
  175|       |  template <class T>
  176|       |  explicit EIGEN_DEVICE_FUNC half(T val)
  177|       |      : half_impl::half_base(half_impl::float_to_half_rtne(static_cast<float>(val))) {}
  178|      0|  explicit EIGEN_DEVICE_FUNC half(float f) : half_impl::half_base(half_impl::float_to_half_rtne(f)) {}
  179|       |
  180|       |  // Following the convention of numpy, converting between complex and
  181|       |  // float will lead to loss of imag value.
  182|       |  template <typename RealScalar>
  183|       |  explicit EIGEN_DEVICE_FUNC half(std::complex<RealScalar> c)
  184|       |      : half_impl::half_base(half_impl::float_to_half_rtne(static_cast<float>(c.real()))) {}
  185|       |
  186|      0|  EIGEN_DEVICE_FUNC operator float() const {  // NOLINT: Allow implicit conversion to float, because it is lossless.
  187|      0|    return half_impl::half_to_float(*this);
  188|      0|  }
  189|       |
  190|       |#if defined(EIGEN_HAS_GPU_FP16) && !defined(EIGEN_GPU_COMPILE_PHASE)
  191|       |  EIGEN_DEVICE_FUNC operator __half() const {
  192|       |    ::__half_raw hr;
  193|       |    hr.x = x;
  194|       |    return __half(hr);
  195|       |  }
  196|       |#endif
  197|       |};
  198|       |
  199|       |// TODO(majnemer): Get rid of this once we can rely on C++17 inline variables do
  200|       |// solve the ODR issue.
  201|       |namespace half_impl {
  202|       |template <typename = void>
  203|       |struct numeric_limits_half_impl {
  204|       |  static EIGEN_CONSTEXPR const bool is_specialized = true;
  205|       |  static EIGEN_CONSTEXPR const bool is_signed = true;
  206|       |  static EIGEN_CONSTEXPR const bool is_integer = false;
  207|       |  static EIGEN_CONSTEXPR const bool is_exact = false;
  208|       |  static EIGEN_CONSTEXPR const bool has_infinity = true;
  209|       |  static EIGEN_CONSTEXPR const bool has_quiet_NaN = true;
  210|       |  static EIGEN_CONSTEXPR const bool has_signaling_NaN = true;
  211|       |  EIGEN_DIAGNOSTICS(push)
  212|       |  EIGEN_DISABLE_DEPRECATED_WARNING
  213|       |  static EIGEN_CONSTEXPR const std::float_denorm_style has_denorm = std::denorm_present;
  214|       |  static EIGEN_CONSTEXPR const bool has_denorm_loss = false;
  215|       |  EIGEN_DIAGNOSTICS(pop)
  216|       |  static EIGEN_CONSTEXPR const std::float_round_style round_style = std::round_to_nearest;
  217|       |  static EIGEN_CONSTEXPR const bool is_iec559 = true;
  218|       |  // The C++ standard defines this as "true if the set of values representable
  219|       |  // by the type is finite." Half has finite precision.
  220|       |  static EIGEN_CONSTEXPR const bool is_bounded = true;
  221|       |  static EIGEN_CONSTEXPR const bool is_modulo = false;
  222|       |  static EIGEN_CONSTEXPR const int digits = 11;
  223|       |  static EIGEN_CONSTEXPR const int digits10 =
  224|       |      3;  // according to http://half.sourceforge.net/structstd_1_1numeric__limits_3_01half__float_1_1half_01_4.html
  225|       |  static EIGEN_CONSTEXPR const int max_digits10 =
  226|       |      5;  // according to http://half.sourceforge.net/structstd_1_1numeric__limits_3_01half__float_1_1half_01_4.html
  227|       |  static EIGEN_CONSTEXPR const int radix = std::numeric_limits<float>::radix;
  228|       |  static EIGEN_CONSTEXPR const int min_exponent = -13;
  229|       |  static EIGEN_CONSTEXPR const int min_exponent10 = -4;
  230|       |  static EIGEN_CONSTEXPR const int max_exponent = 16;
  231|       |  static EIGEN_CONSTEXPR const int max_exponent10 = 4;
  232|       |  static EIGEN_CONSTEXPR const bool traps = std::numeric_limits<float>::traps;
  233|       |  // IEEE754: "The implementer shall choose how tininess is detected, but shall
  234|       |  // detect tininess in the same way for all operations in radix two"
  235|       |  static EIGEN_CONSTEXPR const bool tinyness_before = std::numeric_limits<float>::tinyness_before;
  236|       |
  237|       |  static EIGEN_CONSTEXPR Eigen::half(min)() { return Eigen::half_impl::raw_uint16_to_half(0x0400); }
  238|       |  static EIGEN_CONSTEXPR Eigen::half lowest() { return Eigen::half_impl::raw_uint16_to_half(0xfbff); }
  239|       |  static EIGEN_CONSTEXPR Eigen::half(max)() { return Eigen::half_impl::raw_uint16_to_half(0x7bff); }
  240|       |  static EIGEN_CONSTEXPR Eigen::half epsilon() { return Eigen::half_impl::raw_uint16_to_half(0x1400); }
  241|       |  static EIGEN_CONSTEXPR Eigen::half round_error() { return Eigen::half_impl::raw_uint16_to_half(0x3800); }
  242|       |  static EIGEN_CONSTEXPR Eigen::half infinity() { return Eigen::half_impl::raw_uint16_to_half(0x7c00); }
  243|       |  static EIGEN_CONSTEXPR Eigen::half quiet_NaN() { return Eigen::half_impl::raw_uint16_to_half(0x7e00); }
  244|       |  static EIGEN_CONSTEXPR Eigen::half signaling_NaN() { return Eigen::half_impl::raw_uint16_to_half(0x7d00); }
  245|       |  static EIGEN_CONSTEXPR Eigen::half denorm_min() { return Eigen::half_impl::raw_uint16_to_half(0x0001); }
  246|       |};
  247|       |
  248|       |template <typename T>
  249|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::is_specialized;
  250|       |template <typename T>
  251|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::is_signed;
  252|       |template <typename T>
  253|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::is_integer;
  254|       |template <typename T>
  255|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::is_exact;
  256|       |template <typename T>
  257|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::has_infinity;
  258|       |template <typename T>
  259|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::has_quiet_NaN;
  260|       |template <typename T>
  261|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::has_signaling_NaN;
  262|       |EIGEN_DIAGNOSTICS(push)
  263|       |EIGEN_DISABLE_DEPRECATED_WARNING
  264|       |template <typename T>
  265|       |EIGEN_CONSTEXPR const std::float_denorm_style numeric_limits_half_impl<T>::has_denorm;
  266|       |template <typename T>
  267|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::has_denorm_loss;
  268|       |EIGEN_DIAGNOSTICS(pop)
  269|       |template <typename T>
  270|       |EIGEN_CONSTEXPR const std::float_round_style numeric_limits_half_impl<T>::round_style;
  271|       |template <typename T>
  272|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::is_iec559;
  273|       |template <typename T>
  274|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::is_bounded;
  275|       |template <typename T>
  276|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::is_modulo;
  277|       |template <typename T>
  278|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::digits;
  279|       |template <typename T>
  280|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::digits10;
  281|       |template <typename T>
  282|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::max_digits10;
  283|       |template <typename T>
  284|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::radix;
  285|       |template <typename T>
  286|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::min_exponent;
  287|       |template <typename T>
  288|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::min_exponent10;
  289|       |template <typename T>
  290|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::max_exponent;
  291|       |template <typename T>
  292|       |EIGEN_CONSTEXPR const int numeric_limits_half_impl<T>::max_exponent10;
  293|       |template <typename T>
  294|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::traps;
  295|       |template <typename T>
  296|       |EIGEN_CONSTEXPR const bool numeric_limits_half_impl<T>::tinyness_before;
  297|       |}  // end namespace half_impl
  298|       |}  // end namespace Eigen
  299|       |
  300|       |namespace std {
  301|       |// If std::numeric_limits<T> is specialized, should also specialize
  302|       |// std::numeric_limits<const T>, std::numeric_limits<volatile T>, and
  303|       |// std::numeric_limits<const volatile T>
  304|       |// https://stackoverflow.com/a/16519653/
  305|       |template <>
  306|       |class numeric_limits<Eigen::half> : public Eigen::half_impl::numeric_limits_half_impl<> {};
  307|       |template <>
  308|       |class numeric_limits<const Eigen::half> : public numeric_limits<Eigen::half> {};
  309|       |template <>
  310|       |class numeric_limits<volatile Eigen::half> : public numeric_limits<Eigen::half> {};
  311|       |template <>
  312|       |class numeric_limits<const volatile Eigen::half> : public numeric_limits<Eigen::half> {};
  313|       |}  // end namespace std
  314|       |
  315|       |namespace Eigen {
  316|       |
  317|       |namespace half_impl {
  318|       |
  319|       |#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 530) || \
  320|       |    (defined(EIGEN_HAS_HIP_FP16) && defined(HIP_DEVICE_COMPILE))
  321|       |// Note: We deliberately do *not* define this to 1 even if we have Arm's native
  322|       |// fp16 type since GPU half types are rather different from native CPU half types.
  323|       |// TODO: Rename to something like EIGEN_HAS_NATIVE_GPU_FP16
  324|       |#define EIGEN_HAS_NATIVE_FP16
  325|       |#endif
  326|       |
  327|       |// Intrinsics for native fp16 support. Note that on current hardware,
  328|       |// these are no faster than fp32 arithmetic (you need to use the half2
  329|       |// versions to get the ALU speed increased), but you do save the
  330|       |// conversion steps back and forth.
  331|       |
  332|       |#if defined(EIGEN_HAS_NATIVE_FP16)
  333|       |EIGEN_STRONG_INLINE __device__ half operator+(const half& a, const half& b) {
  334|       |#if defined(EIGEN_CUDA_SDK_VER) && EIGEN_CUDA_SDK_VER >= 90000
  335|       |  return __hadd(::__half(a), ::__half(b));
  336|       |#else
  337|       |  return __hadd(a, b);
  338|       |#endif
  339|       |}
  340|       |EIGEN_STRONG_INLINE __device__ half operator*(const half& a, const half& b) { return __hmul(a, b); }
  341|       |EIGEN_STRONG_INLINE __device__ half operator-(const half& a, const half& b) { return __hsub(a, b); }
  342|       |EIGEN_STRONG_INLINE __device__ half operator/(const half& a, const half& b) {
  343|       |#if defined(EIGEN_CUDA_SDK_VER) && EIGEN_CUDA_SDK_VER >= 90000
  344|       |  return __hdiv(a, b);
  345|       |#else
  346|       |  float num = __half2float(a);
  347|       |  float denom = __half2float(b);
  348|       |  return __float2half(num / denom);
  349|       |#endif
  350|       |}
  351|       |EIGEN_STRONG_INLINE __device__ half operator-(const half& a) { return __hneg(a); }
  352|       |EIGEN_STRONG_INLINE __device__ half& operator+=(half& a, const half& b) {
  353|       |  a = a + b;
  354|       |  return a;
  355|       |}
  356|       |EIGEN_STRONG_INLINE __device__ half& operator*=(half& a, const half& b) {
  357|       |  a = a * b;
  358|       |  return a;
  359|       |}
  360|       |EIGEN_STRONG_INLINE __device__ half& operator-=(half& a, const half& b) {
  361|       |  a = a - b;
  362|       |  return a;
  363|       |}
  364|       |EIGEN_STRONG_INLINE __device__ half& operator/=(half& a, const half& b) {
  365|       |  a = a / b;
  366|       |  return a;
  367|       |}
  368|       |EIGEN_STRONG_INLINE __device__ bool operator==(const half& a, const half& b) { return __heq(a, b); }
  369|       |EIGEN_STRONG_INLINE __device__ bool operator!=(const half& a, const half& b) { return __hne(a, b); }
  370|       |EIGEN_STRONG_INLINE __device__ bool operator<(const half& a, const half& b) { return __hlt(a, b); }
  371|       |EIGEN_STRONG_INLINE __device__ bool operator<=(const half& a, const half& b) { return __hle(a, b); }
  372|       |EIGEN_STRONG_INLINE __device__ bool operator>(const half& a, const half& b) { return __hgt(a, b); }
  373|       |EIGEN_STRONG_INLINE __device__ bool operator>=(const half& a, const half& b) { return __hge(a, b); }
  374|       |#endif
  375|       |
  376|       |#if defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC) && !defined(EIGEN_GPU_COMPILE_PHASE)
  377|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator+(const half& a, const half& b) { return half(vaddh_f16(a.x, b.x)); }
  378|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator*(const half& a, const half& b) { return half(vmulh_f16(a.x, b.x)); }
  379|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator-(const half& a, const half& b) { return half(vsubh_f16(a.x, b.x)); }
  380|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator/(const half& a, const half& b) { return half(vdivh_f16(a.x, b.x)); }
  381|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator-(const half& a) { return half(vnegh_f16(a.x)); }
  382|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator+=(half& a, const half& b) {
  383|       |  a = half(vaddh_f16(a.x, b.x));
  384|       |  return a;
  385|       |}
  386|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator*=(half& a, const half& b) {
  387|       |  a = half(vmulh_f16(a.x, b.x));
  388|       |  return a;
  389|       |}
  390|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator-=(half& a, const half& b) {
  391|       |  a = half(vsubh_f16(a.x, b.x));
  392|       |  return a;
  393|       |}
  394|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator/=(half& a, const half& b) {
  395|       |  a = half(vdivh_f16(a.x, b.x));
  396|       |  return a;
  397|       |}
  398|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator==(const half& a, const half& b) { return vceqh_f16(a.x, b.x); }
  399|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator!=(const half& a, const half& b) { return !vceqh_f16(a.x, b.x); }
  400|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator<(const half& a, const half& b) { return vclth_f16(a.x, b.x); }
  401|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator<=(const half& a, const half& b) { return vcleh_f16(a.x, b.x); }
  402|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator>(const half& a, const half& b) { return vcgth_f16(a.x, b.x); }
  403|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator>=(const half& a, const half& b) { return vcgeh_f16(a.x, b.x); }
  404|       |// We need to distinguish ‘clang as the CUDA compiler’ from ‘clang as the host compiler,
  405|       |// invoked by NVCC’ (e.g. on MacOS). The former needs to see both host and device implementation
  406|       |// of the functions, while the latter can only deal with one of them.
  407|       |#elif !defined(EIGEN_HAS_NATIVE_FP16) || (EIGEN_COMP_CLANG && !EIGEN_COMP_NVCC)  // Emulate support for half floats
  408|       |
  409|       |#if EIGEN_COMP_CLANG && defined(EIGEN_GPUCC)
  410|       |// We need to provide emulated *host-side* FP16 operators for clang.
  411|       |#pragma push_macro("EIGEN_DEVICE_FUNC")
  412|       |#undef EIGEN_DEVICE_FUNC
  413|       |#if defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_HAS_NATIVE_FP16)
  414|       |#define EIGEN_DEVICE_FUNC __host__
  415|       |#else  // both host and device need emulated ops.
  416|       |#define EIGEN_DEVICE_FUNC __host__ __device__
  417|       |#endif
  418|       |#endif
  419|       |
  420|       |// Definitions for CPUs and older HIP+CUDA, mostly working through conversion
  421|       |// to/from fp32.
  422|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator+(const half& a, const half& b) { return half(float(a) + float(b)); }
  423|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator*(const half& a, const half& b) { return half(float(a) * float(b)); }
  424|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator-(const half& a, const half& b) { return half(float(a) - float(b)); }
  425|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator/(const half& a, const half& b) { return half(float(a) / float(b)); }
  426|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator-(const half& a) {
  427|      0|  half result;
  428|      0|  result.x = a.x ^ 0x8000;
  429|      0|  return result;
  430|      0|}
  431|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator+=(half& a, const half& b) {
  432|      0|  a = half(float(a) + float(b));
  433|      0|  return a;
  434|      0|}
  435|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator*=(half& a, const half& b) {
  436|      0|  a = half(float(a) * float(b));
  437|      0|  return a;
  438|      0|}
  439|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator-=(half& a, const half& b) {
  440|      0|  a = half(float(a) - float(b));
  441|      0|  return a;
  442|      0|}
  443|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half& operator/=(half& a, const half& b) {
  444|      0|  a = half(float(a) / float(b));
  445|      0|  return a;
  446|      0|}
  447|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator==(const half& a, const half& b) {
  448|      0|  return numext::equal_strict(float(a), float(b));
  449|      0|}
  450|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator!=(const half& a, const half& b) {
  451|      0|  return numext::not_equal_strict(float(a), float(b));
  452|      0|}
  453|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator<(const half& a, const half& b) { return float(a) < float(b); }
  454|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator<=(const half& a, const half& b) { return float(a) <= float(b); }
  455|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator>(const half& a, const half& b) { return float(a) > float(b); }
  456|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool operator>=(const half& a, const half& b) { return float(a) >= float(b); }
  457|       |
  458|       |#if EIGEN_COMP_CLANG && defined(EIGEN_GPUCC)
  459|       |#pragma pop_macro("EIGEN_DEVICE_FUNC")
  460|       |#endif
  461|       |#endif  // Emulate support for half floats
  462|       |
  463|       |// Division by an index. Do it in full float precision to avoid accuracy
  464|       |// issues in converting the denominator to half.
  465|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator/(const half& a, Index b) {
  466|      0|  return half(static_cast<float>(a) / static_cast<float>(b));
  467|      0|}
  468|       |
  469|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator++(half& a) {
  470|      0|  a += half(1);
  471|      0|  return a;
  472|      0|}
  473|       |
  474|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator--(half& a) {
  475|      0|  a -= half(1);
  476|      0|  return a;
  477|      0|}
  478|       |
  479|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator++(half& a, int) {
  480|      0|  half original_value = a;
  481|      0|  ++a;
  482|      0|  return original_value;
  483|      0|}
  484|       |
  485|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half operator--(half& a, int) {
  486|      0|  half original_value = a;
  487|      0|  --a;
  488|      0|  return original_value;
  489|      0|}
  490|       |
  491|       |// Conversion routines, including fallbacks for the host or older CUDA.
  492|       |// Note that newer Intel CPUs (Haswell or newer) have vectorized versions of
  493|       |// these in hardware. If we need more performance on older/other CPUs, they are
  494|       |// also possible to vectorize directly.
  495|       |
  496|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR __half_raw raw_uint16_to_half(numext::uint16_t x) {
  497|      0|  // We cannot simply do a "return __half_raw(x)" here, because __half_raw is union type
  498|      0|  // in the hip_fp16 header file, and that will trigger a compile error
  499|      0|  // On the other hand, having anything but a return statement also triggers a compile error
  500|      0|  // because this is constexpr function.
  501|      0|  // Fortunately, since we need to disable EIGEN_CONSTEXPR for GPU anyway, we can get out
  502|      0|  // of this catch22 by having separate bodies for GPU / non GPU
  503|      0|#if defined(EIGEN_HAS_GPU_FP16)
  504|      0|  __half_raw h;
  505|      0|  h.x = x;
  506|      0|  return h;
  507|      0|#else
  508|      0|  return __half_raw(x);
  509|      0|#endif
  510|      0|}
  511|       |
  512|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC numext::uint16_t raw_half_as_uint16(const __half_raw& h) {
  513|      0|  // HIP/CUDA/Default have a member 'x' of type uint16_t.
  514|      0|  // For ARM64 native half, the member 'x' is of type __fp16, so we need to bit-cast.
  515|      0|  // For SYCL, cl::sycl::half is _Float16, so cast directly.
  516|      0|#if defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
  517|      0|  return numext::bit_cast<numext::uint16_t>(h.x);
  518|      0|#elif defined(SYCL_DEVICE_ONLY)
  519|      0|  return numext::bit_cast<numext::uint16_t>(h);
  520|      0|#else
  521|      0|  return h.x;
  522|      0|#endif
  523|      0|}
  524|       |
  525|       |union float32_bits {
  526|       |  unsigned int u;
  527|       |  float f;
  528|       |};
  529|       |
  530|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC __half_raw float_to_half_rtne(float ff) {
  531|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 300) || \
  532|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  533|      0|  __half tmp_ff = __float2half(ff);
  534|      0|  return *(__half_raw*)&tmp_ff;
  535|      0|
  536|      0|#elif defined(EIGEN_HAS_FP16_C)
  537|      0|  __half_raw h;
  538|      0|#if EIGEN_COMP_MSVC
  539|      0|  // MSVC does not have scalar instructions.
  540|      0|  h.x = _mm_extract_epi16(_mm_cvtps_ph(_mm_set_ss(ff), 0), 0);
  541|      0|#else
  542|      0|  h.x = _cvtss_sh(ff, 0);
  543|      0|#endif
  544|      0|  return h;
  545|      0|
  546|      0|#elif defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
  547|      0|  __half_raw h;
  548|      0|  h.x = static_cast<__fp16>(ff);
  549|      0|  return h;
  550|      0|
  551|      0|#else
  552|      0|  float32_bits f;
  553|      0|  f.f = ff;
  554|      0|
  555|      0|  const float32_bits f32infty = {255 << 23};
  556|      0|  const float32_bits f16max = {(127 + 16) << 23};
  557|      0|  const float32_bits denorm_magic = {((127 - 15) + (23 - 10) + 1) << 23};
  558|      0|  unsigned int sign_mask = 0x80000000u;
  559|      0|  __half_raw o;
  560|      0|  o.x = static_cast<numext::uint16_t>(0x0u);
  561|      0|
  562|      0|  unsigned int sign = f.u & sign_mask;
  563|      0|  f.u ^= sign;
  564|      0|
  565|      0|  // NOTE all the integer compares in this function can be safely
  566|      0|  // compiled into signed compares since all operands are below
  567|      0|  // 0x80000000. Important if you want fast straight SSE2 code
  568|      0|  // (since there's no unsigned PCMPGTD).
  569|      0|
  570|      0|  if (f.u >= f16max.u) {                         // result is Inf or NaN (all exponent bits set)
  571|      0|    o.x = (f.u > f32infty.u) ? 0x7e00 : 0x7c00;  // NaN->qNaN and Inf->Inf
  572|      0|  } else {                                       // (De)normalized number or zero
  573|      0|    if (f.u < (113 << 23)) {                     // resulting FP16 is subnormal or zero
  574|      0|      // use a magic value to align our 10 mantissa bits at the bottom of
  575|      0|      // the float. as long as FP addition is round-to-nearest-even this
  576|      0|      // just works.
  577|      0|      f.f += denorm_magic.f;
  578|      0|
  579|      0|      // and one integer subtract of the bias later, we have our final float!
  580|      0|      o.x = static_cast<numext::uint16_t>(f.u - denorm_magic.u);
  581|      0|    } else {
  582|      0|      unsigned int mant_odd = (f.u >> 13) & 1;  // resulting mantissa is odd
  583|      0|
  584|      0|      // update exponent, rounding bias part 1
  585|      0|      // Equivalent to `f.u += ((unsigned int)(15 - 127) << 23) + 0xfff`, but
  586|      0|      // without arithmetic overflow.
  587|      0|      f.u += 0xc8000fffU;
  588|      0|      // rounding bias part 2
  589|      0|      f.u += mant_odd;
  590|      0|      // take the bits!
  591|      0|      o.x = static_cast<numext::uint16_t>(f.u >> 13);
  592|      0|    }
  593|      0|  }
  594|      0|
  595|      0|  o.x |= static_cast<numext::uint16_t>(sign >> 16);
  596|      0|  return o;
  597|      0|#endif
  598|      0|}
  599|       |
  600|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC float half_to_float(__half_raw h) {
  601|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 300) || \
  602|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  603|      0|  return __half2float(h);
  604|      0|#elif defined(EIGEN_HAS_FP16_C)
  605|      0|#if EIGEN_COMP_MSVC
  606|      0|  // MSVC does not have scalar instructions.
  607|      0|  return _mm_cvtss_f32(_mm_cvtph_ps(_mm_set1_epi16(h.x)));
  608|      0|#else
  609|      0|  return _cvtsh_ss(h.x);
  610|      0|#endif
  611|      0|#elif defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
  612|      0|  return static_cast<float>(h.x);
  613|      0|#else
  614|      0|  const float32_bits magic = {113 << 23};
  615|      0|  const unsigned int shifted_exp = 0x7c00 << 13;  // exponent mask after shift
  616|      0|  float32_bits o;
  617|      0|
  618|      0|  o.u = (h.x & 0x7fff) << 13;            // exponent/mantissa bits
  619|      0|  unsigned int exp = shifted_exp & o.u;  // just the exponent
  620|      0|  o.u += (127 - 15) << 23;               // exponent adjust
  621|      0|
  622|      0|  // handle exponent special cases
  623|      0|  if (exp == shifted_exp) {   // Inf/NaN?
  624|      0|    o.u += (128 - 16) << 23;  // extra exp adjust
  625|      0|  } else if (exp == 0) {      // Zero/Denormal?
  626|      0|    o.u += 1 << 23;           // extra exp adjust
  627|      0|    o.f -= magic.f;           // renormalize
  628|      0|  }
  629|      0|
  630|      0|  o.u |= (h.x & 0x8000) << 16;  // sign bit
  631|      0|  return o.f;
  632|      0|#endif
  633|      0|}
  634|       |
  635|       |// --- standard functions ---
  636|       |
  637|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool(isinf)(const half& a) {
  638|      0|#ifdef EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC
  639|      0|  return (numext::bit_cast<numext::uint16_t>(a.x) & 0x7fff) == 0x7c00;
  640|      0|#else
  641|      0|  return (a.x & 0x7fff) == 0x7c00;
  642|      0|#endif
  643|      0|}
  644|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool(isnan)(const half& a) {
  645|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 530) || \
  646|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  647|      0|  return __hisnan(a);
  648|      0|#elif defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
  649|      0|  return (numext::bit_cast<numext::uint16_t>(a.x) & 0x7fff) > 0x7c00;
  650|      0|#else
  651|      0|  return (a.x & 0x7fff) > 0x7c00;
  652|      0|#endif
  653|      0|}
  654|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool(isfinite)(const half& a) {
  655|      0|  return !(isinf EIGEN_NOT_A_MACRO(a)) && !(isnan EIGEN_NOT_A_MACRO(a));
  656|      0|}
  657|       |
  658|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half abs(const half& a) {
  659|      0|#if defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
  660|      0|  return half(vabsh_f16(a.x));
  661|      0|#else
  662|      0|  half result;
  663|      0|  result.x = a.x & 0x7FFF;
  664|      0|  return result;
  665|      0|#endif
  666|      0|}
  667|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp(const half& a) {
  668|      0|#if (EIGEN_CUDA_SDK_VER >= 80000 && defined EIGEN_CUDA_ARCH && EIGEN_CUDA_ARCH >= 530) || \
  669|      0|    defined(EIGEN_HIP_DEVICE_COMPILE)
  670|      0|  return half(hexp(a));
  671|      0|#else
  672|      0|  return half(::expf(float(a)));
  673|      0|#endif
  674|      0|}
  675|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half exp2(const half& a) {
  676|      0|#if (EIGEN_CUDA_SDK_VER >= 80000 && defined EIGEN_CUDA_ARCH && EIGEN_CUDA_ARCH >= 530) || \
  677|      0|    defined(EIGEN_HIP_DEVICE_COMPILE)
  678|      0|  return half(hexp2(a));
  679|      0|#else
  680|      0|  return half(::exp2f(float(a)));
  681|      0|#endif
  682|      0|}
  683|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half expm1(const half& a) { return half(numext::expm1(float(a))); }
  684|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log(const half& a) {
  685|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && EIGEN_CUDA_SDK_VER >= 80000 && defined(EIGEN_CUDA_ARCH) && \
  686|      0|     EIGEN_CUDA_ARCH >= 530) ||                                                                 \
  687|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  688|      0|  return half(hlog(a));
  689|      0|#else
  690|      0|  return half(::logf(float(a)));
  691|      0|#endif
  692|      0|}
  693|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log1p(const half& a) { return half(numext::log1p(float(a))); }
  694|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log10(const half& a) { return half(::log10f(float(a))); }
  695|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half log2(const half& a) {
  696|      0|  return half(static_cast<float>(EIGEN_LOG2E) * ::logf(float(a)));
  697|      0|}
  698|       |
  699|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sqrt(const half& a) {
  700|      0|#if (EIGEN_CUDA_SDK_VER >= 80000 && defined EIGEN_CUDA_ARCH && EIGEN_CUDA_ARCH >= 530) || \
  701|      0|    defined(EIGEN_HIP_DEVICE_COMPILE)
  702|      0|  return half(hsqrt(a));
  703|      0|#else
  704|      0|  return half(::sqrtf(float(a)));
  705|      0|#endif
  706|      0|}
  707|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half pow(const half& a, const half& b) {
  708|      0|  return half(::powf(float(a), float(b)));
  709|      0|}
  710|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan2(const half& a, const half& b) {
  711|      0|  return half(::atan2f(float(a), float(b)));
  712|      0|}
  713|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half sin(const half& a) { return half(::sinf(float(a))); }
  714|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half cos(const half& a) { return half(::cosf(float(a))); }
  715|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tan(const half& a) { return half(::tanf(float(a))); }
  716|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half tanh(const half& a) { return half(::tanhf(float(a))); }
  717|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half asin(const half& a) { return half(::asinf(float(a))); }
  718|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half acos(const half& a) { return half(::acosf(float(a))); }
  719|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atan(const half& a) { return half(::atanf(float(a))); }
  720|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half atanh(const half& a) { return half(::atanhf(float(a))); }
  721|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half floor(const half& a) {
  722|      0|#if (EIGEN_CUDA_SDK_VER >= 80000 && defined EIGEN_CUDA_ARCH && EIGEN_CUDA_ARCH >= 300) || \
  723|      0|    defined(EIGEN_HIP_DEVICE_COMPILE)
  724|      0|  return half(hfloor(a));
  725|      0|#else
  726|      0|  return half(::floorf(float(a)));
  727|      0|#endif
  728|      0|}
  729|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half ceil(const half& a) {
  730|      0|#if (EIGEN_CUDA_SDK_VER >= 80000 && defined EIGEN_CUDA_ARCH && EIGEN_CUDA_ARCH >= 300) || \
  731|      0|    defined(EIGEN_HIP_DEVICE_COMPILE)
  732|      0|  return half(hceil(a));
  733|      0|#else
  734|      0|  return half(::ceilf(float(a)));
  735|      0|#endif
  736|      0|}
  737|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half rint(const half& a) { return half(::rintf(float(a))); }
  738|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half round(const half& a) { return half(::roundf(float(a))); }
  739|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half trunc(const half& a) { return half(::truncf(float(a))); }
  740|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half fmod(const half& a, const half& b) {
  741|      0|  return half(::fmodf(float(a), float(b)));
  742|      0|}
  743|       |
  744|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half(min)(const half& a, const half& b) {
  745|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 530) || \
  746|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  747|      0|  return __hlt(b, a) ? b : a;
  748|      0|#else
  749|      0|  const float f1 = static_cast<float>(a);
  750|      0|  const float f2 = static_cast<float>(b);
  751|      0|  return f2 < f1 ? b : a;
  752|      0|#endif
  753|      0|}
  754|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC half(max)(const half& a, const half& b) {
  755|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 530) || \
  756|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  757|      0|  return __hlt(a, b) ? b : a;
  758|      0|#else
  759|      0|  const float f1 = static_cast<float>(a);
  760|      0|  const float f2 = static_cast<float>(b);
  761|      0|  return f1 < f2 ? b : a;
  762|      0|#endif
  763|      0|}
  764|       |
  765|       |#ifndef EIGEN_NO_IO
  766|      0|EIGEN_ALWAYS_INLINE std::ostream& operator<<(std::ostream& os, const half& v) {
  767|      0|  os << static_cast<float>(v);
  768|      0|  return os;
  769|      0|}
  770|       |#endif
  771|       |
  772|       |}  // end namespace half_impl
  773|       |
  774|       |// import Eigen::half_impl::half into Eigen namespace
  775|       |// using half_impl::half;
  776|       |
  777|       |namespace internal {
  778|       |
  779|       |template <>
  780|       |struct is_arithmetic<half> {
  781|       |  enum { value = true };
  782|       |};
  783|       |
  784|       |template <>
  785|       |struct random_impl<half> {
  786|       |  enum : int { MantissaBits = 10 };
  787|       |  using Impl = random_impl<float>;
  788|      0|  static EIGEN_DEVICE_FUNC inline half run(const half& x, const half& y) {
  789|      0|    float result = Impl::run(x, y, MantissaBits);
  790|      0|    return half(result);
  791|      0|  }
  792|      0|  static EIGEN_DEVICE_FUNC inline half run() {
  793|      0|    float result = Impl::run(MantissaBits);
  794|      0|    return half(result);
  795|      0|  }
  796|       |};
  797|       |
  798|       |}  // end namespace internal
  799|       |
  800|       |template <>
  801|       |struct NumTraits<Eigen::half> : GenericNumTraits<Eigen::half> {
  802|       |  enum { IsSigned = true, IsInteger = false, IsComplex = false, RequireInitialization = false };
  803|       |
  804|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::half epsilon() {
  805|      0|    return half_impl::raw_uint16_to_half(0x0800);
  806|      0|  }
  807|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::half dummy_precision() {
  808|      0|    return half_impl::raw_uint16_to_half(0x211f);  //  Eigen::half(1e-2f);
  809|      0|  }
  810|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::half highest() {
  811|      0|    return half_impl::raw_uint16_to_half(0x7bff);
  812|      0|  }
  813|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::half lowest() {
  814|      0|    return half_impl::raw_uint16_to_half(0xfbff);
  815|      0|  }
  816|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::half infinity() {
  817|      0|    return half_impl::raw_uint16_to_half(0x7c00);
  818|      0|  }
  819|      0|  EIGEN_DEVICE_FUNC EIGEN_CONSTEXPR static EIGEN_STRONG_INLINE Eigen::half quiet_NaN() {
  820|      0|    return half_impl::raw_uint16_to_half(0x7e00);
  821|      0|  }
  822|       |};
  823|       |
  824|       |}  // end namespace Eigen
  825|       |
  826|       |#if defined(EIGEN_HAS_GPU_FP16) || defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
  827|       |#pragma pop_macro("EIGEN_CONSTEXPR")
  828|       |#endif
  829|       |
  830|       |namespace Eigen {
  831|       |namespace numext {
  832|       |
  833|       |#if defined(EIGEN_GPU_COMPILE_PHASE)
  834|       |
  835|       |template <>
  836|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE bool(isnan)(const Eigen::half& h) {
  837|       |  return (half_impl::isnan)(h);
  838|       |}
  839|       |
  840|       |template <>
  841|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE bool(isinf)(const Eigen::half& h) {
  842|       |  return (half_impl::isinf)(h);
  843|       |}
  844|       |
  845|       |template <>
  846|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE bool(isfinite)(const Eigen::half& h) {
  847|       |  return (half_impl::isfinite)(h);
  848|       |}
  849|       |
  850|       |#endif
  851|       |
  852|       |template <>
  853|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC Eigen::half bit_cast<Eigen::half, uint16_t>(const uint16_t& src) {
  854|      0|  return Eigen::half(Eigen::half_impl::raw_uint16_to_half(src));
  855|      0|}
  856|       |
  857|       |template <>
  858|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC uint16_t bit_cast<uint16_t, Eigen::half>(const Eigen::half& src) {
  859|      0|  return Eigen::half_impl::raw_half_as_uint16(src);
  860|      0|}
  861|       |
  862|       |}  // namespace numext
  863|       |}  // namespace Eigen
  864|       |
  865|       |// Add the missing shfl* intrinsics.
  866|       |// The __shfl* functions are only valid on HIP or _CUDA_ARCH_ >= 300.
  867|       |//   CUDA defines them for (__CUDA_ARCH__ >= 300 || !defined(__CUDA_ARCH__))
  868|       |//
  869|       |// HIP and CUDA prior to SDK 9.0 define
  870|       |//    __shfl, __shfl_up, __shfl_down, __shfl_xor for int and float
  871|       |// CUDA since 9.0 deprecates those and instead defines
  872|       |//    __shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync,
  873|       |//    with native support for __half and __nv_bfloat16
  874|       |//
  875|       |// Note that the following are __device__ - only functions.
  876|       |#if (defined(EIGEN_CUDACC) && (!defined(EIGEN_CUDA_ARCH) || EIGEN_CUDA_ARCH >= 300)) || defined(EIGEN_HIPCC)
  877|       |
  878|       |#if defined(EIGEN_HAS_CUDA_FP16) && EIGEN_CUDA_SDK_VER >= 90000
  879|       |
  880|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl_sync(unsigned mask, Eigen::half var, int srcLane,
  881|       |                                                       int width = warpSize) {
  882|       |  const __half h = var;
  883|       |  return static_cast<Eigen::half>(__shfl_sync(mask, h, srcLane, width));
  884|       |}
  885|       |
  886|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl_up_sync(unsigned mask, Eigen::half var, unsigned int delta,
  887|       |                                                          int width = warpSize) {
  888|       |  const __half h = var;
  889|       |  return static_cast<Eigen::half>(__shfl_up_sync(mask, h, delta, width));
  890|       |}
  891|       |
  892|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl_down_sync(unsigned mask, Eigen::half var, unsigned int delta,
  893|       |                                                            int width = warpSize) {
  894|       |  const __half h = var;
  895|       |  return static_cast<Eigen::half>(__shfl_down_sync(mask, h, delta, width));
  896|       |}
  897|       |
  898|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl_xor_sync(unsigned mask, Eigen::half var, int laneMask,
  899|       |                                                           int width = warpSize) {
  900|       |  const __half h = var;
  901|       |  return static_cast<Eigen::half>(__shfl_xor_sync(mask, h, laneMask, width));
  902|       |}
  903|       |
  904|       |#else  // HIP or CUDA SDK < 9.0
  905|       |
  906|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl(Eigen::half var, int srcLane, int width = warpSize) {
  907|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  908|       |  return Eigen::numext::bit_cast<Eigen::half>(static_cast<Eigen::numext::uint16_t>(__shfl(ivar, srcLane, width)));
  909|       |}
  910|       |
  911|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl_up(Eigen::half var, unsigned int delta, int width = warpSize) {
  912|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  913|       |  return Eigen::numext::bit_cast<Eigen::half>(static_cast<Eigen::numext::uint16_t>(__shfl_up(ivar, delta, width)));
  914|       |}
  915|       |
  916|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl_down(Eigen::half var, unsigned int delta, int width = warpSize) {
  917|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  918|       |  return Eigen::numext::bit_cast<Eigen::half>(static_cast<Eigen::numext::uint16_t>(__shfl_down(ivar, delta, width)));
  919|       |}
  920|       |
  921|       |__device__ EIGEN_STRONG_INLINE Eigen::half __shfl_xor(Eigen::half var, int laneMask, int width = warpSize) {
  922|       |  const int ivar = static_cast<int>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(var));
  923|       |  return Eigen::numext::bit_cast<Eigen::half>(static_cast<Eigen::numext::uint16_t>(__shfl_xor(ivar, laneMask, width)));
  924|       |}
  925|       |
  926|       |#endif  // HIP vs CUDA
  927|       |#endif  // __shfl*
  928|       |
  929|       |// ldg() has an overload for __half_raw, but we also need one for Eigen::half.
  930|       |#if (defined(EIGEN_CUDACC) && (!defined(EIGEN_CUDA_ARCH) || EIGEN_CUDA_ARCH >= 350)) || defined(EIGEN_HIPCC)
  931|       |EIGEN_STRONG_INLINE __device__ Eigen::half __ldg(const Eigen::half* ptr) {
  932|       |  return Eigen::half_impl::raw_uint16_to_half(__ldg(reinterpret_cast<const Eigen::numext::uint16_t*>(ptr)));
  933|       |}
  934|       |#endif  // __ldg
  935|       |
  936|       |#if EIGEN_HAS_STD_HASH
  937|       |namespace std {
  938|       |template <>
  939|       |struct hash<Eigen::half> {
  940|      0|  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE std::size_t operator()(const Eigen::half& a) const {
  941|      0|    return static_cast<std::size_t>(Eigen::numext::bit_cast<Eigen::numext::uint16_t>(a));
  942|      0|  }
  943|       |};
  944|       |}  // end namespace std
  945|       |#endif
  946|       |
  947|       |namespace Eigen {
  948|       |namespace internal {
  949|       |
  950|       |template <>
  951|       |struct cast_impl<float, half> {
  952|      0|  EIGEN_DEVICE_FUNC static inline half run(const float& a) {
  953|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 300) || \
  954|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  955|      0|    return __float2half(a);
  956|      0|#else
  957|      0|    return half(a);
  958|      0|#endif
  959|      0|  }
  960|       |};
  961|       |
  962|       |template <>
  963|       |struct cast_impl<int, half> {
  964|      0|  EIGEN_DEVICE_FUNC static inline half run(const int& a) {
  965|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 300) || \
  966|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  967|      0|    return __float2half(static_cast<float>(a));
  968|      0|#else
  969|      0|    return half(static_cast<float>(a));
  970|      0|#endif
  971|      0|  }
  972|       |};
  973|       |
  974|       |template <>
  975|       |struct cast_impl<half, float> {
  976|      0|  EIGEN_DEVICE_FUNC static inline float run(const half& a) {
  977|      0|#if (defined(EIGEN_HAS_CUDA_FP16) && defined(EIGEN_CUDA_ARCH) && EIGEN_CUDA_ARCH >= 300) || \
  978|      0|    (defined(EIGEN_HAS_HIP_FP16) && defined(EIGEN_HIP_DEVICE_COMPILE))
  979|      0|    return __half2float(a);
  980|      0|#else
  981|      0|    return static_cast<float>(a);
  982|      0|#endif
  983|      0|  }
  984|       |};
  985|       |
  986|       |}  // namespace internal
  987|       |}  // namespace Eigen
  988|       |
  989|       |#endif  // EIGEN_HALF_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/SSE/Complex.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2010 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_COMPLEX_SSE_H
   11|       |#define EIGEN_COMPLEX_SSE_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |//---------- float ----------
   21|       |struct Packet2cf {
   22|      0|  EIGEN_STRONG_INLINE Packet2cf() {}
   23|      0|  EIGEN_STRONG_INLINE explicit Packet2cf(const __m128& a) : v(a) {}
   24|       |  Packet4f v;
   25|       |};
   26|       |
   27|       |// Use the packet_traits defined in AVX/PacketMath.h instead if we're going
   28|       |// to leverage AVX instructions.
   29|       |#ifndef EIGEN_VECTORIZE_AVX
   30|       |template <>
   31|       |struct packet_traits<std::complex<float> > : default_packet_traits {
   32|       |  typedef Packet2cf type;
   33|       |  typedef Packet2cf half;
   34|       |  enum {
   35|       |    Vectorizable = 1,
   36|       |    AlignedOnScalar = 1,
   37|       |    size = 2,
   38|       |
   39|       |    HasAdd = 1,
   40|       |    HasSub = 1,
   41|       |    HasMul = 1,
   42|       |    HasDiv = 1,
   43|       |    HasNegate = 1,
   44|       |    HasSqrt = 1,
   45|       |    HasLog = 1,
   46|       |    HasExp = 1,
   47|       |    HasAbs = 0,
   48|       |    HasAbs2 = 0,
   49|       |    HasMin = 0,
   50|       |    HasMax = 0,
   51|       |    HasSetLinear = 0,
   52|       |    HasBlend = 1
   53|       |  };
   54|       |};
   55|       |#endif
   56|       |
   57|       |template <>
   58|       |struct unpacket_traits<Packet2cf> {
   59|       |  typedef std::complex<float> type;
   60|       |  typedef Packet2cf half;
   61|       |  typedef Packet4f as_real;
   62|       |  enum {
   63|       |    size = 2,
   64|       |    alignment = Aligned16,
   65|       |    vectorizable = true,
   66|       |    masked_load_available = false,
   67|       |    masked_store_available = false
   68|       |  };
   69|       |};
   70|       |
   71|       |template <>
   72|      0|EIGEN_STRONG_INLINE Packet2cf padd<Packet2cf>(const Packet2cf& a, const Packet2cf& b) {
   73|      0|  return Packet2cf(_mm_add_ps(a.v, b.v));
   74|      0|}
   75|       |template <>
   76|      0|EIGEN_STRONG_INLINE Packet2cf psub<Packet2cf>(const Packet2cf& a, const Packet2cf& b) {
   77|      0|  return Packet2cf(_mm_sub_ps(a.v, b.v));
   78|      0|}
   79|       |
   80|       |template <>
   81|      0|EIGEN_STRONG_INLINE Packet2cf pnegate(const Packet2cf& a) {
   82|      0|  const __m128 mask = _mm_castsi128_ps(_mm_setr_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
   83|      0|  return Packet2cf(_mm_xor_ps(a.v, mask));
   84|      0|}
   85|       |template <>
   86|      0|EIGEN_STRONG_INLINE Packet2cf pconj(const Packet2cf& a) {
   87|      0|  const __m128 mask = _mm_castsi128_ps(_mm_setr_epi32(0x00000000, 0x80000000, 0x00000000, 0x80000000));
   88|      0|  return Packet2cf(_mm_xor_ps(a.v, mask));
   89|      0|}
   90|       |
   91|       |template <>
   92|      0|EIGEN_STRONG_INLINE Packet2cf pmul(const Packet2cf& a, const Packet2cf& b) {
   93|      0|#ifdef EIGEN_VECTORIZE_SSE3
   94|      0|  __m128 tmp1 = _mm_mul_ps(_mm_movehdup_ps(a.v), vec4f_swizzle1(b.v, 1, 0, 3, 2));
   95|      0|  __m128 tmp2 = _mm_moveldup_ps(a.v);
   96|      0|#else
   97|      0|  __m128 tmp1 = _mm_mul_ps(vec4f_swizzle1(a.v, 1, 1, 3, 3), vec4f_swizzle1(b.v, 1, 0, 3, 2));
   98|      0|  __m128 tmp2 = vec4f_swizzle1(a.v, 0, 0, 2, 2);
   99|      0|#endif
  100|      0|#ifdef EIGEN_VECTORIZE_FMA
  101|      0|  __m128 result = _mm_fmaddsub_ps(tmp2, b.v, tmp1);
  102|      0|#else
  103|      0|#ifdef EIGEN_VECTORIZE_SSE3
  104|      0|  __m128 result = _mm_addsub_ps(_mm_mul_ps(tmp2, b.v), tmp1);
  105|      0|#else
  106|      0|  const __m128 mask = _mm_setr_ps(-0.0f, 0.0f, -0.0f, 0.0f);
  107|      0|  __m128 result = _mm_add_ps(_mm_mul_ps(tmp2, b.v), _mm_xor_ps(tmp1, mask));
  108|      0|#endif
  109|      0|#endif
  110|      0|  return Packet2cf(result);
  111|      0|}
  112|       |
  113|       |template <>
  114|      0|EIGEN_STRONG_INLINE Packet2cf ptrue<Packet2cf>(const Packet2cf& a) {
  115|      0|  return Packet2cf(ptrue(Packet4f(a.v)));
  116|      0|}
  117|       |template <>
  118|      0|EIGEN_STRONG_INLINE Packet2cf pand<Packet2cf>(const Packet2cf& a, const Packet2cf& b) {
  119|      0|  return Packet2cf(_mm_and_ps(a.v, b.v));
  120|      0|}
  121|       |template <>
  122|      0|EIGEN_STRONG_INLINE Packet2cf por<Packet2cf>(const Packet2cf& a, const Packet2cf& b) {
  123|      0|  return Packet2cf(_mm_or_ps(a.v, b.v));
  124|      0|}
  125|       |template <>
  126|      0|EIGEN_STRONG_INLINE Packet2cf pxor<Packet2cf>(const Packet2cf& a, const Packet2cf& b) {
  127|      0|  return Packet2cf(_mm_xor_ps(a.v, b.v));
  128|      0|}
  129|       |template <>
  130|      0|EIGEN_STRONG_INLINE Packet2cf pandnot<Packet2cf>(const Packet2cf& a, const Packet2cf& b) {
  131|      0|  return Packet2cf(_mm_andnot_ps(b.v, a.v));
  132|      0|}
  133|       |
  134|       |template <>
  135|      0|EIGEN_STRONG_INLINE Packet2cf pload<Packet2cf>(const std::complex<float>* from) {
  136|      0|  EIGEN_DEBUG_ALIGNED_LOAD return Packet2cf(_mm_load_ps(&numext::real_ref(*from)));
  137|      0|}
  138|       |template <>
  139|      0|EIGEN_STRONG_INLINE Packet2cf ploadu<Packet2cf>(const std::complex<float>* from) {
  140|      0|  EIGEN_DEBUG_UNALIGNED_LOAD return Packet2cf(_mm_loadu_ps(&numext::real_ref(*from)));
  141|      0|}
  142|       |
  143|       |template <>
  144|      0|EIGEN_STRONG_INLINE Packet2cf pset1<Packet2cf>(const std::complex<float>& from) {
  145|      0|  const float re = std::real(from);
  146|      0|  const float im = std::imag(from);
  147|      0|  return Packet2cf(_mm_set_ps(im, re, im, re));
  148|      0|}
  149|       |
  150|       |template <>
  151|      0|EIGEN_STRONG_INLINE Packet2cf ploaddup<Packet2cf>(const std::complex<float>* from) {
  152|      0|  return pset1<Packet2cf>(*from);
  153|      0|}
  154|       |
  155|       |template <>
  156|      0|EIGEN_STRONG_INLINE void pstore<std::complex<float> >(std::complex<float>* to, const Packet2cf& from) {
  157|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_ps(&numext::real_ref(*to), from.v);
  158|      0|}
  159|       |template <>
  160|      0|EIGEN_STRONG_INLINE void pstoreu<std::complex<float> >(std::complex<float>* to, const Packet2cf& from) {
  161|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_ps(&numext::real_ref(*to), from.v);
  162|      0|}
  163|       |
  164|       |template <>
  165|       |EIGEN_DEVICE_FUNC inline Packet2cf pgather<std::complex<float>, Packet2cf>(const std::complex<float>* from,
  166|      0|                                                                           Index stride) {
  167|      0|  return Packet2cf(_mm_set_ps(std::imag(from[1 * stride]), std::real(from[1 * stride]), std::imag(from[0 * stride]),
  168|      0|                              std::real(from[0 * stride])));
  169|      0|}
  170|       |
  171|       |template <>
  172|       |EIGEN_DEVICE_FUNC inline void pscatter<std::complex<float>, Packet2cf>(std::complex<float>* to, const Packet2cf& from,
  173|      0|                                                                       Index stride) {
  174|      0|  to[stride * 0] = std::complex<float>(_mm_cvtss_f32(_mm_shuffle_ps(from.v, from.v, 0)),
  175|      0|                                       _mm_cvtss_f32(_mm_shuffle_ps(from.v, from.v, 1)));
  176|      0|  to[stride * 1] = std::complex<float>(_mm_cvtss_f32(_mm_shuffle_ps(from.v, from.v, 2)),
  177|      0|                                       _mm_cvtss_f32(_mm_shuffle_ps(from.v, from.v, 3)));
  178|      0|}
  179|       |
  180|       |template <>
  181|      0|EIGEN_STRONG_INLINE void prefetch<std::complex<float> >(const std::complex<float>* addr) {
  182|      0|  _mm_prefetch((SsePrefetchPtrType)(addr), _MM_HINT_T0);
  183|      0|}
  184|       |
  185|       |template <>
  186|      0|EIGEN_STRONG_INLINE std::complex<float> pfirst<Packet2cf>(const Packet2cf& a) {
  187|      0|  alignas(alignof(__m64)) std::complex<float> res;
  188|      0|  _mm_storel_pi((__m64*)&res, a.v);
  189|      0|  return res;
  190|      0|}
  191|       |
  192|       |template <>
  193|      0|EIGEN_STRONG_INLINE Packet2cf preverse(const Packet2cf& a) {
  194|      0|  return Packet2cf(_mm_castpd_ps(preverse(Packet2d(_mm_castps_pd(a.v)))));
  195|      0|}
  196|       |
  197|       |template <>
  198|      0|EIGEN_STRONG_INLINE std::complex<float> predux<Packet2cf>(const Packet2cf& a) {
  199|      0|  return pfirst(Packet2cf(_mm_add_ps(a.v, _mm_movehl_ps(a.v, a.v))));
  200|      0|}
  201|       |
  202|       |template <>
  203|      0|EIGEN_STRONG_INLINE std::complex<float> predux_mul<Packet2cf>(const Packet2cf& a) {
  204|      0|  return pfirst(pmul(a, Packet2cf(_mm_movehl_ps(a.v, a.v))));
  205|      0|}
  206|       |
  207|      0|EIGEN_STRONG_INLINE Packet2cf pcplxflip /* <Packet2cf> */ (const Packet2cf& x) {
  208|      0|  return Packet2cf(vec4f_swizzle1(x.v, 1, 0, 3, 2));
  209|      0|}
  210|       |
  211|       |EIGEN_MAKE_CONJ_HELPER_CPLX_REAL(Packet2cf, Packet4f)
  212|       |
  213|       |template <>
  214|      0|EIGEN_STRONG_INLINE Packet2cf pdiv<Packet2cf>(const Packet2cf& a, const Packet2cf& b) {
  215|      0|  return pdiv_complex(a, b);
  216|      0|}
  217|       |
  218|       |//---------- double ----------
  219|       |struct Packet1cd {
  220|      0|  EIGEN_STRONG_INLINE Packet1cd() {}
  221|      0|  EIGEN_STRONG_INLINE explicit Packet1cd(const __m128d& a) : v(a) {}
  222|       |  Packet2d v;
  223|       |};
  224|       |
  225|       |// Use the packet_traits defined in AVX/PacketMath.h instead if we're going
  226|       |// to leverage AVX instructions.
  227|       |#ifndef EIGEN_VECTORIZE_AVX
  228|       |template <>
  229|       |struct packet_traits<std::complex<double> > : default_packet_traits {
  230|       |  typedef Packet1cd type;
  231|       |  typedef Packet1cd half;
  232|       |  enum {
  233|       |    Vectorizable = 1,
  234|       |    AlignedOnScalar = 0,
  235|       |    size = 1,
  236|       |
  237|       |    HasAdd = 1,
  238|       |    HasSub = 1,
  239|       |    HasMul = 1,
  240|       |    HasDiv = 1,
  241|       |    HasNegate = 1,
  242|       |    HasSqrt = 1,
  243|       |    HasLog = 1,
  244|       |    HasAbs = 0,
  245|       |    HasAbs2 = 0,
  246|       |    HasMin = 0,
  247|       |    HasMax = 0,
  248|       |    HasSetLinear = 0
  249|       |  };
  250|       |};
  251|       |#endif
  252|       |
  253|       |template <>
  254|       |struct unpacket_traits<Packet1cd> {
  255|       |  typedef std::complex<double> type;
  256|       |  typedef Packet1cd half;
  257|       |  typedef Packet2d as_real;
  258|       |  enum {
  259|       |    size = 1,
  260|       |    alignment = Aligned16,
  261|       |    vectorizable = true,
  262|       |    masked_load_available = false,
  263|       |    masked_store_available = false
  264|       |  };
  265|       |};
  266|       |
  267|       |template <>
  268|      0|EIGEN_STRONG_INLINE Packet1cd padd<Packet1cd>(const Packet1cd& a, const Packet1cd& b) {
  269|      0|  return Packet1cd(_mm_add_pd(a.v, b.v));
  270|      0|}
  271|       |template <>
  272|      0|EIGEN_STRONG_INLINE Packet1cd psub<Packet1cd>(const Packet1cd& a, const Packet1cd& b) {
  273|      0|  return Packet1cd(_mm_sub_pd(a.v, b.v));
  274|      0|}
  275|       |template <>
  276|      0|EIGEN_STRONG_INLINE Packet1cd pnegate(const Packet1cd& a) {
  277|      0|  return Packet1cd(pnegate(Packet2d(a.v)));
  278|      0|}
  279|       |template <>
  280|      0|EIGEN_STRONG_INLINE Packet1cd pconj(const Packet1cd& a) {
  281|      0|  const __m128d mask = _mm_castsi128_pd(_mm_set_epi32(0x80000000, 0x0, 0x0, 0x0));
  282|      0|  return Packet1cd(_mm_xor_pd(a.v, mask));
  283|      0|}
  284|       |
  285|       |template <>
  286|      0|EIGEN_STRONG_INLINE Packet1cd pmul(const Packet1cd& a, const Packet1cd& b) {
  287|      0|  __m128d tmp1 = _mm_mul_pd(_mm_unpackhi_pd(a.v, a.v), vec2d_swizzle1(b.v, 1, 0));
  288|      0|#ifdef EIGEN_VECTORIZE_SSE3
  289|      0|  __m128d tmp2 = _mm_movedup_pd(a.v);
  290|      0|#else
  291|      0|  __m128d tmp2 = _mm_unpacklo_pd(a.v, a.v);
  292|      0|#endif
  293|      0|#ifdef EIGEN_VECTORIZE_FMA
  294|      0|  __m128d result = _mm_fmaddsub_pd(tmp2, b.v, tmp1);
  295|      0|#else
  296|      0|#ifdef EIGEN_VECTORIZE_SSE3
  297|      0|  __m128d result = _mm_addsub_pd(_mm_mul_pd(tmp2, b.v), tmp1);
  298|      0|#else
  299|      0|  const __m128d mask = _mm_setr_pd(-0.0, 0.0);
  300|      0|  __m128d result = _mm_add_pd(_mm_mul_pd(tmp2, b.v), _mm_xor_pd(tmp1, mask));
  301|      0|#endif
  302|      0|#endif
  303|      0|  return Packet1cd(result);
  304|      0|}
  305|       |
  306|       |template <>
  307|      0|EIGEN_STRONG_INLINE Packet1cd ptrue<Packet1cd>(const Packet1cd& a) {
  308|      0|  return Packet1cd(ptrue(Packet2d(a.v)));
  309|      0|}
  310|       |template <>
  311|      0|EIGEN_STRONG_INLINE Packet1cd pand<Packet1cd>(const Packet1cd& a, const Packet1cd& b) {
  312|      0|  return Packet1cd(_mm_and_pd(a.v, b.v));
  313|      0|}
  314|       |template <>
  315|      0|EIGEN_STRONG_INLINE Packet1cd por<Packet1cd>(const Packet1cd& a, const Packet1cd& b) {
  316|      0|  return Packet1cd(_mm_or_pd(a.v, b.v));
  317|      0|}
  318|       |template <>
  319|      0|EIGEN_STRONG_INLINE Packet1cd pxor<Packet1cd>(const Packet1cd& a, const Packet1cd& b) {
  320|      0|  return Packet1cd(_mm_xor_pd(a.v, b.v));
  321|      0|}
  322|       |template <>
  323|      0|EIGEN_STRONG_INLINE Packet1cd pandnot<Packet1cd>(const Packet1cd& a, const Packet1cd& b) {
  324|      0|  return Packet1cd(_mm_andnot_pd(b.v, a.v));
  325|      0|}
  326|       |
  327|       |// FIXME force unaligned load, this is a temporary fix
  328|       |template <>
  329|      0|EIGEN_STRONG_INLINE Packet1cd pload<Packet1cd>(const std::complex<double>* from) {
  330|      0|  EIGEN_DEBUG_ALIGNED_LOAD return Packet1cd(_mm_load_pd((const double*)from));
  331|      0|}
  332|       |template <>
  333|      0|EIGEN_STRONG_INLINE Packet1cd ploadu<Packet1cd>(const std::complex<double>* from) {
  334|      0|  EIGEN_DEBUG_UNALIGNED_LOAD return Packet1cd(_mm_loadu_pd((const double*)from));
  335|      0|}
  336|       |template <>
  337|       |EIGEN_STRONG_INLINE Packet1cd
  338|      0|pset1<Packet1cd>(const std::complex<double>& from) { /* here we really have to use unaligned loads :( */
  339|      0|  return ploadu<Packet1cd>(&from);
  340|      0|}
  341|       |
  342|       |template <>
  343|      0|EIGEN_STRONG_INLINE Packet1cd ploaddup<Packet1cd>(const std::complex<double>* from) {
  344|      0|  return pset1<Packet1cd>(*from);
  345|      0|}
  346|       |
  347|       |// FIXME force unaligned store, this is a temporary fix
  348|       |template <>
  349|      0|EIGEN_STRONG_INLINE void pstore<std::complex<double> >(std::complex<double>* to, const Packet1cd& from) {
  350|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_pd((double*)to, from.v);
  351|      0|}
  352|       |template <>
  353|      0|EIGEN_STRONG_INLINE void pstoreu<std::complex<double> >(std::complex<double>* to, const Packet1cd& from) {
  354|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_pd((double*)to, from.v);
  355|      0|}
  356|       |
  357|       |template <>
  358|      0|EIGEN_STRONG_INLINE void prefetch<std::complex<double> >(const std::complex<double>* addr) {
  359|      0|  _mm_prefetch((SsePrefetchPtrType)(addr), _MM_HINT_T0);
  360|      0|}
  361|       |
  362|       |template <>
  363|      0|EIGEN_STRONG_INLINE std::complex<double> pfirst<Packet1cd>(const Packet1cd& a) {
  364|      0|  EIGEN_ALIGN16 double res[2];
  365|      0|  _mm_store_pd(res, a.v);
  366|      0|  return std::complex<double>(res[0], res[1]);
  367|      0|}
  368|       |
  369|       |template <>
  370|      0|EIGEN_STRONG_INLINE Packet1cd preverse(const Packet1cd& a) {
  371|      0|  return a;
  372|      0|}
  373|       |
  374|       |template <>
  375|      0|EIGEN_STRONG_INLINE std::complex<double> predux<Packet1cd>(const Packet1cd& a) {
  376|      0|  return pfirst(a);
  377|      0|}
  378|       |
  379|       |template <>
  380|      0|EIGEN_STRONG_INLINE std::complex<double> predux_mul<Packet1cd>(const Packet1cd& a) {
  381|      0|  return pfirst(a);
  382|      0|}
  383|       |
  384|       |EIGEN_MAKE_CONJ_HELPER_CPLX_REAL(Packet1cd, Packet2d)
  385|       |
  386|       |template <>
  387|      0|EIGEN_STRONG_INLINE Packet1cd pdiv<Packet1cd>(const Packet1cd& a, const Packet1cd& b) {
  388|      0|  return pdiv_complex(a, b);
  389|      0|}
  390|       |
  391|      0|EIGEN_STRONG_INLINE Packet1cd pcplxflip /* <Packet1cd> */ (const Packet1cd& x) {
  392|      0|  return Packet1cd(preverse(Packet2d(x.v)));
  393|      0|}
  394|       |
  395|      0|EIGEN_DEVICE_FUNC inline void ptranspose(PacketBlock<Packet2cf, 2>& kernel) {
  396|      0|  __m128d w1 = _mm_castps_pd(kernel.packet[0].v);
  397|      0|  __m128d w2 = _mm_castps_pd(kernel.packet[1].v);
  398|      0|
  399|      0|  __m128 tmp = _mm_castpd_ps(_mm_unpackhi_pd(w1, w2));
  400|      0|  kernel.packet[0].v = _mm_castpd_ps(_mm_unpacklo_pd(w1, w2));
  401|      0|  kernel.packet[1].v = tmp;
  402|      0|}
  403|       |
  404|       |template <>
  405|      0|EIGEN_STRONG_INLINE Packet2cf pcmp_eq(const Packet2cf& a, const Packet2cf& b) {
  406|      0|  __m128 eq = _mm_cmpeq_ps(a.v, b.v);
  407|      0|  return Packet2cf(pand<Packet4f>(eq, vec4f_swizzle1(eq, 1, 0, 3, 2)));
  408|      0|}
  409|       |
  410|       |template <>
  411|      0|EIGEN_STRONG_INLINE Packet1cd pcmp_eq(const Packet1cd& a, const Packet1cd& b) {
  412|      0|  __m128d eq = _mm_cmpeq_pd(a.v, b.v);
  413|      0|  return Packet1cd(pand<Packet2d>(eq, vec2d_swizzle1(eq, 1, 0)));
  414|      0|}
  415|       |
  416|       |template <>
  417|       |EIGEN_STRONG_INLINE Packet2cf pblend(const Selector<2>& ifPacket, const Packet2cf& thenPacket,
  418|      0|                                     const Packet2cf& elsePacket) {
  419|      0|  __m128d result = pblend<Packet2d>(ifPacket, _mm_castps_pd(thenPacket.v), _mm_castps_pd(elsePacket.v));
  420|      0|  return Packet2cf(_mm_castpd_ps(result));
  421|      0|}
  422|       |
  423|       |template <>
  424|      0|EIGEN_STRONG_INLINE Packet1cd psqrt<Packet1cd>(const Packet1cd& a) {
  425|      0|  return psqrt_complex<Packet1cd>(a);
  426|      0|}
  427|       |
  428|       |template <>
  429|      0|EIGEN_STRONG_INLINE Packet2cf psqrt<Packet2cf>(const Packet2cf& a) {
  430|      0|  return psqrt_complex<Packet2cf>(a);
  431|      0|}
  432|       |
  433|       |template <>
  434|      0|EIGEN_STRONG_INLINE Packet1cd plog<Packet1cd>(const Packet1cd& a) {
  435|      0|  return plog_complex<Packet1cd>(a);
  436|      0|}
  437|       |
  438|       |template <>
  439|      0|EIGEN_STRONG_INLINE Packet2cf plog<Packet2cf>(const Packet2cf& a) {
  440|      0|  return plog_complex<Packet2cf>(a);
  441|      0|}
  442|       |
  443|       |template <>
  444|      0|EIGEN_STRONG_INLINE Packet2cf pexp<Packet2cf>(const Packet2cf& a) {
  445|      0|  return pexp_complex<Packet2cf>(a);
  446|      0|}
  447|       |
  448|       |#ifdef EIGEN_VECTORIZE_FMA
  449|       |// std::complex<float>
  450|       |template <>
  451|       |EIGEN_STRONG_INLINE Packet2cf pmadd(const Packet2cf& a, const Packet2cf& b, const Packet2cf& c) {
  452|       |  __m128 a_odd = _mm_movehdup_ps(a.v);
  453|       |  __m128 a_even = _mm_moveldup_ps(a.v);
  454|       |  __m128 b_swap = _mm_permute_ps(b.v, _MM_SHUFFLE(2, 3, 0, 1));
  455|       |  __m128 result = _mm_fmaddsub_ps(a_even, b.v, _mm_fmaddsub_ps(a_odd, b_swap, c.v));
  456|       |  return Packet2cf(result);
  457|       |}
  458|       |template <>
  459|       |EIGEN_STRONG_INLINE Packet2cf pmsub(const Packet2cf& a, const Packet2cf& b, const Packet2cf& c) {
  460|       |  __m128 a_odd = _mm_movehdup_ps(a.v);
  461|       |  __m128 a_even = _mm_moveldup_ps(a.v);
  462|       |  __m128 b_swap = _mm_permute_ps(b.v, _MM_SHUFFLE(2, 3, 0, 1));
  463|       |  __m128 result = _mm_fmaddsub_ps(a_even, b.v, _mm_fmsubadd_ps(a_odd, b_swap, c.v));
  464|       |  return Packet2cf(result);
  465|       |}
  466|       |template <>
  467|       |EIGEN_STRONG_INLINE Packet2cf pnmadd(const Packet2cf& a, const Packet2cf& b, const Packet2cf& c) {
  468|       |  __m128 a_odd = _mm_movehdup_ps(a.v);
  469|       |  __m128 a_even = _mm_moveldup_ps(a.v);
  470|       |  __m128 b_swap = _mm_permute_ps(b.v, _MM_SHUFFLE(2, 3, 0, 1));
  471|       |  __m128 result = _mm_fmaddsub_ps(a_odd, b_swap, _mm_fmaddsub_ps(a_even, b.v, c.v));
  472|       |  return Packet2cf(result);
  473|       |}
  474|       |template <>
  475|       |EIGEN_STRONG_INLINE Packet2cf pnmsub(const Packet2cf& a, const Packet2cf& b, const Packet2cf& c) {
  476|       |  __m128 a_odd = _mm_movehdup_ps(a.v);
  477|       |  __m128 a_even = _mm_moveldup_ps(a.v);
  478|       |  __m128 b_swap = _mm_permute_ps(b.v, _MM_SHUFFLE(2, 3, 0, 1));
  479|       |  __m128 result = _mm_fmaddsub_ps(a_odd, b_swap, _mm_fmsubadd_ps(a_even, b.v, c.v));
  480|       |  return Packet2cf(result);
  481|       |}
  482|       |// std::complex<double>
  483|       |template <>
  484|       |EIGEN_STRONG_INLINE Packet1cd pmadd(const Packet1cd& a, const Packet1cd& b, const Packet1cd& c) {
  485|       |  __m128d a_odd = _mm_permute_pd(a.v, 0x3);
  486|       |  __m128d a_even = _mm_movedup_pd(a.v);
  487|       |  __m128d b_swap = _mm_permute_pd(b.v, 0x1);
  488|       |  __m128d result = _mm_fmaddsub_pd(a_even, b.v, _mm_fmaddsub_pd(a_odd, b_swap, c.v));
  489|       |  return Packet1cd(result);
  490|       |}
  491|       |template <>
  492|       |EIGEN_STRONG_INLINE Packet1cd pmsub(const Packet1cd& a, const Packet1cd& b, const Packet1cd& c) {
  493|       |  __m128d a_odd = _mm_permute_pd(a.v, 0x3);
  494|       |  __m128d a_even = _mm_movedup_pd(a.v);
  495|       |  __m128d b_swap = _mm_permute_pd(b.v, 0x1);
  496|       |  __m128d result = _mm_fmaddsub_pd(a_even, b.v, _mm_fmsubadd_pd(a_odd, b_swap, c.v));
  497|       |  return Packet1cd(result);
  498|       |}
  499|       |template <>
  500|       |EIGEN_STRONG_INLINE Packet1cd pnmadd(const Packet1cd& a, const Packet1cd& b, const Packet1cd& c) {
  501|       |  __m128d a_odd = _mm_permute_pd(a.v, 0x3);
  502|       |  __m128d a_even = _mm_movedup_pd(a.v);
  503|       |  __m128d b_swap = _mm_permute_pd(b.v, 0x1);
  504|       |  __m128d result = _mm_fmaddsub_pd(a_odd, b_swap, _mm_fmaddsub_pd(a_even, b.v, c.v));
  505|       |  return Packet1cd(result);
  506|       |}
  507|       |template <>
  508|       |EIGEN_STRONG_INLINE Packet1cd pnmsub(const Packet1cd& a, const Packet1cd& b, const Packet1cd& c) {
  509|       |  __m128d a_odd = _mm_permute_pd(a.v, 0x3);
  510|       |  __m128d a_even = _mm_movedup_pd(a.v);
  511|       |  __m128d b_swap = _mm_permute_pd(b.v, 0x1);
  512|       |  __m128d result = _mm_fmaddsub_pd(a_odd, b_swap, _mm_fmsubadd_pd(a_even, b.v, c.v));
  513|       |  return Packet1cd(result);
  514|       |}
  515|       |#endif
  516|       |}  // end namespace internal
  517|       |}  // end namespace Eigen
  518|       |
  519|       |#endif  // EIGEN_COMPLEX_SSE_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/SSE/MathFunctions.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2007 Julien Pommier
    5|       |// Copyright (C) 2009 Gael Guennebaud <gael.guennebaud@inria.fr>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |/* The sin and cos and functions of this file come from
   12|       | * Julien Pommier's sse math library: http://gruntthepeon.free.fr/ssemath/
   13|       | */
   14|       |
   15|       |#ifndef EIGEN_MATH_FUNCTIONS_SSE_H
   16|       |#define EIGEN_MATH_FUNCTIONS_SSE_H
   17|       |
   18|       |// IWYU pragma: private
   19|       |#include "../../InternalHeaderCheck.h"
   20|       |
   21|       |namespace Eigen {
   22|       |
   23|       |namespace internal {
   24|       |
   25|       |EIGEN_INSTANTIATE_GENERIC_MATH_FUNCS_FLOAT(Packet4f)
   26|       |EIGEN_INSTANTIATE_GENERIC_MATH_FUNCS_DOUBLE(Packet2d)
   27|       |
   28|       |// Notice that for newer processors, it is counterproductive to use Newton
   29|       |// iteration for square root. In particular, Skylake and Zen2 processors
   30|       |// have approximately doubled throughput of the _mm_sqrt_ps instruction
   31|       |// compared to their predecessors.
   32|       |template <>
   33|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet4f psqrt<Packet4f>(const Packet4f& x) {
   34|      0|  return _mm_sqrt_ps(x);
   35|      0|}
   36|       |template <>
   37|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet2d psqrt<Packet2d>(const Packet2d& x) {
   38|      0|  return _mm_sqrt_pd(x);
   39|      0|}
   40|       |template <>
   41|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS Packet16b psqrt<Packet16b>(const Packet16b& x) {
   42|      0|  return x;
   43|      0|}
   44|       |
   45|       |#if EIGEN_FAST_MATH
   46|       |// Even on Skylake, using Newton iteration is a win for reciprocal square root.
   47|       |template <>
   48|      0|EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS EIGEN_UNUSED Packet4f prsqrt<Packet4f>(const Packet4f& x) {
   49|      0|  return generic_rsqrt_newton_step<Packet4f, /*Steps=*/1>::run(x, _mm_rsqrt_ps(x));
   50|      0|}
   51|       |
   52|       |#ifdef EIGEN_VECTORIZE_FMA
   53|       |// Trying to speed up reciprocal using Newton-Raphson is counterproductive
   54|       |// unless FMA is available. Without FMA pdiv(pset1<Packet>(Scalar(1),a)) is
   55|       |// 30% faster.
   56|       |template <>
   57|       |EIGEN_STRONG_INLINE Packet4f preciprocal<Packet4f>(const Packet4f& x) {
   58|       |  return generic_reciprocal_newton_step<Packet4f, /*Steps=*/1>::run(x, _mm_rcp_ps(x));
   59|       |}
   60|       |#endif
   61|       |
   62|       |#endif
   63|       |
   64|       |}  // end namespace internal
   65|       |
   66|       |namespace numext {
   67|       |
   68|       |template <>
   69|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE float sqrt(const float& x) {
   70|      0|  return internal::pfirst(internal::Packet4f(_mm_sqrt_ss(_mm_set_ss(x))));
   71|      0|}
   72|       |
   73|       |template <>
   74|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE double sqrt(const double& x) {
   75|      0|#if EIGEN_COMP_GNUC_STRICT
   76|      0|  // This works around a GCC bug generating poor code for _mm_sqrt_pd
   77|      0|  // See https://gitlab.com/libeigen/eigen/commit/8dca9f97e38970
   78|      0|  return internal::pfirst(internal::Packet2d(__builtin_ia32_sqrtsd(_mm_set_sd(x))));
   79|      0|#else
   80|      0|  return internal::pfirst(internal::Packet2d(_mm_sqrt_pd(_mm_set_sd(x))));
   81|      0|#endif
   82|      0|}
   83|       |
   84|       |}  // namespace numext
   85|       |
   86|       |}  // end namespace Eigen
   87|       |
   88|       |#endif  // EIGEN_MATH_FUNCTIONS_SSE_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/SSE/PacketMath.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2009 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_PACKET_MATH_SSE_H
   11|       |#define EIGEN_PACKET_MATH_SSE_H
   12|       |
   13|       |#include <cstdint>
   14|       |// IWYU pragma: private
   15|       |#include "../../InternalHeaderCheck.h"
   16|       |
   17|       |namespace Eigen {
   18|       |
   19|       |namespace internal {
   20|       |
   21|       |#ifndef EIGEN_CACHEFRIENDLY_PRODUCT_THRESHOLD
   22|       |#define EIGEN_CACHEFRIENDLY_PRODUCT_THRESHOLD 8
   23|       |#endif
   24|       |
   25|       |#if !defined(EIGEN_VECTORIZE_AVX) && !defined(EIGEN_ARCH_DEFAULT_NUMBER_OF_REGISTERS)
   26|       |// 32 bits =>  8 registers
   27|       |// 64 bits => 16 registers
   28|       |#define EIGEN_ARCH_DEFAULT_NUMBER_OF_REGISTERS (2 * sizeof(void*))
   29|       |#endif
   30|       |
   31|       |#ifdef EIGEN_VECTORIZE_FMA
   32|       |#ifndef EIGEN_HAS_SINGLE_INSTRUCTION_MADD
   33|       |#define EIGEN_HAS_SINGLE_INSTRUCTION_MADD
   34|       |#endif
   35|       |#endif
   36|       |
   37|       |#if ((defined EIGEN_VECTORIZE_AVX) && (EIGEN_COMP_GNUC_STRICT || EIGEN_COMP_MINGW || EIGEN_COMP_LCC) && \
   38|       |     (__GXX_ABI_VERSION < 1004)) ||                                                                     \
   39|       |    EIGEN_OS_QNX
   40|       |// With GCC's default ABI version, a __m128 or __m256 are the same types and therefore we cannot
   41|       |// have overloads for both types without linking error.
   42|       |// One solution is to increase ABI version using -fabi-version=4 (or greater).
   43|       |// Otherwise, we workaround this inconvenience by wrapping 128bit types into the following helper
   44|       |// structure:
   45|       |typedef eigen_packet_wrapper<__m128> Packet4f;
   46|       |typedef eigen_packet_wrapper<__m128d> Packet2d;
   47|       |#else
   48|       |typedef __m128 Packet4f;
   49|       |typedef __m128d Packet2d;
   50|       |#endif
   51|       |
   52|       |typedef eigen_packet_wrapper<__m128i, 0> Packet4i;
   53|       |typedef eigen_packet_wrapper<__m128i, 1> Packet16b;
   54|       |typedef eigen_packet_wrapper<__m128i, 4> Packet4ui;
   55|       |typedef eigen_packet_wrapper<__m128i, 5> Packet2l;
   56|       |
   57|       |template <>
   58|       |struct is_arithmetic<__m128> {
   59|       |  enum { value = true };
   60|       |};
   61|       |template <>
   62|       |struct is_arithmetic<__m128i> {
   63|       |  enum { value = true };
   64|       |};
   65|       |template <>
   66|       |struct is_arithmetic<__m128d> {
   67|       |  enum { value = true };
   68|       |};
   69|       |template <>
   70|       |struct is_arithmetic<Packet4i> {
   71|       |  enum { value = true };
   72|       |};
   73|       |template <>
   74|       |struct is_arithmetic<Packet2l> {
   75|       |  enum { value = true };
   76|       |};
   77|       |// Note that `Packet4ui` uses the underlying type `__m128i`, which is
   78|       |// interpreted as a vector of _signed_ `int32`s, which breaks some arithmetic
   79|       |// operations used in `GenericPacketMath.h`.
   80|       |template <>
   81|       |struct is_arithmetic<Packet4ui> {
   82|       |  enum { value = false };
   83|       |};
   84|       |template <>
   85|       |struct is_arithmetic<Packet16b> {
   86|       |  enum { value = true };
   87|       |};
   88|       |
   89|       |template <int p, int q, int r, int s>
   90|       |struct shuffle_mask {
   91|       |  enum { mask = (s) << 6 | (r) << 4 | (q) << 2 | (p) };
   92|       |};
   93|       |
   94|       |// TODO: change the implementation of all swizzle* ops from macro to template,
   95|       |#define vec4f_swizzle1(v, p, q, r, s) \
   96|       |  Packet4f(_mm_castsi128_ps(_mm_shuffle_epi32(_mm_castps_si128(v), (shuffle_mask<p, q, r, s>::mask))))
   97|       |
   98|       |#define vec4i_swizzle1(v, p, q, r, s) Packet4i(_mm_shuffle_epi32(v, (shuffle_mask<p, q, r, s>::mask)))
   99|       |
  100|       |#define vec4ui_swizzle1(v, p, q, r, s) Packet4ui(vec4i_swizzle1(v, p, q, r, s))
  101|       |
  102|       |#define vec2d_swizzle1(v, p, q) \
  103|       |  Packet2d(_mm_castsi128_pd(    \
  104|       |      _mm_shuffle_epi32(_mm_castpd_si128(v), (shuffle_mask<2 * p, 2 * p + 1, 2 * q, 2 * q + 1>::mask))))
  105|       |
  106|       |#define vec4f_swizzle2(a, b, p, q, r, s) Packet4f(_mm_shuffle_ps((a), (b), (shuffle_mask<p, q, r, s>::mask)))
  107|       |
  108|       |#define vec4i_swizzle2(a, b, p, q, r, s) \
  109|       |  Packet4i(                              \
  110|       |      _mm_castps_si128((_mm_shuffle_ps(_mm_castsi128_ps(a), _mm_castsi128_ps(b), (shuffle_mask<p, q, r, s>::mask)))))
  111|       |
  112|       |#define vec4ui_swizzle2(a, b, p, q, r, s) Packet4i(vec4i_swizzle2(a, b, p, q, r, s))
  113|       |
  114|      0|EIGEN_STRONG_INLINE Packet4f vec4f_movelh(const Packet4f& a, const Packet4f& b) {
  115|      0|  return Packet4f(_mm_movelh_ps(a, b));
  116|      0|}
  117|      0|EIGEN_STRONG_INLINE Packet4f vec4f_movehl(const Packet4f& a, const Packet4f& b) {
  118|      0|  return Packet4f(_mm_movehl_ps(a, b));
  119|      0|}
  120|      0|EIGEN_STRONG_INLINE Packet4f vec4f_unpacklo(const Packet4f& a, const Packet4f& b) {
  121|      0|  return Packet4f(_mm_unpacklo_ps(a, b));
  122|      0|}
  123|      0|EIGEN_STRONG_INLINE Packet4f vec4f_unpackhi(const Packet4f& a, const Packet4f& b) {
  124|      0|  return Packet4f(_mm_unpackhi_ps(a, b));
  125|      0|}
  126|       |#define vec4f_duplane(a, p) vec4f_swizzle2(a, a, p, p, p, p)
  127|       |
  128|       |#define vec2d_swizzle2(a, b, mask) Packet2d(_mm_shuffle_pd(a, b, mask))
  129|       |
  130|      0|EIGEN_STRONG_INLINE Packet2d vec2d_unpacklo(const Packet2d& a, const Packet2d& b) {
  131|      0|  return Packet2d(_mm_unpacklo_pd(a, b));
  132|      0|}
  133|      0|EIGEN_STRONG_INLINE Packet2d vec2d_unpackhi(const Packet2d& a, const Packet2d& b) {
  134|      0|  return Packet2d(_mm_unpackhi_pd(a, b));
  135|      0|}
  136|       |#define vec2d_duplane(a, p) vec2d_swizzle2(a, a, (p << 1) | p)
  137|       |
  138|       |#define EIGEN_DECLARE_CONST_Packet4f(NAME, X) const Packet4f p4f_##NAME = pset1<Packet4f>(X)
  139|       |
  140|       |#define EIGEN_DECLARE_CONST_Packet2d(NAME, X) const Packet2d p2d_##NAME = pset1<Packet2d>(X)
  141|       |
  142|       |#define EIGEN_DECLARE_CONST_Packet4f_FROM_INT(NAME, X) const Packet4f p4f_##NAME = pset1frombits<Packet4f>(X)
  143|       |
  144|       |#define EIGEN_DECLARE_CONST_Packet4i(NAME, X) const Packet4i p4i_##NAME = pset1<Packet4i>(X)
  145|       |
  146|       |#define EIGEN_DECLARE_CONST_Packet4ui(NAME, X) const Packet4ui p4ui_##NAME = pset1<Packet4ui>(X)
  147|       |
  148|       |// Work around lack of extract/cvt for epi64 when compiling for 32-bit.
  149|       |#if EIGEN_ARCH_x86_64
  150|      0|EIGEN_ALWAYS_INLINE int64_t _mm_extract_epi64_0(const __m128i& a) { return _mm_cvtsi128_si64(a); }
  151|       |#ifdef EIGEN_VECTORIZE_SSE4_1
  152|       |EIGEN_ALWAYS_INLINE int64_t _mm_extract_epi64_1(const __m128i& a) { return _mm_extract_epi64(a, 1); }
  153|       |#else
  154|      0|EIGEN_ALWAYS_INLINE int64_t _mm_extract_epi64_1(const __m128i& a) {
  155|      0|  return _mm_cvtsi128_si64(_mm_castpd_si128(_mm_shuffle_pd(_mm_castsi128_pd(a), _mm_castsi128_pd(a), 0x1)));
  156|      0|}
  157|       |#endif
  158|       |#else
  159|       |// epi64 instructions are not available.  The following seems to generate the same instructions
  160|       |// with -O2 in GCC/Clang.
  161|       |EIGEN_ALWAYS_INLINE int64_t _mm_extract_epi64_0(const __m128i& a) {
  162|       |  return numext::bit_cast<int64_t>(_mm_cvtsd_f64(_mm_castsi128_pd(a)));
  163|       |}
  164|       |EIGEN_ALWAYS_INLINE int64_t _mm_extract_epi64_1(const __m128i& a) {
  165|       |  return numext::bit_cast<int64_t>(_mm_cvtsd_f64(_mm_shuffle_pd(_mm_castsi128_pd(a), _mm_castsi128_pd(a), 0x1)));
  166|       |}
  167|       |#endif
  168|       |
  169|       |// Use the packet_traits defined in AVX/PacketMath.h instead if we're going
  170|       |// to leverage AVX instructions.
  171|       |#ifndef EIGEN_VECTORIZE_AVX
  172|       |template <>
  173|       |struct packet_traits<float> : default_packet_traits {
  174|       |  typedef Packet4f type;
  175|       |  typedef Packet4f half;
  176|       |  enum {
  177|       |    Vectorizable = 1,
  178|       |    AlignedOnScalar = 1,
  179|       |    size = 4,
  180|       |
  181|       |    HasCmp = 1,
  182|       |    HasDiv = 1,
  183|       |    HasReciprocal = EIGEN_FAST_MATH,
  184|       |    HasSin = EIGEN_FAST_MATH,
  185|       |    HasCos = EIGEN_FAST_MATH,
  186|       |    HasACos = 1,
  187|       |    HasASin = 1,
  188|       |    HasATan = 1,
  189|       |    HasATanh = 1,
  190|       |    HasLog = 1,
  191|       |    HasLog1p = 1,
  192|       |    HasExpm1 = 1,
  193|       |    HasNdtri = 1,
  194|       |    HasExp = 1,
  195|       |    HasBessel = 1,
  196|       |    HasSqrt = 1,
  197|       |    HasRsqrt = 1,
  198|       |    HasTanh = EIGEN_FAST_MATH,
  199|       |    HasErf = EIGEN_FAST_MATH,
  200|       |    HasErfc = EIGEN_FAST_MATH,
  201|       |    HasBlend = 1,
  202|       |    HasSign = 0  // The manually vectorized version is slightly slower for SSE.
  203|       |  };
  204|       |};
  205|       |template <>
  206|       |struct packet_traits<double> : default_packet_traits {
  207|       |  typedef Packet2d type;
  208|       |  typedef Packet2d half;
  209|       |  enum {
  210|       |    Vectorizable = 1,
  211|       |    AlignedOnScalar = 1,
  212|       |    size = 2,
  213|       |
  214|       |    HasCmp = 1,
  215|       |    HasDiv = 1,
  216|       |    HasSin = EIGEN_FAST_MATH,
  217|       |    HasCos = EIGEN_FAST_MATH,
  218|       |    HasTanh = EIGEN_FAST_MATH,
  219|       |    HasLog = 1,
  220|       |    HasErf = EIGEN_FAST_MATH,
  221|       |    HasErfc = EIGEN_FAST_MATH,
  222|       |    HasExp = 1,
  223|       |    HasSqrt = 1,
  224|       |    HasRsqrt = 1,
  225|       |    HasATan = 1,
  226|       |    HasATanh = 1,
  227|       |    HasBlend = 1
  228|       |  };
  229|       |};
  230|       |template <>
  231|       |struct packet_traits<int> : default_packet_traits {
  232|       |  typedef Packet4i type;
  233|       |  typedef Packet4i half;
  234|       |  enum {
  235|       |    Vectorizable = 1,
  236|       |    AlignedOnScalar = 1,
  237|       |    size = 4,
  238|       |
  239|       |    HasCmp = 1,
  240|       |    HasDiv = 1,
  241|       |    HasShift = 1,
  242|       |    HasBlend = 1
  243|       |  };
  244|       |};
  245|       |template <>
  246|       |struct packet_traits<uint32_t> : default_packet_traits {
  247|       |  typedef Packet4ui type;
  248|       |  typedef Packet4ui half;
  249|       |  enum {
  250|       |    Vectorizable = 1,
  251|       |    AlignedOnScalar = 1,
  252|       |    size = 4,
  253|       |
  254|       |    HasDiv = 0,
  255|       |    HasNegate = 0,
  256|       |    HasCmp = 1,
  257|       |    HasShift = 1,
  258|       |    HasBlend = 1
  259|       |  };
  260|       |};
  261|       |template <>
  262|       |struct packet_traits<int64_t> : default_packet_traits {
  263|       |  typedef Packet2l type;
  264|       |  typedef Packet2l half;
  265|       |  enum {
  266|       |    Vectorizable = 1,
  267|       |    AlignedOnScalar = 1,
  268|       |    size = 2,
  269|       |
  270|       |    HasDiv = 0,
  271|       |    HasCmp = 1,
  272|       |    HasShift = 1,
  273|       |    HasBlend = 1
  274|       |  };
  275|       |};
  276|       |#endif
  277|       |template <>
  278|       |struct packet_traits<bool> : default_packet_traits {
  279|       |  typedef Packet16b type;
  280|       |  typedef Packet16b half;
  281|       |  enum {
  282|       |    Vectorizable = 1,
  283|       |    AlignedOnScalar = 1,
  284|       |    size = 16,
  285|       |
  286|       |    HasCmp = 1,  // note -- only pcmp_eq is defined
  287|       |    HasShift = 0,
  288|       |    HasAbs = 0,
  289|       |    HasAbs2 = 0,
  290|       |    HasMin = 0,
  291|       |    HasMax = 0,
  292|       |    HasConj = 0,
  293|       |    HasSqrt = 1,
  294|       |    HasNegate = 0,
  295|       |    HasSign = 0  // Don't try to vectorize psign<bool> = identity.
  296|       |  };
  297|       |};
  298|       |
  299|       |template <>
  300|       |struct unpacket_traits<Packet4f> {
  301|       |  typedef float type;
  302|       |  typedef Packet4f half;
  303|       |  typedef Packet4i integer_packet;
  304|       |  enum {
  305|       |    size = 4,
  306|       |    alignment = Aligned16,
  307|       |    vectorizable = true,
  308|       |    masked_load_available = false,
  309|       |    masked_store_available = false
  310|       |  };
  311|       |};
  312|       |template <>
  313|       |struct unpacket_traits<Packet2d> {
  314|       |  typedef double type;
  315|       |  typedef Packet2d half;
  316|       |  typedef Packet2l integer_packet;
  317|       |  enum {
  318|       |    size = 2,
  319|       |    alignment = Aligned16,
  320|       |    vectorizable = true,
  321|       |    masked_load_available = false,
  322|       |    masked_store_available = false
  323|       |  };
  324|       |};
  325|       |template <>
  326|       |struct unpacket_traits<Packet2l> {
  327|       |  typedef int64_t type;
  328|       |  typedef Packet2l half;
  329|       |  enum {
  330|       |    size = 2,
  331|       |    alignment = Aligned16,
  332|       |    vectorizable = true,
  333|       |    masked_load_available = false,
  334|       |    masked_store_available = false
  335|       |  };
  336|       |};
  337|       |template <>
  338|       |struct unpacket_traits<Packet4i> {
  339|       |  typedef int type;
  340|       |  typedef Packet4i half;
  341|       |  enum {
  342|       |    size = 4,
  343|       |    alignment = Aligned16,
  344|       |    vectorizable = true,
  345|       |    masked_load_available = false,
  346|       |    masked_store_available = false
  347|       |  };
  348|       |};
  349|       |template <>
  350|       |struct unpacket_traits<Packet4ui> {
  351|       |  typedef uint32_t type;
  352|       |  typedef Packet4ui half;
  353|       |  enum {
  354|       |    size = 4,
  355|       |    alignment = Aligned16,
  356|       |    vectorizable = true,
  357|       |    masked_load_available = false,
  358|       |    masked_store_available = false
  359|       |  };
  360|       |};
  361|       |template <>
  362|       |struct unpacket_traits<Packet16b> {
  363|       |  typedef bool type;
  364|       |  typedef Packet16b half;
  365|       |  enum {
  366|       |    size = 16,
  367|       |    alignment = Aligned16,
  368|       |    vectorizable = true,
  369|       |    masked_load_available = false,
  370|       |    masked_store_available = false
  371|       |  };
  372|       |};
  373|       |
  374|       |#ifndef EIGEN_VECTORIZE_AVX
  375|       |template <>
  376|       |struct scalar_div_cost<float, true> {
  377|       |  enum { value = 7 };
  378|       |};
  379|       |template <>
  380|       |struct scalar_div_cost<double, true> {
  381|       |  enum { value = 8 };
  382|       |};
  383|       |#endif
  384|       |
  385|       |template <>
  386|      0|EIGEN_STRONG_INLINE Packet4f pset1<Packet4f>(const float& from) {
  387|      0|  return _mm_set_ps1(from);
  388|      0|}
  389|       |template <>
  390|      0|EIGEN_STRONG_INLINE Packet2d pset1<Packet2d>(const double& from) {
  391|      0|  return _mm_set1_pd(from);
  392|      0|}
  393|       |template <>
  394|      0|EIGEN_STRONG_INLINE Packet2l pset1<Packet2l>(const int64_t& from) {
  395|      0|  return _mm_set1_epi64x(from);
  396|      0|}
  397|       |template <>
  398|      0|EIGEN_STRONG_INLINE Packet4i pset1<Packet4i>(const int& from) {
  399|      0|  return _mm_set1_epi32(from);
  400|      0|}
  401|       |template <>
  402|      0|EIGEN_STRONG_INLINE Packet4ui pset1<Packet4ui>(const uint32_t& from) {
  403|      0|  return _mm_set1_epi32(numext::bit_cast<int32_t>(from));
  404|      0|}
  405|       |template <>
  406|      0|EIGEN_STRONG_INLINE Packet16b pset1<Packet16b>(const bool& from) {
  407|      0|  return _mm_set1_epi8(static_cast<char>(from));
  408|      0|}
  409|       |
  410|       |template <>
  411|      0|EIGEN_STRONG_INLINE Packet4f pset1frombits<Packet4f>(unsigned int from) {
  412|      0|  return _mm_castsi128_ps(pset1<Packet4i>(from));
  413|      0|}
  414|       |template <>
  415|      0|EIGEN_STRONG_INLINE Packet2d pset1frombits<Packet2d>(uint64_t from) {
  416|      0|  return _mm_castsi128_pd(_mm_set1_epi64x(from));
  417|      0|}
  418|       |
  419|       |template <>
  420|      0|EIGEN_STRONG_INLINE Packet4f peven_mask(const Packet4f& /*a*/) {
  421|      0|  return _mm_castsi128_ps(_mm_set_epi32(0, -1, 0, -1));
  422|      0|}
  423|       |template <>
  424|      0|EIGEN_STRONG_INLINE Packet2l peven_mask(const Packet2l& /*a*/) {
  425|      0|  return _mm_set_epi32(0, 0, -1, -1);
  426|      0|}
  427|       |template <>
  428|      0|EIGEN_STRONG_INLINE Packet4i peven_mask(const Packet4i& /*a*/) {
  429|      0|  return _mm_set_epi32(0, -1, 0, -1);
  430|      0|}
  431|       |template <>
  432|      0|EIGEN_STRONG_INLINE Packet4ui peven_mask(const Packet4ui& /*a*/) {
  433|      0|  return _mm_set_epi32(0, -1, 0, -1);
  434|      0|}
  435|       |template <>
  436|      0|EIGEN_STRONG_INLINE Packet2d peven_mask(const Packet2d& /*a*/) {
  437|      0|  return _mm_castsi128_pd(_mm_set_epi32(0, 0, -1, -1));
  438|      0|}
  439|       |
  440|       |template <>
  441|      0|EIGEN_STRONG_INLINE Packet4f pzero(const Packet4f& /*a*/) {
  442|      0|  return _mm_setzero_ps();
  443|      0|}
  444|       |template <>
  445|      0|EIGEN_STRONG_INLINE Packet2d pzero(const Packet2d& /*a*/) {
  446|      0|  return _mm_setzero_pd();
  447|      0|}
  448|       |template <>
  449|      0|EIGEN_STRONG_INLINE Packet2l pzero(const Packet2l& /*a*/) {
  450|      0|  return _mm_setzero_si128();
  451|      0|}
  452|       |template <>
  453|      0|EIGEN_STRONG_INLINE Packet4i pzero(const Packet4i& /*a*/) {
  454|      0|  return _mm_setzero_si128();
  455|      0|}
  456|       |template <>
  457|      0|EIGEN_STRONG_INLINE Packet4ui pzero(const Packet4ui& /*a*/) {
  458|      0|  return _mm_setzero_si128();
  459|      0|}
  460|       |
  461|       |// GCC generates a shufps instruction for _mm_set1_ps/_mm_load1_ps instead of the more efficient pshufd instruction.
  462|       |// However, using inrinsics for pset1 makes gcc to generate crappy code in some cases (see bug 203)
  463|       |// Using inline assembly is also not an option because then gcc fails to reorder properly the instructions.
  464|       |// Therefore, we introduced the pload1 functions to be used in product kernels for which bug 203 does not apply.
  465|       |// Also note that with AVX, we want it to generate a vbroadcastss.
  466|       |#if EIGEN_COMP_GNUC_STRICT && (!defined __AVX__)
  467|       |template <>
  468|       |EIGEN_STRONG_INLINE Packet4f pload1<Packet4f>(const float* from) {
  469|       |  return vec4f_swizzle1(_mm_load_ss(from), 0, 0, 0, 0);
  470|       |}
  471|       |#endif
  472|       |
  473|       |template <>
  474|      0|EIGEN_STRONG_INLINE Packet4f plset<Packet4f>(const float& a) {
  475|      0|  return _mm_add_ps(pset1<Packet4f>(a), _mm_set_ps(3, 2, 1, 0));
  476|      0|}
  477|       |template <>
  478|      0|EIGEN_STRONG_INLINE Packet2d plset<Packet2d>(const double& a) {
  479|      0|  return _mm_add_pd(pset1<Packet2d>(a), _mm_set_pd(1, 0));
  480|      0|}
  481|       |template <>
  482|      0|EIGEN_STRONG_INLINE Packet2l plset<Packet2l>(const int64_t& a) {
  483|      0|  return _mm_add_epi32(pset1<Packet2l>(a), _mm_set_epi64x(1, 0));
  484|      0|}
  485|       |template <>
  486|      0|EIGEN_STRONG_INLINE Packet4i plset<Packet4i>(const int& a) {
  487|      0|  return _mm_add_epi32(pset1<Packet4i>(a), _mm_set_epi32(3, 2, 1, 0));
  488|      0|}
  489|       |template <>
  490|      0|EIGEN_STRONG_INLINE Packet4ui plset<Packet4ui>(const uint32_t& a) {
  491|      0|  return _mm_add_epi32(pset1<Packet4ui>(a), _mm_set_epi32(3, 2, 1, 0));
  492|      0|}
  493|       |
  494|       |template <>
  495|      0|EIGEN_STRONG_INLINE Packet4f padd<Packet4f>(const Packet4f& a, const Packet4f& b) {
  496|      0|  return _mm_add_ps(a, b);
  497|      0|}
  498|       |template <>
  499|      0|EIGEN_STRONG_INLINE Packet2d padd<Packet2d>(const Packet2d& a, const Packet2d& b) {
  500|      0|  return _mm_add_pd(a, b);
  501|      0|}
  502|       |template <>
  503|      0|EIGEN_STRONG_INLINE Packet2l padd<Packet2l>(const Packet2l& a, const Packet2l& b) {
  504|      0|  return _mm_add_epi64(a, b);
  505|      0|}
  506|       |template <>
  507|      0|EIGEN_STRONG_INLINE Packet4i padd<Packet4i>(const Packet4i& a, const Packet4i& b) {
  508|      0|  return _mm_add_epi32(a, b);
  509|      0|}
  510|       |template <>
  511|      0|EIGEN_STRONG_INLINE Packet4ui padd<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
  512|      0|  return _mm_add_epi32(a, b);
  513|      0|}
  514|       |
  515|       |template <>
  516|      0|EIGEN_STRONG_INLINE Packet16b padd<Packet16b>(const Packet16b& a, const Packet16b& b) {
  517|      0|  return _mm_or_si128(a, b);
  518|      0|}
  519|       |
  520|       |template <typename Packet>
  521|       |EIGEN_STRONG_INLINE Packet padds(const Packet& a, const Packet& b);
  522|       |template <>
  523|      0|EIGEN_STRONG_INLINE Packet4f padds<Packet4f>(const Packet4f& a, const Packet4f& b) {
  524|      0|  return _mm_add_ss(a, b);
  525|      0|}
  526|       |template <>
  527|      0|EIGEN_STRONG_INLINE Packet2d padds<Packet2d>(const Packet2d& a, const Packet2d& b) {
  528|      0|  return _mm_add_sd(a, b);
  529|      0|}
  530|       |
  531|       |template <>
  532|      0|EIGEN_STRONG_INLINE Packet4f psub<Packet4f>(const Packet4f& a, const Packet4f& b) {
  533|      0|  return _mm_sub_ps(a, b);
  534|      0|}
  535|       |template <>
  536|      0|EIGEN_STRONG_INLINE Packet2d psub<Packet2d>(const Packet2d& a, const Packet2d& b) {
  537|      0|  return _mm_sub_pd(a, b);
  538|      0|}
  539|       |template <>
  540|      0|EIGEN_STRONG_INLINE Packet2l psub<Packet2l>(const Packet2l& a, const Packet2l& b) {
  541|      0|  return _mm_sub_epi64(a, b);
  542|      0|}
  543|       |template <>
  544|      0|EIGEN_STRONG_INLINE Packet4i psub<Packet4i>(const Packet4i& a, const Packet4i& b) {
  545|      0|  return _mm_sub_epi32(a, b);
  546|      0|}
  547|       |template <>
  548|      0|EIGEN_STRONG_INLINE Packet4ui psub<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
  549|      0|  return _mm_sub_epi32(a, b);
  550|      0|}
  551|       |template <>
  552|      0|EIGEN_STRONG_INLINE Packet16b psub<Packet16b>(const Packet16b& a, const Packet16b& b) {
  553|      0|  return _mm_xor_si128(a, b);
  554|      0|}
  555|       |
  556|       |template <>
  557|       |EIGEN_STRONG_INLINE Packet4f pxor<Packet4f>(const Packet4f& a, const Packet4f& b);
  558|       |template <>
  559|      0|EIGEN_STRONG_INLINE Packet4f paddsub<Packet4f>(const Packet4f& a, const Packet4f& b) {
  560|      0|#ifdef EIGEN_VECTORIZE_SSE3
  561|      0|  return _mm_addsub_ps(a, b);
  562|      0|#else
  563|      0|  const Packet4f mask = _mm_castsi128_ps(_mm_setr_epi32(0x80000000, 0x0, 0x80000000, 0x0));
  564|      0|  return padd(a, pxor(mask, b));
  565|      0|#endif
  566|      0|}
  567|       |
  568|       |template <>
  569|       |EIGEN_STRONG_INLINE Packet2d pxor<Packet2d>(const Packet2d&, const Packet2d&);
  570|       |template <>
  571|      0|EIGEN_STRONG_INLINE Packet2d paddsub<Packet2d>(const Packet2d& a, const Packet2d& b) {
  572|      0|#ifdef EIGEN_VECTORIZE_SSE3
  573|      0|  return _mm_addsub_pd(a, b);
  574|      0|#else
  575|      0|  const Packet2d mask = _mm_castsi128_pd(_mm_setr_epi32(0x0, 0x80000000, 0x0, 0x0));
  576|      0|  return padd(a, pxor(mask, b));
  577|      0|#endif
  578|      0|}
  579|       |
  580|       |template <>
  581|      0|EIGEN_STRONG_INLINE Packet4f pnegate(const Packet4f& a) {
  582|      0|  const Packet4f mask = _mm_castsi128_ps(_mm_setr_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000));
  583|      0|  return _mm_xor_ps(a, mask);
  584|      0|}
  585|       |template <>
  586|      0|EIGEN_STRONG_INLINE Packet2d pnegate(const Packet2d& a) {
  587|      0|  const Packet2d mask = _mm_castsi128_pd(_mm_setr_epi32(0x0, 0x80000000, 0x0, 0x80000000));
  588|      0|  return _mm_xor_pd(a, mask);
  589|      0|}
  590|       |template <>
  591|      0|EIGEN_STRONG_INLINE Packet2l pnegate(const Packet2l& a) {
  592|      0|  return psub(pzero(a), a);
  593|      0|}
  594|       |
  595|       |template <>
  596|      0|EIGEN_STRONG_INLINE Packet4i pnegate(const Packet4i& a) {
  597|      0|  return psub(pzero(a), a);
  598|      0|}
  599|       |
  600|       |template <>
  601|      0|EIGEN_STRONG_INLINE Packet4f pconj(const Packet4f& a) {
  602|      0|  return a;
  603|      0|}
  604|       |template <>
  605|      0|EIGEN_STRONG_INLINE Packet2d pconj(const Packet2d& a) {
  606|      0|  return a;
  607|      0|}
  608|       |template <>
  609|      0|EIGEN_STRONG_INLINE Packet2l pconj(const Packet2l& a) {
  610|      0|  return a;
  611|      0|}
  612|       |template <>
  613|      0|EIGEN_STRONG_INLINE Packet4i pconj(const Packet4i& a) {
  614|      0|  return a;
  615|      0|}
  616|       |
  617|       |template <>
  618|      0|EIGEN_STRONG_INLINE Packet4f pmul<Packet4f>(const Packet4f& a, const Packet4f& b) {
  619|      0|  return _mm_mul_ps(a, b);
  620|      0|}
  621|       |template <>
  622|      0|EIGEN_STRONG_INLINE Packet2d pmul<Packet2d>(const Packet2d& a, const Packet2d& b) {
  623|      0|  return _mm_mul_pd(a, b);
  624|      0|}
  625|       |template <>
  626|      0|EIGEN_STRONG_INLINE Packet2l pmul<Packet2l>(const Packet2l& a, const Packet2l& b) {
  627|      0|  // 64-bit mul requires avx512, so do this with 32-bit multiplication
  628|      0|  __m128i upper32_a = _mm_srli_epi64(a, 32);
  629|      0|  __m128i upper32_b = _mm_srli_epi64(b, 32);
  630|      0|
  631|      0|  // upper * lower
  632|      0|  __m128i mul1 = _mm_mul_epu32(upper32_a, b);
  633|      0|  __m128i mul2 = _mm_mul_epu32(upper32_b, a);
  634|      0|  // Gives us both upper*upper and lower*lower
  635|      0|  __m128i mul3 = _mm_mul_epu32(a, b);
  636|      0|
  637|      0|  __m128i high = _mm_slli_epi64(_mm_add_epi64(mul1, mul2), 32);
  638|      0|  return _mm_add_epi64(high, mul3);
  639|      0|}
  640|       |template <>
  641|      0|EIGEN_STRONG_INLINE Packet4i pmul<Packet4i>(const Packet4i& a, const Packet4i& b) {
  642|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
  643|      0|  return _mm_mullo_epi32(a, b);
  644|      0|#else
  645|      0|  // this version is slightly faster than 4 scalar products
  646|      0|  return vec4i_swizzle1(
  647|      0|      vec4i_swizzle2(_mm_mul_epu32(a, b), _mm_mul_epu32(vec4i_swizzle1(a, 1, 0, 3, 2), vec4i_swizzle1(b, 1, 0, 3, 2)),
  648|      0|                     0, 2, 0, 2),
  649|      0|      0, 2, 1, 3);
  650|      0|#endif
  651|      0|}
  652|       |template <>
  653|      0|EIGEN_STRONG_INLINE Packet4ui pmul<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
  654|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
  655|      0|  return _mm_mullo_epi32(a, b);
  656|      0|#else
  657|      0|  // this version is slightly faster than 4 scalar products
  658|      0|  return vec4ui_swizzle1(
  659|      0|      vec4ui_swizzle2(_mm_mul_epu32(a, b),
  660|      0|                      _mm_mul_epu32(vec4ui_swizzle1(a, 1, 0, 3, 2), vec4ui_swizzle1(b, 1, 0, 3, 2)), 0, 2, 0, 2),
  661|      0|      0, 2, 1, 3);
  662|      0|#endif
  663|      0|}
  664|       |
  665|       |template <>
  666|      0|EIGEN_STRONG_INLINE Packet16b pmul<Packet16b>(const Packet16b& a, const Packet16b& b) {
  667|      0|  return _mm_and_si128(a, b);
  668|      0|}
  669|       |
  670|       |template <>
  671|      0|EIGEN_STRONG_INLINE Packet4f pdiv<Packet4f>(const Packet4f& a, const Packet4f& b) {
  672|      0|  return _mm_div_ps(a, b);
  673|      0|}
  674|       |template <>
  675|      0|EIGEN_STRONG_INLINE Packet2d pdiv<Packet2d>(const Packet2d& a, const Packet2d& b) {
  676|      0|  return _mm_div_pd(a, b);
  677|      0|}
  678|       |
  679|       |template <>
  680|      0|EIGEN_STRONG_INLINE Packet4i pdiv<Packet4i>(const Packet4i& a, const Packet4i& b) {
  681|      0|#ifdef EIGEN_VECTORIZE_AVX
  682|      0|  return _mm256_cvttpd_epi32(_mm256_div_pd(_mm256_cvtepi32_pd(a), _mm256_cvtepi32_pd(b)));
  683|      0|#else
  684|      0|  __m128i q_lo = _mm_cvttpd_epi32(_mm_div_pd(_mm_cvtepi32_pd(a), _mm_cvtepi32_pd(b)));
  685|      0|  __m128i q_hi = _mm_cvttpd_epi32(
  686|      0|      _mm_div_pd(_mm_cvtepi32_pd(vec4i_swizzle1(a, 2, 3, 0, 1)), _mm_cvtepi32_pd(vec4i_swizzle1(b, 2, 3, 0, 1))));
  687|      0|  return vec4i_swizzle1(_mm_unpacklo_epi32(q_lo, q_hi), 0, 2, 1, 3);
  688|      0|#endif
  689|      0|}
  690|       |
  691|       |#ifdef EIGEN_VECTORIZE_FMA
  692|       |template <>
  693|       |EIGEN_STRONG_INLINE Packet4f pmadd(const Packet4f& a, const Packet4f& b, const Packet4f& c) {
  694|       |  return _mm_fmadd_ps(a, b, c);
  695|       |}
  696|       |template <>
  697|       |EIGEN_STRONG_INLINE Packet2d pmadd(const Packet2d& a, const Packet2d& b, const Packet2d& c) {
  698|       |  return _mm_fmadd_pd(a, b, c);
  699|       |}
  700|       |template <>
  701|       |EIGEN_STRONG_INLINE Packet4f pmsub(const Packet4f& a, const Packet4f& b, const Packet4f& c) {
  702|       |  return _mm_fmsub_ps(a, b, c);
  703|       |}
  704|       |template <>
  705|       |EIGEN_STRONG_INLINE Packet2d pmsub(const Packet2d& a, const Packet2d& b, const Packet2d& c) {
  706|       |  return _mm_fmsub_pd(a, b, c);
  707|       |}
  708|       |template <>
  709|       |EIGEN_STRONG_INLINE Packet4f pnmadd(const Packet4f& a, const Packet4f& b, const Packet4f& c) {
  710|       |  return _mm_fnmadd_ps(a, b, c);
  711|       |}
  712|       |template <>
  713|       |EIGEN_STRONG_INLINE Packet2d pnmadd(const Packet2d& a, const Packet2d& b, const Packet2d& c) {
  714|       |  return _mm_fnmadd_pd(a, b, c);
  715|       |}
  716|       |template <>
  717|       |EIGEN_STRONG_INLINE Packet4f pnmsub(const Packet4f& a, const Packet4f& b, const Packet4f& c) {
  718|       |  return _mm_fnmsub_ps(a, b, c);
  719|       |}
  720|       |template <>
  721|       |EIGEN_STRONG_INLINE Packet2d pnmsub(const Packet2d& a, const Packet2d& b, const Packet2d& c) {
  722|       |  return _mm_fnmsub_pd(a, b, c);
  723|       |}
  724|       |
  725|       |template <typename Packet>
  726|       |EIGEN_STRONG_INLINE Packet pmadds(const Packet& a, const Packet& b, const Packet& c);
  727|       |template <>
  728|       |EIGEN_STRONG_INLINE Packet4f pmadds<Packet4f>(const Packet4f& a, const Packet4f& b, const Packet4f& c) {
  729|       |  return _mm_fmadd_ss(a, b, c);
  730|       |}
  731|       |template <>
  732|       |EIGEN_STRONG_INLINE Packet2d pmadds<Packet2d>(const Packet2d& a, const Packet2d& b, const Packet2d& c) {
  733|       |  return _mm_fmadd_sd(a, b, c);
  734|       |}
  735|       |#endif
  736|       |
  737|       |#ifdef EIGEN_VECTORIZE_SSE4_1
  738|       |template <>
  739|       |EIGEN_STRONG_INLINE Packet4f pselect(const Packet4f& mask, const Packet4f& a, const Packet4f& b) {
  740|       |  return _mm_blendv_ps(b, a, mask);
  741|       |}
  742|       |
  743|       |template <>
  744|       |EIGEN_STRONG_INLINE Packet2l pselect(const Packet2l& mask, const Packet2l& a, const Packet2l& b) {
  745|       |  return _mm_castpd_si128(_mm_blendv_pd(_mm_castsi128_pd(b), _mm_castsi128_pd(a), _mm_castsi128_pd(mask)));
  746|       |}
  747|       |
  748|       |template <>
  749|       |EIGEN_STRONG_INLINE Packet4i pselect(const Packet4i& mask, const Packet4i& a, const Packet4i& b) {
  750|       |  return _mm_castps_si128(_mm_blendv_ps(_mm_castsi128_ps(b), _mm_castsi128_ps(a), _mm_castsi128_ps(mask)));
  751|       |}
  752|       |
  753|       |template <>
  754|       |EIGEN_STRONG_INLINE Packet4ui pselect(const Packet4ui& mask, const Packet4ui& a, const Packet4ui& b) {
  755|       |  return _mm_castps_si128(_mm_blendv_ps(_mm_castsi128_ps(b), _mm_castsi128_ps(a), _mm_castsi128_ps(mask)));
  756|       |}
  757|       |
  758|       |template <>
  759|       |EIGEN_STRONG_INLINE Packet2d pselect(const Packet2d& mask, const Packet2d& a, const Packet2d& b) {
  760|       |  return _mm_blendv_pd(b, a, mask);
  761|       |}
  762|       |#endif
  763|       |
  764|       |template <>
  765|      0|EIGEN_STRONG_INLINE Packet2l ptrue<Packet2l>(const Packet2l& a) {
  766|      0|  return _mm_cmpeq_epi32(a, a);
  767|      0|}
  768|       |template <>
  769|      0|EIGEN_STRONG_INLINE Packet4i ptrue<Packet4i>(const Packet4i& a) {
  770|      0|  return _mm_cmpeq_epi32(a, a);
  771|      0|}
  772|       |template <>
  773|      0|EIGEN_STRONG_INLINE Packet16b ptrue<Packet16b>(const Packet16b& /*a*/) {
  774|      0|  return pset1<Packet16b>(true);
  775|      0|}
  776|       |template <>
  777|      0|EIGEN_STRONG_INLINE Packet4f ptrue<Packet4f>(const Packet4f& a) {
  778|      0|  Packet4i b = _mm_castps_si128(a);
  779|      0|  return _mm_castsi128_ps(_mm_cmpeq_epi32(b, b));
  780|      0|}
  781|       |template <>
  782|      0|EIGEN_STRONG_INLINE Packet2d ptrue<Packet2d>(const Packet2d& a) {
  783|      0|  Packet4i b = _mm_castpd_si128(a);
  784|      0|  return _mm_castsi128_pd(_mm_cmpeq_epi32(b, b));
  785|      0|}
  786|       |
  787|       |template <>
  788|      0|EIGEN_STRONG_INLINE Packet4f pand<Packet4f>(const Packet4f& a, const Packet4f& b) {
  789|      0|  return _mm_and_ps(a, b);
  790|      0|}
  791|       |template <>
  792|      0|EIGEN_STRONG_INLINE Packet2d pand<Packet2d>(const Packet2d& a, const Packet2d& b) {
  793|      0|  return _mm_and_pd(a, b);
  794|      0|}
  795|       |template <>
  796|      0|EIGEN_STRONG_INLINE Packet2l pand<Packet2l>(const Packet2l& a, const Packet2l& b) {
  797|      0|  return _mm_and_si128(a, b);
  798|      0|}
  799|       |template <>
  800|      0|EIGEN_STRONG_INLINE Packet4i pand<Packet4i>(const Packet4i& a, const Packet4i& b) {
  801|      0|  return _mm_and_si128(a, b);
  802|      0|}
  803|       |template <>
  804|      0|EIGEN_STRONG_INLINE Packet4ui pand<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
  805|      0|  return _mm_and_si128(a, b);
  806|      0|}
  807|       |template <>
  808|      0|EIGEN_STRONG_INLINE Packet16b pand<Packet16b>(const Packet16b& a, const Packet16b& b) {
  809|      0|  return _mm_and_si128(a, b);
  810|      0|}
  811|       |
  812|       |template <>
  813|      0|EIGEN_STRONG_INLINE Packet4f por<Packet4f>(const Packet4f& a, const Packet4f& b) {
  814|      0|  return _mm_or_ps(a, b);
  815|      0|}
  816|       |template <>
  817|      0|EIGEN_STRONG_INLINE Packet2d por<Packet2d>(const Packet2d& a, const Packet2d& b) {
  818|      0|  return _mm_or_pd(a, b);
  819|      0|}
  820|       |template <>
  821|      0|EIGEN_STRONG_INLINE Packet2l por<Packet2l>(const Packet2l& a, const Packet2l& b) {
  822|      0|  return _mm_or_si128(a, b);
  823|      0|}
  824|       |template <>
  825|      0|EIGEN_STRONG_INLINE Packet4i por<Packet4i>(const Packet4i& a, const Packet4i& b) {
  826|      0|  return _mm_or_si128(a, b);
  827|      0|}
  828|       |template <>
  829|      0|EIGEN_STRONG_INLINE Packet4ui por<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
  830|      0|  return _mm_or_si128(a, b);
  831|      0|}
  832|       |template <>
  833|      0|EIGEN_STRONG_INLINE Packet16b por<Packet16b>(const Packet16b& a, const Packet16b& b) {
  834|      0|  return _mm_or_si128(a, b);
  835|      0|}
  836|       |
  837|       |template <>
  838|      0|EIGEN_STRONG_INLINE Packet4f pxor<Packet4f>(const Packet4f& a, const Packet4f& b) {
  839|      0|  return _mm_xor_ps(a, b);
  840|      0|}
  841|       |template <>
  842|      0|EIGEN_STRONG_INLINE Packet2d pxor<Packet2d>(const Packet2d& a, const Packet2d& b) {
  843|      0|  return _mm_xor_pd(a, b);
  844|      0|}
  845|       |template <>
  846|      0|EIGEN_STRONG_INLINE Packet2l pxor<Packet2l>(const Packet2l& a, const Packet2l& b) {
  847|      0|  return _mm_xor_si128(a, b);
  848|      0|}
  849|       |template <>
  850|      0|EIGEN_STRONG_INLINE Packet4i pxor<Packet4i>(const Packet4i& a, const Packet4i& b) {
  851|      0|  return _mm_xor_si128(a, b);
  852|      0|}
  853|       |template <>
  854|      0|EIGEN_STRONG_INLINE Packet4ui pxor<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
  855|      0|  return _mm_xor_si128(a, b);
  856|      0|}
  857|       |template <>
  858|      0|EIGEN_STRONG_INLINE Packet16b pxor<Packet16b>(const Packet16b& a, const Packet16b& b) {
  859|      0|  return _mm_xor_si128(a, b);
  860|      0|}
  861|       |
  862|       |template <>
  863|      0|EIGEN_STRONG_INLINE Packet4f pandnot<Packet4f>(const Packet4f& a, const Packet4f& b) {
  864|      0|  return _mm_andnot_ps(b, a);
  865|      0|}
  866|       |template <>
  867|      0|EIGEN_STRONG_INLINE Packet2d pandnot<Packet2d>(const Packet2d& a, const Packet2d& b) {
  868|      0|  return _mm_andnot_pd(b, a);
  869|      0|}
  870|       |template <>
  871|      0|EIGEN_STRONG_INLINE Packet2l pandnot<Packet2l>(const Packet2l& a, const Packet2l& b) {
  872|      0|  return _mm_andnot_si128(b, a);
  873|      0|}
  874|       |template <>
  875|      0|EIGEN_STRONG_INLINE Packet4i pandnot<Packet4i>(const Packet4i& a, const Packet4i& b) {
  876|      0|  return _mm_andnot_si128(b, a);
  877|      0|}
  878|       |template <>
  879|      0|EIGEN_STRONG_INLINE Packet4ui pandnot<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
  880|      0|  return _mm_andnot_si128(b, a);
  881|      0|}
  882|       |
  883|       |template <>
  884|      0|EIGEN_STRONG_INLINE Packet4f pcmp_le(const Packet4f& a, const Packet4f& b) {
  885|      0|  return _mm_cmple_ps(a, b);
  886|      0|}
  887|       |template <>
  888|      0|EIGEN_STRONG_INLINE Packet4f pcmp_lt(const Packet4f& a, const Packet4f& b) {
  889|      0|  return _mm_cmplt_ps(a, b);
  890|      0|}
  891|       |template <>
  892|      0|EIGEN_STRONG_INLINE Packet4f pcmp_lt_or_nan(const Packet4f& a, const Packet4f& b) {
  893|      0|  return _mm_cmpnge_ps(a, b);
  894|      0|}
  895|       |template <>
  896|      0|EIGEN_STRONG_INLINE Packet4f pcmp_eq(const Packet4f& a, const Packet4f& b) {
  897|      0|  return _mm_cmpeq_ps(a, b);
  898|      0|}
  899|       |
  900|       |template <>
  901|      0|EIGEN_STRONG_INLINE Packet2d pcmp_le(const Packet2d& a, const Packet2d& b) {
  902|      0|  return _mm_cmple_pd(a, b);
  903|      0|}
  904|       |template <>
  905|      0|EIGEN_STRONG_INLINE Packet2d pcmp_lt(const Packet2d& a, const Packet2d& b) {
  906|      0|  return _mm_cmplt_pd(a, b);
  907|      0|}
  908|       |template <>
  909|      0|EIGEN_STRONG_INLINE Packet2d pcmp_lt_or_nan(const Packet2d& a, const Packet2d& b) {
  910|      0|  return _mm_cmpnge_pd(a, b);
  911|      0|}
  912|       |template <>
  913|      0|EIGEN_STRONG_INLINE Packet2d pcmp_eq(const Packet2d& a, const Packet2d& b) {
  914|      0|  return _mm_cmpeq_pd(a, b);
  915|      0|}
  916|       |template <>
  917|      0|EIGEN_STRONG_INLINE Packet4i pcmp_lt(const Packet4i& a, const Packet4i& b) {
  918|      0|  return _mm_cmplt_epi32(a, b);
  919|      0|}
  920|       |template <>
  921|      0|EIGEN_STRONG_INLINE Packet4i pcmp_eq(const Packet4i& a, const Packet4i& b) {
  922|      0|  return _mm_cmpeq_epi32(a, b);
  923|      0|}
  924|       |template <>
  925|      0|EIGEN_STRONG_INLINE Packet4i pcmp_le(const Packet4i& a, const Packet4i& b) {
  926|      0|  return por(pcmp_lt(a, b), pcmp_eq(a, b));
  927|      0|}
  928|       |template <>
  929|      0|EIGEN_STRONG_INLINE Packet2l pcmp_lt(const Packet2l& a, const Packet2l& b) {
  930|      0|#ifdef EIGEN_VECTORIZE_SSE4_2
  931|      0|  return _mm_cmpgt_epi64(b, a);
  932|      0|#else
  933|      0|  Packet4i eq = pcmp_eq<Packet4i>(Packet4i(a), Packet4i(b));
  934|      0|  Packet2l hi_eq = Packet2l(_mm_shuffle_epi32(eq, (shuffle_mask<1, 1, 3, 3>::mask)));
  935|      0|  Packet4i lt = pcmp_lt<Packet4i>(Packet4i(a), Packet4i(b));
  936|      0|  Packet2l hi_lt = Packet2l(_mm_shuffle_epi32(lt, (shuffle_mask<1, 1, 3, 3>::mask)));
  937|      0|  Packet2l lo_lt = Packet2l(_mm_shuffle_epi32(lt, (shuffle_mask<0, 0, 2, 2>::mask)));
  938|      0|  // return hi(a) < hi(b) || (hi(a) == hi(b) && lo(a) < lo(b))
  939|      0|  return por(hi_lt, pand(hi_eq, lo_lt));
  940|      0|#endif
  941|      0|}
  942|       |template <>
  943|      0|EIGEN_STRONG_INLINE Packet2l pcmp_eq(const Packet2l& a, const Packet2l& b) {
  944|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
  945|      0|  return _mm_cmpeq_epi64(a, b);
  946|      0|#else
  947|      0|  Packet4i tmp = pcmp_eq<Packet4i>(Packet4i(a), Packet4i(b));
  948|      0|  return Packet2l(pand<Packet4i>(tmp, _mm_shuffle_epi32(tmp, (shuffle_mask<1, 0, 3, 2>::mask))));
  949|      0|#endif
  950|      0|}
  951|       |template <>
  952|      0|EIGEN_STRONG_INLINE Packet2l pcmp_le(const Packet2l& a, const Packet2l& b) {
  953|      0|  return por(pcmp_lt(a, b), pcmp_eq(a, b));
  954|      0|}
  955|       |template <>
  956|      0|EIGEN_STRONG_INLINE Packet16b pcmp_eq(const Packet16b& a, const Packet16b& b) {
  957|      0|  // Mask out invalid bool bits to avoid UB.
  958|      0|  const Packet16b kBoolMask = pset1<Packet16b>(true);
  959|      0|  return _mm_and_si128(_mm_cmpeq_epi8(a, b), kBoolMask);
  960|      0|}
  961|       |template <>
  962|      0|EIGEN_STRONG_INLINE Packet4ui pcmp_eq(const Packet4ui& a, const Packet4ui& b) {
  963|      0|  return _mm_cmpeq_epi32(a, b);
  964|      0|}
  965|       |
  966|       |template <>
  967|      0|EIGEN_STRONG_INLINE Packet4f pmin<Packet4f>(const Packet4f& a, const Packet4f& b) {
  968|      0|#if EIGEN_GNUC_STRICT_LESS_THAN(6, 3, 0)
  969|      0|// There appears to be a bug in GCC, by which the optimizer may
  970|      0|// flip the argument order in calls to _mm_min_ps, so we have to
  971|      0|// resort to inline ASM here. This is supposed to be fixed in gcc6.3,
  972|      0|// see also: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=72867
  973|      0|#ifdef EIGEN_VECTORIZE_AVX
  974|      0|  Packet4f res;
  975|      0|  asm("vminps %[a], %[b], %[res]" : [res] "=x"(res) : [a] "x"(a), [b] "x"(b));
  976|      0|#else
  977|      0|  Packet4f res = b;
  978|      0|  asm("minps %[a], %[res]" : [res] "+x"(res) : [a] "x"(a));
  979|      0|#endif
  980|      0|  return res;
  981|      0|#else
  982|      0|  // Arguments are reversed to match NaN propagation behavior of std::min.
  983|      0|  return _mm_min_ps(b, a);
  984|      0|#endif
  985|      0|}
  986|       |template <>
  987|      0|EIGEN_STRONG_INLINE Packet2d pmin<Packet2d>(const Packet2d& a, const Packet2d& b) {
  988|      0|#if EIGEN_GNUC_STRICT_LESS_THAN(6, 3, 0)
  989|      0|// There appears to be a bug in GCC, by which the optimizer may
  990|      0|// flip the argument order in calls to _mm_min_pd, so we have to
  991|      0|// resort to inline ASM here. This is supposed to be fixed in gcc6.3,
  992|      0|// see also: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=72867
  993|      0|#ifdef EIGEN_VECTORIZE_AVX
  994|      0|  Packet2d res;
  995|      0|  asm("vminpd %[a], %[b], %[res]" : [res] "=x"(res) : [a] "x"(a), [b] "x"(b));
  996|      0|#else
  997|      0|  Packet2d res = b;
  998|      0|  asm("minpd %[a], %[res]" : [res] "+x"(res) : [a] "x"(a));
  999|      0|#endif
 1000|      0|  return res;
 1001|      0|#else
 1002|      0|  // Arguments are reversed to match NaN propagation behavior of std::min.
 1003|      0|  return _mm_min_pd(b, a);
 1004|      0|#endif
 1005|      0|}
 1006|       |template <>
 1007|      0|EIGEN_STRONG_INLINE Packet2l pmin<Packet2l>(const Packet2l& a, const Packet2l& b) {
 1008|      0|  Packet2l a_lt_mask = pcmp_lt(a, b);
 1009|      0|  return por(pandnot(b, a_lt_mask), pand(a, a_lt_mask));
 1010|      0|}
 1011|       |template <>
 1012|      0|EIGEN_STRONG_INLINE Packet4i pmin<Packet4i>(const Packet4i& a, const Packet4i& b) {
 1013|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1014|      0|  return _mm_min_epi32(a, b);
 1015|      0|#else
 1016|      0|  // after some bench, this version *is* faster than a scalar implementation
 1017|      0|  Packet4i mask = _mm_cmplt_epi32(a, b);
 1018|      0|  return _mm_or_si128(_mm_and_si128(mask, a), _mm_andnot_si128(mask, b));
 1019|      0|#endif
 1020|      0|}
 1021|       |template <>
 1022|      0|EIGEN_STRONG_INLINE Packet4ui pmin<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
 1023|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1024|      0|  return _mm_min_epu32(a, b);
 1025|      0|#else
 1026|      0|  return padd((Packet4ui)pmin((Packet4i)psub(a, pset1<Packet4ui>(0x80000000UL)),
 1027|      0|                              (Packet4i)psub(b, pset1<Packet4ui>(0x80000000UL))),
 1028|      0|              pset1<Packet4ui>(0x80000000UL));
 1029|      0|#endif
 1030|      0|}
 1031|       |
 1032|       |template <>
 1033|      0|EIGEN_STRONG_INLINE Packet4f pmax<Packet4f>(const Packet4f& a, const Packet4f& b) {
 1034|      0|#if EIGEN_GNUC_STRICT_LESS_THAN(6, 3, 0)
 1035|      0|// There appears to be a bug in GCC, by which the optimizer may
 1036|      0|// flip the argument order in calls to _mm_max_ps, so we have to
 1037|      0|// resort to inline ASM here. This is supposed to be fixed in gcc6.3,
 1038|      0|// see also: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=72867
 1039|      0|#ifdef EIGEN_VECTORIZE_AVX
 1040|      0|  Packet4f res;
 1041|      0|  asm("vmaxps %[a], %[b], %[res]" : [res] "=x"(res) : [a] "x"(a), [b] "x"(b));
 1042|      0|#else
 1043|      0|  Packet4f res = b;
 1044|      0|  asm("maxps %[a], %[res]" : [res] "+x"(res) : [a] "x"(a));
 1045|      0|#endif
 1046|      0|  return res;
 1047|      0|#else
 1048|      0|  // Arguments are reversed to match NaN propagation behavior of std::max.
 1049|      0|  return _mm_max_ps(b, a);
 1050|      0|#endif
 1051|      0|}
 1052|       |template <>
 1053|      0|EIGEN_STRONG_INLINE Packet2d pmax<Packet2d>(const Packet2d& a, const Packet2d& b) {
 1054|      0|#if EIGEN_GNUC_STRICT_LESS_THAN(6, 3, 0)
 1055|      0|// There appears to be a bug in GCC, by which the optimizer may
 1056|      0|// flip the argument order in calls to _mm_max_pd, so we have to
 1057|      0|// resort to inline ASM here. This is supposed to be fixed in gcc6.3,
 1058|      0|// see also: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=72867
 1059|      0|#ifdef EIGEN_VECTORIZE_AVX
 1060|      0|  Packet2d res;
 1061|      0|  asm("vmaxpd %[a], %[b], %[res]" : [res] "=x"(res) : [a] "x"(a), [b] "x"(b));
 1062|      0|#else
 1063|      0|  Packet2d res = b;
 1064|      0|  asm("maxpd %[a], %[res]" : [res] "+x"(res) : [a] "x"(a));
 1065|      0|#endif
 1066|      0|  return res;
 1067|      0|#else
 1068|      0|  // Arguments are reversed to match NaN propagation behavior of std::max.
 1069|      0|  return _mm_max_pd(b, a);
 1070|      0|#endif
 1071|      0|}
 1072|       |template <>
 1073|      0|EIGEN_STRONG_INLINE Packet2l pmax<Packet2l>(const Packet2l& a, const Packet2l& b) {
 1074|      0|  Packet2l a_lt_mask = pcmp_lt(a, b);
 1075|      0|  return por(pandnot(a, a_lt_mask), pand(b, a_lt_mask));
 1076|      0|}
 1077|       |template <>
 1078|      0|EIGEN_STRONG_INLINE Packet4i pmax<Packet4i>(const Packet4i& a, const Packet4i& b) {
 1079|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1080|      0|  return _mm_max_epi32(a, b);
 1081|      0|#else
 1082|      0|  // after some bench, this version *is* faster than a scalar implementation
 1083|      0|  Packet4i mask = _mm_cmpgt_epi32(a, b);
 1084|      0|  return _mm_or_si128(_mm_and_si128(mask, a), _mm_andnot_si128(mask, b));
 1085|      0|#endif
 1086|      0|}
 1087|       |template <>
 1088|      0|EIGEN_STRONG_INLINE Packet4ui pmax<Packet4ui>(const Packet4ui& a, const Packet4ui& b) {
 1089|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1090|      0|  return _mm_max_epu32(a, b);
 1091|      0|#else
 1092|      0|  return padd((Packet4ui)pmax((Packet4i)psub(a, pset1<Packet4ui>(0x80000000UL)),
 1093|      0|                              (Packet4i)psub(b, pset1<Packet4ui>(0x80000000UL))),
 1094|      0|              pset1<Packet4ui>(0x80000000UL));
 1095|      0|#endif
 1096|      0|}
 1097|       |
 1098|       |template <>
 1099|      0|EIGEN_STRONG_INLINE Packet4ui pcmp_lt(const Packet4ui& a, const Packet4ui& b) {
 1100|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1101|      0|  return pxor(pcmp_eq(a, pmax(a, b)), ptrue(a));
 1102|      0|#else
 1103|      0|  return (Packet4ui)pcmp_lt((Packet4i)psub(a, pset1<Packet4ui>(0x80000000UL)),
 1104|      0|                            (Packet4i)psub(b, pset1<Packet4ui>(0x80000000UL)));
 1105|      0|#endif
 1106|      0|}
 1107|       |template <>
 1108|      0|EIGEN_STRONG_INLINE Packet4ui pcmp_le(const Packet4ui& a, const Packet4ui& b) {
 1109|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1110|      0|  return pcmp_eq(a, pmin(a, b));
 1111|      0|#else
 1112|      0|  return (Packet4ui)pcmp_le((Packet4i)psub(a, pset1<Packet4ui>(0x80000000UL)),
 1113|      0|                            (Packet4i)psub(b, pset1<Packet4ui>(0x80000000UL)));
 1114|      0|#endif
 1115|      0|}
 1116|       |
 1117|       |template <typename Packet, typename Op>
 1118|      0|EIGEN_STRONG_INLINE Packet pminmax_propagate_numbers(const Packet& a, const Packet& b, Op op) {
 1119|      0|  // In this implementation, we take advantage of the fact that pmin/pmax for SSE
 1120|      0|  // always return a if either a or b is NaN.
 1121|      0|  Packet not_nan_mask_a = pcmp_eq(a, a);
 1122|      0|  Packet m = op(a, b);
 1123|      0|  return pselect<Packet>(not_nan_mask_a, m, b);
 1124|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25pminmax_propagate_numbersIDv4_fPFS2_RKS2_S4_EEET_RKS7_S9_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal25pminmax_propagate_numbersIDv2_dPFS2_RKS2_S4_EEET_RKS7_S9_T0_
  ------------------
 1125|       |
 1126|       |template <typename Packet, typename Op>
 1127|      0|EIGEN_STRONG_INLINE Packet pminmax_propagate_nan(const Packet& a, const Packet& b, Op op) {
 1128|      0|  // In this implementation, we take advantage of the fact that pmin/pmax for SSE
 1129|      0|  // always return a if either a or b is NaN.
 1130|      0|  Packet not_nan_mask_a = pcmp_eq(a, a);
 1131|      0|  Packet m = op(b, a);
 1132|      0|  return pselect<Packet>(not_nan_mask_a, m, a);
 1133|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal21pminmax_propagate_nanIDv4_fPFS2_RKS2_S4_EEET_RKS7_S9_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal21pminmax_propagate_nanIDv2_dPFS2_RKS2_S4_EEET_RKS7_S9_T0_
  ------------------
 1134|       |
 1135|       |// Add specializations for min/max with prescribed NaN propagation.
 1136|       |template <>
 1137|      0|EIGEN_STRONG_INLINE Packet4f pmin<PropagateNumbers, Packet4f>(const Packet4f& a, const Packet4f& b) {
 1138|      0|  return pminmax_propagate_numbers(a, b, pmin<Packet4f>);
 1139|      0|}
 1140|       |template <>
 1141|      0|EIGEN_STRONG_INLINE Packet2d pmin<PropagateNumbers, Packet2d>(const Packet2d& a, const Packet2d& b) {
 1142|      0|  return pminmax_propagate_numbers(a, b, pmin<Packet2d>);
 1143|      0|}
 1144|       |template <>
 1145|      0|EIGEN_STRONG_INLINE Packet4f pmax<PropagateNumbers, Packet4f>(const Packet4f& a, const Packet4f& b) {
 1146|      0|  return pminmax_propagate_numbers(a, b, pmax<Packet4f>);
 1147|      0|}
 1148|       |template <>
 1149|      0|EIGEN_STRONG_INLINE Packet2d pmax<PropagateNumbers, Packet2d>(const Packet2d& a, const Packet2d& b) {
 1150|      0|  return pminmax_propagate_numbers(a, b, pmax<Packet2d>);
 1151|      0|}
 1152|       |template <>
 1153|      0|EIGEN_STRONG_INLINE Packet4f pmin<PropagateNaN, Packet4f>(const Packet4f& a, const Packet4f& b) {
 1154|      0|  return pminmax_propagate_nan(a, b, pmin<Packet4f>);
 1155|      0|}
 1156|       |template <>
 1157|      0|EIGEN_STRONG_INLINE Packet2d pmin<PropagateNaN, Packet2d>(const Packet2d& a, const Packet2d& b) {
 1158|      0|  return pminmax_propagate_nan(a, b, pmin<Packet2d>);
 1159|      0|}
 1160|       |template <>
 1161|      0|EIGEN_STRONG_INLINE Packet4f pmax<PropagateNaN, Packet4f>(const Packet4f& a, const Packet4f& b) {
 1162|      0|  return pminmax_propagate_nan(a, b, pmax<Packet4f>);
 1163|      0|}
 1164|       |template <>
 1165|      0|EIGEN_STRONG_INLINE Packet2d pmax<PropagateNaN, Packet2d>(const Packet2d& a, const Packet2d& b) {
 1166|      0|  return pminmax_propagate_nan(a, b, pmax<Packet2d>);
 1167|      0|}
 1168|       |
 1169|       |template <>
 1170|      0|EIGEN_STRONG_INLINE Packet4f psignbit(const Packet4f& a) {
 1171|      0|  return _mm_castsi128_ps(_mm_srai_epi32(_mm_castps_si128(a), 31));
 1172|      0|}
 1173|       |template <>
 1174|      0|EIGEN_STRONG_INLINE Packet2d psignbit(const Packet2d& a) {
 1175|      0|  Packet4f tmp = psignbit<Packet4f>(_mm_castpd_ps(a));
 1176|      0|#ifdef EIGEN_VECTORIZE_AVX
 1177|      0|  return _mm_castps_pd(_mm_permute_ps(tmp, (shuffle_mask<1, 1, 3, 3>::mask)));
 1178|      0|#else
 1179|      0|  return _mm_castps_pd(_mm_shuffle_ps(tmp, tmp, (shuffle_mask<1, 1, 3, 3>::mask)));
 1180|      0|#endif  // EIGEN_VECTORIZE_AVX
 1181|      0|}
 1182|       |template <>
 1183|      0|EIGEN_STRONG_INLINE Packet4i psignbit(const Packet4i& a) {
 1184|      0|  return _mm_srai_epi32(a, 31);
 1185|      0|}
 1186|       |template <>
 1187|      0|EIGEN_STRONG_INLINE Packet4ui psignbit(const Packet4ui& a) {
 1188|      0|  return pzero(a);
 1189|      0|}
 1190|       |template <>
 1191|      0|EIGEN_STRONG_INLINE Packet2l psignbit(const Packet2l& a) {
 1192|      0|  Packet4i tmp = psignbit<Packet4i>(Packet4i(a));
 1193|      0|  return Packet2l(_mm_shuffle_epi32(tmp, (shuffle_mask<1, 1, 3, 3>::mask)));
 1194|      0|}
 1195|       |
 1196|       |template <int N>
 1197|       |EIGEN_STRONG_INLINE Packet2l parithmetic_shift_right(const Packet2l& a) {
 1198|       |  Packet2l signbit = psignbit(a);
 1199|       |  return por(_mm_slli_epi64(signbit, 64 - N), _mm_srli_epi64(a, N));
 1200|       |}
 1201|       |template <int N>
 1202|       |EIGEN_STRONG_INLINE Packet2l plogical_shift_right(const Packet2l& a) {
 1203|       |  return _mm_srli_epi64(a, N);
 1204|       |}
 1205|       |template <int N>
 1206|      0|EIGEN_STRONG_INLINE Packet2l plogical_shift_left(const Packet2l& a) {
 1207|      0|  return _mm_slli_epi64(a, N);
 1208|      0|}
 1209|       |template <int N>
 1210|      0|EIGEN_STRONG_INLINE Packet4i parithmetic_shift_right(const Packet4i& a) {
 1211|      0|  return _mm_srai_epi32(a, N);
 1212|      0|}
 1213|       |template <int N>
 1214|      0|EIGEN_STRONG_INLINE Packet4i plogical_shift_right(const Packet4i& a) {
 1215|      0|  return _mm_srli_epi32(a, N);
 1216|      0|}
 1217|       |template <int N>
 1218|      0|EIGEN_STRONG_INLINE Packet4i plogical_shift_left(const Packet4i& a) {
 1219|      0|  return _mm_slli_epi32(a, N);
 1220|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal19plogical_shift_leftILi23EEENS0_20eigen_packet_wrapperIDv2_xLi0EEERKS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal19plogical_shift_leftILi30EEENS0_20eigen_packet_wrapperIDv2_xLi0EEERKS4_
  ------------------
 1221|       |template <int N>
 1222|       |EIGEN_STRONG_INLINE Packet4ui parithmetic_shift_right(const Packet4ui& a) {
 1223|       |  return _mm_srli_epi32(a, N);
 1224|       |}
 1225|       |template <int N>
 1226|       |EIGEN_STRONG_INLINE Packet4ui plogical_shift_right(const Packet4ui& a) {
 1227|       |  return _mm_srli_epi32(a, N);
 1228|       |}
 1229|       |template <int N>
 1230|       |EIGEN_STRONG_INLINE Packet4ui plogical_shift_left(const Packet4ui& a) {
 1231|       |  return _mm_slli_epi32(a, N);
 1232|       |}
 1233|       |
 1234|       |template <>
 1235|      0|EIGEN_STRONG_INLINE Packet4f pabs(const Packet4f& a) {
 1236|      0|  const __m128i mask = _mm_setr_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF);
 1237|      0|  return _mm_castsi128_ps(_mm_and_si128(mask, _mm_castps_si128(a)));
 1238|      0|}
 1239|       |template <>
 1240|      0|EIGEN_STRONG_INLINE Packet2d pabs(const Packet2d& a) {
 1241|      0|  const __m128i mask = _mm_setr_epi32(0xFFFFFFFF, 0x7FFFFFFF, 0xFFFFFFFF, 0x7FFFFFFF);
 1242|      0|  return _mm_castsi128_pd(_mm_and_si128(mask, _mm_castpd_si128(a)));
 1243|      0|}
 1244|       |template <>
 1245|      0|EIGEN_STRONG_INLINE Packet2l pabs(const Packet2l& a) {
 1246|      0|  Packet2l signbit = psignbit(a);
 1247|      0|  return _mm_sub_epi64(_mm_xor_si128(a, signbit), signbit);
 1248|      0|}
 1249|       |template <>
 1250|      0|EIGEN_STRONG_INLINE Packet4i pabs(const Packet4i& a) {
 1251|      0|#ifdef EIGEN_VECTORIZE_SSSE3
 1252|      0|  return _mm_abs_epi32(a);
 1253|      0|#else
 1254|      0|  Packet4i signbit = psignbit(a);
 1255|      0|  return _mm_sub_epi32(_mm_xor_si128(a, signbit), signbit);
 1256|      0|#endif
 1257|      0|}
 1258|       |template <>
 1259|      0|EIGEN_STRONG_INLINE Packet4ui pabs(const Packet4ui& a) {
 1260|      0|  return a;
 1261|      0|}
 1262|       |
 1263|       |#ifdef EIGEN_VECTORIZE_SSE4_1
 1264|       |template <>
 1265|       |EIGEN_STRONG_INLINE Packet4f pround<Packet4f>(const Packet4f& a) {
 1266|       |  // Unfortunately _mm_round_ps doesn't have a rounding mode to implement numext::round.
 1267|       |  const Packet4f mask = pset1frombits<Packet4f>(0x80000000u);
 1268|       |  const Packet4f prev0dot5 = pset1frombits<Packet4f>(0x3EFFFFFFu);
 1269|       |  return _mm_round_ps(padd(por(pand(a, mask), prev0dot5), a), _MM_FROUND_TO_ZERO);
 1270|       |}
 1271|       |
 1272|       |template <>
 1273|       |EIGEN_STRONG_INLINE Packet2d pround<Packet2d>(const Packet2d& a) {
 1274|       |  const Packet2d mask = _mm_castsi128_pd(_mm_set_epi64x(0x8000000000000000ull, 0x8000000000000000ull));
 1275|       |  const Packet2d prev0dot5 = _mm_castsi128_pd(_mm_set_epi64x(0x3FDFFFFFFFFFFFFFull, 0x3FDFFFFFFFFFFFFFull));
 1276|       |  return _mm_round_pd(padd(por(pand(a, mask), prev0dot5), a), _MM_FROUND_TO_ZERO);
 1277|       |}
 1278|       |
 1279|       |template <>
 1280|       |EIGEN_STRONG_INLINE Packet4f print<Packet4f>(const Packet4f& a) {
 1281|       |  return _mm_round_ps(a, _MM_FROUND_CUR_DIRECTION);
 1282|       |}
 1283|       |template <>
 1284|       |EIGEN_STRONG_INLINE Packet2d print<Packet2d>(const Packet2d& a) {
 1285|       |  return _mm_round_pd(a, _MM_FROUND_CUR_DIRECTION);
 1286|       |}
 1287|       |
 1288|       |template <>
 1289|       |EIGEN_STRONG_INLINE Packet4f pceil<Packet4f>(const Packet4f& a) {
 1290|       |  return _mm_ceil_ps(a);
 1291|       |}
 1292|       |template <>
 1293|       |EIGEN_STRONG_INLINE Packet2d pceil<Packet2d>(const Packet2d& a) {
 1294|       |  return _mm_ceil_pd(a);
 1295|       |}
 1296|       |
 1297|       |template <>
 1298|       |EIGEN_STRONG_INLINE Packet4f pfloor<Packet4f>(const Packet4f& a) {
 1299|       |  return _mm_floor_ps(a);
 1300|       |}
 1301|       |template <>
 1302|       |EIGEN_STRONG_INLINE Packet2d pfloor<Packet2d>(const Packet2d& a) {
 1303|       |  return _mm_floor_pd(a);
 1304|       |}
 1305|       |
 1306|       |template <>
 1307|       |EIGEN_STRONG_INLINE Packet4f ptrunc<Packet4f>(const Packet4f& a) {
 1308|       |  return _mm_round_ps(a, _MM_FROUND_TRUNC);
 1309|       |}
 1310|       |template <>
 1311|       |EIGEN_STRONG_INLINE Packet2d ptrunc<Packet2d>(const Packet2d& a) {
 1312|       |  return _mm_round_pd(a, _MM_FROUND_TRUNC);
 1313|       |}
 1314|       |#endif
 1315|       |
 1316|       |template <>
 1317|      0|EIGEN_STRONG_INLINE Packet4f pload<Packet4f>(const float* from) {
 1318|      0|  EIGEN_DEBUG_ALIGNED_LOAD return _mm_load_ps(from);
 1319|      0|}
 1320|       |template <>
 1321|      0|EIGEN_STRONG_INLINE Packet2d pload<Packet2d>(const double* from) {
 1322|      0|  EIGEN_DEBUG_ALIGNED_LOAD return _mm_load_pd(from);
 1323|      0|}
 1324|       |template <>
 1325|      0|EIGEN_STRONG_INLINE Packet2l pload<Packet2l>(const int64_t* from) {
 1326|      0|  EIGEN_DEBUG_ALIGNED_LOAD return _mm_load_si128(reinterpret_cast<const __m128i*>(from));
 1327|      0|}
 1328|       |template <>
 1329|      0|EIGEN_STRONG_INLINE Packet4i pload<Packet4i>(const int* from) {
 1330|      0|  EIGEN_DEBUG_ALIGNED_LOAD return _mm_load_si128(reinterpret_cast<const __m128i*>(from));
 1331|      0|}
 1332|       |template <>
 1333|      0|EIGEN_STRONG_INLINE Packet4ui pload<Packet4ui>(const uint32_t* from) {
 1334|      0|  EIGEN_DEBUG_ALIGNED_LOAD return _mm_load_si128(reinterpret_cast<const __m128i*>(from));
 1335|      0|}
 1336|       |template <>
 1337|      0|EIGEN_STRONG_INLINE Packet16b pload<Packet16b>(const bool* from) {
 1338|      0|  EIGEN_DEBUG_ALIGNED_LOAD return _mm_load_si128(reinterpret_cast<const __m128i*>(from));
 1339|      0|}
 1340|       |
 1341|       |#if EIGEN_COMP_MSVC
 1342|       |template <>
 1343|       |EIGEN_STRONG_INLINE Packet4f ploadu<Packet4f>(const float* from) {
 1344|       |  EIGEN_DEBUG_UNALIGNED_LOAD
 1345|       |  return _mm_loadu_ps(from);
 1346|       |}
 1347|       |#else
 1348|       |// NOTE: with the code below, MSVC's compiler crashes!
 1349|       |
 1350|       |template <>
 1351|      0|EIGEN_STRONG_INLINE Packet4f ploadu<Packet4f>(const float* from) {
 1352|      0|  EIGEN_DEBUG_UNALIGNED_LOAD
 1353|      0|  return _mm_loadu_ps(from);
 1354|      0|}
 1355|       |#endif
 1356|       |
 1357|       |template <>
 1358|      0|EIGEN_STRONG_INLINE Packet2d ploadu<Packet2d>(const double* from) {
 1359|      0|  EIGEN_DEBUG_UNALIGNED_LOAD
 1360|      0|  return _mm_loadu_pd(from);
 1361|      0|}
 1362|       |template <>
 1363|      0|EIGEN_STRONG_INLINE Packet2l ploadu<Packet2l>(const int64_t* from) {
 1364|      0|  EIGEN_DEBUG_UNALIGNED_LOAD
 1365|      0|  return _mm_loadu_si128(reinterpret_cast<const __m128i*>(from));
 1366|      0|}
 1367|       |template <>
 1368|      0|EIGEN_STRONG_INLINE Packet4i ploadu<Packet4i>(const int* from) {
 1369|      0|  EIGEN_DEBUG_UNALIGNED_LOAD
 1370|      0|  return _mm_loadu_si128(reinterpret_cast<const __m128i*>(from));
 1371|      0|}
 1372|       |template <>
 1373|      0|EIGEN_STRONG_INLINE Packet4ui ploadu<Packet4ui>(const uint32_t* from) {
 1374|      0|  EIGEN_DEBUG_UNALIGNED_LOAD
 1375|      0|  return _mm_loadu_si128(reinterpret_cast<const __m128i*>(from));
 1376|      0|}
 1377|       |template <>
 1378|      0|EIGEN_STRONG_INLINE Packet16b ploadu<Packet16b>(const bool* from) {
 1379|      0|  EIGEN_DEBUG_UNALIGNED_LOAD
 1380|      0|  return _mm_loadu_si128(reinterpret_cast<const __m128i*>(from));
 1381|      0|}
 1382|       |
 1383|       |// Load lower part of packet zero extending.
 1384|       |template <typename Packet>
 1385|       |EIGEN_STRONG_INLINE Packet ploadl(const typename unpacket_traits<Packet>::type* from);
 1386|       |template <>
 1387|      0|EIGEN_STRONG_INLINE Packet4f ploadl<Packet4f>(const float* from) {
 1388|      0|  EIGEN_DEBUG_UNALIGNED_LOAD return _mm_castpd_ps(_mm_load_sd(reinterpret_cast<const double*>(from)));
 1389|      0|}
 1390|       |template <>
 1391|      0|EIGEN_STRONG_INLINE Packet2d ploadl<Packet2d>(const double* from) {
 1392|      0|  EIGEN_DEBUG_UNALIGNED_LOAD return _mm_load_sd(from);
 1393|      0|}
 1394|       |
 1395|       |// Load scalar
 1396|       |template <typename Packet>
 1397|       |EIGEN_STRONG_INLINE Packet ploads(const typename unpacket_traits<Packet>::type* from);
 1398|       |template <>
 1399|      0|EIGEN_STRONG_INLINE Packet4f ploads<Packet4f>(const float* from) {
 1400|      0|  EIGEN_DEBUG_UNALIGNED_LOAD return _mm_load_ss(from);
 1401|      0|}
 1402|       |template <>
 1403|      0|EIGEN_STRONG_INLINE Packet2d ploads<Packet2d>(const double* from) {
 1404|      0|  EIGEN_DEBUG_UNALIGNED_LOAD return _mm_load_sd(from);
 1405|      0|}
 1406|       |
 1407|       |template <>
 1408|      0|EIGEN_STRONG_INLINE Packet4f ploaddup<Packet4f>(const float* from) {
 1409|      0|  return vec4f_swizzle1(_mm_castpd_ps(_mm_load_sd(reinterpret_cast<const double*>(from))), 0, 0, 1, 1);
 1410|      0|}
 1411|       |template <>
 1412|      0|EIGEN_STRONG_INLINE Packet2d ploaddup<Packet2d>(const double* from) {
 1413|      0|  return pset1<Packet2d>(from[0]);
 1414|      0|}
 1415|       |template <>
 1416|      0|EIGEN_STRONG_INLINE Packet2l ploaddup<Packet2l>(const int64_t* from) {
 1417|      0|  return pset1<Packet2l>(from[0]);
 1418|      0|}
 1419|       |template <>
 1420|      0|EIGEN_STRONG_INLINE Packet4i ploaddup<Packet4i>(const int* from) {
 1421|      0|  Packet4i tmp;
 1422|      0|  tmp = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(from));
 1423|      0|  return vec4i_swizzle1(tmp, 0, 0, 1, 1);
 1424|      0|}
 1425|       |template <>
 1426|      0|EIGEN_STRONG_INLINE Packet4ui ploaddup<Packet4ui>(const uint32_t* from) {
 1427|      0|  Packet4ui tmp;
 1428|      0|  tmp = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(from));
 1429|      0|  return vec4ui_swizzle1(tmp, 0, 0, 1, 1);
 1430|      0|}
 1431|       |
 1432|       |// Loads 8 bools from memory and returns the packet
 1433|       |// {b0, b0, b1, b1, b2, b2, b3, b3, b4, b4, b5, b5, b6, b6, b7, b7}
 1434|       |template <>
 1435|      0|EIGEN_STRONG_INLINE Packet16b ploaddup<Packet16b>(const bool* from) {
 1436|      0|  __m128i tmp = _mm_castpd_si128(pload1<Packet2d>(reinterpret_cast<const double*>(from)));
 1437|      0|  return _mm_unpacklo_epi8(tmp, tmp);
 1438|      0|}
 1439|       |
 1440|       |// Loads 4 bools from memory and returns the packet
 1441|       |// {b0, b0  b0, b0, b1, b1, b1, b1, b2, b2, b2, b2, b3, b3, b3, b3}
 1442|       |template <>
 1443|      0|EIGEN_STRONG_INLINE Packet16b ploadquad<Packet16b>(const bool* from) {
 1444|      0|  __m128i tmp = _mm_castps_si128(pload1<Packet4f>(reinterpret_cast<const float*>(from)));
 1445|      0|  tmp = _mm_unpacklo_epi8(tmp, tmp);
 1446|      0|  return _mm_unpacklo_epi16(tmp, tmp);
 1447|      0|}
 1448|       |
 1449|       |template <>
 1450|      0|EIGEN_STRONG_INLINE void pstore<float>(float* to, const Packet4f& from) {
 1451|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_ps(to, from);
 1452|      0|}
 1453|       |template <>
 1454|      0|EIGEN_STRONG_INLINE void pstore<double>(double* to, const Packet2d& from) {
 1455|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_pd(to, from);
 1456|      0|}
 1457|       |template <>
 1458|      0|EIGEN_STRONG_INLINE void pstore<int64_t>(int64_t* to, const Packet2l& from) {
 1459|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_si128(reinterpret_cast<__m128i*>(to), from);
 1460|      0|}
 1461|       |template <>
 1462|      0|EIGEN_STRONG_INLINE void pstore<int>(int* to, const Packet4i& from) {
 1463|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_si128(reinterpret_cast<__m128i*>(to), from);
 1464|      0|}
 1465|       |template <>
 1466|      0|EIGEN_STRONG_INLINE void pstore<uint32_t>(uint32_t* to, const Packet4ui& from) {
 1467|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_si128(reinterpret_cast<__m128i*>(to), from);
 1468|      0|}
 1469|       |template <>
 1470|      0|EIGEN_STRONG_INLINE void pstore<bool>(bool* to, const Packet16b& from) {
 1471|      0|  EIGEN_DEBUG_ALIGNED_STORE _mm_store_si128(reinterpret_cast<__m128i*>(to), from);
 1472|      0|}
 1473|       |
 1474|       |template <>
 1475|      0|EIGEN_STRONG_INLINE void pstoreu<double>(double* to, const Packet2d& from) {
 1476|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_pd(to, from);
 1477|      0|}
 1478|       |template <>
 1479|      0|EIGEN_STRONG_INLINE void pstoreu<float>(float* to, const Packet4f& from) {
 1480|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_ps(to, from);
 1481|      0|}
 1482|       |template <>
 1483|      0|EIGEN_STRONG_INLINE void pstoreu<int64_t>(int64_t* to, const Packet2l& from) {
 1484|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_si128(reinterpret_cast<__m128i*>(to), from);
 1485|      0|}
 1486|       |template <>
 1487|      0|EIGEN_STRONG_INLINE void pstoreu<int>(int* to, const Packet4i& from) {
 1488|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_si128(reinterpret_cast<__m128i*>(to), from);
 1489|      0|}
 1490|       |template <>
 1491|      0|EIGEN_STRONG_INLINE void pstoreu<uint32_t>(uint32_t* to, const Packet4ui& from) {
 1492|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_si128(reinterpret_cast<__m128i*>(to), from);
 1493|      0|}
 1494|       |template <>
 1495|      0|EIGEN_STRONG_INLINE void pstoreu<bool>(bool* to, const Packet16b& from) {
 1496|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storeu_si128(reinterpret_cast<__m128i*>(to), from);
 1497|      0|}
 1498|       |
 1499|       |template <typename Scalar, typename Packet>
 1500|       |EIGEN_STRONG_INLINE void pstorel(Scalar* to, const Packet& from);
 1501|       |template <>
 1502|      0|EIGEN_STRONG_INLINE void pstorel(float* to, const Packet4f& from) {
 1503|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storel_pi(reinterpret_cast<__m64*>(to), from);
 1504|      0|}
 1505|       |template <>
 1506|      0|EIGEN_STRONG_INLINE void pstorel(double* to, const Packet2d& from) {
 1507|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_storel_pd(to, from);
 1508|      0|}
 1509|       |
 1510|       |template <typename Scalar, typename Packet>
 1511|       |EIGEN_STRONG_INLINE void pstores(Scalar* to, const Packet& from);
 1512|       |template <>
 1513|      0|EIGEN_STRONG_INLINE void pstores(float* to, const Packet4f& from) {
 1514|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_store_ss(to, from);
 1515|      0|}
 1516|       |template <>
 1517|      0|EIGEN_STRONG_INLINE void pstores(double* to, const Packet2d& from) {
 1518|      0|  EIGEN_DEBUG_UNALIGNED_STORE _mm_store_sd(to, from);
 1519|      0|}
 1520|       |
 1521|       |template <>
 1522|      0|EIGEN_STRONG_INLINE Packet4f preverse(const Packet4f& a) {
 1523|      0|  return _mm_shuffle_ps(a, a, 0x1B);
 1524|      0|}
 1525|       |template <>
 1526|      0|EIGEN_STRONG_INLINE Packet2d preverse(const Packet2d& a) {
 1527|      0|  return _mm_shuffle_pd(a, a, 0x1);
 1528|      0|}
 1529|       |template <>
 1530|      0|EIGEN_STRONG_INLINE Packet2l preverse(const Packet2l& a) {
 1531|      0|  return _mm_castpd_si128(preverse(_mm_castsi128_pd(a)));
 1532|      0|}
 1533|       |template <>
 1534|      0|EIGEN_STRONG_INLINE Packet4i preverse(const Packet4i& a) {
 1535|      0|  return _mm_shuffle_epi32(a, 0x1B);
 1536|      0|}
 1537|       |template <>
 1538|      0|EIGEN_STRONG_INLINE Packet4ui preverse(const Packet4ui& a) {
 1539|      0|  return _mm_shuffle_epi32(a, 0x1B);
 1540|      0|}
 1541|       |template <>
 1542|      0|EIGEN_STRONG_INLINE Packet16b preverse(const Packet16b& a) {
 1543|      0|#ifdef EIGEN_VECTORIZE_SSSE3
 1544|      0|  __m128i mask = _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
 1545|      0|  return _mm_shuffle_epi8(a, mask);
 1546|      0|#else
 1547|      0|  Packet16b tmp = _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 1, 2, 3));
 1548|      0|  tmp = _mm_shufflehi_epi16(_mm_shufflelo_epi16(tmp, _MM_SHUFFLE(2, 3, 0, 1)), _MM_SHUFFLE(2, 3, 0, 1));
 1549|      0|  return _mm_or_si128(_mm_slli_epi16(tmp, 8), _mm_srli_epi16(tmp, 8));
 1550|      0|#endif
 1551|      0|}
 1552|       |
 1553|       |#if EIGEN_COMP_MSVC_STRICT && EIGEN_OS_WIN64
 1554|       |// The temporary variable fixes an internal compilation error in vs <= 2008 and a wrong-result bug in vs 2010
 1555|       |// Direct of the struct members fixed bug #62.
 1556|       |template <>
 1557|       |EIGEN_STRONG_INLINE float pfirst<Packet4f>(const Packet4f& a) {
 1558|       |  return a.m128_f32[0];
 1559|       |}
 1560|       |template <>
 1561|       |EIGEN_STRONG_INLINE double pfirst<Packet2d>(const Packet2d& a) {
 1562|       |  return a.m128d_f64[0];
 1563|       |}
 1564|       |template <>
 1565|       |EIGEN_STRONG_INLINE int64_t pfirst<Packet2l>(const Packet2l& a) {
 1566|       |  int64_t x = _mm_extract_epi64_0(a);
 1567|       |  return x;
 1568|       |}
 1569|       |template <>
 1570|       |EIGEN_STRONG_INLINE int pfirst<Packet4i>(const Packet4i& a) {
 1571|       |  int x = _mm_cvtsi128_si32(a);
 1572|       |  return x;
 1573|       |}
 1574|       |template <>
 1575|       |EIGEN_STRONG_INLINE uint32_t pfirst<Packet4ui>(const Packet4ui& a) {
 1576|       |  uint32_t x = numext::bit_cast<uint32_t>(_mm_cvtsi128_si32(a));
 1577|       |  return x;
 1578|       |}
 1579|       |#elif EIGEN_COMP_MSVC_STRICT
 1580|       |// The temporary variable fixes an internal compilation error in vs <= 2008 and a wrong-result bug in vs 2010
 1581|       |template <>
 1582|       |EIGEN_STRONG_INLINE float pfirst<Packet4f>(const Packet4f& a) {
 1583|       |  float x = _mm_cvtss_f32(a);
 1584|       |  return x;
 1585|       |}
 1586|       |template <>
 1587|       |EIGEN_STRONG_INLINE double pfirst<Packet2d>(const Packet2d& a) {
 1588|       |  double x = _mm_cvtsd_f64(a);
 1589|       |  return x;
 1590|       |}
 1591|       |template <>
 1592|       |EIGEN_STRONG_INLINE int64_t pfirst<Packet2l>(const Packet2l& a) {
 1593|       |  int64_t x = _mm_extract_epi64_0(a);
 1594|       |  return x;
 1595|       |}
 1596|       |template <>
 1597|       |EIGEN_STRONG_INLINE int pfirst<Packet4i>(const Packet4i& a) {
 1598|       |  int x = _mm_cvtsi128_si32(a);
 1599|       |  return x;
 1600|       |}
 1601|       |template <>
 1602|       |EIGEN_STRONG_INLINE uint32_t pfirst<Packet4ui>(const Packet4ui& a) {
 1603|       |  uint32_t x = numext::bit_cast<uint32_t>(_mm_cvtsi128_si32(a));
 1604|       |  return x;
 1605|       |}
 1606|       |#else
 1607|       |template <>
 1608|      0|EIGEN_STRONG_INLINE float pfirst<Packet4f>(const Packet4f& a) {
 1609|      0|  return _mm_cvtss_f32(a);
 1610|      0|}
 1611|       |template <>
 1612|      0|EIGEN_STRONG_INLINE double pfirst<Packet2d>(const Packet2d& a) {
 1613|      0|  return _mm_cvtsd_f64(a);
 1614|      0|}
 1615|       |template <>
 1616|      0|EIGEN_STRONG_INLINE int64_t pfirst<Packet2l>(const Packet2l& a) {
 1617|      0|  return _mm_extract_epi64_0(a);
 1618|      0|}
 1619|       |template <>
 1620|      0|EIGEN_STRONG_INLINE int pfirst<Packet4i>(const Packet4i& a) {
 1621|      0|  return _mm_cvtsi128_si32(a);
 1622|      0|}
 1623|       |template <>
 1624|      0|EIGEN_STRONG_INLINE uint32_t pfirst<Packet4ui>(const Packet4ui& a) {
 1625|      0|  return numext::bit_cast<uint32_t>(_mm_cvtsi128_si32(a));
 1626|      0|}
 1627|       |#endif
 1628|       |template <>
 1629|      0|EIGEN_STRONG_INLINE bool pfirst<Packet16b>(const Packet16b& a) {
 1630|      0|  int x = _mm_cvtsi128_si32(a);
 1631|      0|  return static_cast<bool>(x & 1);
 1632|      0|}
 1633|       |
 1634|       |template <>
 1635|      0|EIGEN_STRONG_INLINE Packet4f pgather<float, Packet4f>(const float* from, Index stride) {
 1636|      0|  return _mm_set_ps(from[3 * stride], from[2 * stride], from[1 * stride], from[0 * stride]);
 1637|      0|}
 1638|       |template <>
 1639|      0|EIGEN_STRONG_INLINE Packet2d pgather<double, Packet2d>(const double* from, Index stride) {
 1640|      0|  return _mm_set_pd(from[1 * stride], from[0 * stride]);
 1641|      0|}
 1642|       |template <>
 1643|      0|EIGEN_STRONG_INLINE Packet2l pgather<int64_t, Packet2l>(const int64_t* from, Index stride) {
 1644|      0|  return _mm_set_epi64x(from[1 * stride], from[0 * stride]);
 1645|      0|}
 1646|       |template <>
 1647|      0|EIGEN_STRONG_INLINE Packet4i pgather<int, Packet4i>(const int* from, Index stride) {
 1648|      0|  return _mm_set_epi32(from[3 * stride], from[2 * stride], from[1 * stride], from[0 * stride]);
 1649|      0|}
 1650|       |template <>
 1651|      0|EIGEN_STRONG_INLINE Packet4ui pgather<uint32_t, Packet4ui>(const uint32_t* from, Index stride) {
 1652|      0|  return _mm_set_epi32(numext::bit_cast<int32_t>(from[3 * stride]), numext::bit_cast<int32_t>(from[2 * stride]),
 1653|      0|                       numext::bit_cast<int32_t>(from[1 * stride]), numext::bit_cast<int32_t>(from[0 * stride]));
 1654|      0|}
 1655|       |
 1656|       |template <>
 1657|      0|EIGEN_STRONG_INLINE Packet16b pgather<bool, Packet16b>(const bool* from, Index stride) {
 1658|      0|  return _mm_set_epi8(from[15 * stride], from[14 * stride], from[13 * stride], from[12 * stride], from[11 * stride],
 1659|      0|                      from[10 * stride], from[9 * stride], from[8 * stride], from[7 * stride], from[6 * stride],
 1660|      0|                      from[5 * stride], from[4 * stride], from[3 * stride], from[2 * stride], from[1 * stride],
 1661|      0|                      from[0 * stride]);
 1662|      0|}
 1663|       |
 1664|       |template <>
 1665|      0|EIGEN_STRONG_INLINE void pscatter<float, Packet4f>(float* to, const Packet4f& from, Index stride) {
 1666|      0|  to[stride * 0] = pfirst(from);
 1667|      0|  to[stride * 1] = pfirst(_mm_shuffle_ps(from, from, 1));
 1668|      0|  to[stride * 2] = pfirst(_mm_shuffle_ps(from, from, 2));
 1669|      0|  to[stride * 3] = pfirst(_mm_shuffle_ps(from, from, 3));
 1670|      0|}
 1671|       |template <>
 1672|      0|EIGEN_STRONG_INLINE void pscatter<double, Packet2d>(double* to, const Packet2d& from, Index stride) {
 1673|      0|  to[stride * 0] = pfirst(from);
 1674|      0|  to[stride * 1] = pfirst(preverse(from));
 1675|      0|}
 1676|       |template <>
 1677|      0|EIGEN_STRONG_INLINE void pscatter<int64_t, Packet2l>(int64_t* to, const Packet2l& from, Index stride) {
 1678|      0|  to[stride * 0] = pfirst(from);
 1679|      0|  to[stride * 1] = pfirst(preverse(from));
 1680|      0|}
 1681|       |template <>
 1682|      0|EIGEN_STRONG_INLINE void pscatter<int, Packet4i>(int* to, const Packet4i& from, Index stride) {
 1683|      0|  to[stride * 0] = _mm_cvtsi128_si32(from);
 1684|      0|  to[stride * 1] = _mm_cvtsi128_si32(_mm_shuffle_epi32(from, 1));
 1685|      0|  to[stride * 2] = _mm_cvtsi128_si32(_mm_shuffle_epi32(from, 2));
 1686|      0|  to[stride * 3] = _mm_cvtsi128_si32(_mm_shuffle_epi32(from, 3));
 1687|      0|}
 1688|       |template <>
 1689|      0|EIGEN_STRONG_INLINE void pscatter<uint32_t, Packet4ui>(uint32_t* to, const Packet4ui& from, Index stride) {
 1690|      0|  to[stride * 0] = numext::bit_cast<uint32_t>(_mm_cvtsi128_si32(from));
 1691|      0|  to[stride * 1] = numext::bit_cast<uint32_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(from, 1)));
 1692|      0|  to[stride * 2] = numext::bit_cast<uint32_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(from, 2)));
 1693|      0|  to[stride * 3] = numext::bit_cast<uint32_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(from, 3)));
 1694|      0|}
 1695|       |template <>
 1696|      0|EIGEN_STRONG_INLINE void pscatter<bool, Packet16b>(bool* to, const Packet16b& from, Index stride) {
 1697|      0|  to[4 * stride * 0] = _mm_cvtsi128_si32(from);
 1698|      0|  to[4 * stride * 1] = _mm_cvtsi128_si32(_mm_shuffle_epi32(from, 1));
 1699|      0|  to[4 * stride * 2] = _mm_cvtsi128_si32(_mm_shuffle_epi32(from, 2));
 1700|      0|  to[4 * stride * 3] = _mm_cvtsi128_si32(_mm_shuffle_epi32(from, 3));
 1701|      0|}
 1702|       |
 1703|       |// some compilers might be tempted to perform multiple moves instead of using a vector path.
 1704|       |template <>
 1705|      0|EIGEN_STRONG_INLINE void pstore1<Packet4f>(float* to, const float& a) {
 1706|      0|  Packet4f pa = _mm_set_ss(a);
 1707|      0|  pstore(to, Packet4f(vec4f_swizzle1(pa, 0, 0, 0, 0)));
 1708|      0|}
 1709|       |// some compilers might be tempted to perform multiple moves instead of using a vector path.
 1710|       |template <>
 1711|      0|EIGEN_STRONG_INLINE void pstore1<Packet2d>(double* to, const double& a) {
 1712|      0|  Packet2d pa = _mm_set_sd(a);
 1713|      0|  pstore(to, Packet2d(vec2d_swizzle1(pa, 0, 0)));
 1714|      0|}
 1715|       |
 1716|       |#if EIGEN_COMP_PGI && EIGEN_COMP_PGI < 1900
 1717|       |typedef const void* SsePrefetchPtrType;
 1718|       |#else
 1719|       |typedef const char* SsePrefetchPtrType;
 1720|       |#endif
 1721|       |
 1722|       |#ifndef EIGEN_VECTORIZE_AVX
 1723|       |template <>
 1724|      0|EIGEN_STRONG_INLINE void prefetch<float>(const float* addr) {
 1725|      0|  _mm_prefetch((SsePrefetchPtrType)(addr), _MM_HINT_T0);
 1726|      0|}
 1727|       |template <>
 1728|      0|EIGEN_STRONG_INLINE void prefetch<double>(const double* addr) {
 1729|      0|  _mm_prefetch((SsePrefetchPtrType)(addr), _MM_HINT_T0);
 1730|      0|}
 1731|       |template <>
 1732|      0|EIGEN_STRONG_INLINE void prefetch<int>(const int* addr) {
 1733|      0|  _mm_prefetch((SsePrefetchPtrType)(addr), _MM_HINT_T0);
 1734|      0|}
 1735|       |template <>
 1736|      0|EIGEN_STRONG_INLINE void prefetch<int64_t>(const int64_t* addr) {
 1737|      0|  _mm_prefetch((SsePrefetchPtrType)(addr), _MM_HINT_T0);
 1738|      0|}
 1739|       |template <>
 1740|      0|EIGEN_STRONG_INLINE void prefetch<uint32_t>(const uint32_t* addr) {
 1741|      0|  _mm_prefetch((SsePrefetchPtrType)(addr), _MM_HINT_T0);
 1742|      0|}
 1743|       |#endif
 1744|       |
 1745|       |template <>
 1746|      0|EIGEN_STRONG_INLINE Packet4f pfrexp<Packet4f>(const Packet4f& a, Packet4f& exponent) {
 1747|      0|  return pfrexp_generic(a, exponent);
 1748|      0|}
 1749|       |
 1750|       |// Extract exponent without existence of Packet2l.
 1751|       |template <>
 1752|      0|EIGEN_STRONG_INLINE Packet2d pfrexp_generic_get_biased_exponent(const Packet2d& a) {
 1753|      0|  const Packet2d cst_exp_mask = pset1frombits<Packet2d>(static_cast<uint64_t>(0x7ff0000000000000ull));
 1754|      0|  __m128i a_expo = _mm_srli_epi64(_mm_castpd_si128(pand(a, cst_exp_mask)), 52);
 1755|      0|  return _mm_cvtepi32_pd(vec4i_swizzle1(a_expo, 0, 2, 1, 3));
 1756|      0|}
 1757|       |
 1758|       |template <>
 1759|      0|EIGEN_STRONG_INLINE Packet2d pfrexp<Packet2d>(const Packet2d& a, Packet2d& exponent) {
 1760|      0|  return pfrexp_generic(a, exponent);
 1761|      0|}
 1762|       |
 1763|       |template <>
 1764|      0|EIGEN_STRONG_INLINE Packet4f pldexp<Packet4f>(const Packet4f& a, const Packet4f& exponent) {
 1765|      0|  return pldexp_generic(a, exponent);
 1766|      0|}
 1767|       |
 1768|       |// We specialize pldexp here, since the generic implementation uses Packet2l, which is not well
 1769|       |// supported by SSE, and has more range than is needed for exponents.
 1770|       |template <>
 1771|      0|EIGEN_STRONG_INLINE Packet2d pldexp<Packet2d>(const Packet2d& a, const Packet2d& exponent) {
 1772|      0|  // Clamp exponent to [-2099, 2099]
 1773|      0|  const Packet2d max_exponent = pset1<Packet2d>(2099.0);
 1774|      0|  const Packet2d e = pmin(pmax(exponent, pnegate(max_exponent)), max_exponent);
 1775|      0|
 1776|      0|  // Convert e to integer and swizzle to low-order bits.
 1777|      0|  const Packet4i ei = vec4i_swizzle1(_mm_cvtpd_epi32(e), 0, 3, 1, 3);
 1778|      0|
 1779|      0|  // Split 2^e into four factors and multiply:
 1780|      0|  const Packet4i bias = _mm_set_epi32(0, 1023, 0, 1023);
 1781|      0|  Packet4i b = parithmetic_shift_right<2>(ei);                       // floor(e/4)
 1782|      0|  Packet2d c = _mm_castsi128_pd(_mm_slli_epi64(padd(b, bias), 52));  // 2^b
 1783|      0|  Packet2d out = pmul(pmul(pmul(a, c), c), c);                       // a * 2^(3b)
 1784|      0|  b = psub(psub(psub(ei, b), b), b);                                 // e - 3b
 1785|      0|  c = _mm_castsi128_pd(_mm_slli_epi64(padd(b, bias), 52));           // 2^(e - 3b)
 1786|      0|  out = pmul(out, c);                                                // a * 2^e
 1787|      0|  return out;
 1788|      0|}
 1789|       |
 1790|       |// We specialize pldexp here, since the generic implementation uses Packet2l, which is not well
 1791|       |// supported by SSE, and has more range than is needed for exponents.
 1792|       |template <>
 1793|      0|EIGEN_STRONG_INLINE Packet2d pldexp_fast<Packet2d>(const Packet2d& a, const Packet2d& exponent) {
 1794|      0|  // Clamp exponent to [-1023, 1024]
 1795|      0|  const Packet2d min_exponent = pset1<Packet2d>(-1023.0);
 1796|      0|  const Packet2d max_exponent = pset1<Packet2d>(1024.0);
 1797|      0|  const Packet2d e = pmin(pmax(exponent, min_exponent), max_exponent);
 1798|      0|
 1799|      0|  // Convert e to integer and swizzle to low-order bits.
 1800|      0|  const Packet4i ei = vec4i_swizzle1(_mm_cvtpd_epi32(e), 0, 3, 1, 3);
 1801|      0|
 1802|      0|  // Compute 2^e multiply:
 1803|      0|  const Packet4i bias = _mm_set_epi32(0, 1023, 0, 1023);
 1804|      0|  const Packet2d c = _mm_castsi128_pd(_mm_slli_epi64(padd(ei, bias), 52));  // 2^e
 1805|      0|  return pmul(a, c);
 1806|      0|}
 1807|       |
 1808|       |// with AVX, the default implementations based on pload1 are faster
 1809|       |#ifndef __AVX__
 1810|       |template <>
 1811|      0|EIGEN_STRONG_INLINE void pbroadcast4<Packet4f>(const float* a, Packet4f& a0, Packet4f& a1, Packet4f& a2, Packet4f& a3) {
 1812|      0|  a3 = pload<Packet4f>(a);
 1813|      0|  a0 = vec4f_swizzle1(a3, 0, 0, 0, 0);
 1814|      0|  a1 = vec4f_swizzle1(a3, 1, 1, 1, 1);
 1815|      0|  a2 = vec4f_swizzle1(a3, 2, 2, 2, 2);
 1816|      0|  a3 = vec4f_swizzle1(a3, 3, 3, 3, 3);
 1817|      0|}
 1818|       |template <>
 1819|       |EIGEN_STRONG_INLINE void pbroadcast4<Packet2d>(const double* a, Packet2d& a0, Packet2d& a1, Packet2d& a2,
 1820|      0|                                               Packet2d& a3) {
 1821|      0|#ifdef EIGEN_VECTORIZE_SSE3
 1822|      0|  a0 = _mm_loaddup_pd(a + 0);
 1823|      0|  a1 = _mm_loaddup_pd(a + 1);
 1824|      0|  a2 = _mm_loaddup_pd(a + 2);
 1825|      0|  a3 = _mm_loaddup_pd(a + 3);
 1826|      0|#else
 1827|      0|  a1 = pload<Packet2d>(a);
 1828|      0|  a0 = vec2d_swizzle1(a1, 0, 0);
 1829|      0|  a1 = vec2d_swizzle1(a1, 1, 1);
 1830|      0|  a3 = pload<Packet2d>(a + 2);
 1831|      0|  a2 = vec2d_swizzle1(a3, 0, 0);
 1832|      0|  a3 = vec2d_swizzle1(a3, 1, 1);
 1833|      0|#endif
 1834|      0|}
 1835|       |#endif
 1836|       |
 1837|      0|EIGEN_STRONG_INLINE void punpackp(Packet4f* vecs) {
 1838|      0|  vecs[1] = _mm_castsi128_ps(_mm_shuffle_epi32(_mm_castps_si128(vecs[0]), 0x55));
 1839|      0|  vecs[2] = _mm_castsi128_ps(_mm_shuffle_epi32(_mm_castps_si128(vecs[0]), 0xAA));
 1840|      0|  vecs[3] = _mm_castsi128_ps(_mm_shuffle_epi32(_mm_castps_si128(vecs[0]), 0xFF));
 1841|      0|  vecs[0] = _mm_castsi128_ps(_mm_shuffle_epi32(_mm_castps_si128(vecs[0]), 0x00));
 1842|      0|}
 1843|       |
 1844|       |template <>
 1845|      0|EIGEN_STRONG_INLINE float predux<Packet4f>(const Packet4f& a) {
 1846|      0|  // Disable SSE3 _mm_hadd_pd that is extremely slow on all existing Intel's architectures
 1847|      0|  // (from Nehalem to Haswell)
 1848|      0|  // #ifdef EIGEN_VECTORIZE_SSE3
 1849|      0|  //   Packet4f tmp = _mm_add_ps(a, vec4f_swizzle1(a,2,3,2,3));
 1850|      0|  //   return pfirst<Packet4f>(_mm_hadd_ps(tmp, tmp));
 1851|      0|  // #else
 1852|      0|  Packet4f tmp = _mm_add_ps(a, _mm_movehl_ps(a, a));
 1853|      0|  return pfirst<Packet4f>(_mm_add_ss(tmp, _mm_shuffle_ps(tmp, tmp, 1)));
 1854|      0|  // #endif
 1855|      0|}
 1856|       |
 1857|       |template <>
 1858|      0|EIGEN_STRONG_INLINE double predux<Packet2d>(const Packet2d& a) {
 1859|      0|  // Disable SSE3 _mm_hadd_pd that is extremely slow on all existing Intel's architectures
 1860|      0|  // (from Nehalem to Haswell)
 1861|      0|  // #ifdef EIGEN_VECTORIZE_SSE3
 1862|      0|  //   return pfirst<Packet2d>(_mm_hadd_pd(a, a));
 1863|      0|  // #else
 1864|      0|  return pfirst<Packet2d>(_mm_add_sd(a, _mm_unpackhi_pd(a, a)));
 1865|      0|  // #endif
 1866|      0|}
 1867|       |
 1868|       |template <>
 1869|      0|EIGEN_STRONG_INLINE int64_t predux<Packet2l>(const Packet2l& a) {
 1870|      0|  return pfirst<Packet2l>(_mm_add_epi64(a, _mm_unpackhi_epi64(a, a)));
 1871|      0|}
 1872|       |
 1873|       |#ifdef EIGEN_VECTORIZE_SSSE3
 1874|       |template <>
 1875|       |EIGEN_STRONG_INLINE int predux<Packet4i>(const Packet4i& a) {
 1876|       |  Packet4i tmp0 = _mm_hadd_epi32(a, a);
 1877|       |  return pfirst<Packet4i>(_mm_hadd_epi32(tmp0, tmp0));
 1878|       |}
 1879|       |template <>
 1880|       |EIGEN_STRONG_INLINE uint32_t predux<Packet4ui>(const Packet4ui& a) {
 1881|       |  Packet4ui tmp0 = _mm_hadd_epi32(a, a);
 1882|       |  return pfirst<Packet4ui>(_mm_hadd_epi32(tmp0, tmp0));
 1883|       |}
 1884|       |#else
 1885|       |template <>
 1886|      0|EIGEN_STRONG_INLINE int predux<Packet4i>(const Packet4i& a) {
 1887|      0|  Packet4i tmp = _mm_add_epi32(a, _mm_unpackhi_epi64(a, a));
 1888|      0|  return pfirst(tmp) + pfirst<Packet4i>(_mm_shuffle_epi32(tmp, 1));
 1889|      0|}
 1890|       |template <>
 1891|      0|EIGEN_STRONG_INLINE uint32_t predux<Packet4ui>(const Packet4ui& a) {
 1892|      0|  Packet4ui tmp = _mm_add_epi32(a, _mm_unpackhi_epi64(a, a));
 1893|      0|  return pfirst(tmp) + pfirst<Packet4ui>(_mm_shuffle_epi32(tmp, 1));
 1894|      0|}
 1895|       |#endif
 1896|       |
 1897|       |template <>
 1898|      0|EIGEN_STRONG_INLINE bool predux<Packet16b>(const Packet16b& a) {
 1899|      0|  Packet4i tmp = _mm_or_si128(a, _mm_unpackhi_epi64(a, a));
 1900|      0|  return (pfirst(tmp) != 0) || (pfirst<Packet4i>(_mm_shuffle_epi32(tmp, 1)) != 0);
 1901|      0|}
 1902|       |
 1903|       |// Other reduction functions:
 1904|       |
 1905|       |// mul
 1906|       |template <>
 1907|      0|EIGEN_STRONG_INLINE float predux_mul<Packet4f>(const Packet4f& a) {
 1908|      0|  Packet4f tmp = _mm_mul_ps(a, _mm_movehl_ps(a, a));
 1909|      0|  return pfirst<Packet4f>(_mm_mul_ss(tmp, _mm_shuffle_ps(tmp, tmp, 1)));
 1910|      0|}
 1911|       |template <>
 1912|      0|EIGEN_STRONG_INLINE double predux_mul<Packet2d>(const Packet2d& a) {
 1913|      0|  return pfirst<Packet2d>(_mm_mul_sd(a, _mm_unpackhi_pd(a, a)));
 1914|      0|}
 1915|       |template <>
 1916|      0|EIGEN_STRONG_INLINE int64_t predux_mul<Packet2l>(const Packet2l& a) {
 1917|      0|  EIGEN_ALIGN16 int64_t aux[2];
 1918|      0|  pstore(aux, a);
 1919|      0|  return aux[0] * aux[1];
 1920|      0|}
 1921|       |template <>
 1922|      0|EIGEN_STRONG_INLINE int predux_mul<Packet4i>(const Packet4i& a) {
 1923|      0|  // after some experiments, it is seems this is the fastest way to implement it
 1924|      0|  // for GCC (e.g., reusing pmul is very slow!)
 1925|      0|  // TODO try to call _mm_mul_epu32 directly
 1926|      0|  EIGEN_ALIGN16 int aux[4];
 1927|      0|  pstore(aux, a);
 1928|      0|  return (aux[0] * aux[1]) * (aux[2] * aux[3]);
 1929|      0|}
 1930|       |template <>
 1931|      0|EIGEN_STRONG_INLINE uint32_t predux_mul<Packet4ui>(const Packet4ui& a) {
 1932|      0|  // after some experiments, it is seems this is the fastest way to implement it
 1933|      0|  // for GCC (eg., reusing pmul is very slow !)
 1934|      0|  // TODO try to call _mm_mul_epu32 directly
 1935|      0|  EIGEN_ALIGN16 uint32_t aux[4];
 1936|      0|  pstore(aux, a);
 1937|      0|  return (aux[0] * aux[1]) * (aux[2] * aux[3]);
 1938|      0|}
 1939|       |
 1940|       |template <>
 1941|      0|EIGEN_STRONG_INLINE bool predux_mul<Packet16b>(const Packet16b& a) {
 1942|      0|  Packet4i tmp = _mm_and_si128(a, _mm_unpackhi_epi64(a, a));
 1943|      0|  return ((pfirst<Packet4i>(tmp) == 0x01010101) && (pfirst<Packet4i>(_mm_shuffle_epi32(tmp, 1)) == 0x01010101));
 1944|      0|}
 1945|       |
 1946|       |// min
 1947|       |template <>
 1948|      0|EIGEN_STRONG_INLINE float predux_min<Packet4f>(const Packet4f& a) {
 1949|      0|  Packet4f tmp = _mm_min_ps(a, _mm_movehl_ps(a, a));
 1950|      0|  return pfirst<Packet4f>(_mm_min_ss(tmp, _mm_shuffle_ps(tmp, tmp, 1)));
 1951|      0|}
 1952|       |template <>
 1953|      0|EIGEN_STRONG_INLINE double predux_min<Packet2d>(const Packet2d& a) {
 1954|      0|  return pfirst<Packet2d>(_mm_min_sd(a, _mm_unpackhi_pd(a, a)));
 1955|      0|}
 1956|       |template <>
 1957|      0|EIGEN_STRONG_INLINE int predux_min<Packet4i>(const Packet4i& a) {
 1958|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1959|      0|  Packet4i tmp = _mm_min_epi32(a, _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 3, 2)));
 1960|      0|  return pfirst<Packet4i>(_mm_min_epi32(tmp, _mm_shuffle_epi32(tmp, 1)));
 1961|      0|#else
 1962|      0|  // after some experiments, it is seems this is the fastest way to implement it
 1963|      0|  // for GCC (eg., it does not like using std::min after the pstore !!)
 1964|      0|  EIGEN_ALIGN16 int aux[4];
 1965|      0|  pstore(aux, a);
 1966|      0|  int aux0 = aux[0] < aux[1] ? aux[0] : aux[1];
 1967|      0|  int aux2 = aux[2] < aux[3] ? aux[2] : aux[3];
 1968|      0|  return aux0 < aux2 ? aux0 : aux2;
 1969|      0|#endif  // EIGEN_VECTORIZE_SSE4_1
 1970|      0|}
 1971|       |template <>
 1972|      0|EIGEN_STRONG_INLINE uint32_t predux_min<Packet4ui>(const Packet4ui& a) {
 1973|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 1974|      0|  Packet4ui tmp = _mm_min_epu32(a, _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 3, 2)));
 1975|      0|  return pfirst<Packet4ui>(_mm_min_epu32(tmp, _mm_shuffle_epi32(tmp, 1)));
 1976|      0|#else
 1977|      0|  // after some experiments, it is seems this is the fastest way to implement it
 1978|      0|  // for GCC (eg., it does not like using std::min after the pstore !!)
 1979|      0|  EIGEN_ALIGN16 uint32_t aux[4];
 1980|      0|  pstore(aux, a);
 1981|      0|  uint32_t aux0 = aux[0] < aux[1] ? aux[0] : aux[1];
 1982|      0|  uint32_t aux2 = aux[2] < aux[3] ? aux[2] : aux[3];
 1983|      0|  return aux0 < aux2 ? aux0 : aux2;
 1984|      0|#endif  // EIGEN_VECTORIZE_SSE4_1
 1985|      0|}
 1986|       |
 1987|       |// max
 1988|       |template <>
 1989|      0|EIGEN_STRONG_INLINE float predux_max<Packet4f>(const Packet4f& a) {
 1990|      0|  Packet4f tmp = _mm_max_ps(a, _mm_movehl_ps(a, a));
 1991|      0|  return pfirst<Packet4f>(_mm_max_ss(tmp, _mm_shuffle_ps(tmp, tmp, 1)));
 1992|      0|}
 1993|       |template <>
 1994|      0|EIGEN_STRONG_INLINE double predux_max<Packet2d>(const Packet2d& a) {
 1995|      0|  return pfirst<Packet2d>(_mm_max_sd(a, _mm_unpackhi_pd(a, a)));
 1996|      0|}
 1997|       |template <>
 1998|      0|EIGEN_STRONG_INLINE int predux_max<Packet4i>(const Packet4i& a) {
 1999|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 2000|      0|  Packet4i tmp = _mm_max_epi32(a, _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 3, 2)));
 2001|      0|  return pfirst<Packet4i>(_mm_max_epi32(tmp, _mm_shuffle_epi32(tmp, 1)));
 2002|      0|#else
 2003|      0|  // after some experiments, it is seems this is the fastest way to implement it
 2004|      0|  // for GCC (eg., it does not like using std::min after the pstore !!)
 2005|      0|  EIGEN_ALIGN16 int aux[4];
 2006|      0|  pstore(aux, a);
 2007|      0|  int aux0 = aux[0] > aux[1] ? aux[0] : aux[1];
 2008|      0|  int aux2 = aux[2] > aux[3] ? aux[2] : aux[3];
 2009|      0|  return aux0 > aux2 ? aux0 : aux2;
 2010|      0|#endif  // EIGEN_VECTORIZE_SSE4_1
 2011|      0|}
 2012|       |template <>
 2013|      0|EIGEN_STRONG_INLINE uint32_t predux_max<Packet4ui>(const Packet4ui& a) {
 2014|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
 2015|      0|  Packet4ui tmp = _mm_max_epu32(a, _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 3, 2)));
 2016|      0|  return pfirst<Packet4ui>(_mm_max_epu32(tmp, _mm_shuffle_epi32(tmp, 1)));
 2017|      0|#else
 2018|      0|  // after some experiments, it is seems this is the fastest way to implement it
 2019|      0|  // for GCC (eg., it does not like using std::min after the pstore !!)
 2020|      0|  EIGEN_ALIGN16 uint32_t aux[4];
 2021|      0|  pstore(aux, a);
 2022|      0|  uint32_t aux0 = aux[0] > aux[1] ? aux[0] : aux[1];
 2023|      0|  uint32_t aux2 = aux[2] > aux[3] ? aux[2] : aux[3];
 2024|      0|  return aux0 > aux2 ? aux0 : aux2;
 2025|      0|#endif  // EIGEN_VECTORIZE_SSE4_1
 2026|      0|}
 2027|       |
 2028|       |// not needed yet
 2029|       |// template<> EIGEN_STRONG_INLINE bool predux_all(const Packet4f& x)
 2030|       |// {
 2031|       |//   return _mm_movemask_ps(x) == 0xF;
 2032|       |// }
 2033|       |
 2034|       |template <>
 2035|      0|EIGEN_STRONG_INLINE bool predux_any(const Packet2d& x) {
 2036|      0|  return _mm_movemask_pd(x) != 0x0;
 2037|      0|}
 2038|       |
 2039|       |template <>
 2040|      0|EIGEN_STRONG_INLINE bool predux_any(const Packet4f& x) {
 2041|      0|  return _mm_movemask_ps(x) != 0x0;
 2042|      0|}
 2043|       |
 2044|       |template <>
 2045|      0|EIGEN_STRONG_INLINE bool predux_any(const Packet2l& x) {
 2046|      0|  return _mm_movemask_pd(_mm_castsi128_pd(x)) != 0x0;
 2047|      0|}
 2048|       |
 2049|       |template <>
 2050|      0|EIGEN_STRONG_INLINE bool predux_any(const Packet4i& x) {
 2051|      0|  return _mm_movemask_ps(_mm_castsi128_ps(x)) != 0x0;
 2052|      0|}
 2053|       |template <>
 2054|      0|EIGEN_STRONG_INLINE bool predux_any(const Packet4ui& x) {
 2055|      0|  return _mm_movemask_ps(_mm_castsi128_ps(x)) != 0x0;
 2056|      0|}
 2057|       |
 2058|      0|EIGEN_STRONG_INLINE void ptranspose(PacketBlock<Packet4f, 4>& kernel) {
 2059|      0|  _MM_TRANSPOSE4_PS(kernel.packet[0], kernel.packet[1], kernel.packet[2], kernel.packet[3]);
 2060|      0|}
 2061|       |
 2062|      0|EIGEN_STRONG_INLINE void ptranspose(PacketBlock<Packet2d, 2>& kernel) {
 2063|      0|  __m128d tmp = _mm_unpackhi_pd(kernel.packet[0], kernel.packet[1]);
 2064|      0|  kernel.packet[0] = _mm_unpacklo_pd(kernel.packet[0], kernel.packet[1]);
 2065|      0|  kernel.packet[1] = tmp;
 2066|      0|}
 2067|       |
 2068|      0|EIGEN_STRONG_INLINE void ptranspose(PacketBlock<Packet2l, 2>& kernel) {
 2069|      0|  __m128i tmp = _mm_unpackhi_epi64(kernel.packet[0], kernel.packet[1]);
 2070|      0|  kernel.packet[0] = _mm_unpacklo_epi64(kernel.packet[0], kernel.packet[1]);
 2071|      0|  kernel.packet[1] = tmp;
 2072|      0|}
 2073|       |
 2074|      0|EIGEN_STRONG_INLINE void ptranspose(PacketBlock<Packet4i, 4>& kernel) {
 2075|      0|  __m128i T0 = _mm_unpacklo_epi32(kernel.packet[0], kernel.packet[1]);
 2076|      0|  __m128i T1 = _mm_unpacklo_epi32(kernel.packet[2], kernel.packet[3]);
 2077|      0|  __m128i T2 = _mm_unpackhi_epi32(kernel.packet[0], kernel.packet[1]);
 2078|      0|  __m128i T3 = _mm_unpackhi_epi32(kernel.packet[2], kernel.packet[3]);
 2079|      0|
 2080|      0|  kernel.packet[0] = _mm_unpacklo_epi64(T0, T1);
 2081|      0|  kernel.packet[1] = _mm_unpackhi_epi64(T0, T1);
 2082|      0|  kernel.packet[2] = _mm_unpacklo_epi64(T2, T3);
 2083|      0|  kernel.packet[3] = _mm_unpackhi_epi64(T2, T3);
 2084|      0|}
 2085|      0|EIGEN_STRONG_INLINE void ptranspose(PacketBlock<Packet4ui, 4>& kernel) {
 2086|      0|  ptranspose((PacketBlock<Packet4i, 4>&)kernel);
 2087|      0|}
 2088|       |
 2089|      0|EIGEN_STRONG_INLINE void ptranspose(PacketBlock<Packet16b, 4>& kernel) {
 2090|      0|  __m128i T0 = _mm_unpacklo_epi8(kernel.packet[0], kernel.packet[1]);
 2091|      0|  __m128i T1 = _mm_unpackhi_epi8(kernel.packet[0], kernel.packet[1]);
 2092|      0|  __m128i T2 = _mm_unpacklo_epi8(kernel.packet[2], kernel.packet[3]);
 2093|      0|  __m128i T3 = _mm_unpackhi_epi8(kernel.packet[2], kernel.packet[3]);
 2094|      0|  kernel.packet[0] = _mm_unpacklo_epi16(T0, T2);
 2095|      0|  kernel.packet[1] = _mm_unpackhi_epi16(T0, T2);
 2096|      0|  kernel.packet[2] = _mm_unpacklo_epi16(T1, T3);
 2097|      0|  kernel.packet[3] = _mm_unpackhi_epi16(T1, T3);
 2098|      0|}
 2099|       |
 2100|      0|EIGEN_STRONG_INLINE void ptranspose(PacketBlock<Packet16b, 16>& kernel) {
 2101|      0|  // If we number the elements in the input thus:
 2102|      0|  // kernel.packet[ 0] = {00, 01, 02, 03, 04, 05, 06, 07, 08, 09, 0a, 0b, 0c, 0d, 0e, 0f}
 2103|      0|  // kernel.packet[ 1] = {10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1a, 1b, 1c, 1d, 1e, 1f}
 2104|      0|  // ...
 2105|      0|  // kernel.packet[15] = {f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, fa, fb, fc, fd, fe, ff},
 2106|      0|  //
 2107|      0|  // the desired output is:
 2108|      0|  // kernel.packet[ 0] = {00, 10, 20, 30, 40, 50, 60, 70, 80, 90, a0, b0, c0, d0, e0, f0}
 2109|      0|  // kernel.packet[ 1] = {01, 11, 21, 31, 41, 51, 61, 71, 81, 91, a1, b1, c1, d1, e1, f1}
 2110|      0|  // ...
 2111|      0|  // kernel.packet[15] = {0f, 1f, 2f, 3f, 4f, 5f, 6f, 7f, 8f, 9f, af, bf, cf, df, ef, ff},
 2112|      0|  __m128i t0 =
 2113|      0|      _mm_unpacklo_epi8(kernel.packet[0], kernel.packet[1]);  // 00 10 01 11 02 12 03 13 04 14 05 15 06 16 07 17
 2114|      0|  __m128i t1 =
 2115|      0|      _mm_unpackhi_epi8(kernel.packet[0], kernel.packet[1]);  // 08 18 09 19 0a 1a 0b 1b 0c 1c 0d 1d 0e 1e 0f 1f
 2116|      0|  __m128i t2 =
 2117|      0|      _mm_unpacklo_epi8(kernel.packet[2], kernel.packet[3]);  // 20 30 21 31 22 32 ...                     27 37
 2118|      0|  __m128i t3 =
 2119|      0|      _mm_unpackhi_epi8(kernel.packet[2], kernel.packet[3]);  // 28 38 29 39 2a 3a ...                     2f 3f
 2120|      0|  __m128i t4 =
 2121|      0|      _mm_unpacklo_epi8(kernel.packet[4], kernel.packet[5]);  // 40 50 41 51 42 52                         47 57
 2122|      0|  __m128i t5 = _mm_unpackhi_epi8(kernel.packet[4], kernel.packet[5]);  // 48 58 49 59 4a 5a
 2123|      0|  __m128i t6 = _mm_unpacklo_epi8(kernel.packet[6], kernel.packet[7]);
 2124|      0|  __m128i t7 = _mm_unpackhi_epi8(kernel.packet[6], kernel.packet[7]);
 2125|      0|  __m128i t8 = _mm_unpacklo_epi8(kernel.packet[8], kernel.packet[9]);
 2126|      0|  __m128i t9 = _mm_unpackhi_epi8(kernel.packet[8], kernel.packet[9]);
 2127|      0|  __m128i ta = _mm_unpacklo_epi8(kernel.packet[10], kernel.packet[11]);
 2128|      0|  __m128i tb = _mm_unpackhi_epi8(kernel.packet[10], kernel.packet[11]);
 2129|      0|  __m128i tc = _mm_unpacklo_epi8(kernel.packet[12], kernel.packet[13]);
 2130|      0|  __m128i td = _mm_unpackhi_epi8(kernel.packet[12], kernel.packet[13]);
 2131|      0|  __m128i te = _mm_unpacklo_epi8(kernel.packet[14], kernel.packet[15]);
 2132|      0|  __m128i tf = _mm_unpackhi_epi8(kernel.packet[14], kernel.packet[15]);
 2133|      0|
 2134|      0|  __m128i s0 = _mm_unpacklo_epi16(t0, t2);  // 00 10 20 30 01 11 21 31 02 12 22 32 03 13 23 33
 2135|      0|  __m128i s1 = _mm_unpackhi_epi16(t0, t2);  // 04 14 24 34
 2136|      0|  __m128i s2 = _mm_unpacklo_epi16(t1, t3);  // 08 18 28 38 ...
 2137|      0|  __m128i s3 = _mm_unpackhi_epi16(t1, t3);  // 0c 1c 2c 3c ...
 2138|      0|  __m128i s4 = _mm_unpacklo_epi16(t4, t6);  // 40 50 60 70 41 51 61 71 42 52 62 72 43 53 63 73
 2139|      0|  __m128i s5 = _mm_unpackhi_epi16(t4, t6);  // 44 54 64 74 ...
 2140|      0|  __m128i s6 = _mm_unpacklo_epi16(t5, t7);
 2141|      0|  __m128i s7 = _mm_unpackhi_epi16(t5, t7);
 2142|      0|  __m128i s8 = _mm_unpacklo_epi16(t8, ta);
 2143|      0|  __m128i s9 = _mm_unpackhi_epi16(t8, ta);
 2144|      0|  __m128i sa = _mm_unpacklo_epi16(t9, tb);
 2145|      0|  __m128i sb = _mm_unpackhi_epi16(t9, tb);
 2146|      0|  __m128i sc = _mm_unpacklo_epi16(tc, te);
 2147|      0|  __m128i sd = _mm_unpackhi_epi16(tc, te);
 2148|      0|  __m128i se = _mm_unpacklo_epi16(td, tf);
 2149|      0|  __m128i sf = _mm_unpackhi_epi16(td, tf);
 2150|      0|
 2151|      0|  __m128i u0 = _mm_unpacklo_epi32(s0, s4);  // 00 10 20 30 40 50 60 70 01 11 21 31 41 51 61 71
 2152|      0|  __m128i u1 = _mm_unpackhi_epi32(s0, s4);  // 02 12 22 32 42 52 62 72 03 13 23 33 43 53 63 73
 2153|      0|  __m128i u2 = _mm_unpacklo_epi32(s1, s5);
 2154|      0|  __m128i u3 = _mm_unpackhi_epi32(s1, s5);
 2155|      0|  __m128i u4 = _mm_unpacklo_epi32(s2, s6);
 2156|      0|  __m128i u5 = _mm_unpackhi_epi32(s2, s6);
 2157|      0|  __m128i u6 = _mm_unpacklo_epi32(s3, s7);
 2158|      0|  __m128i u7 = _mm_unpackhi_epi32(s3, s7);
 2159|      0|  __m128i u8 = _mm_unpacklo_epi32(s8, sc);
 2160|      0|  __m128i u9 = _mm_unpackhi_epi32(s8, sc);
 2161|      0|  __m128i ua = _mm_unpacklo_epi32(s9, sd);
 2162|      0|  __m128i ub = _mm_unpackhi_epi32(s9, sd);
 2163|      0|  __m128i uc = _mm_unpacklo_epi32(sa, se);
 2164|      0|  __m128i ud = _mm_unpackhi_epi32(sa, se);
 2165|      0|  __m128i ue = _mm_unpacklo_epi32(sb, sf);
 2166|      0|  __m128i uf = _mm_unpackhi_epi32(sb, sf);
 2167|      0|
 2168|      0|  kernel.packet[0] = _mm_unpacklo_epi64(u0, u8);
 2169|      0|  kernel.packet[1] = _mm_unpackhi_epi64(u0, u8);
 2170|      0|  kernel.packet[2] = _mm_unpacklo_epi64(u1, u9);
 2171|      0|  kernel.packet[3] = _mm_unpackhi_epi64(u1, u9);
 2172|      0|  kernel.packet[4] = _mm_unpacklo_epi64(u2, ua);
 2173|      0|  kernel.packet[5] = _mm_unpackhi_epi64(u2, ua);
 2174|      0|  kernel.packet[6] = _mm_unpacklo_epi64(u3, ub);
 2175|      0|  kernel.packet[7] = _mm_unpackhi_epi64(u3, ub);
 2176|      0|  kernel.packet[8] = _mm_unpacklo_epi64(u4, uc);
 2177|      0|  kernel.packet[9] = _mm_unpackhi_epi64(u4, uc);
 2178|      0|  kernel.packet[10] = _mm_unpacklo_epi64(u5, ud);
 2179|      0|  kernel.packet[11] = _mm_unpackhi_epi64(u5, ud);
 2180|      0|  kernel.packet[12] = _mm_unpacklo_epi64(u6, ue);
 2181|      0|  kernel.packet[13] = _mm_unpackhi_epi64(u6, ue);
 2182|      0|  kernel.packet[14] = _mm_unpacklo_epi64(u7, uf);
 2183|      0|  kernel.packet[15] = _mm_unpackhi_epi64(u7, uf);
 2184|      0|}
 2185|       |
 2186|      0|EIGEN_STRONG_INLINE __m128i sse_blend_mask(const Selector<2>& ifPacket) {
 2187|      0|  return _mm_set_epi64x(0 - ifPacket.select[1], 0 - ifPacket.select[0]);
 2188|      0|}
 2189|       |
 2190|      0|EIGEN_STRONG_INLINE __m128i sse_blend_mask(const Selector<4>& ifPacket) {
 2191|      0|  return _mm_set_epi32(0 - ifPacket.select[3], 0 - ifPacket.select[2], 0 - ifPacket.select[1], 0 - ifPacket.select[0]);
 2192|      0|}
 2193|       |
 2194|       |template <>
 2195|       |EIGEN_STRONG_INLINE Packet2l pblend(const Selector<2>& ifPacket, const Packet2l& thenPacket,
 2196|      0|                                    const Packet2l& elsePacket) {
 2197|      0|  const __m128i true_mask = sse_blend_mask(ifPacket);
 2198|      0|  return pselect<Packet2l>(true_mask, thenPacket, elsePacket);
 2199|      0|}
 2200|       |template <>
 2201|       |EIGEN_STRONG_INLINE Packet4i pblend(const Selector<4>& ifPacket, const Packet4i& thenPacket,
 2202|      0|                                    const Packet4i& elsePacket) {
 2203|      0|  const __m128i true_mask = sse_blend_mask(ifPacket);
 2204|      0|  return pselect<Packet4i>(true_mask, thenPacket, elsePacket);
 2205|      0|}
 2206|       |template <>
 2207|       |EIGEN_STRONG_INLINE Packet4ui pblend(const Selector<4>& ifPacket, const Packet4ui& thenPacket,
 2208|      0|                                     const Packet4ui& elsePacket) {
 2209|      0|  return (Packet4ui)pblend(ifPacket, (Packet4i)thenPacket, (Packet4i)elsePacket);
 2210|      0|}
 2211|       |template <>
 2212|       |EIGEN_STRONG_INLINE Packet4f pblend(const Selector<4>& ifPacket, const Packet4f& thenPacket,
 2213|      0|                                    const Packet4f& elsePacket) {
 2214|      0|  const __m128i true_mask = sse_blend_mask(ifPacket);
 2215|      0|  return pselect<Packet4f>(_mm_castsi128_ps(true_mask), thenPacket, elsePacket);
 2216|      0|}
 2217|       |template <>
 2218|       |EIGEN_STRONG_INLINE Packet2d pblend(const Selector<2>& ifPacket, const Packet2d& thenPacket,
 2219|      0|                                    const Packet2d& elsePacket) {
 2220|      0|  const __m128i true_mask = sse_blend_mask(ifPacket);
 2221|      0|  return pselect<Packet2d>(_mm_castsi128_pd(true_mask), thenPacket, elsePacket);
 2222|      0|}
 2223|       |
 2224|       |// Scalar path for pmadd with FMA to ensure consistency with vectorized path.
 2225|       |#ifdef EIGEN_VECTORIZE_FMA
 2226|       |template <>
 2227|       |EIGEN_STRONG_INLINE float pmadd(const float& a, const float& b, const float& c) {
 2228|       |  return ::fmaf(a, b, c);
 2229|       |}
 2230|       |template <>
 2231|       |EIGEN_STRONG_INLINE double pmadd(const double& a, const double& b, const double& c) {
 2232|       |  return ::fma(a, b, c);
 2233|       |}
 2234|       |template <>
 2235|       |EIGEN_STRONG_INLINE float pmsub(const float& a, const float& b, const float& c) {
 2236|       |  return ::fmaf(a, b, -c);
 2237|       |}
 2238|       |template <>
 2239|       |EIGEN_STRONG_INLINE double pmsub(const double& a, const double& b, const double& c) {
 2240|       |  return ::fma(a, b, -c);
 2241|       |}
 2242|       |template <>
 2243|       |EIGEN_STRONG_INLINE float pnmadd(const float& a, const float& b, const float& c) {
 2244|       |  return ::fmaf(-a, b, c);
 2245|       |}
 2246|       |template <>
 2247|       |EIGEN_STRONG_INLINE double pnmadd(const double& a, const double& b, const double& c) {
 2248|       |  return ::fma(-a, b, c);
 2249|       |}
 2250|       |template <>
 2251|       |EIGEN_STRONG_INLINE float pnmsub(const float& a, const float& b, const float& c) {
 2252|       |  return ::fmaf(-a, b, -c);
 2253|       |}
 2254|       |template <>
 2255|       |EIGEN_STRONG_INLINE double pnmsub(const double& a, const double& b, const double& c) {
 2256|       |  return ::fma(-a, b, -c);
 2257|       |}
 2258|       |#endif
 2259|       |
 2260|       |#ifdef EIGEN_VECTORIZE_SSE4_1
 2261|       |// Helpers for half->float and float->half conversions.
 2262|       |// Currently only used by the AVX code.
 2263|       |EIGEN_STRONG_INLINE __m128i half2floatsse(__m128i h) {
 2264|       |  __m128i input = _mm_cvtepu16_epi32(h);
 2265|       |
 2266|       |  // Direct vectorization of half_to_float, C parts in the comments.
 2267|       |  __m128i shifted_exp = _mm_set1_epi32(0x7c00 << 13);
 2268|       |  // o.u = (h.x & 0x7fff) << 13; // exponent/mantissa bits
 2269|       |  __m128i ou = _mm_slli_epi32(_mm_and_si128(input, _mm_set1_epi32(0x7fff)), 13);
 2270|       |  // exp = shifted_exp & o.u;   // just the exponent
 2271|       |  __m128i exp = _mm_and_si128(ou, shifted_exp);
 2272|       |  // o.u += (127 - 15) << 23;
 2273|       |  ou = _mm_add_epi32(ou, _mm_set1_epi32((127 - 15) << 23));
 2274|       |
 2275|       |  // Inf/NaN?
 2276|       |  __m128i naninf_mask = _mm_cmpeq_epi32(exp, shifted_exp);
 2277|       |  // Inf/NaN adjust
 2278|       |  __m128i naninf_adj = _mm_and_si128(_mm_set1_epi32((128 - 16) << 23), naninf_mask);
 2279|       |  // extra exp adjust for  Inf/NaN
 2280|       |  ou = _mm_add_epi32(ou, naninf_adj);
 2281|       |
 2282|       |  // Zero/Denormal?
 2283|       |  __m128i zeroden_mask = _mm_cmpeq_epi32(exp, _mm_setzero_si128());
 2284|       |  __m128i zeroden_adj = _mm_and_si128(zeroden_mask, _mm_set1_epi32(1 << 23));
 2285|       |  // o.u += 1 << 23;
 2286|       |  ou = _mm_add_epi32(ou, zeroden_adj);
 2287|       |  // magic.u = 113 << 23
 2288|       |  __m128i magic = _mm_and_si128(zeroden_mask, _mm_set1_epi32(113 << 23));
 2289|       |  // o.f -= magic.f
 2290|       |  ou = _mm_castps_si128(_mm_sub_ps(_mm_castsi128_ps(ou), _mm_castsi128_ps(magic)));
 2291|       |
 2292|       |  __m128i sign = _mm_slli_epi32(_mm_and_si128(input, _mm_set1_epi32(0x8000)), 16);
 2293|       |  // o.u |= (h.x & 0x8000) << 16;    // sign bit
 2294|       |  ou = _mm_or_si128(ou, sign);
 2295|       |  // return o.f;
 2296|       |  // We are actually returning uint version, to make
 2297|       |  // _mm256_insertf128_si256 work.
 2298|       |  return ou;
 2299|       |}
 2300|       |
 2301|       |EIGEN_STRONG_INLINE __m128i float2half(__m128 f) {
 2302|       |  // unsigned int sign_mask = 0x80000000u;
 2303|       |  __m128i sign = _mm_set1_epi32(0x80000000u);
 2304|       |  // unsigned int sign = f.u & sign_mask;
 2305|       |  sign = _mm_and_si128(sign, _mm_castps_si128(f));
 2306|       |  // f.u ^= sign;
 2307|       |  f = _mm_xor_ps(f, _mm_castsi128_ps(sign));
 2308|       |
 2309|       |  __m128i fu = _mm_castps_si128(f);
 2310|       |
 2311|       |  __m128i f16max = _mm_set1_epi32((127 + 16) << 23);
 2312|       |  __m128i f32infty = _mm_set1_epi32(255 << 23);
 2313|       |  // if (f.u >= f16max.u) // result is Inf or NaN (all exponent bits set)
 2314|       |  // there is no _mm_cmpge_epi32, so use lt and swap operands
 2315|       |  __m128i infnan_mask = _mm_cmplt_epi32(f16max, _mm_castps_si128(f));
 2316|       |  __m128i inf_mask = _mm_cmpgt_epi32(_mm_castps_si128(f), f32infty);
 2317|       |  __m128i nan_mask = _mm_andnot_si128(inf_mask, infnan_mask);
 2318|       |  __m128i inf_value = _mm_and_si128(inf_mask, _mm_set1_epi32(0x7e00));
 2319|       |  __m128i nan_value = _mm_and_si128(nan_mask, _mm_set1_epi32(0x7c00));
 2320|       |  // o.x = (f.u > f32infty.u) ? 0x7e00 : 0x7c00; // NaN->qNaN and Inf->Inf
 2321|       |  __m128i naninf_value = _mm_or_si128(inf_value, nan_value);
 2322|       |
 2323|       |  __m128i denorm_magic = _mm_set1_epi32(((127 - 15) + (23 - 10) + 1) << 23);
 2324|       |  __m128i subnorm_mask = _mm_cmplt_epi32(_mm_castps_si128(f), _mm_set1_epi32(113 << 23));
 2325|       |  //  f.f += denorm_magic.f;
 2326|       |  f = _mm_add_ps(f, _mm_castsi128_ps(denorm_magic));
 2327|       |  // f.u - denorm_magic.u
 2328|       |  __m128i o = _mm_sub_epi32(_mm_castps_si128(f), denorm_magic);
 2329|       |  o = _mm_and_si128(o, subnorm_mask);
 2330|       |  // Correct result for inf/nan/zero/subnormal, 0 otherwise
 2331|       |  o = _mm_or_si128(o, naninf_value);
 2332|       |
 2333|       |  __m128i mask = _mm_or_si128(infnan_mask, subnorm_mask);
 2334|       |  o = _mm_and_si128(o, mask);
 2335|       |
 2336|       |  // mant_odd = (f.u >> 13) & 1;
 2337|       |  __m128i mand_odd = _mm_and_si128(_mm_srli_epi32(fu, 13), _mm_set1_epi32(0x1));
 2338|       |  // f.u += 0xc8000fffU;
 2339|       |  fu = _mm_add_epi32(fu, _mm_set1_epi32(0xc8000fffU));
 2340|       |  // f.u += mant_odd;
 2341|       |  fu = _mm_add_epi32(fu, mand_odd);
 2342|       |  fu = _mm_andnot_si128(mask, fu);
 2343|       |  // f.u >> 13
 2344|       |  fu = _mm_srli_epi32(fu, 13);
 2345|       |  o = _mm_or_si128(fu, o);
 2346|       |
 2347|       |  // o.x |= static_cast<numext::uint16_t>(sign >> 16);
 2348|       |  o = _mm_or_si128(o, _mm_srli_epi32(sign, 16));
 2349|       |
 2350|       |  // 16 bit values
 2351|       |  return _mm_and_si128(o, _mm_set1_epi32(0xffff));
 2352|       |}
 2353|       |#endif
 2354|       |
 2355|       |// Packet math for Eigen::half
 2356|       |// Disable the following code since it's broken on too many platforms / compilers.
 2357|       |// #elif defined(EIGEN_VECTORIZE_SSE) && (!EIGEN_ARCH_x86_64) && (!EIGEN_COMP_MSVC)
 2358|       |#if 0
 2359|       |
 2360|       |typedef struct {
 2361|       |  __m64 x;
 2362|       |} Packet4h;
 2363|       |
 2364|       |
 2365|       |template<> struct is_arithmetic<Packet4h> { enum { value = true }; };
 2366|       |
 2367|       |template <>
 2368|       |struct packet_traits<Eigen::half> : default_packet_traits {
 2369|       |  typedef Packet4h type;
 2370|       |  // There is no half-size packet for Packet4h.
 2371|       |  typedef Packet4h half;
 2372|       |  enum {
 2373|       |    Vectorizable = 1,
 2374|       |    AlignedOnScalar = 1,
 2375|       |    size = 4,
 2376|       |    HasAdd    = 1,
 2377|       |    HasSub    = 1,
 2378|       |    HasMul    = 1,
 2379|       |    HasDiv    = 1,
 2380|       |    HasNegate = 0,
 2381|       |    HasAbs    = 0,
 2382|       |    HasAbs2   = 0,
 2383|       |    HasMin    = 0,
 2384|       |    HasMax    = 0,
 2385|       |    HasConj   = 0,
 2386|       |    HasSetLinear = 0,
 2387|       |  };
 2388|       |};
 2389|       |
 2390|       |
 2391|       |template<> struct unpacket_traits<Packet4h> { typedef Eigen::half type; enum {size=4, alignment=Aligned16, vectorizable=true, masked_load_available=false, masked_store_available=false}; typedef Packet4h half; };
 2392|       |
 2393|       |template<> EIGEN_STRONG_INLINE Packet4h pset1<Packet4h>(const Eigen::half& from) {
 2394|       |  Packet4h result;
 2395|       |  result.x = _mm_set1_pi16(from.x);
 2396|       |  return result;
 2397|       |}
 2398|       |
 2399|       |template<> EIGEN_STRONG_INLINE Eigen::half pfirst<Packet4h>(const Packet4h& from) {
 2400|       |  return half_impl::raw_uint16_to_half(static_cast<unsigned short>(_mm_cvtsi64_si32(from.x)));
 2401|       |}
 2402|       |
 2403|       |template<> EIGEN_STRONG_INLINE Packet4h pconj(const Packet4h& a) { return a; }
 2404|       |
 2405|       |template<> EIGEN_STRONG_INLINE Packet4h padd<Packet4h>(const Packet4h& a, const Packet4h& b) {
 2406|       |  __int64_t a64 = _mm_cvtm64_si64(a.x);
 2407|       |  __int64_t b64 = _mm_cvtm64_si64(b.x);
 2408|       |
 2409|       |  Eigen::half h[4];
 2410|       |
 2411|       |  Eigen::half ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64));
 2412|       |  Eigen::half hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64));
 2413|       |  h[0] = ha + hb;
 2414|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 16));
 2415|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 16));
 2416|       |  h[1] = ha + hb;
 2417|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 32));
 2418|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 32));
 2419|       |  h[2] = ha + hb;
 2420|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 48));
 2421|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 48));
 2422|       |  h[3] = ha + hb;
 2423|       |  Packet4h result;
 2424|       |  result.x = _mm_set_pi16(h[3].x, h[2].x, h[1].x, h[0].x);
 2425|       |  return result;
 2426|       |}
 2427|       |
 2428|       |template<> EIGEN_STRONG_INLINE Packet4h psub<Packet4h>(const Packet4h& a, const Packet4h& b) {
 2429|       |  __int64_t a64 = _mm_cvtm64_si64(a.x);
 2430|       |  __int64_t b64 = _mm_cvtm64_si64(b.x);
 2431|       |
 2432|       |  Eigen::half h[4];
 2433|       |
 2434|       |  Eigen::half ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64));
 2435|       |  Eigen::half hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64));
 2436|       |  h[0] = ha - hb;
 2437|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 16));
 2438|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 16));
 2439|       |  h[1] = ha - hb;
 2440|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 32));
 2441|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 32));
 2442|       |  h[2] = ha - hb;
 2443|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 48));
 2444|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 48));
 2445|       |  h[3] = ha - hb;
 2446|       |  Packet4h result;
 2447|       |  result.x = _mm_set_pi16(h[3].x, h[2].x, h[1].x, h[0].x);
 2448|       |  return result;
 2449|       |}
 2450|       |
 2451|       |template<> EIGEN_STRONG_INLINE Packet4h pmul<Packet4h>(const Packet4h& a, const Packet4h& b) {
 2452|       |  __int64_t a64 = _mm_cvtm64_si64(a.x);
 2453|       |  __int64_t b64 = _mm_cvtm64_si64(b.x);
 2454|       |
 2455|       |  Eigen::half h[4];
 2456|       |
 2457|       |  Eigen::half ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64));
 2458|       |  Eigen::half hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64));
 2459|       |  h[0] = ha * hb;
 2460|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 16));
 2461|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 16));
 2462|       |  h[1] = ha * hb;
 2463|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 32));
 2464|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 32));
 2465|       |  h[2] = ha * hb;
 2466|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 48));
 2467|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 48));
 2468|       |  h[3] = ha * hb;
 2469|       |  Packet4h result;
 2470|       |  result.x = _mm_set_pi16(h[3].x, h[2].x, h[1].x, h[0].x);
 2471|       |  return result;
 2472|       |}
 2473|       |
 2474|       |template<> EIGEN_STRONG_INLINE Packet4h pdiv<Packet4h>(const Packet4h& a, const Packet4h& b) {
 2475|       |  __int64_t a64 = _mm_cvtm64_si64(a.x);
 2476|       |  __int64_t b64 = _mm_cvtm64_si64(b.x);
 2477|       |
 2478|       |  Eigen::half h[4];
 2479|       |
 2480|       |  Eigen::half ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64));
 2481|       |  Eigen::half hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64));
 2482|       |  h[0] = ha / hb;
 2483|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 16));
 2484|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 16));
 2485|       |  h[1] = ha / hb;
 2486|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 32));
 2487|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 32));
 2488|       |  h[2] = ha / hb;
 2489|       |  ha = half_impl::raw_uint16_to_half(static_cast<unsigned short>(a64 >> 48));
 2490|       |  hb = half_impl::raw_uint16_to_half(static_cast<unsigned short>(b64 >> 48));
 2491|       |  h[3] = ha / hb;
 2492|       |  Packet4h result;
 2493|       |  result.x = _mm_set_pi16(h[3].x, h[2].x, h[1].x, h[0].x);
 2494|       |  return result;
 2495|       |}
 2496|       |
 2497|       |template<> EIGEN_STRONG_INLINE Packet4h pload<Packet4h>(const Eigen::half* from) {
 2498|       |  Packet4h result;
 2499|       |  result.x = _mm_cvtsi64_m64(*reinterpret_cast<const __int64_t*>(from));
 2500|       |  return result;
 2501|       |}
 2502|       |
 2503|       |template<> EIGEN_STRONG_INLINE Packet4h ploadu<Packet4h>(const Eigen::half* from) {
 2504|       |  Packet4h result;
 2505|       |  result.x = _mm_cvtsi64_m64(*reinterpret_cast<const __int64_t*>(from));
 2506|       |  return result;
 2507|       |}
 2508|       |
 2509|       |template<> EIGEN_STRONG_INLINE void pstore<Eigen::half>(Eigen::half* to, const Packet4h& from) {
 2510|       |  __int64_t r = _mm_cvtm64_si64(from.x);
 2511|       |  *(reinterpret_cast<__int64_t*>(to)) = r;
 2512|       |}
 2513|       |
 2514|       |template<> EIGEN_STRONG_INLINE void pstoreu<Eigen::half>(Eigen::half* to, const Packet4h& from) {
 2515|       |  __int64_t r = _mm_cvtm64_si64(from.x);
 2516|       |  *(reinterpret_cast<__int64_t*>(to)) = r;
 2517|       |}
 2518|       |
 2519|       |template<> EIGEN_STRONG_INLINE Packet4h
 2520|       |ploadquad<Packet4h>(const Eigen::half* from) {
 2521|       |  return pset1<Packet4h>(*from);
 2522|       |}
 2523|       |
 2524|       |template<> EIGEN_STRONG_INLINE Packet4h pgather<Eigen::half, Packet4h>(const Eigen::half* from, Index stride)
 2525|       |{
 2526|       |  Packet4h result;
 2527|       |  result.x = _mm_set_pi16(from[3*stride].x, from[2*stride].x, from[1*stride].x, from[0*stride].x);
 2528|       |  return result;
 2529|       |}
 2530|       |
 2531|       |template<> EIGEN_STRONG_INLINE void pscatter<Eigen::half, Packet4h>(Eigen::half* to, const Packet4h& from, Index stride)
 2532|       |{
 2533|       |  __int64_t a = _mm_cvtm64_si64(from.x);
 2534|       |  to[stride*0].x = static_cast<unsigned short>(a);
 2535|       |  to[stride*1].x = static_cast<unsigned short>(a >> 16);
 2536|       |  to[stride*2].x = static_cast<unsigned short>(a >> 32);
 2537|       |  to[stride*3].x = static_cast<unsigned short>(a >> 48);
 2538|       |}
 2539|       |
 2540|       |EIGEN_STRONG_INLINE void
 2541|       |ptranspose(PacketBlock<Packet4h,4>& kernel) {
 2542|       |  __m64 T0 = _mm_unpacklo_pi16(kernel.packet[0].x, kernel.packet[1].x);
 2543|       |  __m64 T1 = _mm_unpacklo_pi16(kernel.packet[2].x, kernel.packet[3].x);
 2544|       |  __m64 T2 = _mm_unpackhi_pi16(kernel.packet[0].x, kernel.packet[1].x);
 2545|       |  __m64 T3 = _mm_unpackhi_pi16(kernel.packet[2].x, kernel.packet[3].x);
 2546|       |
 2547|       |  kernel.packet[0].x = _mm_unpacklo_pi32(T0, T1);
 2548|       |  kernel.packet[1].x = _mm_unpackhi_pi32(T0, T1);
 2549|       |  kernel.packet[2].x = _mm_unpacklo_pi32(T2, T3);
 2550|       |  kernel.packet[3].x = _mm_unpackhi_pi32(T2, T3);
 2551|       |}
 2552|       |
 2553|       |#endif
 2554|       |
 2555|       |}  // end namespace internal
 2556|       |
 2557|       |}  // end namespace Eigen
 2558|       |
 2559|       |#if EIGEN_COMP_PGI && EIGEN_COMP_PGI < 1900
 2560|       |// PGI++ does not define the following intrinsics in C++ mode.
 2561|       |static inline __m128 _mm_castpd_ps(__m128d x) { return reinterpret_cast<__m128&>(x); }
 2562|       |static inline __m128i _mm_castpd_si128(__m128d x) { return reinterpret_cast<__m128i&>(x); }
 2563|       |static inline __m128d _mm_castps_pd(__m128 x) { return reinterpret_cast<__m128d&>(x); }
 2564|       |static inline __m128i _mm_castps_si128(__m128 x) { return reinterpret_cast<__m128i&>(x); }
 2565|       |static inline __m128 _mm_castsi128_ps(__m128i x) { return reinterpret_cast<__m128&>(x); }
 2566|       |static inline __m128d _mm_castsi128_pd(__m128i x) { return reinterpret_cast<__m128d&>(x); }
 2567|       |#endif
 2568|       |
 2569|       |#endif  // EIGEN_PACKET_MATH_SSE_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/arch/SSE/TypeCasting.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2015 Benoit Steiner <benoit.steiner.goog@gmail.com>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_TYPE_CASTING_SSE_H
   11|       |#define EIGEN_TYPE_CASTING_SSE_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |#ifndef EIGEN_VECTORIZE_AVX
   21|       |template <>
   22|       |struct type_casting_traits<float, bool> : vectorized_type_casting_traits<float, bool> {};
   23|       |template <>
   24|       |struct type_casting_traits<bool, float> : vectorized_type_casting_traits<bool, float> {};
   25|       |
   26|       |template <>
   27|       |struct type_casting_traits<float, int> : vectorized_type_casting_traits<float, int> {};
   28|       |template <>
   29|       |struct type_casting_traits<int, float> : vectorized_type_casting_traits<int, float> {};
   30|       |
   31|       |template <>
   32|       |struct type_casting_traits<float, double> : vectorized_type_casting_traits<float, double> {};
   33|       |template <>
   34|       |struct type_casting_traits<double, float> : vectorized_type_casting_traits<double, float> {};
   35|       |
   36|       |template <>
   37|       |struct type_casting_traits<double, int> : vectorized_type_casting_traits<double, int> {};
   38|       |template <>
   39|       |struct type_casting_traits<int, double> : vectorized_type_casting_traits<int, double> {};
   40|       |
   41|       |#ifndef EIGEN_VECTORIZE_AVX2
   42|       |template <>
   43|       |struct type_casting_traits<double, int64_t> : vectorized_type_casting_traits<double, int64_t> {};
   44|       |template <>
   45|       |struct type_casting_traits<int64_t, double> : vectorized_type_casting_traits<int64_t, double> {};
   46|       |#endif
   47|       |#endif
   48|       |
   49|       |template <>
   50|       |EIGEN_STRONG_INLINE Packet16b pcast<Packet4f, Packet16b>(const Packet4f& a, const Packet4f& b, const Packet4f& c,
   51|      0|                                                         const Packet4f& d) {
   52|      0|  __m128 zero = pzero(a);
   53|      0|  __m128 nonzero_a = _mm_cmpneq_ps(a, zero);
   54|      0|  __m128 nonzero_b = _mm_cmpneq_ps(b, zero);
   55|      0|  __m128 nonzero_c = _mm_cmpneq_ps(c, zero);
   56|      0|  __m128 nonzero_d = _mm_cmpneq_ps(d, zero);
   57|      0|  __m128i ab_bytes = _mm_packs_epi32(_mm_castps_si128(nonzero_a), _mm_castps_si128(nonzero_b));
   58|      0|  __m128i cd_bytes = _mm_packs_epi32(_mm_castps_si128(nonzero_c), _mm_castps_si128(nonzero_d));
   59|      0|  __m128i merged = _mm_packs_epi16(ab_bytes, cd_bytes);
   60|      0|  return _mm_and_si128(merged, _mm_set1_epi8(1));
   61|      0|}
   62|       |
   63|       |template <>
   64|      0|EIGEN_STRONG_INLINE Packet4f pcast<Packet16b, Packet4f>(const Packet16b& a) {
   65|      0|  const __m128 cst_one = _mm_set_ps1(1.0f);
   66|      0|#ifdef EIGEN_VECTORIZE_SSE4_1
   67|      0|  __m128i a_extended = _mm_cvtepi8_epi32(a);
   68|      0|  __m128i abcd = _mm_cmpeq_epi32(a_extended, _mm_setzero_si128());
   69|      0|#else
   70|      0|  __m128i abcd_efhg_ijkl_mnop = _mm_cmpeq_epi8(a, _mm_setzero_si128());
   71|      0|  __m128i aabb_ccdd_eeff_gghh = _mm_unpacklo_epi8(abcd_efhg_ijkl_mnop, abcd_efhg_ijkl_mnop);
   72|      0|  __m128i abcd = _mm_unpacklo_epi8(aabb_ccdd_eeff_gghh, aabb_ccdd_eeff_gghh);
   73|      0|#endif
   74|      0|  __m128 result = _mm_andnot_ps(_mm_castsi128_ps(abcd), cst_one);
   75|      0|  return result;
   76|      0|}
   77|       |
   78|       |template <>
   79|      0|EIGEN_STRONG_INLINE Packet4i pcast<Packet4f, Packet4i>(const Packet4f& a) {
   80|      0|  return _mm_cvttps_epi32(a);
   81|      0|}
   82|       |
   83|       |template <>
   84|      0|EIGEN_STRONG_INLINE Packet4i pcast<Packet2d, Packet4i>(const Packet2d& a, const Packet2d& b) {
   85|      0|  return _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(_mm_cvttpd_epi32(a)), _mm_castsi128_ps(_mm_cvttpd_epi32(b)),
   86|      0|                                         (1 << 2) | (1 << 6)));
   87|      0|}
   88|       |
   89|       |template <>
   90|      0|EIGEN_STRONG_INLINE Packet2l pcast<Packet2d, Packet2l>(const Packet2d& a) {
   91|      0|#if EIGEN_ARCH_x86_64
   92|      0|  return _mm_set_epi64x(_mm_cvttsd_si64(preverse(a)), _mm_cvttsd_si64(a));
   93|      0|#else
   94|      0|  return _mm_set_epi64x(static_cast<int64_t>(pfirst(preverse(a))), static_cast<int64_t>(pfirst(a)));
   95|      0|#endif
   96|      0|}
   97|       |
   98|       |template <>
   99|      0|EIGEN_STRONG_INLINE Packet2d pcast<Packet2l, Packet2d>(const Packet2l& a) {
  100|      0|  EIGEN_ALIGN16 int64_t aux[2];
  101|      0|  pstore(aux, a);
  102|      0|  return _mm_set_pd(static_cast<double>(aux[1]), static_cast<double>(aux[0]));
  103|      0|}
  104|       |
  105|       |template <>
  106|      0|EIGEN_STRONG_INLINE Packet4f pcast<Packet4i, Packet4f>(const Packet4i& a) {
  107|      0|  return _mm_cvtepi32_ps(a);
  108|      0|}
  109|       |
  110|       |template <>
  111|      0|EIGEN_STRONG_INLINE Packet4f pcast<Packet2d, Packet4f>(const Packet2d& a, const Packet2d& b) {
  112|      0|  return _mm_shuffle_ps(_mm_cvtpd_ps(a), _mm_cvtpd_ps(b), (1 << 2) | (1 << 6));
  113|      0|}
  114|       |
  115|       |template <>
  116|      0|EIGEN_STRONG_INLINE Packet2d pcast<Packet4i, Packet2d>(const Packet4i& a) {
  117|      0|  // Simply discard the second half of the input
  118|      0|  return _mm_cvtepi32_pd(a);
  119|      0|}
  120|       |
  121|       |template <>
  122|      0|EIGEN_STRONG_INLINE Packet2d pcast<Packet4f, Packet2d>(const Packet4f& a) {
  123|      0|  // Simply discard the second half of the input
  124|      0|  return _mm_cvtps_pd(a);
  125|      0|}
  126|       |
  127|       |template <>
  128|      0|EIGEN_STRONG_INLINE Packet2d preinterpret<Packet2d, Packet4f>(const Packet4f& a) {
  129|      0|  return _mm_castps_pd(a);
  130|      0|}
  131|       |
  132|       |template <>
  133|      0|EIGEN_STRONG_INLINE Packet4f preinterpret<Packet4f, Packet2d>(const Packet2d& a) {
  134|      0|  return _mm_castpd_ps(a);
  135|      0|}
  136|       |
  137|       |template <>
  138|      0|EIGEN_STRONG_INLINE Packet4i preinterpret<Packet4i, Packet4f>(const Packet4f& a) {
  139|      0|  return _mm_castps_si128(a);
  140|      0|}
  141|       |
  142|       |template <>
  143|      0|EIGEN_STRONG_INLINE Packet4f preinterpret<Packet4f, Packet4i>(const Packet4i& a) {
  144|      0|  return _mm_castsi128_ps(a);
  145|      0|}
  146|       |
  147|       |template <>
  148|      0|EIGEN_STRONG_INLINE Packet2d preinterpret<Packet2d, Packet4i>(const Packet4i& a) {
  149|      0|  return _mm_castsi128_pd(a);
  150|      0|}
  151|       |
  152|       |template <>
  153|      0|EIGEN_STRONG_INLINE Packet2d preinterpret<Packet2d, Packet2l>(const Packet2l& a) {
  154|      0|  return _mm_castsi128_pd(a);
  155|      0|}
  156|       |template <>
  157|      0|EIGEN_STRONG_INLINE Packet2l preinterpret<Packet2l, Packet2d>(const Packet2d& a) {
  158|      0|  return _mm_castpd_si128(a);
  159|      0|}
  160|       |
  161|       |template <>
  162|      0|EIGEN_STRONG_INLINE Packet4i preinterpret<Packet4i, Packet2d>(const Packet2d& a) {
  163|      0|  return _mm_castpd_si128(a);
  164|      0|}
  165|       |
  166|       |template <>
  167|      0|EIGEN_STRONG_INLINE Packet4ui preinterpret<Packet4ui, Packet4i>(const Packet4i& a) {
  168|      0|  return Packet4ui(a);
  169|      0|}
  170|       |
  171|       |template <>
  172|      0|EIGEN_STRONG_INLINE Packet4i preinterpret<Packet4i, Packet4ui>(const Packet4ui& a) {
  173|      0|  return Packet4i(a);
  174|      0|}
  175|       |
  176|       |// Disable the following code since it's broken on too many platforms / compilers.
  177|       |// #elif defined(EIGEN_VECTORIZE_SSE) && (!EIGEN_ARCH_x86_64) && (!EIGEN_COMP_MSVC)
  178|       |#if 0
  179|       |
  180|       |template <>
  181|       |struct type_casting_traits<Eigen::half, float> {
  182|       |  enum {
  183|       |    VectorizedCast = 1,
  184|       |    SrcCoeffRatio = 1,
  185|       |    TgtCoeffRatio = 1
  186|       |  };
  187|       |};
  188|       |
  189|       |template<> EIGEN_STRONG_INLINE Packet4f pcast<Packet4h, Packet4f>(const Packet4h& a) {
  190|       |  __int64_t a64 = _mm_cvtm64_si64(a.x);
  191|       |  Eigen::half h = raw_uint16_to_half(static_cast<unsigned short>(a64));
  192|       |  float f1 = static_cast<float>(h);
  193|       |  h = raw_uint16_to_half(static_cast<unsigned short>(a64 >> 16));
  194|       |  float f2 = static_cast<float>(h);
  195|       |  h = raw_uint16_to_half(static_cast<unsigned short>(a64 >> 32));
  196|       |  float f3 = static_cast<float>(h);
  197|       |  h = raw_uint16_to_half(static_cast<unsigned short>(a64 >> 48));
  198|       |  float f4 = static_cast<float>(h);
  199|       |  return _mm_set_ps(f4, f3, f2, f1);
  200|       |}
  201|       |
  202|       |template <>
  203|       |struct type_casting_traits<float, Eigen::half> {
  204|       |  enum {
  205|       |    VectorizedCast = 1,
  206|       |    SrcCoeffRatio = 1,
  207|       |    TgtCoeffRatio = 1
  208|       |  };
  209|       |};
  210|       |
  211|       |template<> EIGEN_STRONG_INLINE Packet4h pcast<Packet4f, Packet4h>(const Packet4f& a) {
  212|       |  EIGEN_ALIGN16 float aux[4];
  213|       |  pstore(aux, a);
  214|       |  Eigen::half h0(aux[0]);
  215|       |  Eigen::half h1(aux[1]);
  216|       |  Eigen::half h2(aux[2]);
  217|       |  Eigen::half h3(aux[3]);
  218|       |
  219|       |  Packet4h result;
  220|       |  result.x = _mm_set_pi16(h3.x, h2.x, h1.x, h0.x);
  221|       |  return result;
  222|       |}
  223|       |
  224|       |#endif
  225|       |
  226|       |}  // end namespace internal
  227|       |
  228|       |}  // end namespace Eigen
  229|       |
  230|       |#endif  // EIGEN_TYPE_CASTING_SSE_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/functors/BinaryFunctors.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2010 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_BINARY_FUNCTORS_H
   11|       |#define EIGEN_BINARY_FUNCTORS_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |//---------- associative binary functors ----------
   21|       |
   22|       |template <typename Arg1, typename Arg2>
   23|       |struct binary_op_base {
   24|       |  typedef Arg1 first_argument_type;
   25|       |  typedef Arg2 second_argument_type;
   26|       |};
   27|       |
   28|       |/** \internal
   29|       | * \brief Template functor to compute the sum of two scalars
   30|       | *
   31|       | * \sa class CwiseBinaryOp, MatrixBase::operator+, class VectorwiseOp, DenseBase::sum()
   32|       | */
   33|       |template <typename LhsScalar, typename RhsScalar>
   34|       |struct scalar_sum_op : binary_op_base<LhsScalar, RhsScalar> {
   35|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_sum_op>::ReturnType result_type;
   36|       |#ifdef EIGEN_SCALAR_BINARY_OP_PLUGIN
   37|       |  scalar_sum_op(){EIGEN_SCALAR_BINARY_OP_PLUGIN}
   38|       |#endif
   39|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type
   40|       |  operator()(const LhsScalar& a, const RhsScalar& b) const {
   41|       |    return a + b;
   42|       |  }
   43|       |  template <typename Packet>
   44|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
   45|       |    return internal::padd(a, b);
   46|       |  }
   47|       |  template <typename Packet>
   48|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type predux(const Packet& a) const {
   49|       |    return internal::predux(a);
   50|       |  }
   51|       |};
   52|       |template <typename LhsScalar, typename RhsScalar>
   53|       |struct functor_traits<scalar_sum_op<LhsScalar, RhsScalar>> {
   54|       |  enum {
   55|       |    Cost = (int(NumTraits<LhsScalar>::AddCost) + int(NumTraits<RhsScalar>::AddCost)) / 2,  // rough estimate!
   56|       |    PacketAccess =
   57|       |        is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasAdd && packet_traits<RhsScalar>::HasAdd
   58|       |    // TODO vectorize mixed sum
   59|       |  };
   60|       |};
   61|       |
   62|       |template <>
   63|      0|EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool scalar_sum_op<bool, bool>::operator()(const bool& a, const bool& b) const {
   64|      0|  return a || b;
   65|      0|}
   66|       |
   67|       |/** \internal
   68|       | * \brief Template functor to compute the product of two scalars
   69|       | *
   70|       | * \sa class CwiseBinaryOp, Cwise::operator*(), class VectorwiseOp, MatrixBase::redux()
   71|       | */
   72|       |template <typename LhsScalar, typename RhsScalar>
   73|       |struct scalar_product_op : binary_op_base<LhsScalar, RhsScalar> {
   74|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_product_op>::ReturnType result_type;
   75|       |#ifdef EIGEN_SCALAR_BINARY_OP_PLUGIN
   76|       |  scalar_product_op(){EIGEN_SCALAR_BINARY_OP_PLUGIN}
   77|       |#endif
   78|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type
   79|       |  operator()(const LhsScalar& a, const RhsScalar& b) const {
   80|       |    return a * b;
   81|       |  }
   82|       |  template <typename Packet>
   83|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
   84|       |    return internal::pmul(a, b);
   85|       |  }
   86|       |  template <typename Packet>
   87|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type predux(const Packet& a) const {
   88|       |    return internal::predux_mul(a);
   89|       |  }
   90|       |};
   91|       |template <typename LhsScalar, typename RhsScalar>
   92|       |struct functor_traits<scalar_product_op<LhsScalar, RhsScalar>> {
   93|       |  enum {
   94|       |    Cost = (int(NumTraits<LhsScalar>::MulCost) + int(NumTraits<RhsScalar>::MulCost)) / 2,  // rough estimate!
   95|       |    PacketAccess =
   96|       |        is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasMul && packet_traits<RhsScalar>::HasMul
   97|       |    // TODO vectorize mixed product
   98|       |  };
   99|       |};
  100|       |
  101|       |template <>
  102|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool scalar_product_op<bool, bool>::operator()(const bool& a,
  103|      0|                                                                                     const bool& b) const {
  104|      0|  return a && b;
  105|      0|}
  106|       |
  107|       |/** \internal
  108|       | * \brief Template functor to compute the conjugate product of two scalars
  109|       | *
  110|       | * This is a short cut for conj(x) * y which is needed for optimization purpose; in Eigen2 support mode, this becomes x
  111|       | * * conj(y)
  112|       | */
  113|       |template <typename LhsScalar, typename RhsScalar>
  114|       |struct scalar_conj_product_op : binary_op_base<LhsScalar, RhsScalar> {
  115|       |  enum { Conj = NumTraits<LhsScalar>::IsComplex };
  116|       |
  117|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_conj_product_op>::ReturnType result_type;
  118|       |
  119|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  120|       |    return conj_helper<LhsScalar, RhsScalar, Conj, false>().pmul(a, b);
  121|       |  }
  122|       |
  123|       |  template <typename Packet>
  124|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  125|       |    return conj_helper<Packet, Packet, Conj, false>().pmul(a, b);
  126|       |  }
  127|       |};
  128|       |template <typename LhsScalar, typename RhsScalar>
  129|       |struct functor_traits<scalar_conj_product_op<LhsScalar, RhsScalar>> {
  130|       |  enum {
  131|       |    Cost = NumTraits<LhsScalar>::MulCost,
  132|       |    PacketAccess = internal::is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasMul
  133|       |  };
  134|       |};
  135|       |
  136|       |/** \internal
  137|       | * \brief Template functor to compute the min of two scalars
  138|       | *
  139|       | * \sa class CwiseBinaryOp, MatrixBase::cwiseMin, class VectorwiseOp, MatrixBase::minCoeff()
  140|       | */
  141|       |template <typename LhsScalar, typename RhsScalar, int NaNPropagation>
  142|       |struct scalar_min_op : binary_op_base<LhsScalar, RhsScalar> {
  143|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_min_op>::ReturnType result_type;
  144|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  145|       |    return internal::pmin<NaNPropagation>(a, b);
  146|       |  }
  147|       |  template <typename Packet>
  148|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  149|       |    return internal::pmin<NaNPropagation>(a, b);
  150|       |  }
  151|       |  template <typename Packet>
  152|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type predux(const Packet& a) const {
  153|       |    return internal::predux_min<NaNPropagation>(a);
  154|       |  }
  155|       |};
  156|       |
  157|       |template <typename LhsScalar, typename RhsScalar, int NaNPropagation>
  158|       |struct functor_traits<scalar_min_op<LhsScalar, RhsScalar, NaNPropagation>> {
  159|       |  enum {
  160|       |    Cost = (NumTraits<LhsScalar>::AddCost + NumTraits<RhsScalar>::AddCost) / 2,
  161|       |    PacketAccess = internal::is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasMin
  162|       |  };
  163|       |};
  164|       |
  165|       |/** \internal
  166|       | * \brief Template functor to compute the max of two scalars
  167|       | *
  168|       | * \sa class CwiseBinaryOp, MatrixBase::cwiseMax, class VectorwiseOp, MatrixBase::maxCoeff()
  169|       | */
  170|       |template <typename LhsScalar, typename RhsScalar, int NaNPropagation>
  171|       |struct scalar_max_op : binary_op_base<LhsScalar, RhsScalar> {
  172|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_max_op>::ReturnType result_type;
  173|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  174|       |    return internal::pmax<NaNPropagation>(a, b);
  175|       |  }
  176|       |  template <typename Packet>
  177|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  178|       |    return internal::pmax<NaNPropagation>(a, b);
  179|       |  }
  180|       |  template <typename Packet>
  181|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type predux(const Packet& a) const {
  182|       |    return internal::predux_max<NaNPropagation>(a);
  183|       |  }
  184|       |};
  185|       |
  186|       |template <typename LhsScalar, typename RhsScalar, int NaNPropagation>
  187|       |struct functor_traits<scalar_max_op<LhsScalar, RhsScalar, NaNPropagation>> {
  188|       |  enum {
  189|       |    Cost = (NumTraits<LhsScalar>::AddCost + NumTraits<RhsScalar>::AddCost) / 2,
  190|       |    PacketAccess = internal::is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasMax
  191|       |  };
  192|       |};
  193|       |
  194|       |/** \internal
  195|       | * \brief Template functors for comparison of two scalars
  196|       | * \todo Implement packet-comparisons
  197|       | */
  198|       |template <typename LhsScalar, typename RhsScalar, ComparisonName cmp, bool UseTypedComparators = false>
  199|       |struct scalar_cmp_op;
  200|       |
  201|       |template <typename LhsScalar, typename RhsScalar, ComparisonName cmp, bool UseTypedComparators>
  202|       |struct functor_traits<scalar_cmp_op<LhsScalar, RhsScalar, cmp, UseTypedComparators>> {
  203|       |  enum {
  204|       |    Cost = (NumTraits<LhsScalar>::AddCost + NumTraits<RhsScalar>::AddCost) / 2,
  205|       |    PacketAccess = (UseTypedComparators || is_same<LhsScalar, bool>::value) && is_same<LhsScalar, RhsScalar>::value &&
  206|       |                   packet_traits<LhsScalar>::HasCmp
  207|       |  };
  208|       |};
  209|       |
  210|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  211|       |struct typed_cmp_helper {
  212|       |  static constexpr bool SameType = is_same<LhsScalar, RhsScalar>::value;
  213|       |  static constexpr bool IsNumeric = is_arithmetic<typename NumTraits<LhsScalar>::Real>::value;
  214|       |  static constexpr bool UseTyped = UseTypedComparators && SameType && IsNumeric;
  215|       |  using type = typename conditional<UseTyped, LhsScalar, bool>::type;
  216|       |};
  217|       |
  218|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  219|       |using cmp_return_t = typename typed_cmp_helper<LhsScalar, RhsScalar, UseTypedComparators>::type;
  220|       |
  221|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  222|       |struct scalar_cmp_op<LhsScalar, RhsScalar, cmp_EQ, UseTypedComparators> : binary_op_base<LhsScalar, RhsScalar> {
  223|       |  using result_type = cmp_return_t<LhsScalar, RhsScalar, UseTypedComparators>;
  224|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  225|       |    return a == b ? result_type(1) : result_type(0);
  226|       |  }
  227|       |  template <typename Packet>
  228|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  229|       |    const Packet cst_one = pset1<Packet>(result_type(1));
  230|       |    return pand(pcmp_eq(a, b), cst_one);
  231|       |  }
  232|       |};
  233|       |
  234|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  235|       |struct scalar_cmp_op<LhsScalar, RhsScalar, cmp_LT, UseTypedComparators> : binary_op_base<LhsScalar, RhsScalar> {
  236|       |  using result_type = cmp_return_t<LhsScalar, RhsScalar, UseTypedComparators>;
  237|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  238|       |    return a < b ? result_type(1) : result_type(0);
  239|       |  }
  240|       |  template <typename Packet>
  241|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  242|       |    const Packet cst_one = pset1<Packet>(result_type(1));
  243|       |    return pand(pcmp_lt(a, b), cst_one);
  244|       |  }
  245|       |};
  246|       |
  247|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  248|       |struct scalar_cmp_op<LhsScalar, RhsScalar, cmp_LE, UseTypedComparators> : binary_op_base<LhsScalar, RhsScalar> {
  249|       |  using result_type = cmp_return_t<LhsScalar, RhsScalar, UseTypedComparators>;
  250|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  251|       |    return a <= b ? result_type(1) : result_type(0);
  252|       |  }
  253|       |  template <typename Packet>
  254|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  255|       |    const Packet cst_one = pset1<Packet>(result_type(1));
  256|       |    return pand(cst_one, pcmp_le(a, b));
  257|       |  }
  258|       |};
  259|       |
  260|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  261|       |struct scalar_cmp_op<LhsScalar, RhsScalar, cmp_GT, UseTypedComparators> : binary_op_base<LhsScalar, RhsScalar> {
  262|       |  using result_type = cmp_return_t<LhsScalar, RhsScalar, UseTypedComparators>;
  263|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  264|       |    return a > b ? result_type(1) : result_type(0);
  265|       |  }
  266|       |  template <typename Packet>
  267|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  268|       |    const Packet cst_one = pset1<Packet>(result_type(1));
  269|       |    return pand(cst_one, pcmp_lt(b, a));
  270|       |  }
  271|       |};
  272|       |
  273|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  274|       |struct scalar_cmp_op<LhsScalar, RhsScalar, cmp_GE, UseTypedComparators> : binary_op_base<LhsScalar, RhsScalar> {
  275|       |  using result_type = cmp_return_t<LhsScalar, RhsScalar, UseTypedComparators>;
  276|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  277|       |    return a >= b ? result_type(1) : result_type(0);
  278|       |  }
  279|       |  template <typename Packet>
  280|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  281|       |    const Packet cst_one = pset1<Packet>(result_type(1));
  282|       |    return pand(cst_one, pcmp_le(b, a));
  283|       |  }
  284|       |};
  285|       |
  286|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  287|       |struct scalar_cmp_op<LhsScalar, RhsScalar, cmp_UNORD, UseTypedComparators> : binary_op_base<LhsScalar, RhsScalar> {
  288|       |  using result_type = cmp_return_t<LhsScalar, RhsScalar, UseTypedComparators>;
  289|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  290|       |    return !(a <= b || b <= a) ? result_type(1) : result_type(0);
  291|       |  }
  292|       |  template <typename Packet>
  293|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  294|       |    const Packet cst_one = pset1<Packet>(result_type(1));
  295|       |    return pandnot(cst_one, por(pcmp_le(a, b), pcmp_le(b, a)));
  296|       |  }
  297|       |};
  298|       |
  299|       |template <typename LhsScalar, typename RhsScalar, bool UseTypedComparators>
  300|       |struct scalar_cmp_op<LhsScalar, RhsScalar, cmp_NEQ, UseTypedComparators> : binary_op_base<LhsScalar, RhsScalar> {
  301|       |  using result_type = cmp_return_t<LhsScalar, RhsScalar, UseTypedComparators>;
  302|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const LhsScalar& a, const RhsScalar& b) const {
  303|       |    return a != b ? result_type(1) : result_type(0);
  304|       |  }
  305|       |  template <typename Packet>
  306|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  307|       |    const Packet cst_one = pset1<Packet>(result_type(1));
  308|       |    return pandnot(cst_one, pcmp_eq(a, b));
  309|       |  }
  310|       |};
  311|       |
  312|       |/** \internal
  313|       | * \brief Template functor to compute the hypot of two \b positive \b and \b real scalars
  314|       | *
  315|       | * \sa MatrixBase::stableNorm(), class Redux
  316|       | */
  317|       |template <typename Scalar>
  318|       |struct scalar_hypot_op<Scalar, Scalar> : binary_op_base<Scalar, Scalar> {
  319|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& x, const Scalar& y) const {
  320|       |    // This functor is used by hypotNorm only for which it is faster to first apply abs
  321|       |    // on all coefficients prior to reduction through hypot.
  322|       |    // This way we avoid calling abs on positive and real entries, and this also permits
  323|       |    // to seamlessly handle complexes. Otherwise we would have to handle both real and complexes
  324|       |    // through the same functor...
  325|       |    return internal::positive_real_hypot(x, y);
  326|       |  }
  327|       |};
  328|       |template <typename Scalar>
  329|       |struct functor_traits<scalar_hypot_op<Scalar, Scalar>> {
  330|       |  enum {
  331|       |    Cost = 3 * NumTraits<Scalar>::AddCost + 2 * NumTraits<Scalar>::MulCost + 2 * scalar_div_cost<Scalar, false>::value,
  332|       |    PacketAccess = false
  333|       |  };
  334|       |};
  335|       |
  336|       |/** \internal
  337|       | * \brief Template functor to compute the pow of two scalars
  338|       | * See the specification of pow in https://en.cppreference.com/w/cpp/numeric/math/pow
  339|       | */
  340|       |template <typename Scalar, typename Exponent>
  341|       |struct scalar_pow_op : binary_op_base<Scalar, Exponent> {
  342|       |  typedef typename ScalarBinaryOpTraits<Scalar, Exponent, scalar_pow_op>::ReturnType result_type;
  343|       |#ifdef EIGEN_SCALAR_BINARY_OP_PLUGIN
  344|       |  scalar_pow_op() {
  345|       |    typedef Scalar LhsScalar;
  346|       |    typedef Exponent RhsScalar;
  347|       |    EIGEN_SCALAR_BINARY_OP_PLUGIN
  348|       |  }
  349|       |#endif
  350|       |
  351|       |  EIGEN_DEVICE_FUNC inline result_type operator()(const Scalar& a, const Exponent& b) const {
  352|       |    return numext::pow(a, b);
  353|       |  }
  354|       |
  355|       |  template <typename Packet>
  356|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a, const Packet& b) const {
  357|       |    return generic_pow(a, b);
  358|       |  }
  359|       |};
  360|       |
  361|       |template <typename Scalar, typename Exponent>
  362|       |struct functor_traits<scalar_pow_op<Scalar, Exponent>> {
  363|       |  enum {
  364|       |    Cost = 5 * NumTraits<Scalar>::MulCost,
  365|       |    PacketAccess = (!NumTraits<Scalar>::IsComplex && !NumTraits<Scalar>::IsInteger && packet_traits<Scalar>::HasExp &&
  366|       |                    packet_traits<Scalar>::HasLog && packet_traits<Scalar>::HasRound && packet_traits<Scalar>::HasCmp &&
  367|       |                    // Temporarily disable packet access for half/bfloat16 until
  368|       |                    // accuracy is improved.
  369|       |                    !is_same<Scalar, half>::value && !is_same<Scalar, bfloat16>::value)
  370|       |  };
  371|       |};
  372|       |
  373|       |//---------- non associative binary functors ----------
  374|       |
  375|       |/** \internal
  376|       | * \brief Template functor to compute the difference of two scalars
  377|       | *
  378|       | * \sa class CwiseBinaryOp, MatrixBase::operator-
  379|       | */
  380|       |template <typename LhsScalar, typename RhsScalar>
  381|       |struct scalar_difference_op : binary_op_base<LhsScalar, RhsScalar> {
  382|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_difference_op>::ReturnType result_type;
  383|       |#ifdef EIGEN_SCALAR_BINARY_OP_PLUGIN
  384|       |  scalar_difference_op(){EIGEN_SCALAR_BINARY_OP_PLUGIN}
  385|       |#endif
  386|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type
  387|       |  operator()(const LhsScalar& a, const RhsScalar& b) const {
  388|       |    return a - b;
  389|       |  }
  390|       |  template <typename Packet>
  391|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a, const Packet& b) const {
  392|       |    return internal::psub(a, b);
  393|       |  }
  394|       |};
  395|       |template <typename LhsScalar, typename RhsScalar>
  396|       |struct functor_traits<scalar_difference_op<LhsScalar, RhsScalar>> {
  397|       |  enum {
  398|       |    Cost = (int(NumTraits<LhsScalar>::AddCost) + int(NumTraits<RhsScalar>::AddCost)) / 2,
  399|       |    PacketAccess =
  400|       |        is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasSub && packet_traits<RhsScalar>::HasSub
  401|       |  };
  402|       |};
  403|       |
  404|       |template <typename Packet, bool IsInteger = NumTraits<typename unpacket_traits<Packet>::type>::IsInteger>
  405|       |struct maybe_raise_div_by_zero {
  406|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void run(Packet x) { EIGEN_UNUSED_VARIABLE(x); }
  407|       |};
  408|       |
  409|       |#ifndef EIGEN_GPU_COMPILE_PHASE
  410|       |template <typename Packet>
  411|       |struct maybe_raise_div_by_zero<Packet, true> {
  412|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void run(Packet x) {
  413|       |    if (EIGEN_PREDICT_FALSE(predux_any(pcmp_eq(x, pzero(x))))) {
  414|       |      // Use volatile variables to force a division by zero, which will
  415|       |      // result in the default platform behaviour (usually SIGFPE).
  416|       |      volatile typename unpacket_traits<Packet>::type zero = 0;
  417|       |      volatile typename unpacket_traits<Packet>::type val = 1;
  418|       |      val = val / zero;
  419|       |    }
  420|       |  }
  421|       |};
  422|       |#endif
  423|       |
  424|       |/** \internal
  425|       | * \brief Template functor to compute the quotient of two scalars
  426|       | *
  427|       | * \sa class CwiseBinaryOp, Cwise::operator/()
  428|       | */
  429|       |template <typename LhsScalar, typename RhsScalar>
  430|       |struct scalar_quotient_op : binary_op_base<LhsScalar, RhsScalar> {
  431|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_quotient_op>::ReturnType result_type;
  432|       |#ifdef EIGEN_SCALAR_BINARY_OP_PLUGIN
  433|       |  scalar_quotient_op(){EIGEN_SCALAR_BINARY_OP_PLUGIN}
  434|       |#endif
  435|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type
  436|       |  operator()(const LhsScalar& a, const RhsScalar& b) const {
  437|       |    return a / b;
  438|       |  }
  439|       |  template <typename Packet>
  440|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a, const Packet& b) const {
  441|       |    maybe_raise_div_by_zero<Packet>::run(b);
  442|       |    return internal::pdiv(a, b);
  443|       |  }
  444|       |};
  445|       |template <typename LhsScalar, typename RhsScalar>
  446|       |struct functor_traits<scalar_quotient_op<LhsScalar, RhsScalar>> {
  447|       |  typedef typename scalar_quotient_op<LhsScalar, RhsScalar>::result_type result_type;
  448|       |  enum {
  449|       |    PacketAccess =
  450|       |        is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasDiv && packet_traits<RhsScalar>::HasDiv,
  451|       |    Cost = scalar_div_cost<result_type, PacketAccess>::value
  452|       |  };
  453|       |};
  454|       |
  455|       |/** \internal
  456|       | * \brief Template functor to compute the and of two scalars as if they were booleans
  457|       | *
  458|       | * \sa class CwiseBinaryOp, ArrayBase::operator&&
  459|       | */
  460|       |template <typename Scalar>
  461|       |struct scalar_boolean_and_op {
  462|       |  using result_type = Scalar;
  463|       |  // `false` any value `a` that satisfies `a == Scalar(0)`
  464|       |  // `true` is the complement of `false`
  465|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a, const Scalar& b) const {
  466|       |    return (a != Scalar(0)) && (b != Scalar(0)) ? Scalar(1) : Scalar(0);
  467|       |  }
  468|       |  template <typename Packet>
  469|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  470|       |    const Packet cst_one = pset1<Packet>(Scalar(1));
  471|       |    // and(a,b) == !or(!a,!b)
  472|       |    Packet not_a = pcmp_eq(a, pzero(a));
  473|       |    Packet not_b = pcmp_eq(b, pzero(b));
  474|       |    Packet a_nand_b = por(not_a, not_b);
  475|       |    return pandnot(cst_one, a_nand_b);
  476|       |  }
  477|       |};
  478|       |template <typename Scalar>
  479|       |struct functor_traits<scalar_boolean_and_op<Scalar>> {
  480|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasCmp };
  481|       |};
  482|       |
  483|       |/** \internal
  484|       | * \brief Template functor to compute the or of two scalars as if they were booleans
  485|       | *
  486|       | * \sa class CwiseBinaryOp, ArrayBase::operator||
  487|       | */
  488|       |template <typename Scalar>
  489|       |struct scalar_boolean_or_op {
  490|       |  using result_type = Scalar;
  491|       |  // `false` any value `a` that satisfies `a == Scalar(0)`
  492|       |  // `true` is the complement of `false`
  493|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a, const Scalar& b) const {
  494|       |    return (a != Scalar(0)) || (b != Scalar(0)) ? Scalar(1) : Scalar(0);
  495|       |  }
  496|       |  template <typename Packet>
  497|       |  EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  498|       |    const Packet cst_one = pset1<Packet>(Scalar(1));
  499|       |    // if or(a,b) == 0, then a == 0 and b == 0
  500|       |    // or(a,b) == !nor(a,b)
  501|       |    Packet a_nor_b = pcmp_eq(por(a, b), pzero(a));
  502|       |    return pandnot(cst_one, a_nor_b);
  503|       |  }
  504|       |};
  505|       |template <typename Scalar>
  506|       |struct functor_traits<scalar_boolean_or_op<Scalar>> {
  507|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasCmp };
  508|       |};
  509|       |
  510|       |/** \internal
  511|       | * \brief Template functor to compute the xor of two scalars as if they were booleans
  512|       | *
  513|       | * \sa class CwiseBinaryOp, ArrayBase::operator^
  514|       | */
  515|       |template <typename Scalar>
  516|       |struct scalar_boolean_xor_op {
  517|       |  using result_type = Scalar;
  518|       |  // `false` any value `a` that satisfies `a == Scalar(0)`
  519|       |  // `true` is the complement of `false`
  520|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a, const Scalar& b) const {
  521|       |    return (a != Scalar(0)) != (b != Scalar(0)) ? Scalar(1) : Scalar(0);
  522|       |  }
  523|       |  template <typename Packet>
  524|       |  EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  525|       |    const Packet cst_one = pset1<Packet>(Scalar(1));
  526|       |    // xor(a,b) == xor(!a,!b)
  527|       |    Packet not_a = pcmp_eq(a, pzero(a));
  528|       |    Packet not_b = pcmp_eq(b, pzero(b));
  529|       |    Packet a_xor_b = pxor(not_a, not_b);
  530|       |    return pand(cst_one, a_xor_b);
  531|       |  }
  532|       |};
  533|       |template <typename Scalar>
  534|       |struct functor_traits<scalar_boolean_xor_op<Scalar>> {
  535|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasCmp };
  536|       |};
  537|       |
  538|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
  539|       |struct bitwise_binary_impl {
  540|       |  static constexpr size_t Size = sizeof(Scalar);
  541|       |  using uint_t = typename numext::get_integer_by_size<Size>::unsigned_type;
  542|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_and(const Scalar& a, const Scalar& b) {
  543|       |    uint_t a_as_uint = numext::bit_cast<uint_t, Scalar>(a);
  544|       |    uint_t b_as_uint = numext::bit_cast<uint_t, Scalar>(b);
  545|       |    uint_t result = a_as_uint & b_as_uint;
  546|       |    return numext::bit_cast<Scalar, uint_t>(result);
  547|       |  }
  548|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_or(const Scalar& a, const Scalar& b) {
  549|       |    uint_t a_as_uint = numext::bit_cast<uint_t, Scalar>(a);
  550|       |    uint_t b_as_uint = numext::bit_cast<uint_t, Scalar>(b);
  551|       |    uint_t result = a_as_uint | b_as_uint;
  552|       |    return numext::bit_cast<Scalar, uint_t>(result);
  553|       |  }
  554|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_xor(const Scalar& a, const Scalar& b) {
  555|       |    uint_t a_as_uint = numext::bit_cast<uint_t, Scalar>(a);
  556|       |    uint_t b_as_uint = numext::bit_cast<uint_t, Scalar>(b);
  557|       |    uint_t result = a_as_uint ^ b_as_uint;
  558|       |    return numext::bit_cast<Scalar, uint_t>(result);
  559|       |  }
  560|       |};
  561|       |
  562|       |template <typename Scalar>
  563|       |struct bitwise_binary_impl<Scalar, true> {
  564|       |  using Real = typename NumTraits<Scalar>::Real;
  565|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_and(const Scalar& a, const Scalar& b) {
  566|       |    Real real_result = bitwise_binary_impl<Real>::run_and(numext::real(a), numext::real(b));
  567|       |    Real imag_result = bitwise_binary_impl<Real>::run_and(numext::imag(a), numext::imag(b));
  568|       |    return Scalar(real_result, imag_result);
  569|       |  }
  570|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_or(const Scalar& a, const Scalar& b) {
  571|       |    Real real_result = bitwise_binary_impl<Real>::run_or(numext::real(a), numext::real(b));
  572|       |    Real imag_result = bitwise_binary_impl<Real>::run_or(numext::imag(a), numext::imag(b));
  573|       |    return Scalar(real_result, imag_result);
  574|       |  }
  575|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_xor(const Scalar& a, const Scalar& b) {
  576|       |    Real real_result = bitwise_binary_impl<Real>::run_xor(numext::real(a), numext::real(b));
  577|       |    Real imag_result = bitwise_binary_impl<Real>::run_xor(numext::imag(a), numext::imag(b));
  578|       |    return Scalar(real_result, imag_result);
  579|       |  }
  580|       |};
  581|       |
  582|       |/** \internal
  583|       | * \brief Template functor to compute the bitwise and of two scalars
  584|       | *
  585|       | * \sa class CwiseBinaryOp, ArrayBase::operator&
  586|       | */
  587|       |template <typename Scalar>
  588|       |struct scalar_bitwise_and_op {
  589|       |  EIGEN_STATIC_ASSERT(!NumTraits<Scalar>::RequireInitialization,
  590|       |                      BITWISE OPERATIONS MAY ONLY BE PERFORMED ON PLAIN DATA TYPES)
  591|       |  EIGEN_STATIC_ASSERT((!internal::is_same<Scalar, bool>::value), DONT USE BITWISE OPS ON BOOLEAN TYPES)
  592|       |  using result_type = Scalar;
  593|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a, const Scalar& b) const {
  594|       |    return bitwise_binary_impl<Scalar>::run_and(a, b);
  595|       |  }
  596|       |  template <typename Packet>
  597|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  598|       |    return pand(a, b);
  599|       |  }
  600|       |};
  601|       |template <typename Scalar>
  602|       |struct functor_traits<scalar_bitwise_and_op<Scalar>> {
  603|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = true };
  604|       |};
  605|       |
  606|       |/** \internal
  607|       | * \brief Template functor to compute the bitwise or of two scalars
  608|       | *
  609|       | * \sa class CwiseBinaryOp, ArrayBase::operator|
  610|       | */
  611|       |template <typename Scalar>
  612|       |struct scalar_bitwise_or_op {
  613|       |  EIGEN_STATIC_ASSERT(!NumTraits<Scalar>::RequireInitialization,
  614|       |                      BITWISE OPERATIONS MAY ONLY BE PERFORMED ON PLAIN DATA TYPES)
  615|       |  EIGEN_STATIC_ASSERT((!internal::is_same<Scalar, bool>::value), DONT USE BITWISE OPS ON BOOLEAN TYPES)
  616|       |  using result_type = Scalar;
  617|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a, const Scalar& b) const {
  618|       |    return bitwise_binary_impl<Scalar>::run_or(a, b);
  619|       |  }
  620|       |  template <typename Packet>
  621|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  622|       |    return por(a, b);
  623|       |  }
  624|       |};
  625|       |template <typename Scalar>
  626|       |struct functor_traits<scalar_bitwise_or_op<Scalar>> {
  627|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = true };
  628|       |};
  629|       |
  630|       |/** \internal
  631|       | * \brief Template functor to compute the bitwise xor of two scalars
  632|       | *
  633|       | * \sa class CwiseBinaryOp, ArrayBase::operator^
  634|       | */
  635|       |template <typename Scalar>
  636|       |struct scalar_bitwise_xor_op {
  637|       |  EIGEN_STATIC_ASSERT(!NumTraits<Scalar>::RequireInitialization,
  638|       |                      BITWISE OPERATIONS MAY ONLY BE PERFORMED ON PLAIN DATA TYPES)
  639|       |  EIGEN_STATIC_ASSERT((!internal::is_same<Scalar, bool>::value), DONT USE BITWISE OPS ON BOOLEAN TYPES)
  640|       |  using result_type = Scalar;
  641|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a, const Scalar& b) const {
  642|       |    return bitwise_binary_impl<Scalar>::run_xor(a, b);
  643|       |  }
  644|       |  template <typename Packet>
  645|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a, const Packet& b) const {
  646|       |    return pxor(a, b);
  647|       |  }
  648|       |};
  649|       |template <typename Scalar>
  650|       |struct functor_traits<scalar_bitwise_xor_op<Scalar>> {
  651|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = true };
  652|       |};
  653|       |
  654|       |/** \internal
  655|       | * \brief Template functor to compute the absolute difference of two scalars
  656|       | *
  657|       | * \sa class CwiseBinaryOp, MatrixBase::absolute_difference
  658|       | */
  659|       |template <typename LhsScalar, typename RhsScalar>
  660|       |struct scalar_absolute_difference_op : binary_op_base<LhsScalar, RhsScalar> {
  661|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar, scalar_absolute_difference_op>::ReturnType result_type;
  662|       |#ifdef EIGEN_SCALAR_BINARY_OP_PLUGIN
  663|       |  scalar_absolute_difference_op(){EIGEN_SCALAR_BINARY_OP_PLUGIN}
  664|       |#endif
  665|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type
  666|       |  operator()(const LhsScalar& a, const RhsScalar& b) const {
  667|       |    return numext::absdiff(a, b);
  668|       |  }
  669|       |  template <typename Packet>
  670|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a, const Packet& b) const {
  671|       |    return internal::pabsdiff(a, b);
  672|       |  }
  673|       |};
  674|       |template <typename LhsScalar, typename RhsScalar>
  675|       |struct functor_traits<scalar_absolute_difference_op<LhsScalar, RhsScalar>> {
  676|       |  enum {
  677|       |    Cost = (NumTraits<LhsScalar>::AddCost + NumTraits<RhsScalar>::AddCost) / 2,
  678|       |    PacketAccess = is_same<LhsScalar, RhsScalar>::value && packet_traits<LhsScalar>::HasAbsDiff
  679|       |  };
  680|       |};
  681|       |
  682|       |template <typename LhsScalar, typename RhsScalar>
  683|       |struct scalar_atan2_op {
  684|       |  using Scalar = LhsScalar;
  685|       |
  686|       |  static constexpr bool Enable =
  687|       |      is_same<LhsScalar, RhsScalar>::value && !NumTraits<Scalar>::IsInteger && !NumTraits<Scalar>::IsComplex;
  688|       |  EIGEN_STATIC_ASSERT(Enable, "LhsScalar and RhsScalar must be the same non-integer, non-complex type")
  689|       |
  690|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& y, const Scalar& x) const {
  691|       |    return numext::atan2(y, x);
  692|       |  }
  693|       |  template <typename Packet>
  694|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& y, const Packet& x) const {
  695|       |    return internal::patan2(y, x);
  696|       |  }
  697|       |};
  698|       |
  699|       |template <typename LhsScalar, typename RhsScalar>
  700|       |struct functor_traits<scalar_atan2_op<LhsScalar, RhsScalar>> {
  701|       |  using Scalar = LhsScalar;
  702|       |  enum {
  703|       |    PacketAccess = is_same<LhsScalar, RhsScalar>::value && packet_traits<Scalar>::HasATan &&
  704|       |                   packet_traits<Scalar>::HasDiv && !NumTraits<Scalar>::IsInteger && !NumTraits<Scalar>::IsComplex,
  705|       |    Cost = int(scalar_div_cost<Scalar, PacketAccess>::value) + int(functor_traits<scalar_atan_op<Scalar>>::Cost)
  706|       |  };
  707|       |};
  708|       |
  709|       |//---------- binary functors bound to a constant, thus appearing as a unary functor ----------
  710|       |
  711|       |// The following two classes permits to turn any binary functor into a unary one with one argument bound to a constant
  712|       |// value. They are analogues to std::binder1st/binder2nd but with the following differences:
  713|       |//  - they are compatible with packetOp
  714|       |//  - they are portable across C++ versions (the std::binder* are deprecated in C++11)
  715|       |template <typename BinaryOp>
  716|       |struct bind1st_op : BinaryOp {
  717|       |  typedef typename BinaryOp::first_argument_type first_argument_type;
  718|       |  typedef typename BinaryOp::second_argument_type second_argument_type;
  719|       |  typedef typename BinaryOp::result_type result_type;
  720|       |
  721|       |  EIGEN_DEVICE_FUNC explicit bind1st_op(const first_argument_type& val) : m_value(val) {}
  722|       |
  723|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type operator()(const second_argument_type& b) const {
  724|       |    return BinaryOp::operator()(m_value, b);
  725|       |  }
  726|       |
  727|       |  template <typename Packet>
  728|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& b) const {
  729|       |    return BinaryOp::packetOp(internal::pset1<Packet>(m_value), b);
  730|       |  }
  731|       |
  732|       |  first_argument_type m_value;
  733|       |};
  734|       |template <typename BinaryOp>
  735|       |struct functor_traits<bind1st_op<BinaryOp>> : functor_traits<BinaryOp> {};
  736|       |
  737|       |template <typename BinaryOp>
  738|       |struct bind2nd_op : BinaryOp {
  739|       |  typedef typename BinaryOp::first_argument_type first_argument_type;
  740|       |  typedef typename BinaryOp::second_argument_type second_argument_type;
  741|       |  typedef typename BinaryOp::result_type result_type;
  742|       |
  743|       |  EIGEN_DEVICE_FUNC explicit bind2nd_op(const second_argument_type& val) : m_value(val) {}
  744|       |
  745|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type operator()(const first_argument_type& a) const {
  746|       |    return BinaryOp::operator()(a, m_value);
  747|       |  }
  748|       |
  749|       |  template <typename Packet>
  750|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
  751|       |    return BinaryOp::packetOp(a, internal::pset1<Packet>(m_value));
  752|       |  }
  753|       |
  754|       |  second_argument_type m_value;
  755|       |};
  756|       |template <typename BinaryOp>
  757|       |struct functor_traits<bind2nd_op<BinaryOp>> : functor_traits<BinaryOp> {};
  758|       |
  759|       |}  // end namespace internal
  760|       |
  761|       |}  // end namespace Eigen
  762|       |
  763|       |#endif  // EIGEN_BINARY_FUNCTORS_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/functors/UnaryFunctors.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2016 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_UNARY_FUNCTORS_H
   11|       |#define EIGEN_UNARY_FUNCTORS_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |/** \internal
   21|       | * \brief Template functor to compute the opposite of a scalar
   22|       | *
   23|       | * \sa class CwiseUnaryOp, MatrixBase::operator-
   24|       | */
   25|       |template <typename Scalar>
   26|       |struct scalar_opposite_op {
   27|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const { return numext::negate(a); }
   28|       |  template <typename Packet>
   29|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
   30|       |    return internal::pnegate(a);
   31|       |  }
   32|       |};
   33|       |template <typename Scalar>
   34|       |struct functor_traits<scalar_opposite_op<Scalar>> {
   35|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasNegate };
   36|       |};
   37|       |
   38|       |/** \internal
   39|       | * \brief Template functor to compute the absolute value of a scalar
   40|       | *
   41|       | * \sa class CwiseUnaryOp, Cwise::abs
   42|       | */
   43|       |template <typename Scalar>
   44|       |struct scalar_abs_op {
   45|       |  typedef typename NumTraits<Scalar>::Real result_type;
   46|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type operator()(const Scalar& a) const { return numext::abs(a); }
   47|       |  template <typename Packet>
   48|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
   49|       |    return internal::pabs(a);
   50|       |  }
   51|       |};
   52|       |template <typename Scalar>
   53|       |struct functor_traits<scalar_abs_op<Scalar>> {
   54|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasAbs };
   55|       |};
   56|       |
   57|       |/** \internal
   58|       | * \brief Template functor to compute the score of a scalar, to chose a pivot
   59|       | *
   60|       | * \sa class CwiseUnaryOp
   61|       | */
   62|       |template <typename Scalar>
   63|       |struct scalar_score_coeff_op : scalar_abs_op<Scalar> {
   64|       |  typedef void Score_is_abs;
   65|       |};
   66|       |template <typename Scalar>
   67|       |struct functor_traits<scalar_score_coeff_op<Scalar>> : functor_traits<scalar_abs_op<Scalar>> {};
   68|       |
   69|       |/* Avoid recomputing abs when we know the score and they are the same. Not a true Eigen functor.  */
   70|       |template <typename Scalar, typename = void>
   71|       |struct abs_knowing_score {
   72|       |  typedef typename NumTraits<Scalar>::Real result_type;
   73|       |  template <typename Score>
   74|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type operator()(const Scalar& a, const Score&) const {
   75|       |    return numext::abs(a);
   76|       |  }
   77|       |};
   78|       |template <typename Scalar>
   79|       |struct abs_knowing_score<Scalar, typename scalar_score_coeff_op<Scalar>::Score_is_abs> {
   80|       |  typedef typename NumTraits<Scalar>::Real result_type;
   81|       |  template <typename Scal>
   82|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type operator()(const Scal&, const result_type& a) const {
   83|       |    return a;
   84|       |  }
   85|       |};
   86|       |
   87|       |/** \internal
   88|       | * \brief Template functor to compute the squared absolute value of a scalar
   89|       | *
   90|       | * \sa class CwiseUnaryOp, Cwise::abs2
   91|       | */
   92|       |template <typename Scalar>
   93|       |struct scalar_abs2_op {
   94|       |  typedef typename NumTraits<Scalar>::Real result_type;
   95|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type operator()(const Scalar& a) const { return numext::abs2(a); }
   96|       |  template <typename Packet>
   97|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
   98|       |    return internal::pmul(a, a);
   99|       |  }
  100|       |};
  101|       |template <typename Scalar>
  102|       |struct functor_traits<scalar_abs2_op<Scalar>> {
  103|       |  enum { Cost = NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasAbs2 };
  104|       |};
  105|       |
  106|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
  107|       |struct squared_norm_functor {
  108|       |  typedef Scalar result_type;
  109|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const {
  110|       |    return Scalar(numext::real(a) * numext::real(a), numext::imag(a) * numext::imag(a));
  111|       |  }
  112|       |  template <typename Packet>
  113|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
  114|       |    return Packet(pmul(a.v, a.v));
  115|       |  }
  116|       |};
  117|       |template <typename Scalar>
  118|       |struct squared_norm_functor<Scalar, false> : scalar_abs2_op<Scalar> {};
  119|       |
  120|       |template <typename Scalar>
  121|       |struct functor_traits<squared_norm_functor<Scalar>> {
  122|       |  using Real = typename NumTraits<Scalar>::Real;
  123|       |  enum { Cost = NumTraits<Real>::MulCost, PacketAccess = packet_traits<Real>::HasMul };
  124|       |};
  125|       |
  126|       |/** \internal
  127|       | * \brief Template functor to compute the conjugate of a complex value
  128|       | *
  129|       | * \sa class CwiseUnaryOp, MatrixBase::conjugate()
  130|       | */
  131|       |template <typename Scalar>
  132|       |struct scalar_conjugate_op {
  133|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const { return numext::conj(a); }
  134|       |  template <typename Packet>
  135|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
  136|       |    return internal::pconj(a);
  137|       |  }
  138|       |};
  139|       |template <typename Scalar>
  140|       |struct functor_traits<scalar_conjugate_op<Scalar>> {
  141|       |  enum {
  142|       |    Cost = 0,
  143|       |    // Yes the cost is zero even for complexes because in most cases for which
  144|       |    // the cost is used, conjugation turns to be a no-op. Some examples:
  145|       |    //   cost(a*conj(b)) == cost(a*b)
  146|       |    //   cost(a+conj(b)) == cost(a+b)
  147|       |    //   <etc.
  148|       |    // If we don't set it to zero, then:
  149|       |    //   A.conjugate().lazyProduct(B.conjugate())
  150|       |    // will bake its operands. We definitely don't want that!
  151|       |    PacketAccess = packet_traits<Scalar>::HasConj
  152|       |  };
  153|       |};
  154|       |
  155|       |/** \internal
  156|       | * \brief Template functor to compute the phase angle of a complex
  157|       | *
  158|       | * \sa class CwiseUnaryOp, Cwise::arg
  159|       | */
  160|       |template <typename Scalar>
  161|       |struct scalar_arg_op {
  162|       |  typedef typename NumTraits<Scalar>::Real result_type;
  163|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type operator()(const Scalar& a) const { return numext::arg(a); }
  164|       |  template <typename Packet>
  165|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
  166|       |    return internal::parg(a);
  167|       |  }
  168|       |};
  169|       |template <typename Scalar>
  170|       |struct functor_traits<scalar_arg_op<Scalar>> {
  171|       |  enum {
  172|       |    Cost = NumTraits<Scalar>::IsComplex ? 5 * NumTraits<Scalar>::MulCost : NumTraits<Scalar>::AddCost,
  173|       |    PacketAccess = packet_traits<Scalar>::HasArg
  174|       |  };
  175|       |};
  176|       |
  177|       |/** \internal
  178|       | * \brief Template functor to compute the complex argument, returned as a complex type
  179|       | *
  180|       | * \sa class CwiseUnaryOp, Cwise::carg
  181|       | */
  182|       |template <typename Scalar>
  183|       |struct scalar_carg_op {
  184|       |  using result_type = Scalar;
  185|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const {
  186|       |    return Scalar(numext::arg(a));
  187|       |  }
  188|       |  template <typename Packet>
  189|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
  190|       |    return pcarg(a);
  191|       |  }
  192|       |};
  193|       |template <typename Scalar>
  194|       |struct functor_traits<scalar_carg_op<Scalar>> {
  195|       |  using RealScalar = typename NumTraits<Scalar>::Real;
  196|       |  enum { Cost = functor_traits<scalar_atan2_op<RealScalar>>::Cost, PacketAccess = packet_traits<RealScalar>::HasATan };
  197|       |};
  198|       |
  199|       |/** \internal
  200|       | * \brief Template functor to cast a scalar to another type
  201|       | *
  202|       | * \sa class CwiseUnaryOp, MatrixBase::cast()
  203|       | */
  204|       |template <typename Scalar, typename NewType>
  205|       |struct scalar_cast_op {
  206|       |  typedef NewType result_type;
  207|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const NewType operator()(const Scalar& a) const {
  208|       |    return cast<Scalar, NewType>(a);
  209|       |  }
  210|       |};
  211|       |
  212|       |template <typename Scalar, typename NewType>
  213|       |struct functor_traits<scalar_cast_op<Scalar, NewType>> {
  214|       |  enum { Cost = is_same<Scalar, NewType>::value ? 0 : NumTraits<NewType>::AddCost, PacketAccess = false };
  215|       |};
  216|       |
  217|       |/** \internal
  218|       | * `core_cast_op` serves to distinguish the vectorized implementation from that of the legacy `scalar_cast_op` for
  219|       | * backwards compatibility. The manner in which packet ops are handled is defined by the specialized unary_evaluator:
  220|       | * `unary_evaluator<CwiseUnaryOp<core_cast_op<SrcType, DstType>, ArgType>, IndexBased>` in CoreEvaluators.h
  221|       | * Otherwise, the non-vectorized behavior is identical to that of `scalar_cast_op`
  222|       | */
  223|       |template <typename SrcType, typename DstType>
  224|       |struct core_cast_op : scalar_cast_op<SrcType, DstType> {};
  225|       |
  226|       |template <typename SrcType, typename DstType>
  227|       |struct functor_traits<core_cast_op<SrcType, DstType>> {
  228|       |  using CastingTraits = type_casting_traits<SrcType, DstType>;
  229|       |  enum {
  230|       |    Cost = is_same<SrcType, DstType>::value ? 0 : NumTraits<DstType>::AddCost,
  231|       |    PacketAccess = CastingTraits::VectorizedCast && (CastingTraits::SrcCoeffRatio <= 8)
  232|       |  };
  233|       |};
  234|       |
  235|       |/** \internal
  236|       | * \brief Template functor to arithmetically shift a scalar right by a number of bits
  237|       | *
  238|       | * \sa class CwiseUnaryOp, MatrixBase::shift_right()
  239|       | */
  240|       |template <typename Scalar, int N>
  241|       |struct scalar_shift_right_op {
  242|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const {
  243|       |    return numext::arithmetic_shift_right(a);
  244|       |  }
  245|       |  template <typename Packet>
  246|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
  247|       |    return internal::parithmetic_shift_right<N>(a);
  248|       |  }
  249|       |};
  250|       |template <typename Scalar, int N>
  251|       |struct functor_traits<scalar_shift_right_op<Scalar, N>> {
  252|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasShift };
  253|       |};
  254|       |
  255|       |/** \internal
  256|       | * \brief Template functor to logically shift a scalar left by a number of bits
  257|       | *
  258|       | * \sa class CwiseUnaryOp, MatrixBase::shift_left()
  259|       | */
  260|       |template <typename Scalar, int N>
  261|       |struct scalar_shift_left_op {
  262|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const {
  263|       |    return numext::logical_shift_left(a);
  264|       |  }
  265|       |  template <typename Packet>
  266|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Packet packetOp(const Packet& a) const {
  267|       |    return internal::plogical_shift_left<N>(a);
  268|       |  }
  269|       |};
  270|       |template <typename Scalar, int N>
  271|       |struct functor_traits<scalar_shift_left_op<Scalar, N>> {
  272|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasShift };
  273|       |};
  274|       |
  275|       |/** \internal
  276|       | * \brief Template functor to extract the real part of a complex
  277|       | *
  278|       | * \sa class CwiseUnaryOp, MatrixBase::real()
  279|       | */
  280|       |template <typename Scalar>
  281|       |struct scalar_real_op {
  282|       |  typedef typename NumTraits<Scalar>::Real result_type;
  283|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const Scalar& a) const { return numext::real(a); }
  284|       |};
  285|       |template <typename Scalar>
  286|       |struct functor_traits<scalar_real_op<Scalar>> {
  287|       |  enum { Cost = 0, PacketAccess = false };
  288|       |};
  289|       |
  290|       |/** \internal
  291|       | * \brief Template functor to extract the imaginary part of a complex
  292|       | *
  293|       | * \sa class CwiseUnaryOp, MatrixBase::imag()
  294|       | */
  295|       |template <typename Scalar>
  296|       |struct scalar_imag_op {
  297|       |  typedef typename NumTraits<Scalar>::Real result_type;
  298|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const Scalar& a) const { return numext::imag(a); }
  299|       |};
  300|       |template <typename Scalar>
  301|       |struct functor_traits<scalar_imag_op<Scalar>> {
  302|       |  enum { Cost = 0, PacketAccess = false };
  303|       |};
  304|       |
  305|       |/** \internal
  306|       | * \brief Template functor to extract the real part of a complex as a reference
  307|       | *
  308|       | * \sa class CwiseUnaryOp, MatrixBase::real()
  309|       | */
  310|       |template <typename Scalar>
  311|       |struct scalar_real_ref_op {
  312|       |  typedef typename NumTraits<Scalar>::Real result_type;
  313|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type& operator()(const Scalar& a) const {
  314|       |    return numext::real_ref(a);
  315|       |  }
  316|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type& operator()(Scalar& a) const { return numext::real_ref(a); }
  317|       |};
  318|       |template <typename Scalar>
  319|       |struct functor_traits<scalar_real_ref_op<Scalar>> {
  320|       |  enum { Cost = 0, PacketAccess = false };
  321|       |};
  322|       |
  323|       |/** \internal
  324|       | * \brief Template functor to extract the imaginary part of a complex as a reference
  325|       | *
  326|       | * \sa class CwiseUnaryOp, MatrixBase::imag()
  327|       | */
  328|       |template <typename Scalar>
  329|       |struct scalar_imag_ref_op {
  330|       |  typedef typename NumTraits<Scalar>::Real result_type;
  331|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type& operator()(Scalar& a) const { return numext::imag_ref(a); }
  332|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const result_type& operator()(const Scalar& a) const {
  333|       |    return numext::imag_ref(a);
  334|       |  }
  335|       |};
  336|       |template <typename Scalar>
  337|       |struct functor_traits<scalar_imag_ref_op<Scalar>> {
  338|       |  enum { Cost = 0, PacketAccess = false };
  339|       |};
  340|       |
  341|       |/** \internal
  342|       | *
  343|       | * \brief Template functor to compute the exponential of a scalar
  344|       | *
  345|       | * \sa class CwiseUnaryOp, Cwise::exp()
  346|       | */
  347|       |template <typename Scalar>
  348|       |struct scalar_exp_op {
  349|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return internal::pexp(a); }
  350|       |  template <typename Packet>
  351|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  352|       |    return internal::pexp(a);
  353|       |  }
  354|       |};
  355|       |template <typename Scalar>
  356|       |struct functor_traits<scalar_exp_op<Scalar>> {
  357|       |  enum {
  358|       |    PacketAccess = packet_traits<Scalar>::HasExp,
  359|       |  // The following numbers are based on the AVX implementation.
  360|       |#ifdef EIGEN_VECTORIZE_FMA
  361|       |    // Haswell can issue 2 add/mul/madd per cycle.
  362|       |    Cost = (sizeof(Scalar) == 4
  363|       |                // float: 8 pmadd, 4 pmul, 2 padd/psub, 6 other
  364|       |                ? (8 * NumTraits<Scalar>::AddCost + 6 * NumTraits<Scalar>::MulCost)
  365|       |                // double: 7 pmadd, 5 pmul, 3 padd/psub, 1 div,  13 other
  366|       |                : (14 * NumTraits<Scalar>::AddCost + 6 * NumTraits<Scalar>::MulCost +
  367|       |                   scalar_div_cost<Scalar, packet_traits<Scalar>::HasDiv>::value))
  368|       |#else
  369|       |    Cost = (sizeof(Scalar) == 4
  370|       |                // float: 7 pmadd, 6 pmul, 4 padd/psub, 10 other
  371|       |                ? (21 * NumTraits<Scalar>::AddCost + 13 * NumTraits<Scalar>::MulCost)
  372|       |                // double: 7 pmadd, 5 pmul, 3 padd/psub, 1 div,  13 other
  373|       |                : (23 * NumTraits<Scalar>::AddCost + 12 * NumTraits<Scalar>::MulCost +
  374|       |                   scalar_div_cost<Scalar, packet_traits<Scalar>::HasDiv>::value))
  375|       |#endif
  376|       |  };
  377|       |};
  378|       |
  379|       |template <typename Scalar>
  380|       |struct scalar_exp2_op {
  381|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return internal::pexp2(a); }
  382|       |  template <typename Packet>
  383|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  384|       |    return internal::pexp2(a);
  385|       |  }
  386|       |};
  387|       |template <typename Scalar>
  388|       |struct functor_traits<scalar_exp2_op<Scalar>> {
  389|       |  enum {
  390|       |    PacketAccess = packet_traits<Scalar>::HasExp,
  391|       |    Cost = functor_traits<scalar_exp_op<Scalar>>::Cost  // TODO measure cost of exp2
  392|       |  };
  393|       |};
  394|       |
  395|       |/** \internal
  396|       | *
  397|       | * \brief Template functor to compute the exponential of a scalar - 1.
  398|       | *
  399|       | * \sa class CwiseUnaryOp, ArrayBase::expm1()
  400|       | */
  401|       |template <typename Scalar>
  402|       |struct scalar_expm1_op {
  403|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::expm1(a); }
  404|       |  template <typename Packet>
  405|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  406|       |    return internal::pexpm1(a);
  407|       |  }
  408|       |};
  409|       |template <typename Scalar>
  410|       |struct functor_traits<scalar_expm1_op<Scalar>> {
  411|       |  enum {
  412|       |    PacketAccess = packet_traits<Scalar>::HasExpm1,
  413|       |    Cost = functor_traits<scalar_exp_op<Scalar>>::Cost  // TODO measure cost of expm1
  414|       |  };
  415|       |};
  416|       |
  417|       |/** \internal
  418|       | *
  419|       | * \brief Template functor to compute the logarithm of a scalar
  420|       | *
  421|       | * \sa class CwiseUnaryOp, ArrayBase::log()
  422|       | */
  423|       |template <typename Scalar>
  424|       |struct scalar_log_op {
  425|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::log(a); }
  426|       |  template <typename Packet>
  427|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  428|       |    return internal::plog(a);
  429|       |  }
  430|       |};
  431|       |template <typename Scalar>
  432|       |struct functor_traits<scalar_log_op<Scalar>> {
  433|       |  enum {
  434|       |    PacketAccess = packet_traits<Scalar>::HasLog,
  435|       |    Cost = (PacketAccess
  436|       |  // The following numbers are based on the AVX implementation.
  437|       |#ifdef EIGEN_VECTORIZE_FMA
  438|       |                // 8 pmadd, 6 pmul, 8 padd/psub, 16 other, can issue 2 add/mul/madd per cycle.
  439|       |                ? (20 * NumTraits<Scalar>::AddCost + 7 * NumTraits<Scalar>::MulCost)
  440|       |#else
  441|       |                // 8 pmadd, 6 pmul, 8 padd/psub, 20 other
  442|       |                ? (36 * NumTraits<Scalar>::AddCost + 14 * NumTraits<Scalar>::MulCost)
  443|       |#endif
  444|       |                // Measured cost of std::log.
  445|       |                : sizeof(Scalar) == 4 ? 40 : 85)
  446|       |  };
  447|       |};
  448|       |
  449|       |/** \internal
  450|       | *
  451|       | * \brief Template functor to compute the logarithm of 1 plus a scalar value
  452|       | *
  453|       | * \sa class CwiseUnaryOp, ArrayBase::log1p()
  454|       | */
  455|       |template <typename Scalar>
  456|       |struct scalar_log1p_op {
  457|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::log1p(a); }
  458|       |  template <typename Packet>
  459|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  460|       |    return internal::plog1p(a);
  461|       |  }
  462|       |};
  463|       |template <typename Scalar>
  464|       |struct functor_traits<scalar_log1p_op<Scalar>> {
  465|       |  enum {
  466|       |    PacketAccess = packet_traits<Scalar>::HasLog1p,
  467|       |    Cost = functor_traits<scalar_log_op<Scalar>>::Cost  // TODO measure cost of log1p
  468|       |  };
  469|       |};
  470|       |
  471|       |/** \internal
  472|       | *
  473|       | * \brief Template functor to compute the base-10 logarithm of a scalar
  474|       | *
  475|       | * \sa class CwiseUnaryOp, Cwise::log10()
  476|       | */
  477|       |template <typename Scalar>
  478|       |struct scalar_log10_op {
  479|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { EIGEN_USING_STD(log10) return log10(a); }
  480|       |  template <typename Packet>
  481|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  482|       |    return internal::plog10(a);
  483|       |  }
  484|       |};
  485|       |template <typename Scalar>
  486|       |struct functor_traits<scalar_log10_op<Scalar>> {
  487|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasLog10 };
  488|       |};
  489|       |
  490|       |/** \internal
  491|       | *
  492|       | * \brief Template functor to compute the base-2 logarithm of a scalar
  493|       | *
  494|       | * \sa class CwiseUnaryOp, Cwise::log2()
  495|       | */
  496|       |template <typename Scalar>
  497|       |struct scalar_log2_op {
  498|       |  using RealScalar = typename NumTraits<Scalar>::Real;
  499|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const {
  500|       |    return Scalar(RealScalar(EIGEN_LOG2E)) * numext::log(a);
  501|       |  }
  502|       |  template <typename Packet>
  503|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  504|       |    return internal::plog2(a);
  505|       |  }
  506|       |};
  507|       |template <typename Scalar>
  508|       |struct functor_traits<scalar_log2_op<Scalar>> {
  509|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasLog };
  510|       |};
  511|       |
  512|       |/** \internal
  513|       | * \brief Template functor to compute the square root of a scalar
  514|       | * \sa class CwiseUnaryOp, Cwise::sqrt()
  515|       | */
  516|       |template <typename Scalar>
  517|       |struct scalar_sqrt_op {
  518|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::sqrt(a); }
  519|       |  template <typename Packet>
  520|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  521|       |    return internal::psqrt(a);
  522|       |  }
  523|       |};
  524|       |template <typename Scalar>
  525|       |struct functor_traits<scalar_sqrt_op<Scalar>> {
  526|       |  enum {
  527|       |#if EIGEN_FAST_MATH
  528|       |    // The following numbers are based on the AVX implementation.
  529|       |    Cost = (sizeof(Scalar) == 8 ? 28
  530|       |                                // 4 pmul, 1 pmadd, 3 other
  531|       |                                : (3 * NumTraits<Scalar>::AddCost + 5 * NumTraits<Scalar>::MulCost)),
  532|       |#else
  533|       |    // The following numbers are based on min VSQRT throughput on Haswell.
  534|       |    Cost = (sizeof(Scalar) == 8 ? 28 : 14),
  535|       |#endif
  536|       |    PacketAccess = packet_traits<Scalar>::HasSqrt
  537|       |  };
  538|       |};
  539|       |
  540|       |// Boolean specialization to eliminate -Wimplicit-conversion-floating-point-to-bool warnings.
  541|       |template <>
  542|       |struct scalar_sqrt_op<bool> {
  543|      0|  EIGEN_DEPRECATED EIGEN_DEVICE_FUNC inline bool operator()(const bool& a) const { return a; }
  544|       |  template <typename Packet>
  545|       |  EIGEN_DEPRECATED EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  546|       |    return a;
  547|       |  }
  548|       |};
  549|       |template <>
  550|       |struct functor_traits<scalar_sqrt_op<bool>> {
  551|       |  enum { Cost = 1, PacketAccess = packet_traits<bool>::Vectorizable };
  552|       |};
  553|       |
  554|       |/** \internal
  555|       | * \brief Template functor to compute the cube root of a scalar
  556|       | * \sa class CwiseUnaryOp, Cwise::sqrt()
  557|       | */
  558|       |template <typename Scalar>
  559|       |struct scalar_cbrt_op {
  560|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::cbrt(a); }
  561|       |};
  562|       |
  563|       |template <typename Scalar>
  564|       |struct functor_traits<scalar_cbrt_op<Scalar>> {
  565|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = false };
  566|       |};
  567|       |
  568|       |/** \internal
  569|       | * \brief Template functor to compute the reciprocal square root of a scalar
  570|       | * \sa class CwiseUnaryOp, Cwise::rsqrt()
  571|       | */
  572|       |template <typename Scalar>
  573|       |struct scalar_rsqrt_op {
  574|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::rsqrt(a); }
  575|       |  template <typename Packet>
  576|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  577|       |    return internal::prsqrt(a);
  578|       |  }
  579|       |};
  580|       |
  581|       |template <typename Scalar>
  582|       |struct functor_traits<scalar_rsqrt_op<Scalar>> {
  583|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasRsqrt };
  584|       |};
  585|       |
  586|       |/** \internal
  587|       | * \brief Template functor to compute the cosine of a scalar
  588|       | * \sa class CwiseUnaryOp, ArrayBase::cos()
  589|       | */
  590|       |template <typename Scalar>
  591|       |struct scalar_cos_op {
  592|       |  EIGEN_DEVICE_FUNC inline Scalar operator()(const Scalar& a) const { return numext::cos(a); }
  593|       |  template <typename Packet>
  594|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  595|       |    return internal::pcos(a);
  596|       |  }
  597|       |};
  598|       |template <typename Scalar>
  599|       |struct functor_traits<scalar_cos_op<Scalar>> {
  600|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasCos };
  601|       |};
  602|       |
  603|       |/** \internal
  604|       | * \brief Template functor to compute the sine of a scalar
  605|       | * \sa class CwiseUnaryOp, ArrayBase::sin()
  606|       | */
  607|       |template <typename Scalar>
  608|       |struct scalar_sin_op {
  609|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::sin(a); }
  610|       |  template <typename Packet>
  611|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  612|       |    return internal::psin(a);
  613|       |  }
  614|       |};
  615|       |template <typename Scalar>
  616|       |struct functor_traits<scalar_sin_op<Scalar>> {
  617|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasSin };
  618|       |};
  619|       |
  620|       |/** \internal
  621|       | * \brief Template functor to compute the tan of a scalar
  622|       | * \sa class CwiseUnaryOp, ArrayBase::tan()
  623|       | */
  624|       |template <typename Scalar>
  625|       |struct scalar_tan_op {
  626|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::tan(a); }
  627|       |  template <typename Packet>
  628|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  629|       |    return internal::ptan(a);
  630|       |  }
  631|       |};
  632|       |template <typename Scalar>
  633|       |struct functor_traits<scalar_tan_op<Scalar>> {
  634|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasTan };
  635|       |};
  636|       |
  637|       |/** \internal
  638|       | * \brief Template functor to compute the arc cosine of a scalar
  639|       | * \sa class CwiseUnaryOp, ArrayBase::acos()
  640|       | */
  641|       |template <typename Scalar>
  642|       |struct scalar_acos_op {
  643|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::acos(a); }
  644|       |  template <typename Packet>
  645|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  646|       |    return internal::pacos(a);
  647|       |  }
  648|       |};
  649|       |template <typename Scalar>
  650|       |struct functor_traits<scalar_acos_op<Scalar>> {
  651|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasACos };
  652|       |};
  653|       |
  654|       |/** \internal
  655|       | * \brief Template functor to compute the arc sine of a scalar
  656|       | * \sa class CwiseUnaryOp, ArrayBase::asin()
  657|       | */
  658|       |template <typename Scalar>
  659|       |struct scalar_asin_op {
  660|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::asin(a); }
  661|       |  template <typename Packet>
  662|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  663|       |    return internal::pasin(a);
  664|       |  }
  665|       |};
  666|       |template <typename Scalar>
  667|       |struct functor_traits<scalar_asin_op<Scalar>> {
  668|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasASin };
  669|       |};
  670|       |
  671|       |/** \internal
  672|       | * \brief Template functor to compute the atan of a scalar
  673|       | * \sa class CwiseUnaryOp, ArrayBase::atan()
  674|       | */
  675|       |template <typename Scalar>
  676|       |struct scalar_atan_op {
  677|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::atan(a); }
  678|       |  template <typename Packet>
  679|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  680|       |    return internal::patan(a);
  681|       |  }
  682|       |};
  683|       |template <typename Scalar>
  684|       |struct functor_traits<scalar_atan_op<Scalar>> {
  685|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasATan };
  686|       |};
  687|       |
  688|       |/** \internal
  689|       | * \brief Template functor to compute the tanh of a scalar
  690|       | * \sa class CwiseUnaryOp, ArrayBase::tanh()
  691|       | */
  692|       |template <typename Scalar>
  693|       |struct scalar_tanh_op {
  694|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::tanh(a); }
  695|       |  template <typename Packet>
  696|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& x) const {
  697|       |    return ptanh(x);
  698|       |  }
  699|       |};
  700|       |
  701|       |template <typename Scalar>
  702|       |struct functor_traits<scalar_tanh_op<Scalar>> {
  703|       |  enum {
  704|       |    PacketAccess = packet_traits<Scalar>::HasTanh,
  705|       |    Cost = ((EIGEN_FAST_MATH && is_same<Scalar, float>::value)
  706|       |// The following numbers are based on the AVX implementation,
  707|       |#ifdef EIGEN_VECTORIZE_FMA
  708|       |                // Haswell can issue 2 add/mul/madd per cycle.
  709|       |                // 9 pmadd, 2 pmul, 1 div, 2 other
  710|       |                ? (2 * NumTraits<Scalar>::AddCost + 6 * NumTraits<Scalar>::MulCost +
  711|       |                   scalar_div_cost<Scalar, packet_traits<Scalar>::HasDiv>::value)
  712|       |#else
  713|       |                ? (11 * NumTraits<Scalar>::AddCost + 11 * NumTraits<Scalar>::MulCost +
  714|       |                   scalar_div_cost<Scalar, packet_traits<Scalar>::HasDiv>::value)
  715|       |#endif
  716|       |                // This number assumes a naive implementation of tanh
  717|       |                : (6 * NumTraits<Scalar>::AddCost + 3 * NumTraits<Scalar>::MulCost +
  718|       |                   2 * scalar_div_cost<Scalar, packet_traits<Scalar>::HasDiv>::value +
  719|       |                   functor_traits<scalar_exp_op<Scalar>>::Cost))
  720|       |  };
  721|       |};
  722|       |
  723|       |/** \internal
  724|       | * \brief Template functor to compute the atanh of a scalar
  725|       | * \sa class CwiseUnaryOp, ArrayBase::atanh()
  726|       | */
  727|       |template <typename Scalar>
  728|       |struct scalar_atanh_op {
  729|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::atanh(a); }
  730|       |  template <typename Packet>
  731|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& x) const {
  732|       |    return patanh(x);
  733|       |  }
  734|       |};
  735|       |
  736|       |template <typename Scalar>
  737|       |struct functor_traits<scalar_atanh_op<Scalar>> {
  738|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasATanh };
  739|       |};
  740|       |
  741|       |/** \internal
  742|       | * \brief Template functor to compute the sinh of a scalar
  743|       | * \sa class CwiseUnaryOp, ArrayBase::sinh()
  744|       | */
  745|       |template <typename Scalar>
  746|       |struct scalar_sinh_op {
  747|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::sinh(a); }
  748|       |  template <typename Packet>
  749|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  750|       |    return internal::psinh(a);
  751|       |  }
  752|       |};
  753|       |template <typename Scalar>
  754|       |struct functor_traits<scalar_sinh_op<Scalar>> {
  755|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasSinh };
  756|       |};
  757|       |
  758|       |/** \internal
  759|       | * \brief Template functor to compute the asinh of a scalar
  760|       | * \sa class CwiseUnaryOp, ArrayBase::asinh()
  761|       | */
  762|       |template <typename Scalar>
  763|       |struct scalar_asinh_op {
  764|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::asinh(a); }
  765|       |};
  766|       |
  767|       |template <typename Scalar>
  768|       |struct functor_traits<scalar_asinh_op<Scalar>> {
  769|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = false };
  770|       |};
  771|       |
  772|       |/** \internal
  773|       | * \brief Template functor to compute the cosh of a scalar
  774|       | * \sa class CwiseUnaryOp, ArrayBase::cosh()
  775|       | */
  776|       |template <typename Scalar>
  777|       |struct scalar_cosh_op {
  778|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::cosh(a); }
  779|       |  template <typename Packet>
  780|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  781|       |    return internal::pcosh(a);
  782|       |  }
  783|       |};
  784|       |template <typename Scalar>
  785|       |struct functor_traits<scalar_cosh_op<Scalar>> {
  786|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasCosh };
  787|       |};
  788|       |
  789|       |/** \internal
  790|       | * \brief Template functor to compute the acosh of a scalar
  791|       | * \sa class CwiseUnaryOp, ArrayBase::acosh()
  792|       | */
  793|       |template <typename Scalar>
  794|       |struct scalar_acosh_op {
  795|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::acosh(a); }
  796|       |};
  797|       |
  798|       |template <typename Scalar>
  799|       |struct functor_traits<scalar_acosh_op<Scalar>> {
  800|       |  enum { Cost = 5 * NumTraits<Scalar>::MulCost, PacketAccess = false };
  801|       |};
  802|       |
  803|       |/** \internal
  804|       | * \brief Template functor to compute the inverse of a scalar
  805|       | * \sa class CwiseUnaryOp, Cwise::inverse()
  806|       | */
  807|       |template <typename Scalar>
  808|       |struct scalar_inverse_op {
  809|       |  EIGEN_DEVICE_FUNC inline Scalar operator()(const Scalar& a) const { return Scalar(1) / a; }
  810|       |  template <typename Packet>
  811|       |  EIGEN_DEVICE_FUNC inline const Packet packetOp(const Packet& a) const {
  812|       |    return internal::preciprocal(a);
  813|       |  }
  814|       |};
  815|       |template <typename Scalar>
  816|       |struct functor_traits<scalar_inverse_op<Scalar>> {
  817|       |  enum {
  818|       |    PacketAccess = packet_traits<Scalar>::HasDiv,
  819|       |    // If packet_traits<Scalar>::HasReciprocal then the Estimated cost is that
  820|       |    // of computing an approximation plus a single Newton-Raphson step, which
  821|       |    // consists of 1 pmul + 1 pmadd.
  822|       |    Cost = (packet_traits<Scalar>::HasReciprocal ? 4 * NumTraits<Scalar>::MulCost
  823|       |                                                 : scalar_div_cost<Scalar, PacketAccess>::value)
  824|       |  };
  825|       |};
  826|       |
  827|       |/** \internal
  828|       | * \brief Template functor to compute the square of a scalar
  829|       | * \sa class CwiseUnaryOp, Cwise::square()
  830|       | */
  831|       |template <typename Scalar>
  832|       |struct scalar_square_op {
  833|       |  EIGEN_DEVICE_FUNC inline Scalar operator()(const Scalar& a) const { return a * a; }
  834|       |  template <typename Packet>
  835|       |  EIGEN_DEVICE_FUNC inline const Packet packetOp(const Packet& a) const {
  836|       |    return internal::pmul(a, a);
  837|       |  }
  838|       |};
  839|       |template <typename Scalar>
  840|       |struct functor_traits<scalar_square_op<Scalar>> {
  841|       |  enum { Cost = NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasMul };
  842|       |};
  843|       |
  844|       |// Boolean specialization to avoid -Wint-in-bool-context warnings on GCC.
  845|       |template <>
  846|       |struct scalar_square_op<bool> {
  847|      0|  EIGEN_DEPRECATED EIGEN_DEVICE_FUNC inline bool operator()(const bool& a) const { return a; }
  848|       |  template <typename Packet>
  849|       |  EIGEN_DEPRECATED EIGEN_DEVICE_FUNC inline const Packet packetOp(const Packet& a) const {
  850|       |    return a;
  851|       |  }
  852|       |};
  853|       |template <>
  854|       |struct functor_traits<scalar_square_op<bool>> {
  855|       |  enum { Cost = 0, PacketAccess = packet_traits<bool>::Vectorizable };
  856|       |};
  857|       |
  858|       |/** \internal
  859|       | * \brief Template functor to compute the cube of a scalar
  860|       | * \sa class CwiseUnaryOp, Cwise::cube()
  861|       | */
  862|       |template <typename Scalar>
  863|       |struct scalar_cube_op {
  864|       |  EIGEN_DEVICE_FUNC inline Scalar operator()(const Scalar& a) const { return a * a * a; }
  865|       |  template <typename Packet>
  866|       |  EIGEN_DEVICE_FUNC inline const Packet packetOp(const Packet& a) const {
  867|       |    return internal::pmul(a, pmul(a, a));
  868|       |  }
  869|       |};
  870|       |template <typename Scalar>
  871|       |struct functor_traits<scalar_cube_op<Scalar>> {
  872|       |  enum { Cost = 2 * NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasMul };
  873|       |};
  874|       |
  875|       |// Boolean specialization to avoid -Wint-in-bool-context warnings on GCC.
  876|       |template <>
  877|       |struct scalar_cube_op<bool> {
  878|      0|  EIGEN_DEPRECATED EIGEN_DEVICE_FUNC inline bool operator()(const bool& a) const { return a; }
  879|       |  template <typename Packet>
  880|       |  EIGEN_DEPRECATED EIGEN_DEVICE_FUNC inline const Packet packetOp(const Packet& a) const {
  881|       |    return a;
  882|       |  }
  883|       |};
  884|       |template <>
  885|       |struct functor_traits<scalar_cube_op<bool>> {
  886|       |  enum { Cost = 0, PacketAccess = packet_traits<bool>::Vectorizable };
  887|       |};
  888|       |
  889|       |/** \internal
  890|       | * \brief Template functor to compute the rounded value of a scalar
  891|       | * \sa class CwiseUnaryOp, ArrayBase::round()
  892|       | */
  893|       |template <typename Scalar>
  894|       |struct scalar_round_op {
  895|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const { return numext::round(a); }
  896|       |  template <typename Packet>
  897|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  898|       |    return internal::pround(a);
  899|       |  }
  900|       |};
  901|       |template <typename Scalar>
  902|       |struct functor_traits<scalar_round_op<Scalar>> {
  903|       |  enum {
  904|       |    Cost = NumTraits<Scalar>::MulCost,
  905|       |    PacketAccess = packet_traits<Scalar>::HasRound || NumTraits<Scalar>::IsInteger
  906|       |  };
  907|       |};
  908|       |
  909|       |/** \internal
  910|       | * \brief Template functor to compute the floor of a scalar
  911|       | * \sa class CwiseUnaryOp, ArrayBase::floor()
  912|       | */
  913|       |template <typename Scalar>
  914|       |struct scalar_floor_op {
  915|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const { return numext::floor(a); }
  916|       |  template <typename Packet>
  917|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  918|       |    return internal::pfloor(a);
  919|       |  }
  920|       |};
  921|       |template <typename Scalar>
  922|       |struct functor_traits<scalar_floor_op<Scalar>> {
  923|       |  enum {
  924|       |    Cost = NumTraits<Scalar>::MulCost,
  925|       |    PacketAccess = packet_traits<Scalar>::HasRound || NumTraits<Scalar>::IsInteger
  926|       |  };
  927|       |};
  928|       |
  929|       |/** \internal
  930|       | * \brief Template functor to compute the rounded (with current rounding mode)  value of a scalar
  931|       | * \sa class CwiseUnaryOp, ArrayBase::rint()
  932|       | */
  933|       |template <typename Scalar>
  934|       |struct scalar_rint_op {
  935|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const { return numext::rint(a); }
  936|       |  template <typename Packet>
  937|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  938|       |    return internal::print(a);
  939|       |  }
  940|       |};
  941|       |template <typename Scalar>
  942|       |struct functor_traits<scalar_rint_op<Scalar>> {
  943|       |  enum {
  944|       |    Cost = NumTraits<Scalar>::MulCost,
  945|       |    PacketAccess = packet_traits<Scalar>::HasRound || NumTraits<Scalar>::IsInteger
  946|       |  };
  947|       |};
  948|       |
  949|       |/** \internal
  950|       | * \brief Template functor to compute the ceil of a scalar
  951|       | * \sa class CwiseUnaryOp, ArrayBase::ceil()
  952|       | */
  953|       |template <typename Scalar>
  954|       |struct scalar_ceil_op {
  955|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const { return numext::ceil(a); }
  956|       |  template <typename Packet>
  957|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  958|       |    return internal::pceil(a);
  959|       |  }
  960|       |};
  961|       |template <typename Scalar>
  962|       |struct functor_traits<scalar_ceil_op<Scalar>> {
  963|       |  enum {
  964|       |    Cost = NumTraits<Scalar>::MulCost,
  965|       |    PacketAccess = packet_traits<Scalar>::HasRound || NumTraits<Scalar>::IsInteger
  966|       |  };
  967|       |};
  968|       |
  969|       |/** \internal
  970|       | * \brief Template functor to compute the truncation of a scalar
  971|       | * \sa class CwiseUnaryOp, ArrayBase::floor()
  972|       | */
  973|       |template <typename Scalar>
  974|       |struct scalar_trunc_op {
  975|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const Scalar operator()(const Scalar& a) const { return numext::trunc(a); }
  976|       |  template <typename Packet>
  977|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
  978|       |    return internal::ptrunc(a);
  979|       |  }
  980|       |};
  981|       |template <typename Scalar>
  982|       |struct functor_traits<scalar_trunc_op<Scalar>> {
  983|       |  enum {
  984|       |    Cost = NumTraits<Scalar>::MulCost,
  985|       |    PacketAccess = packet_traits<Scalar>::HasRound || NumTraits<Scalar>::IsInteger
  986|       |  };
  987|       |};
  988|       |
  989|       |/** \internal
  990|       | * \brief Template functor to compute whether a scalar is NaN
  991|       | * \sa class CwiseUnaryOp, ArrayBase::isnan()
  992|       | */
  993|       |template <typename Scalar, bool UseTypedPredicate = false>
  994|       |struct scalar_isnan_op {
  995|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool operator()(const Scalar& a) const {
  996|       |#if defined(SYCL_DEVICE_ONLY)
  997|       |    return numext::isnan(a);
  998|       |#else
  999|       |    return numext::isnan EIGEN_NOT_A_MACRO(a);
 1000|       |#endif
 1001|       |  }
 1002|       |};
 1003|       |
 1004|       |template <typename Scalar>
 1005|       |struct scalar_isnan_op<Scalar, true> {
 1006|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a) const {
 1007|       |#if defined(SYCL_DEVICE_ONLY)
 1008|       |    return (numext::isnan(a) ? ptrue(a) : pzero(a));
 1009|       |#else
 1010|       |    return (numext::isnan EIGEN_NOT_A_MACRO(a) ? ptrue(a) : pzero(a));
 1011|       |#endif
 1012|       |  }
 1013|       |  template <typename Packet>
 1014|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
 1015|       |    return pisnan(a);
 1016|       |  }
 1017|       |};
 1018|       |
 1019|       |template <typename Scalar, bool UseTypedPredicate>
 1020|       |struct functor_traits<scalar_isnan_op<Scalar, UseTypedPredicate>> {
 1021|       |  enum { Cost = NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasCmp && UseTypedPredicate };
 1022|       |};
 1023|       |
 1024|       |/** \internal
 1025|       | * \brief Template functor to check whether a scalar is +/-inf
 1026|       | * \sa class CwiseUnaryOp, ArrayBase::isinf()
 1027|       | */
 1028|       |template <typename Scalar, bool UseTypedPredicate = false>
 1029|       |struct scalar_isinf_op {
 1030|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool operator()(const Scalar& a) const {
 1031|       |#if defined(SYCL_DEVICE_ONLY)
 1032|       |    return numext::isinf(a);
 1033|       |#else
 1034|       |    return (numext::isinf)(a);
 1035|       |#endif
 1036|       |  }
 1037|       |};
 1038|       |
 1039|       |template <typename Scalar>
 1040|       |struct scalar_isinf_op<Scalar, true> {
 1041|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a) const {
 1042|       |#if defined(SYCL_DEVICE_ONLY)
 1043|       |    return (numext::isinf(a) ? ptrue(a) : pzero(a));
 1044|       |#else
 1045|       |    return (numext::isinf EIGEN_NOT_A_MACRO(a) ? ptrue(a) : pzero(a));
 1046|       |#endif
 1047|       |  }
 1048|       |  template <typename Packet>
 1049|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
 1050|       |    return pisinf(a);
 1051|       |  }
 1052|       |};
 1053|       |template <typename Scalar, bool UseTypedPredicate>
 1054|       |struct functor_traits<scalar_isinf_op<Scalar, UseTypedPredicate>> {
 1055|       |  enum { Cost = NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasCmp && UseTypedPredicate };
 1056|       |};
 1057|       |
 1058|       |/** \internal
 1059|       | * \brief Template functor to check whether a scalar has a finite value
 1060|       | * \sa class CwiseUnaryOp, ArrayBase::isfinite()
 1061|       | */
 1062|       |template <typename Scalar, bool UseTypedPredicate = false>
 1063|       |struct scalar_isfinite_op {
 1064|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE bool operator()(const Scalar& a) const {
 1065|       |#if defined(SYCL_DEVICE_ONLY)
 1066|       |    return numext::isfinite(a);
 1067|       |#else
 1068|       |    return (numext::isfinite)(a);
 1069|       |#endif
 1070|       |  }
 1071|       |};
 1072|       |
 1073|       |template <typename Scalar>
 1074|       |struct scalar_isfinite_op<Scalar, true> {
 1075|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a) const {
 1076|       |#if defined(SYCL_DEVICE_ONLY)
 1077|       |    return (numext::isfinite(a) ? ptrue(a) : pzero(a));
 1078|       |#else
 1079|       |    return (numext::isfinite EIGEN_NOT_A_MACRO(a) ? ptrue(a) : pzero(a));
 1080|       |#endif
 1081|       |  }
 1082|       |  template <typename Packet>
 1083|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
 1084|       |    constexpr Scalar inf = NumTraits<Scalar>::infinity();
 1085|       |    return pcmp_lt(pabs(a), pset1<Packet>(inf));
 1086|       |  }
 1087|       |};
 1088|       |template <typename Scalar, bool UseTypedPredicate>
 1089|       |struct functor_traits<scalar_isfinite_op<Scalar, UseTypedPredicate>> {
 1090|       |  enum { Cost = NumTraits<Scalar>::MulCost, PacketAccess = packet_traits<Scalar>::HasCmp && UseTypedPredicate };
 1091|       |};
 1092|       |
 1093|       |/** \internal
 1094|       | * \brief Template functor to compute the logical not of a scalar as if it were a boolean
 1095|       | *
 1096|       | * \sa class CwiseUnaryOp, ArrayBase::operator!
 1097|       | */
 1098|       |template <typename Scalar>
 1099|       |struct scalar_boolean_not_op {
 1100|       |  using result_type = Scalar;
 1101|       |  // `false` any value `a` that satisfies `a == Scalar(0)`
 1102|       |  // `true` is the complement of `false`
 1103|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a) const {
 1104|       |    return a == Scalar(0) ? Scalar(1) : Scalar(0);
 1105|       |  }
 1106|       |  template <typename Packet>
 1107|       |  EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const {
 1108|       |    const Packet cst_one = pset1<Packet>(Scalar(1));
 1109|       |    Packet not_a = pcmp_eq(a, pzero(a));
 1110|       |    return pand(not_a, cst_one);
 1111|       |  }
 1112|       |};
 1113|       |template <typename Scalar>
 1114|       |struct functor_traits<scalar_boolean_not_op<Scalar>> {
 1115|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = packet_traits<Scalar>::HasCmp };
 1116|       |};
 1117|       |
 1118|       |template <typename Scalar, bool IsComplex = NumTraits<Scalar>::IsComplex>
 1119|       |struct bitwise_unary_impl {
 1120|       |  static constexpr size_t Size = sizeof(Scalar);
 1121|       |  using uint_t = typename numext::get_integer_by_size<Size>::unsigned_type;
 1122|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_not(const Scalar& a) {
 1123|       |    uint_t a_as_uint = numext::bit_cast<uint_t, Scalar>(a);
 1124|       |    uint_t result = ~a_as_uint;
 1125|       |    return numext::bit_cast<Scalar, uint_t>(result);
 1126|       |  }
 1127|       |};
 1128|       |
 1129|       |template <typename Scalar>
 1130|       |struct bitwise_unary_impl<Scalar, true> {
 1131|       |  using Real = typename NumTraits<Scalar>::Real;
 1132|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar run_not(const Scalar& a) {
 1133|       |    Real real_result = bitwise_unary_impl<Real>::run_not(numext::real(a));
 1134|       |    Real imag_result = bitwise_unary_impl<Real>::run_not(numext::imag(a));
 1135|       |    return Scalar(real_result, imag_result);
 1136|       |  }
 1137|       |};
 1138|       |
 1139|       |/** \internal
 1140|       | * \brief Template functor to compute the bitwise not of a scalar
 1141|       | *
 1142|       | * \sa class CwiseUnaryOp, ArrayBase::operator~
 1143|       | */
 1144|       |template <typename Scalar>
 1145|       |struct scalar_bitwise_not_op {
 1146|       |  EIGEN_STATIC_ASSERT(!NumTraits<Scalar>::RequireInitialization,
 1147|       |                      BITWISE OPERATIONS MAY ONLY BE PERFORMED ON PLAIN DATA TYPES)
 1148|       |  EIGEN_STATIC_ASSERT((!internal::is_same<Scalar, bool>::value), DONT USE BITWISE OPS ON BOOLEAN TYPES)
 1149|       |  using result_type = Scalar;
 1150|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a) const {
 1151|       |    return bitwise_unary_impl<Scalar>::run_not(a);
 1152|       |  }
 1153|       |  template <typename Packet>
 1154|       |  EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const {
 1155|       |    return pandnot(ptrue(a), a);
 1156|       |  }
 1157|       |};
 1158|       |template <typename Scalar>
 1159|       |struct functor_traits<scalar_bitwise_not_op<Scalar>> {
 1160|       |  enum { Cost = NumTraits<Scalar>::AddCost, PacketAccess = true };
 1161|       |};
 1162|       |
 1163|       |/** \internal
 1164|       | * \brief Template functor to compute the signum of a scalar
 1165|       | * \sa class CwiseUnaryOp, Cwise::sign()
 1166|       | */
 1167|       |template <typename Scalar>
 1168|       |struct scalar_sign_op {
 1169|       |  EIGEN_DEVICE_FUNC inline const Scalar operator()(const Scalar& a) const { return numext::sign(a); }
 1170|       |
 1171|       |  template <typename Packet>
 1172|       |  EIGEN_DEVICE_FUNC inline Packet packetOp(const Packet& a) const {
 1173|       |    return internal::psign(a);
 1174|       |  }
 1175|       |};
 1176|       |
 1177|       |template <typename Scalar>
 1178|       |struct functor_traits<scalar_sign_op<Scalar>> {
 1179|       |  enum {
 1180|       |    Cost = NumTraits<Scalar>::IsComplex ? (8 * NumTraits<Scalar>::MulCost)  // roughly
 1181|       |                                        : (3 * NumTraits<Scalar>::AddCost),
 1182|       |    PacketAccess = packet_traits<Scalar>::HasSign && packet_traits<Scalar>::Vectorizable
 1183|       |  };
 1184|       |};
 1185|       |
 1186|       |// Real-valued implementation.
 1187|       |template <typename T, typename EnableIf = void>
 1188|       |struct scalar_logistic_op_impl {
 1189|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T operator()(const T& x) const { return packetOp(x); }
 1190|       |
 1191|       |  template <typename Packet>
 1192|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& x) const {
 1193|       |    const Packet one = pset1<Packet>(T(1));
 1194|       |    const Packet inf = pset1<Packet>(NumTraits<T>::infinity());
 1195|       |    const Packet e = pexp(x);
 1196|       |    const Packet inf_mask = pcmp_eq(e, inf);
 1197|       |    return pselect(inf_mask, one, pdiv(e, padd(one, e)));
 1198|       |  }
 1199|       |};
 1200|       |
 1201|       |// Complex-valud implementation.
 1202|       |template <typename T>
 1203|       |struct scalar_logistic_op_impl<T, std::enable_if_t<NumTraits<T>::IsComplex>> {
 1204|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T operator()(const T& x) const {
 1205|       |    const T e = numext::exp(x);
 1206|       |    return (numext::isinf)(numext::real(e)) ? T(1) : e / (e + T(1));
 1207|       |  }
 1208|       |};
 1209|       |
 1210|       |/** \internal
 1211|       | * \brief Template functor to compute the logistic function of a scalar
 1212|       | * \sa class CwiseUnaryOp, ArrayBase::logistic()
 1213|       | */
 1214|       |template <typename T>
 1215|       |struct scalar_logistic_op : scalar_logistic_op_impl<T> {};
 1216|       |
 1217|       |// TODO(rmlarsen): Enable the following on host when integer_packet is defined
 1218|       |// for the relevant packet types.
 1219|       |#ifndef EIGEN_GPUCC
 1220|       |
 1221|       |/** \internal
 1222|       | * \brief Template specialization of the logistic function for float.
 1223|       | * Computes S(x) = exp(x) / (1 + exp(x)), where exp(x) is implemented
 1224|       | * using an algorithm partly adopted from the implementation of
 1225|       | * pexp_float. See the individual steps described in the code below.
 1226|       | * Note that compared to pexp, we use an additional outer multiplicative
 1227|       | * range reduction step using the identity exp(x) = exp(x/2)^2.
 1228|       | * This prevert us from having to call ldexp on values that could produce
 1229|       | * a denormal result, which allows us to call the faster implementation in
 1230|       | * pldexp_fast_impl<Packet>::run(p, m).
 1231|       | * The final squaring, however, doubles the error bound on the final
 1232|       | * approximation. Exhaustive testing shows that we have a worst case error
 1233|       | * of 4.5 ulps (compared to computing S(x) in double precision), which is
 1234|       | * acceptable.
 1235|       | */
 1236|       |template <>
 1237|       |struct scalar_logistic_op<float> {
 1238|      0|  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE float operator()(const float& x) const {
 1239|      0|    // Truncate at the first point where the interpolant is exactly one.
 1240|      0|    const float cst_exp_hi = 16.6355324f;
 1241|      0|    const float e = numext::exp(numext::mini(x, cst_exp_hi));
 1242|      0|    return e / (1.0f + e);
 1243|      0|  }
 1244|       |
 1245|       |  template <typename Packet>
 1246|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& _x) const {
 1247|       |    const Packet cst_zero = pset1<Packet>(0.0f);
 1248|       |    const Packet cst_one = pset1<Packet>(1.0f);
 1249|       |    const Packet cst_half = pset1<Packet>(0.5f);
 1250|       |    // Truncate at the first point where the interpolant is exactly one.
 1251|       |    const Packet cst_exp_hi = pset1<Packet>(16.6355324f);
 1252|       |    const Packet cst_exp_lo = pset1<Packet>(-104.f);
 1253|       |
 1254|       |    // Clamp x to the non-trivial range where S(x). Outside this
 1255|       |    // interval the correctly rounded value of S(x) is either zero
 1256|       |    // or one.
 1257|       |    Packet zero_mask = pcmp_lt(_x, cst_exp_lo);
 1258|       |    Packet x = pmin(_x, cst_exp_hi);
 1259|       |
 1260|       |    // 1. Multiplicative range reduction:
 1261|       |    // Reduce the range of x by a factor of 2. This avoids having
 1262|       |    // to compute exp(x) accurately where the result is a denormalized
 1263|       |    // value.
 1264|       |    x = pmul(x, cst_half);
 1265|       |
 1266|       |    // 2. Subtractive range reduction:
 1267|       |    // Express exp(x) as exp(m*ln(2) + r) = 2^m*exp(r), start by extracting
 1268|       |    // m = floor(x/ln(2) + 0.5), such that x = m*ln(2) + r.
 1269|       |    const Packet cst_cephes_LOG2EF = pset1<Packet>(1.44269504088896341f);
 1270|       |    Packet m = pfloor(pmadd(x, cst_cephes_LOG2EF, cst_half));
 1271|       |    // Get r = x - m*ln(2). We use a trick from Cephes where the term
 1272|       |    // m*ln(2) is subtracted out in two parts, m*C1+m*C2 = m*ln(2),
 1273|       |    // to avoid accumulating truncation errors.
 1274|       |    const Packet cst_cephes_exp_C1 = pset1<Packet>(-0.693359375f);
 1275|       |    const Packet cst_cephes_exp_C2 = pset1<Packet>(2.12194440e-4f);
 1276|       |    Packet r = pmadd(m, cst_cephes_exp_C1, x);
 1277|       |    r = pmadd(m, cst_cephes_exp_C2, r);
 1278|       |
 1279|       |    // 3. Compute an approximation to exp(r) using a degree 5 minimax polynomial.
 1280|       |    // We compute even and odd terms separately to increase instruction level
 1281|       |    // parallelism.
 1282|       |    Packet r2 = pmul(r, r);
 1283|       |    const Packet cst_p2 = pset1<Packet>(0.49999141693115234375f);
 1284|       |    const Packet cst_p3 = pset1<Packet>(0.16666877269744873046875f);
 1285|       |    const Packet cst_p4 = pset1<Packet>(4.1898667812347412109375e-2f);
 1286|       |    const Packet cst_p5 = pset1<Packet>(8.33471305668354034423828125e-3f);
 1287|       |
 1288|       |    const Packet p_even = pmadd(r2, cst_p4, cst_p2);
 1289|       |    const Packet p_odd = pmadd(r2, cst_p5, cst_p3);
 1290|       |    const Packet p_low = padd(r, cst_one);
 1291|       |    Packet p = pmadd(r, p_odd, p_even);
 1292|       |    p = pmadd(r2, p, p_low);
 1293|       |
 1294|       |    // 4. Undo subtractive range reduction exp(m*ln(2) + r) = 2^m * exp(r).
 1295|       |    Packet e = pldexp_fast(p, m);
 1296|       |
 1297|       |    // 5. Undo multiplicative range reduction by using exp(r) = exp(r/2)^2.
 1298|       |    e = pmul(e, e);
 1299|       |
 1300|       |    // Return exp(x) / (1 + exp(x))
 1301|       |    return pselect(zero_mask, cst_zero, pdiv(e, padd(cst_one, e)));
 1302|       |  }
 1303|       |};
 1304|       |#endif  // #ifndef EIGEN_GPU_COMPILE_PHASE
 1305|       |
 1306|       |template <typename T>
 1307|       |struct functor_traits<scalar_logistic_op<T>> {
 1308|       |  enum {
 1309|       |    // The cost estimate for float here here is for the common(?) case where
 1310|       |    // all arguments are greater than -9.
 1311|       |    Cost = scalar_div_cost<T, packet_traits<T>::HasDiv>::value +
 1312|       |           (internal::is_same<T, float>::value ? NumTraits<T>::AddCost * 15 + NumTraits<T>::MulCost * 11
 1313|       |                                               : NumTraits<T>::AddCost * 2 + functor_traits<scalar_exp_op<T>>::Cost),
 1314|       |    PacketAccess = !NumTraits<T>::IsComplex && packet_traits<T>::HasAdd && packet_traits<T>::HasDiv &&
 1315|       |                   (internal::is_same<T, float>::value
 1316|       |                        ? packet_traits<T>::HasMul && packet_traits<T>::HasMax && packet_traits<T>::HasMin
 1317|       |                        : packet_traits<T>::HasNegate && packet_traits<T>::HasExp)
 1318|       |  };
 1319|       |};
 1320|       |
 1321|       |template <typename Scalar, typename ExponentScalar, bool IsBaseInteger = NumTraits<Scalar>::IsInteger,
 1322|       |          bool IsExponentInteger = NumTraits<ExponentScalar>::IsInteger,
 1323|       |          bool IsBaseComplex = NumTraits<Scalar>::IsComplex,
 1324|       |          bool IsExponentComplex = NumTraits<ExponentScalar>::IsComplex>
 1325|       |struct scalar_unary_pow_op {
 1326|       |  typedef typename internal::promote_scalar_arg<
 1327|       |      Scalar, ExponentScalar,
 1328|       |      internal::has_ReturnType<ScalarBinaryOpTraits<Scalar, ExponentScalar, scalar_unary_pow_op>>::value>::type
 1329|       |      PromotedExponent;
 1330|       |  typedef typename ScalarBinaryOpTraits<Scalar, PromotedExponent, scalar_unary_pow_op>::ReturnType result_type;
 1331|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_unary_pow_op(const ExponentScalar& exponent) : m_exponent(exponent) {}
 1332|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(const Scalar& a) const {
 1333|       |    EIGEN_USING_STD(pow);
 1334|       |    return static_cast<result_type>(pow(a, m_exponent));
 1335|       |  }
 1336|       |
 1337|       | private:
 1338|       |  const ExponentScalar m_exponent;
 1339|       |  scalar_unary_pow_op() {}
 1340|       |};
 1341|       |
 1342|       |template <typename T>
 1343|       |constexpr int exponent_digits() {
 1344|       |  return CHAR_BIT * sizeof(T) - NumTraits<T>::digits() - NumTraits<T>::IsSigned;
 1345|       |}
 1346|       |
 1347|       |template <typename From, typename To>
 1348|       |struct is_floating_exactly_representable {
 1349|       |  // TODO(rmlarsen): Add radix to NumTraits and enable this check.
 1350|       |  // (NumTraits<To>::radix == NumTraits<From>::radix) &&
 1351|       |  static constexpr bool value =
 1352|       |      (exponent_digits<To>() >= exponent_digits<From>() && NumTraits<To>::digits() >= NumTraits<From>::digits());
 1353|       |};
 1354|       |
 1355|       |// Specialization for real, non-integer types, non-complex types.
 1356|       |template <typename Scalar, typename ExponentScalar>
 1357|       |struct scalar_unary_pow_op<Scalar, ExponentScalar, false, false, false, false> {
 1358|       |  template <bool IsExactlyRepresentable = is_floating_exactly_representable<ExponentScalar, Scalar>::value>
 1359|       |  std::enable_if_t<IsExactlyRepresentable, void> check_is_representable() const {}
 1360|       |
 1361|       |  // Issue a deprecation warning if we do a narrowing conversion on the exponent.
 1362|       |  template <bool IsExactlyRepresentable = is_floating_exactly_representable<ExponentScalar, Scalar>::value>
 1363|       |  EIGEN_DEPRECATED std::enable_if_t<!IsExactlyRepresentable, void> check_is_representable() const {}
 1364|       |
 1365|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_unary_pow_op(const ExponentScalar& exponent)
 1366|       |      : m_exponent(static_cast<Scalar>(exponent)) {
 1367|       |    check_is_representable();
 1368|       |  }
 1369|       |
 1370|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a) const {
 1371|       |    EIGEN_USING_STD(pow);
 1372|       |    return static_cast<Scalar>(pow(a, m_exponent));
 1373|       |  }
 1374|       |  template <typename Packet>
 1375|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const {
 1376|       |    return unary_pow_impl<Packet, Scalar>::run(a, m_exponent);
 1377|       |  }
 1378|       |
 1379|       | private:
 1380|       |  const Scalar m_exponent;
 1381|       |  scalar_unary_pow_op() {}
 1382|       |};
 1383|       |
 1384|       |template <typename Scalar, typename ExponentScalar, bool BaseIsInteger>
 1385|       |struct scalar_unary_pow_op<Scalar, ExponentScalar, BaseIsInteger, true, false, false> {
 1386|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_unary_pow_op(const ExponentScalar& exponent) : m_exponent(exponent) {}
 1387|       |  // TODO: error handling logic for complex^real_integer
 1388|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Scalar operator()(const Scalar& a) const {
 1389|       |    return unary_pow_impl<Scalar, ExponentScalar>::run(a, m_exponent);
 1390|       |  }
 1391|       |  template <typename Packet>
 1392|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Packet packetOp(const Packet& a) const {
 1393|       |    return unary_pow_impl<Packet, ExponentScalar>::run(a, m_exponent);
 1394|       |  }
 1395|       |
 1396|       | private:
 1397|       |  const ExponentScalar m_exponent;
 1398|       |  scalar_unary_pow_op() {}
 1399|       |};
 1400|       |
 1401|       |template <typename Scalar, typename ExponentScalar>
 1402|       |struct functor_traits<scalar_unary_pow_op<Scalar, ExponentScalar>> {
 1403|       |  enum {
 1404|       |    GenPacketAccess = functor_traits<scalar_pow_op<Scalar, ExponentScalar>>::PacketAccess,
 1405|       |    IntPacketAccess = !NumTraits<Scalar>::IsComplex && packet_traits<Scalar>::HasMul &&
 1406|       |                      (packet_traits<Scalar>::HasDiv || NumTraits<Scalar>::IsInteger) && packet_traits<Scalar>::HasCmp,
 1407|       |    PacketAccess = NumTraits<ExponentScalar>::IsInteger ? IntPacketAccess : (IntPacketAccess && GenPacketAccess),
 1408|       |    Cost = functor_traits<scalar_pow_op<Scalar, ExponentScalar>>::Cost
 1409|       |  };
 1410|       |};
 1411|       |
 1412|       |}  // end namespace internal
 1413|       |
 1414|       |}  // end namespace Eigen
 1415|       |
 1416|       |#endif  // EIGEN_FUNCTORS_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/products/GeneralBlockPanelKernel.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2009 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_GENERAL_BLOCK_PANEL_H
   11|       |#define EIGEN_GENERAL_BLOCK_PANEL_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |enum GEBPPacketSizeType { GEBPPacketFull = 0, GEBPPacketHalf, GEBPPacketQuarter };
   21|       |
   22|       |template <typename LhsScalar_, typename RhsScalar_, bool ConjLhs_ = false, bool ConjRhs_ = false,
   23|       |          int Arch = Architecture::Target, int PacketSize_ = GEBPPacketFull>
   24|       |class gebp_traits;
   25|       |
   26|       |/** \internal \returns b if a<=0, and returns a otherwise. */
   27|      0|inline std::ptrdiff_t manage_caching_sizes_helper(std::ptrdiff_t a, std::ptrdiff_t b) { return a <= 0 ? b : a; }
   28|       |
   29|       |#if defined(EIGEN_DEFAULT_L1_CACHE_SIZE)
   30|       |#define EIGEN_SET_DEFAULT_L1_CACHE_SIZE(val) EIGEN_DEFAULT_L1_CACHE_SIZE
   31|       |#else
   32|       |#define EIGEN_SET_DEFAULT_L1_CACHE_SIZE(val) val
   33|       |#endif  // defined(EIGEN_DEFAULT_L1_CACHE_SIZE)
   34|       |
   35|       |#if defined(EIGEN_DEFAULT_L2_CACHE_SIZE)
   36|       |#define EIGEN_SET_DEFAULT_L2_CACHE_SIZE(val) EIGEN_DEFAULT_L2_CACHE_SIZE
   37|       |#else
   38|       |#define EIGEN_SET_DEFAULT_L2_CACHE_SIZE(val) val
   39|       |#endif  // defined(EIGEN_DEFAULT_L2_CACHE_SIZE)
   40|       |
   41|       |#if defined(EIGEN_DEFAULT_L3_CACHE_SIZE)
   42|       |#define EIGEN_SET_DEFAULT_L3_CACHE_SIZE(val) EIGEN_DEFAULT_L3_CACHE_SIZE
   43|       |#else
   44|       |#define EIGEN_SET_DEFAULT_L3_CACHE_SIZE(val) val
   45|       |#endif  // defined(EIGEN_DEFAULT_L3_CACHE_SIZE)
   46|       |
   47|       |#if EIGEN_ARCH_i386_OR_x86_64
   48|       |const std::ptrdiff_t defaultL1CacheSize = EIGEN_SET_DEFAULT_L1_CACHE_SIZE(32 * 1024);
   49|       |const std::ptrdiff_t defaultL2CacheSize = EIGEN_SET_DEFAULT_L2_CACHE_SIZE(256 * 1024);
   50|       |const std::ptrdiff_t defaultL3CacheSize = EIGEN_SET_DEFAULT_L3_CACHE_SIZE(2 * 1024 * 1024);
   51|       |#elif EIGEN_ARCH_PPC
   52|       |const std::ptrdiff_t defaultL1CacheSize = EIGEN_SET_DEFAULT_L1_CACHE_SIZE(64 * 1024);
   53|       |#ifdef _ARCH_PWR10
   54|       |const std::ptrdiff_t defaultL2CacheSize = EIGEN_SET_DEFAULT_L2_CACHE_SIZE(2 * 1024 * 1024);
   55|       |const std::ptrdiff_t defaultL3CacheSize = EIGEN_SET_DEFAULT_L3_CACHE_SIZE(8 * 1024 * 1024);
   56|       |#else
   57|       |const std::ptrdiff_t defaultL2CacheSize = EIGEN_SET_DEFAULT_L2_CACHE_SIZE(512 * 1024);
   58|       |const std::ptrdiff_t defaultL3CacheSize = EIGEN_SET_DEFAULT_L3_CACHE_SIZE(4 * 1024 * 1024);
   59|       |#endif
   60|       |#else
   61|       |const std::ptrdiff_t defaultL1CacheSize = EIGEN_SET_DEFAULT_L1_CACHE_SIZE(16 * 1024);
   62|       |const std::ptrdiff_t defaultL2CacheSize = EIGEN_SET_DEFAULT_L2_CACHE_SIZE(512 * 1024);
   63|       |const std::ptrdiff_t defaultL3CacheSize = EIGEN_SET_DEFAULT_L3_CACHE_SIZE(512 * 1024);
   64|       |#endif
   65|       |
   66|       |#undef EIGEN_SET_DEFAULT_L1_CACHE_SIZE
   67|       |#undef EIGEN_SET_DEFAULT_L2_CACHE_SIZE
   68|       |#undef EIGEN_SET_DEFAULT_L3_CACHE_SIZE
   69|       |
   70|       |/** \internal */
   71|       |struct CacheSizes {
   72|      0|  CacheSizes() : m_l1(-1), m_l2(-1), m_l3(-1) {
   73|      0|    int l1CacheSize, l2CacheSize, l3CacheSize;
   74|      0|    queryCacheSizes(l1CacheSize, l2CacheSize, l3CacheSize);
   75|      0|    m_l1 = manage_caching_sizes_helper(l1CacheSize, defaultL1CacheSize);
   76|      0|    m_l2 = manage_caching_sizes_helper(l2CacheSize, defaultL2CacheSize);
   77|      0|    m_l3 = manage_caching_sizes_helper(l3CacheSize, defaultL3CacheSize);
   78|      0|  }
   79|       |
   80|       |  std::ptrdiff_t m_l1;
   81|       |  std::ptrdiff_t m_l2;
   82|       |  std::ptrdiff_t m_l3;
   83|       |};
   84|       |
   85|       |/** \internal */
   86|      0|inline void manage_caching_sizes(Action action, std::ptrdiff_t* l1, std::ptrdiff_t* l2, std::ptrdiff_t* l3) {
   87|      0|  static CacheSizes m_cacheSizes;
   88|      0|
   89|      0|  if (action == SetAction) {
   90|      0|    // set the cpu cache size and cache all block sizes from a global cache size in byte
   91|      0|    eigen_internal_assert(l1 != 0 && l2 != 0);
   92|      0|    m_cacheSizes.m_l1 = *l1;
   93|      0|    m_cacheSizes.m_l2 = *l2;
   94|      0|    m_cacheSizes.m_l3 = *l3;
   95|      0|  } else if (action == GetAction) {
   96|      0|    eigen_internal_assert(l1 != 0 && l2 != 0);
   97|      0|    *l1 = m_cacheSizes.m_l1;
   98|      0|    *l2 = m_cacheSizes.m_l2;
   99|      0|    *l3 = m_cacheSizes.m_l3;
  100|      0|  } else {
  101|      0|    eigen_internal_assert(false);
  102|      0|  }
  103|      0|}
  104|       |
  105|       |/* Helper for computeProductBlockingSizes.
  106|       | *
  107|       | * Given a m x k times k x n matrix product of scalar types \c LhsScalar and \c RhsScalar,
  108|       | * this function computes the blocking size parameters along the respective dimensions
  109|       | * for matrix products and related algorithms. The blocking sizes depends on various
  110|       | * parameters:
  111|       | * - the L1 and L2 cache sizes,
  112|       | * - the register level blocking sizes defined by gebp_traits,
  113|       | * - the number of scalars that fit into a packet (when vectorization is enabled).
  114|       | *
  115|       | * \sa setCpuCacheSizes */
  116|       |
  117|       |template <typename LhsScalar, typename RhsScalar, int KcFactor, typename Index>
  118|       |void evaluateProductBlockingSizesHeuristic(Index& k, Index& m, Index& n, Index num_threads = 1) {
  119|       |  typedef gebp_traits<LhsScalar, RhsScalar> Traits;
  120|       |
  121|       |  // Explanations:
  122|       |  // Let's recall that the product algorithms form mc x kc vertical panels A' on the lhs and
  123|       |  // kc x nc blocks B' on the rhs. B' has to fit into L2/L3 cache. Moreover, A' is processed
  124|       |  // per mr x kc horizontal small panels where mr is the blocking size along the m dimension
  125|       |  // at the register level. This small horizontal panel has to stay within L1 cache.
  126|       |  std::ptrdiff_t l1, l2, l3;
  127|       |  manage_caching_sizes(GetAction, &l1, &l2, &l3);
  128|       |#ifdef EIGEN_VECTORIZE_AVX512
  129|       |  // We need to find a rationale for that, but without this adjustment,
  130|       |  // performance with AVX512 is pretty bad, like -20% slower.
  131|       |  // One reason is that with increasing packet-size, the blocking size k
  132|       |  // has to become pretty small if we want that 1 lhs panel fit within L1.
  133|       |  // For instance, with the 3pX4 kernel and double, the size of the lhs+rhs panels are:
  134|       |  //   k*(3*64 + 4*8) Bytes, with l1=32kBytes, and k%8=0, we have k=144.
  135|       |  // This is quite small for a good reuse of the accumulation registers.
  136|       |  l1 *= 4;
  137|       |#endif
  138|       |
  139|       |  if (num_threads > 1) {
  140|       |    typedef typename Traits::ResScalar ResScalar;
  141|       |    enum {
  142|       |      kdiv = KcFactor * (Traits::mr * sizeof(LhsScalar) + Traits::nr * sizeof(RhsScalar)),
  143|       |      ksub = Traits::mr * (Traits::nr * sizeof(ResScalar)),
  144|       |      kr = 8,
  145|       |      mr = Traits::mr,
  146|       |      nr = Traits::nr
  147|       |    };
  148|       |    // Increasing k gives us more time to prefetch the content of the "C"
  149|       |    // registers. However once the latency is hidden there is no point in
  150|       |    // increasing the value of k, so we'll cap it at 320 (value determined
  151|       |    // experimentally).
  152|       |    // To avoid that k vanishes, we make k_cache at least as big as kr
  153|       |    const Index k_cache = numext::maxi<Index>(kr, (numext::mini<Index>)((l1 - ksub) / kdiv, 320));
  154|       |    if (k_cache < k) {
  155|       |      k = k_cache - (k_cache % kr);
  156|       |      eigen_internal_assert(k > 0);
  157|       |    }
  158|       |
  159|       |    const Index n_cache = (l2 - l1) / (nr * sizeof(RhsScalar) * k);
  160|       |    const Index n_per_thread = numext::div_ceil(n, num_threads);
  161|       |    if (n_cache <= n_per_thread) {
  162|       |      // Don't exceed the capacity of the l2 cache.
  163|       |      eigen_internal_assert(n_cache >= static_cast<Index>(nr));
  164|       |      n = n_cache - (n_cache % nr);
  165|       |      eigen_internal_assert(n > 0);
  166|       |    } else {
  167|       |      n = (numext::mini<Index>)(n, (n_per_thread + nr - 1) - ((n_per_thread + nr - 1) % nr));
  168|       |    }
  169|       |
  170|       |    if (l3 > l2) {
  171|       |      // l3 is shared between all cores, so we'll give each thread its own chunk of l3.
  172|       |      const Index m_cache = (l3 - l2) / (sizeof(LhsScalar) * k * num_threads);
  173|       |      const Index m_per_thread = numext::div_ceil(m, num_threads);
  174|       |      if (m_cache < m_per_thread && m_cache >= static_cast<Index>(mr)) {
  175|       |        m = m_cache - (m_cache % mr);
  176|       |        eigen_internal_assert(m > 0);
  177|       |      } else {
  178|       |        m = (numext::mini<Index>)(m, (m_per_thread + mr - 1) - ((m_per_thread + mr - 1) % mr));
  179|       |      }
  180|       |    }
  181|       |  } else {
  182|       |    // In unit tests we do not want to use extra large matrices,
  183|       |    // so we reduce the cache size to check the blocking strategy is not flawed
  184|       |#ifdef EIGEN_DEBUG_SMALL_PRODUCT_BLOCKS
  185|       |    l1 = 9 * 1024;
  186|       |    l2 = 32 * 1024;
  187|       |    l3 = 512 * 1024;
  188|       |#endif
  189|       |
  190|       |    // Early return for small problems because the computation below are time consuming for small problems.
  191|       |    // Perhaps it would make more sense to consider k*n*m??
  192|       |    // Note that for very tiny problem, this function should be bypassed anyway
  193|       |    // because we use the coefficient-based implementation for them.
  194|       |    if ((numext::maxi)(k, (numext::maxi)(m, n)) < 48) return;
  195|       |
  196|       |    typedef typename Traits::ResScalar ResScalar;
  197|       |    enum {
  198|       |      k_peeling = 8,
  199|       |      k_div = KcFactor * (Traits::mr * sizeof(LhsScalar) + Traits::nr * sizeof(RhsScalar)),
  200|       |      k_sub = Traits::mr * (Traits::nr * sizeof(ResScalar))
  201|       |    };
  202|       |
  203|       |    // ---- 1st level of blocking on L1, yields kc ----
  204|       |
  205|       |    // Blocking on the third dimension (i.e., k) is chosen so that an horizontal panel
  206|       |    // of size mr x kc of the lhs plus a vertical panel of kc x nr of the rhs both fits within L1 cache.
  207|       |    // We also include a register-level block of the result (mx x nr).
  208|       |    // (In an ideal world only the lhs panel would stay in L1)
  209|       |    // Moreover, kc has to be a multiple of 8 to be compatible with loop peeling, leading to a maximum blocking size of:
  210|       |    const Index max_kc = numext::maxi<Index>(((l1 - k_sub) / k_div) & (~(k_peeling - 1)), 1);
  211|       |    const Index old_k = k;
  212|       |    if (k > max_kc) {
  213|       |      // We are really blocking on the third dimension:
  214|       |      // -> reduce blocking size to make sure the last block is as large as possible
  215|       |      //    while keeping the same number of sweeps over the result.
  216|       |      k = (k % max_kc) == 0 ? max_kc
  217|       |                            : max_kc - k_peeling * ((max_kc - 1 - (k % max_kc)) / (k_peeling * (k / max_kc + 1)));
  218|       |
  219|       |      eigen_internal_assert(((old_k / k) == (old_k / max_kc)) && "the number of sweeps has to remain the same");
  220|       |    }
  221|       |
  222|       |// ---- 2nd level of blocking on max(L2,L3), yields nc ----
  223|       |
  224|       |// TODO find a reliable way to get the actual amount of cache per core to use for 2nd level blocking, that is:
  225|       |//      actual_l2 = max(l2, l3/nb_core_sharing_l3)
  226|       |// The number below is quite conservative: it is better to underestimate the cache size rather than overestimating it)
  227|       |// For instance, it corresponds to 6MB of L3 shared among 4 cores.
  228|       |#ifdef EIGEN_DEBUG_SMALL_PRODUCT_BLOCKS
  229|       |    const Index actual_l2 = l3;
  230|       |#else
  231|       |    const Index actual_l2 = 1572864;  // == 1.5 MB
  232|       |#endif
  233|       |
  234|       |    // Here, nc is chosen such that a block of kc x nc of the rhs fit within half of L2.
  235|       |    // The second half is implicitly reserved to access the result and lhs coefficients.
  236|       |    // When k<max_kc, then nc can arbitrarily growth. In practice, it seems to be fruitful
  237|       |    // to limit this growth: we bound nc to growth by a factor x1.5.
  238|       |    // However, if the entire lhs block fit within L1, then we are not going to block on the rows at all,
  239|       |    // and it becomes fruitful to keep the packed rhs blocks in L1 if there is enough remaining space.
  240|       |    Index max_nc;
  241|       |    const Index lhs_bytes = m * k * sizeof(LhsScalar);
  242|       |    const Index remaining_l1 = l1 - k_sub - lhs_bytes;
  243|       |    if (remaining_l1 >= Index(Traits::nr * sizeof(RhsScalar)) * k) {
  244|       |      // L1 blocking
  245|       |      max_nc = remaining_l1 / (k * sizeof(RhsScalar));
  246|       |    } else {
  247|       |      // L2 blocking
  248|       |      max_nc = (3 * actual_l2) / (2 * 2 * max_kc * sizeof(RhsScalar));
  249|       |    }
  250|       |    // WARNING Below, we assume that Traits::nr is a power of two.
  251|       |    Index nc = numext::mini<Index>(actual_l2 / (2 * k * sizeof(RhsScalar)), max_nc) & (~(Traits::nr - 1));
  252|       |    if (n > nc) {
  253|       |      // We are really blocking over the columns:
  254|       |      // -> reduce blocking size to make sure the last block is as large as possible
  255|       |      //    while keeping the same number of sweeps over the packed lhs.
  256|       |      //    Here we allow one more sweep if this gives us a perfect match, thus the commented "-1"
  257|       |      n = (n % nc) == 0 ? nc : (nc - Traits::nr * ((nc /*-1*/ - (n % nc)) / (Traits::nr * (n / nc + 1))));
  258|       |    } else if (old_k == k) {
  259|       |      // So far, no blocking at all, i.e., kc==k, and nc==n.
  260|       |      // In this case, let's perform a blocking over the rows such that the packed lhs data is kept in cache L1/L2
  261|       |      // TODO: part of this blocking strategy is now implemented within the kernel itself, so the L1-based heuristic
  262|       |      // here should be obsolete.
  263|       |      Index problem_size = k * n * sizeof(LhsScalar);
  264|       |      Index actual_lm = actual_l2;
  265|       |      Index max_mc = m;
  266|       |      if (problem_size <= 1024) {
  267|       |        // problem is small enough to keep in L1
  268|       |        // Let's choose m such that lhs's block fit in 1/3 of L1
  269|       |        actual_lm = l1;
  270|       |      } else if (l3 != 0 && problem_size <= 32768) {
  271|       |        // we have both L2 and L3, and problem is small enough to be kept in L2
  272|       |        // Let's choose m such that lhs's block fit in 1/3 of L2
  273|       |        actual_lm = l2;
  274|       |        max_mc = (numext::mini<Index>)(576, max_mc);
  275|       |      }
  276|       |      Index mc = (numext::mini<Index>)(actual_lm / (3 * k * sizeof(LhsScalar)), max_mc);
  277|       |      if (mc > Traits::mr)
  278|       |        mc -= mc % Traits::mr;
  279|       |      else if (mc == 0)
  280|       |        return;
  281|       |      m = (m % mc) == 0 ? mc : (mc - Traits::mr * ((mc /*-1*/ - (m % mc)) / (Traits::mr * (m / mc + 1))));
  282|       |    }
  283|       |  }
  284|       |}
  285|       |
  286|       |template <typename Index>
  287|       |inline bool useSpecificBlockingSizes(Index& k, Index& m, Index& n) {
  288|       |#ifdef EIGEN_TEST_SPECIFIC_BLOCKING_SIZES
  289|       |  if (EIGEN_TEST_SPECIFIC_BLOCKING_SIZES) {
  290|       |    k = numext::mini<Index>(k, EIGEN_TEST_SPECIFIC_BLOCKING_SIZE_K);
  291|       |    m = numext::mini<Index>(m, EIGEN_TEST_SPECIFIC_BLOCKING_SIZE_M);
  292|       |    n = numext::mini<Index>(n, EIGEN_TEST_SPECIFIC_BLOCKING_SIZE_N);
  293|       |    return true;
  294|       |  }
  295|       |#else
  296|       |  EIGEN_UNUSED_VARIABLE(k)
  297|       |  EIGEN_UNUSED_VARIABLE(m)
  298|       |  EIGEN_UNUSED_VARIABLE(n)
  299|       |#endif
  300|       |  return false;
  301|       |}
  302|       |
  303|       |/** \brief Computes the blocking parameters for a m x k times k x n matrix product
  304|       | *
  305|       | * \param[in,out] k Input: the third dimension of the product. Output: the blocking size along the same dimension.
  306|       | * \param[in,out] m Input: the number of rows of the left hand side. Output: the blocking size along the same dimension.
  307|       | * \param[in,out] n Input: the number of columns of the right hand side. Output: the blocking size along the same
  308|       | * dimension.
  309|       | *
  310|       | * Given a m x k times k x n matrix product of scalar types \c LhsScalar and \c RhsScalar,
  311|       | * this function computes the blocking size parameters along the respective dimensions
  312|       | * for matrix products and related algorithms.
  313|       | *
  314|       | * The blocking size parameters may be evaluated:
  315|       | *   - either by a heuristic based on cache sizes;
  316|       | *   - or using fixed prescribed values (for testing purposes).
  317|       | *
  318|       | * \sa setCpuCacheSizes */
  319|       |
  320|       |template <typename LhsScalar, typename RhsScalar, int KcFactor, typename Index>
  321|       |void computeProductBlockingSizes(Index& k, Index& m, Index& n, Index num_threads = 1) {
  322|       |  if (!useSpecificBlockingSizes(k, m, n)) {
  323|       |    evaluateProductBlockingSizesHeuristic<LhsScalar, RhsScalar, KcFactor, Index>(k, m, n, num_threads);
  324|       |  }
  325|       |}
  326|       |
  327|       |template <typename LhsScalar, typename RhsScalar, typename Index>
  328|       |inline void computeProductBlockingSizes(Index& k, Index& m, Index& n, Index num_threads = 1) {
  329|       |  computeProductBlockingSizes<LhsScalar, RhsScalar, 1, Index>(k, m, n, num_threads);
  330|       |}
  331|       |
  332|       |template <typename RhsPacket, typename RhsPacketx4, int registers_taken>
  333|       |struct RhsPanelHelper {
  334|       | private:
  335|       |  static constexpr int remaining_registers =
  336|       |      (std::max)(int(EIGEN_ARCH_DEFAULT_NUMBER_OF_REGISTERS) - registers_taken, 0);
  337|       |
  338|       | public:
  339|       |  typedef std::conditional_t<remaining_registers >= 4, RhsPacketx4, RhsPacket> type;
  340|       |};
  341|       |
  342|       |template <typename Packet>
  343|       |struct QuadPacket {
  344|       |  Packet B_0, B1, B2, B3;
  345|       |  const Packet& get(const FixedInt<0>&) const { return B_0; }
  346|       |  const Packet& get(const FixedInt<1>&) const { return B1; }
  347|       |  const Packet& get(const FixedInt<2>&) const { return B2; }
  348|       |  const Packet& get(const FixedInt<3>&) const { return B3; }
  349|       |};
  350|       |
  351|       |template <int N, typename T1, typename T2, typename T3>
  352|       |struct packet_conditional {
  353|       |  typedef T3 type;
  354|       |};
  355|       |
  356|       |template <typename T1, typename T2, typename T3>
  357|       |struct packet_conditional<GEBPPacketFull, T1, T2, T3> {
  358|       |  typedef T1 type;
  359|       |};
  360|       |
  361|       |template <typename T1, typename T2, typename T3>
  362|       |struct packet_conditional<GEBPPacketHalf, T1, T2, T3> {
  363|       |  typedef T2 type;
  364|       |};
  365|       |
  366|       |#define PACKET_DECL_COND_POSTFIX(postfix, name, packet_size)                                               \
  367|       |  typedef typename packet_conditional<                                                                     \
  368|       |      packet_size, typename packet_traits<name##Scalar>::type, typename packet_traits<name##Scalar>::half, \
  369|       |      typename unpacket_traits<typename packet_traits<name##Scalar>::half>::half>::type name##Packet##postfix
  370|       |
  371|       |#define PACKET_DECL_COND(name, packet_size)                                                                \
  372|       |  typedef typename packet_conditional<                                                                     \
  373|       |      packet_size, typename packet_traits<name##Scalar>::type, typename packet_traits<name##Scalar>::half, \
  374|       |      typename unpacket_traits<typename packet_traits<name##Scalar>::half>::half>::type name##Packet
  375|       |
  376|       |#define PACKET_DECL_COND_SCALAR_POSTFIX(postfix, packet_size)                                  \
  377|       |  typedef typename packet_conditional<                                                         \
  378|       |      packet_size, typename packet_traits<Scalar>::type, typename packet_traits<Scalar>::half, \
  379|       |      typename unpacket_traits<typename packet_traits<Scalar>::half>::half>::type ScalarPacket##postfix
  380|       |
  381|       |#define PACKET_DECL_COND_SCALAR(packet_size)                                                   \
  382|       |  typedef typename packet_conditional<                                                         \
  383|       |      packet_size, typename packet_traits<Scalar>::type, typename packet_traits<Scalar>::half, \
  384|       |      typename unpacket_traits<typename packet_traits<Scalar>::half>::half>::type ScalarPacket
  385|       |
  386|       |/* Vectorization logic
  387|       | *  real*real: unpack rhs to constant packets, ...
  388|       | *
  389|       | *  cd*cd : unpack rhs to (b_r,b_r), (b_i,b_i), mul to get (a_r b_r,a_i b_r) (a_r b_i,a_i b_i),
  390|       | *          storing each res packet into two packets (2x2),
  391|       | *          at the end combine them: swap the second and addsub them
  392|       | *  cf*cf : same but with 2x4 blocks
  393|       | *  cplx*real : unpack rhs to constant packets, ...
  394|       | *  real*cplx : load lhs as (a0,a0,a1,a1), and mul as usual
  395|       | */
  396|       |template <typename LhsScalar_, typename RhsScalar_, bool ConjLhs_, bool ConjRhs_, int Arch, int PacketSize_>
  397|       |class gebp_traits {
  398|       | public:
  399|       |  typedef LhsScalar_ LhsScalar;
  400|       |  typedef RhsScalar_ RhsScalar;
  401|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar>::ReturnType ResScalar;
  402|       |
  403|       |  PACKET_DECL_COND_POSTFIX(_, Lhs, PacketSize_);
  404|       |  PACKET_DECL_COND_POSTFIX(_, Rhs, PacketSize_);
  405|       |  PACKET_DECL_COND_POSTFIX(_, Res, PacketSize_);
  406|       |
  407|       |  enum {
  408|       |    ConjLhs = ConjLhs_,
  409|       |    ConjRhs = ConjRhs_,
  410|       |    Vectorizable = unpacket_traits<LhsPacket_>::vectorizable && unpacket_traits<RhsPacket_>::vectorizable,
  411|       |    LhsPacketSize = Vectorizable ? unpacket_traits<LhsPacket_>::size : 1,
  412|       |    RhsPacketSize = Vectorizable ? unpacket_traits<RhsPacket_>::size : 1,
  413|       |    ResPacketSize = Vectorizable ? unpacket_traits<ResPacket_>::size : 1,
  414|       |
  415|       |    NumberOfRegisters = EIGEN_ARCH_DEFAULT_NUMBER_OF_REGISTERS,
  416|       |
  417|       |    // register block size along the N direction must be 1 or 4
  418|       |    nr = 4,
  419|       |
  420|       |    // register block size along the M direction (currently, this one cannot be modified)
  421|       |    default_mr = (plain_enum_min(16, NumberOfRegisters) / 2 / nr) * LhsPacketSize,
  422|       |#if defined(EIGEN_HAS_SINGLE_INSTRUCTION_MADD) && !defined(EIGEN_VECTORIZE_ALTIVEC) && \
  423|       |    !defined(EIGEN_VECTORIZE_VSX) && ((!EIGEN_COMP_MSVC) || (EIGEN_COMP_MSVC >= 1914))
  424|       |    // we assume 16 registers or more
  425|       |    // See bug 992, if the scalar type is not vectorizable but that EIGEN_HAS_SINGLE_INSTRUCTION_MADD is defined,
  426|       |    // then using 3*LhsPacketSize triggers non-implemented paths in syrk.
  427|       |    // Bug 1515: MSVC prior to v19.14 yields to register spilling.
  428|       |    mr = Vectorizable ? 3 * LhsPacketSize : default_mr,
  429|       |#else
  430|       |    mr = default_mr,
  431|       |#endif
  432|       |
  433|       |    LhsProgress = LhsPacketSize,
  434|       |    RhsProgress = 1
  435|       |  };
  436|       |
  437|       |  typedef std::conditional_t<Vectorizable, LhsPacket_, LhsScalar> LhsPacket;
  438|       |  typedef std::conditional_t<Vectorizable, RhsPacket_, RhsScalar> RhsPacket;
  439|       |  typedef std::conditional_t<Vectorizable, ResPacket_, ResScalar> ResPacket;
  440|       |  typedef LhsPacket LhsPacket4Packing;
  441|       |
  442|       |  typedef QuadPacket<RhsPacket> RhsPacketx4;
  443|       |  typedef ResPacket AccPacket;
  444|       |
  445|       |  EIGEN_STRONG_INLINE void initAcc(AccPacket& p) { p = pset1<ResPacket>(ResScalar(0)); }
  446|       |
  447|       |  template <typename RhsPacketType>
  448|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, RhsPacketType& dest) const {
  449|       |    dest = pset1<RhsPacketType>(*b);
  450|       |  }
  451|       |
  452|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, RhsPacketx4& dest) const {
  453|       |    pbroadcast4(b, dest.B_0, dest.B1, dest.B2, dest.B3);
  454|       |  }
  455|       |
  456|       |  template <typename RhsPacketType>
  457|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar* b, RhsPacketType& dest) const {
  458|       |    loadRhs(b, dest);
  459|       |  }
  460|       |
  461|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar*, RhsPacketx4&) const {}
  462|       |
  463|       |  EIGEN_STRONG_INLINE void loadRhsQuad(const RhsScalar* b, RhsPacket& dest) const { dest = ploadquad<RhsPacket>(b); }
  464|       |
  465|       |  template <typename LhsPacketType>
  466|       |  EIGEN_STRONG_INLINE void loadLhs(const LhsScalar* a, LhsPacketType& dest) const {
  467|       |    dest = pload<LhsPacketType>(a);
  468|       |  }
  469|       |
  470|       |  template <typename LhsPacketType>
  471|       |  EIGEN_STRONG_INLINE void loadLhsUnaligned(const LhsScalar* a, LhsPacketType& dest) const {
  472|       |    dest = ploadu<LhsPacketType>(a);
  473|       |  }
  474|       |
  475|       |  template <typename LhsPacketType, typename RhsPacketType, typename AccPacketType, typename LaneIdType>
  476|       |  EIGEN_STRONG_INLINE void madd(const LhsPacketType& a, const RhsPacketType& b, AccPacketType& c, RhsPacketType& tmp,
  477|       |                                const LaneIdType&) const {
  478|       |    conj_helper<LhsPacketType, RhsPacketType, ConjLhs, ConjRhs> cj;
  479|       |    // It would be a lot cleaner to call pmadd all the time. Unfortunately if we
  480|       |    // let gcc allocate the register in which to store the result of the pmul
  481|       |    // (in the case where there is no FMA) gcc fails to figure out how to avoid
  482|       |    // spilling register.
  483|       |#ifdef EIGEN_HAS_SINGLE_INSTRUCTION_MADD
  484|       |    EIGEN_UNUSED_VARIABLE(tmp);
  485|       |    c = cj.pmadd(a, b, c);
  486|       |#else
  487|       |    tmp = b;
  488|       |    tmp = cj.pmul(a, tmp);
  489|       |    c = padd(c, tmp);
  490|       |#endif
  491|       |  }
  492|       |
  493|       |  template <typename LhsPacketType, typename AccPacketType, typename LaneIdType>
  494|       |  EIGEN_STRONG_INLINE void madd(const LhsPacketType& a, const RhsPacketx4& b, AccPacketType& c, RhsPacket& tmp,
  495|       |                                const LaneIdType& lane) const {
  496|       |    madd(a, b.get(lane), c, tmp, lane);
  497|       |  }
  498|       |
  499|       |  EIGEN_STRONG_INLINE void acc(const AccPacket& c, const ResPacket& alpha, ResPacket& r) const {
  500|       |    r = pmadd(c, alpha, r);
  501|       |  }
  502|       |
  503|       |  template <typename ResPacketHalf>
  504|       |  EIGEN_STRONG_INLINE void acc(const ResPacketHalf& c, const ResPacketHalf& alpha, ResPacketHalf& r) const {
  505|       |    r = pmadd(c, alpha, r);
  506|       |  }
  507|       |};
  508|       |
  509|       |template <typename RealScalar, bool ConjLhs_, int Arch, int PacketSize_>
  510|       |class gebp_traits<std::complex<RealScalar>, RealScalar, ConjLhs_, false, Arch, PacketSize_> {
  511|       | public:
  512|       |  typedef std::complex<RealScalar> LhsScalar;
  513|       |  typedef RealScalar RhsScalar;
  514|       |  typedef typename ScalarBinaryOpTraits<LhsScalar, RhsScalar>::ReturnType ResScalar;
  515|       |
  516|       |  PACKET_DECL_COND_POSTFIX(_, Lhs, PacketSize_);
  517|       |  PACKET_DECL_COND_POSTFIX(_, Rhs, PacketSize_);
  518|       |  PACKET_DECL_COND_POSTFIX(_, Res, PacketSize_);
  519|       |
  520|       |  enum {
  521|       |    ConjLhs = ConjLhs_,
  522|       |    ConjRhs = false,
  523|       |    Vectorizable = unpacket_traits<LhsPacket_>::vectorizable && unpacket_traits<RhsPacket_>::vectorizable,
  524|       |    LhsPacketSize = Vectorizable ? unpacket_traits<LhsPacket_>::size : 1,
  525|       |    RhsPacketSize = Vectorizable ? unpacket_traits<RhsPacket_>::size : 1,
  526|       |    ResPacketSize = Vectorizable ? unpacket_traits<ResPacket_>::size : 1,
  527|       |
  528|       |    NumberOfRegisters = EIGEN_ARCH_DEFAULT_NUMBER_OF_REGISTERS,
  529|       |    nr = 4,
  530|       |#if defined(EIGEN_HAS_SINGLE_INSTRUCTION_MADD) && !defined(EIGEN_VECTORIZE_ALTIVEC) && !defined(EIGEN_VECTORIZE_VSX)
  531|       |    // we assume 16 registers
  532|       |    mr = 3 * LhsPacketSize,
  533|       |#else
  534|       |    mr = (plain_enum_min(16, NumberOfRegisters) / 2 / nr) * LhsPacketSize,
  535|       |#endif
  536|       |
  537|       |    LhsProgress = LhsPacketSize,
  538|       |    RhsProgress = 1
  539|       |  };
  540|       |
  541|       |  typedef std::conditional_t<Vectorizable, LhsPacket_, LhsScalar> LhsPacket;
  542|       |  typedef std::conditional_t<Vectorizable, RhsPacket_, RhsScalar> RhsPacket;
  543|       |  typedef std::conditional_t<Vectorizable, ResPacket_, ResScalar> ResPacket;
  544|       |  typedef LhsPacket LhsPacket4Packing;
  545|       |
  546|       |  typedef QuadPacket<RhsPacket> RhsPacketx4;
  547|       |
  548|       |  typedef ResPacket AccPacket;
  549|       |
  550|       |  EIGEN_STRONG_INLINE void initAcc(AccPacket& p) { p = pset1<ResPacket>(ResScalar(0)); }
  551|       |
  552|       |  template <typename RhsPacketType>
  553|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, RhsPacketType& dest) const {
  554|       |    dest = pset1<RhsPacketType>(*b);
  555|       |  }
  556|       |
  557|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, RhsPacketx4& dest) const {
  558|       |    pbroadcast4(b, dest.B_0, dest.B1, dest.B2, dest.B3);
  559|       |  }
  560|       |
  561|       |  template <typename RhsPacketType>
  562|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar* b, RhsPacketType& dest) const {
  563|       |    loadRhs(b, dest);
  564|       |  }
  565|       |
  566|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar*, RhsPacketx4&) const {}
  567|       |
  568|       |  EIGEN_STRONG_INLINE void loadRhsQuad(const RhsScalar* b, RhsPacket& dest) const {
  569|       |    loadRhsQuad_impl(b, dest, std::conditional_t<RhsPacketSize == 16, true_type, false_type>());
  570|       |  }
  571|       |
  572|       |  EIGEN_STRONG_INLINE void loadRhsQuad_impl(const RhsScalar* b, RhsPacket& dest, const true_type&) const {
  573|       |    // FIXME we can do better!
  574|       |    // what we want here is a ploadheight
  575|       |    RhsScalar tmp[4] = {b[0], b[0], b[1], b[1]};
  576|       |    dest = ploadquad<RhsPacket>(tmp);
  577|       |  }
  578|       |
  579|       |  EIGEN_STRONG_INLINE void loadRhsQuad_impl(const RhsScalar* b, RhsPacket& dest, const false_type&) const {
  580|       |    eigen_internal_assert(RhsPacketSize <= 8);
  581|       |    dest = pset1<RhsPacket>(*b);
  582|       |  }
  583|       |
  584|       |  EIGEN_STRONG_INLINE void loadLhs(const LhsScalar* a, LhsPacket& dest) const { dest = pload<LhsPacket>(a); }
  585|       |
  586|       |  template <typename LhsPacketType>
  587|       |  EIGEN_STRONG_INLINE void loadLhsUnaligned(const LhsScalar* a, LhsPacketType& dest) const {
  588|       |    dest = ploadu<LhsPacketType>(a);
  589|       |  }
  590|       |
  591|       |  template <typename LhsPacketType, typename RhsPacketType, typename AccPacketType, typename LaneIdType>
  592|       |  EIGEN_STRONG_INLINE void madd(const LhsPacketType& a, const RhsPacketType& b, AccPacketType& c, RhsPacketType& tmp,
  593|       |                                const LaneIdType&) const {
  594|       |    madd_impl(a, b, c, tmp, std::conditional_t<Vectorizable, true_type, false_type>());
  595|       |  }
  596|       |
  597|       |  template <typename LhsPacketType, typename RhsPacketType, typename AccPacketType>
  598|       |  EIGEN_STRONG_INLINE void madd_impl(const LhsPacketType& a, const RhsPacketType& b, AccPacketType& c,
  599|       |                                     RhsPacketType& tmp, const true_type&) const {
  600|       |#ifdef EIGEN_HAS_SINGLE_INSTRUCTION_MADD
  601|       |    EIGEN_UNUSED_VARIABLE(tmp);
  602|       |    c.v = pmadd(a.v, b, c.v);
  603|       |#else
  604|       |    tmp = b;
  605|       |    tmp = pmul(a.v, tmp);
  606|       |    c.v = padd(c.v, tmp);
  607|       |#endif
  608|       |  }
  609|       |
  610|       |  EIGEN_STRONG_INLINE void madd_impl(const LhsScalar& a, const RhsScalar& b, ResScalar& c, RhsScalar& /*tmp*/,
  611|       |                                     const false_type&) const {
  612|       |    c += a * b;
  613|       |  }
  614|       |
  615|       |  template <typename LhsPacketType, typename AccPacketType, typename LaneIdType>
  616|       |  EIGEN_STRONG_INLINE void madd(const LhsPacketType& a, const RhsPacketx4& b, AccPacketType& c, RhsPacket& tmp,
  617|       |                                const LaneIdType& lane) const {
  618|       |    madd(a, b.get(lane), c, tmp, lane);
  619|       |  }
  620|       |
  621|       |  template <typename ResPacketType, typename AccPacketType>
  622|       |  EIGEN_STRONG_INLINE void acc(const AccPacketType& c, const ResPacketType& alpha, ResPacketType& r) const {
  623|       |    conj_helper<ResPacketType, ResPacketType, ConjLhs, false> cj;
  624|       |    r = cj.pmadd(c, alpha, r);
  625|       |  }
  626|       |
  627|       | protected:
  628|       |};
  629|       |
  630|       |template <typename Packet>
  631|       |struct DoublePacket {
  632|       |  Packet first;
  633|       |  Packet second;
  634|       |};
  635|       |
  636|       |template <typename Packet>
  637|       |DoublePacket<Packet> padd(const DoublePacket<Packet>& a, const DoublePacket<Packet>& b) {
  638|       |  DoublePacket<Packet> res;
  639|       |  res.first = padd(a.first, b.first);
  640|       |  res.second = padd(a.second, b.second);
  641|       |  return res;
  642|       |}
  643|       |
  644|       |// note that for DoublePacket<RealPacket> the "4" in "downto4"
  645|       |// corresponds to the number of complexes, so it means "8"
  646|       |// it terms of real coefficients.
  647|       |
  648|       |template <typename Packet>
  649|       |const DoublePacket<Packet>& predux_half_dowto4(const DoublePacket<Packet>& a,
  650|       |                                               std::enable_if_t<unpacket_traits<Packet>::size <= 8>* = 0) {
  651|       |  return a;
  652|       |}
  653|       |
  654|       |template <typename Packet>
  655|       |DoublePacket<typename unpacket_traits<Packet>::half> predux_half_dowto4(
  656|       |    const DoublePacket<Packet>& a, std::enable_if_t<unpacket_traits<Packet>::size == 16>* = 0) {
  657|       |  // yes, that's pretty hackish :(
  658|       |  DoublePacket<typename unpacket_traits<Packet>::half> res;
  659|       |  typedef std::complex<typename unpacket_traits<Packet>::type> Cplx;
  660|       |  typedef typename packet_traits<Cplx>::type CplxPacket;
  661|       |  res.first = predux_half_dowto4(CplxPacket(a.first)).v;
  662|       |  res.second = predux_half_dowto4(CplxPacket(a.second)).v;
  663|       |  return res;
  664|       |}
  665|       |
  666|       |// same here, "quad" actually means "8" in terms of real coefficients
  667|       |template <typename Scalar, typename RealPacket>
  668|       |void loadQuadToDoublePacket(const Scalar* b, DoublePacket<RealPacket>& dest,
  669|       |                            std::enable_if_t<unpacket_traits<RealPacket>::size <= 8>* = 0) {
  670|       |  dest.first = pset1<RealPacket>(numext::real(*b));
  671|       |  dest.second = pset1<RealPacket>(numext::imag(*b));
  672|       |}
  673|       |
  674|       |template <typename Scalar, typename RealPacket>
  675|       |void loadQuadToDoublePacket(const Scalar* b, DoublePacket<RealPacket>& dest,
  676|       |                            std::enable_if_t<unpacket_traits<RealPacket>::size == 16>* = 0) {
  677|       |  // yes, that's pretty hackish too :(
  678|       |  typedef typename NumTraits<Scalar>::Real RealScalar;
  679|       |  RealScalar r[4] = {numext::real(b[0]), numext::real(b[0]), numext::real(b[1]), numext::real(b[1])};
  680|       |  RealScalar i[4] = {numext::imag(b[0]), numext::imag(b[0]), numext::imag(b[1]), numext::imag(b[1])};
  681|       |  dest.first = ploadquad<RealPacket>(r);
  682|       |  dest.second = ploadquad<RealPacket>(i);
  683|       |}
  684|       |
  685|       |template <typename Packet>
  686|       |struct unpacket_traits<DoublePacket<Packet> > {
  687|       |  typedef DoublePacket<typename unpacket_traits<Packet>::half> half;
  688|       |  enum { size = 2 * unpacket_traits<Packet>::size };
  689|       |};
  690|       |// template<typename Packet>
  691|       |// DoublePacket<Packet> pmadd(const DoublePacket<Packet> &a, const DoublePacket<Packet> &b)
  692|       |// {
  693|       |//   DoublePacket<Packet> res;
  694|       |//   res.first  = padd(a.first, b.first);
  695|       |//   res.second = padd(a.second,b.second);
  696|       |//   return res;
  697|       |// }
  698|       |
  699|       |template <typename RealScalar, bool ConjLhs_, bool ConjRhs_, int Arch, int PacketSize_>
  700|       |class gebp_traits<std::complex<RealScalar>, std::complex<RealScalar>, ConjLhs_, ConjRhs_, Arch, PacketSize_> {
  701|       | public:
  702|       |  typedef std::complex<RealScalar> Scalar;
  703|       |  typedef std::complex<RealScalar> LhsScalar;
  704|       |  typedef std::complex<RealScalar> RhsScalar;
  705|       |  typedef std::complex<RealScalar> ResScalar;
  706|       |
  707|       |  PACKET_DECL_COND_POSTFIX(_, Lhs, PacketSize_);
  708|       |  PACKET_DECL_COND_POSTFIX(_, Rhs, PacketSize_);
  709|       |  PACKET_DECL_COND_POSTFIX(_, Res, PacketSize_);
  710|       |  PACKET_DECL_COND(Real, PacketSize_);
  711|       |  PACKET_DECL_COND_SCALAR(PacketSize_);
  712|       |
  713|       |  enum {
  714|       |    ConjLhs = ConjLhs_,
  715|       |    ConjRhs = ConjRhs_,
  716|       |    Vectorizable = unpacket_traits<RealPacket>::vectorizable && unpacket_traits<ScalarPacket>::vectorizable,
  717|       |    ResPacketSize = Vectorizable ? unpacket_traits<ResPacket_>::size : 1,
  718|       |    LhsPacketSize = Vectorizable ? unpacket_traits<LhsPacket_>::size : 1,
  719|       |    RhsPacketSize = Vectorizable ? unpacket_traits<RhsScalar>::size : 1,
  720|       |    RealPacketSize = Vectorizable ? unpacket_traits<RealPacket>::size : 1,
  721|       |    NumberOfRegisters = EIGEN_ARCH_DEFAULT_NUMBER_OF_REGISTERS,
  722|       |
  723|       |    nr = 4,
  724|       |    mr = (plain_enum_min(16, NumberOfRegisters) / 2 / nr) * ResPacketSize,
  725|       |
  726|       |    LhsProgress = ResPacketSize,
  727|       |    RhsProgress = 1
  728|       |  };
  729|       |
  730|       |  typedef DoublePacket<RealPacket> DoublePacketType;
  731|       |
  732|       |  typedef std::conditional_t<Vectorizable, ScalarPacket, Scalar> LhsPacket4Packing;
  733|       |  typedef std::conditional_t<Vectorizable, RealPacket, Scalar> LhsPacket;
  734|       |  typedef std::conditional_t<Vectorizable, DoublePacketType, Scalar> RhsPacket;
  735|       |  typedef std::conditional_t<Vectorizable, ScalarPacket, Scalar> ResPacket;
  736|       |  typedef std::conditional_t<Vectorizable, DoublePacketType, Scalar> AccPacket;
  737|       |
  738|       |  // this actually holds 8 packets!
  739|       |  typedef QuadPacket<RhsPacket> RhsPacketx4;
  740|       |
  741|       |  EIGEN_STRONG_INLINE void initAcc(Scalar& p) { p = Scalar(0); }
  742|       |
  743|       |  EIGEN_STRONG_INLINE void initAcc(DoublePacketType& p) {
  744|       |    p.first = pset1<RealPacket>(RealScalar(0));
  745|       |    p.second = pset1<RealPacket>(RealScalar(0));
  746|       |  }
  747|       |
  748|       |  // Scalar path
  749|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, ScalarPacket& dest) const { dest = pset1<ScalarPacket>(*b); }
  750|       |
  751|       |  // Vectorized path
  752|       |  template <typename RealPacketType>
  753|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, DoublePacket<RealPacketType>& dest) const {
  754|       |    dest.first = pset1<RealPacketType>(numext::real(*b));
  755|       |    dest.second = pset1<RealPacketType>(numext::imag(*b));
  756|       |  }
  757|       |
  758|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, RhsPacketx4& dest) const {
  759|       |    loadRhs(b, dest.B_0);
  760|       |    loadRhs(b + 1, dest.B1);
  761|       |    loadRhs(b + 2, dest.B2);
  762|       |    loadRhs(b + 3, dest.B3);
  763|       |  }
  764|       |
  765|       |  // Scalar path
  766|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar* b, ScalarPacket& dest) const { loadRhs(b, dest); }
  767|       |
  768|       |  // Vectorized path
  769|       |  template <typename RealPacketType>
  770|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar* b, DoublePacket<RealPacketType>& dest) const {
  771|       |    loadRhs(b, dest);
  772|       |  }
  773|       |
  774|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar*, RhsPacketx4&) const {}
  775|       |
  776|       |  EIGEN_STRONG_INLINE void loadRhsQuad(const RhsScalar* b, ResPacket& dest) const { loadRhs(b, dest); }
  777|       |  EIGEN_STRONG_INLINE void loadRhsQuad(const RhsScalar* b, DoublePacketType& dest) const {
  778|       |    loadQuadToDoublePacket(b, dest);
  779|       |  }
  780|       |
  781|       |  // nothing special here
  782|       |  EIGEN_STRONG_INLINE void loadLhs(const LhsScalar* a, LhsPacket& dest) const {
  783|       |    dest = pload<LhsPacket>((const typename unpacket_traits<LhsPacket>::type*)(a));
  784|       |  }
  785|       |
  786|       |  template <typename LhsPacketType>
  787|       |  EIGEN_STRONG_INLINE void loadLhsUnaligned(const LhsScalar* a, LhsPacketType& dest) const {
  788|       |    dest = ploadu<LhsPacketType>((const typename unpacket_traits<LhsPacketType>::type*)(a));
  789|       |  }
  790|       |
  791|       |  template <typename LhsPacketType, typename RhsPacketType, typename ResPacketType, typename TmpType,
  792|       |            typename LaneIdType>
  793|       |  EIGEN_STRONG_INLINE std::enable_if_t<!is_same<RhsPacketType, RhsPacketx4>::value> madd(const LhsPacketType& a,
  794|       |                                                                                         const RhsPacketType& b,
  795|       |                                                                                         DoublePacket<ResPacketType>& c,
  796|       |                                                                                         TmpType& /*tmp*/,
  797|       |                                                                                         const LaneIdType&) const {
  798|       |    c.first = pmadd(a, b.first, c.first);
  799|       |    c.second = pmadd(a, b.second, c.second);
  800|       |  }
  801|       |
  802|       |  template <typename LaneIdType>
  803|       |  EIGEN_STRONG_INLINE void madd(const LhsPacket& a, const RhsPacket& b, ResPacket& c, RhsPacket& /*tmp*/,
  804|       |                                const LaneIdType&) const {
  805|       |    c = cj.pmadd(a, b, c);
  806|       |  }
  807|       |
  808|       |  template <typename LhsPacketType, typename AccPacketType, typename LaneIdType>
  809|       |  EIGEN_STRONG_INLINE void madd(const LhsPacketType& a, const RhsPacketx4& b, AccPacketType& c, RhsPacket& tmp,
  810|       |                                const LaneIdType& lane) const {
  811|       |    madd(a, b.get(lane), c, tmp, lane);
  812|       |  }
  813|       |
  814|       |  EIGEN_STRONG_INLINE void acc(const Scalar& c, const Scalar& alpha, Scalar& r) const { r += alpha * c; }
  815|       |
  816|       |  template <typename RealPacketType, typename ResPacketType>
  817|       |  EIGEN_STRONG_INLINE void acc(const DoublePacket<RealPacketType>& c, const ResPacketType& alpha,
  818|       |                               ResPacketType& r) const {
  819|       |    // assemble c
  820|       |    ResPacketType tmp;
  821|       |    if ((!ConjLhs) && (!ConjRhs)) {
  822|       |      tmp = pcplxflip(pconj(ResPacketType(c.second)));
  823|       |      tmp = padd(ResPacketType(c.first), tmp);
  824|       |    } else if ((!ConjLhs) && (ConjRhs)) {
  825|       |      tmp = pconj(pcplxflip(ResPacketType(c.second)));
  826|       |      tmp = padd(ResPacketType(c.first), tmp);
  827|       |    } else if ((ConjLhs) && (!ConjRhs)) {
  828|       |      tmp = pcplxflip(ResPacketType(c.second));
  829|       |      tmp = padd(pconj(ResPacketType(c.first)), tmp);
  830|       |    } else if ((ConjLhs) && (ConjRhs)) {
  831|       |      tmp = pcplxflip(ResPacketType(c.second));
  832|       |      tmp = psub(pconj(ResPacketType(c.first)), tmp);
  833|       |    }
  834|       |
  835|       |    r = pmadd(tmp, alpha, r);
  836|       |  }
  837|       |
  838|       | protected:
  839|       |  conj_helper<LhsScalar, RhsScalar, ConjLhs, ConjRhs> cj;
  840|       |};
  841|       |
  842|       |template <typename RealScalar, bool ConjRhs_, int Arch, int PacketSize_>
  843|       |class gebp_traits<RealScalar, std::complex<RealScalar>, false, ConjRhs_, Arch, PacketSize_> {
  844|       | public:
  845|       |  typedef std::complex<RealScalar> Scalar;
  846|       |  typedef RealScalar LhsScalar;
  847|       |  typedef Scalar RhsScalar;
  848|       |  typedef Scalar ResScalar;
  849|       |
  850|       |  PACKET_DECL_COND_POSTFIX(_, Lhs, PacketSize_);
  851|       |  PACKET_DECL_COND_POSTFIX(_, Rhs, PacketSize_);
  852|       |  PACKET_DECL_COND_POSTFIX(_, Res, PacketSize_);
  853|       |  PACKET_DECL_COND_POSTFIX(_, Real, PacketSize_);
  854|       |  PACKET_DECL_COND_SCALAR_POSTFIX(_, PacketSize_);
  855|       |
  856|       |#undef PACKET_DECL_COND_SCALAR_POSTFIX
  857|       |#undef PACKET_DECL_COND_POSTFIX
  858|       |#undef PACKET_DECL_COND_SCALAR
  859|       |#undef PACKET_DECL_COND
  860|       |
  861|       |  enum {
  862|       |    ConjLhs = false,
  863|       |    ConjRhs = ConjRhs_,
  864|       |    Vectorizable = unpacket_traits<RealPacket_>::vectorizable && unpacket_traits<ScalarPacket_>::vectorizable,
  865|       |    LhsPacketSize = Vectorizable ? unpacket_traits<LhsPacket_>::size : 1,
  866|       |    RhsPacketSize = Vectorizable ? unpacket_traits<RhsPacket_>::size : 1,
  867|       |    ResPacketSize = Vectorizable ? unpacket_traits<ResPacket_>::size : 1,
  868|       |
  869|       |    NumberOfRegisters = EIGEN_ARCH_DEFAULT_NUMBER_OF_REGISTERS,
  870|       |    // FIXME: should depend on NumberOfRegisters
  871|       |    nr = 4,
  872|       |    mr = (plain_enum_min(16, NumberOfRegisters) / 2 / nr) * ResPacketSize,
  873|       |
  874|       |    LhsProgress = ResPacketSize,
  875|       |    RhsProgress = 1
  876|       |  };
  877|       |
  878|       |  typedef std::conditional_t<Vectorizable, LhsPacket_, LhsScalar> LhsPacket;
  879|       |  typedef std::conditional_t<Vectorizable, RhsPacket_, RhsScalar> RhsPacket;
  880|       |  typedef std::conditional_t<Vectorizable, ResPacket_, ResScalar> ResPacket;
  881|       |  typedef LhsPacket LhsPacket4Packing;
  882|       |  typedef QuadPacket<RhsPacket> RhsPacketx4;
  883|       |  typedef ResPacket AccPacket;
  884|       |
  885|       |  EIGEN_STRONG_INLINE void initAcc(AccPacket& p) { p = pset1<ResPacket>(ResScalar(0)); }
  886|       |
  887|       |  template <typename RhsPacketType>
  888|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, RhsPacketType& dest) const {
  889|       |    dest = pset1<RhsPacketType>(*b);
  890|       |  }
  891|       |
  892|       |  EIGEN_STRONG_INLINE void loadRhs(const RhsScalar* b, RhsPacketx4& dest) const {
  893|       |    pbroadcast4(b, dest.B_0, dest.B1, dest.B2, dest.B3);
  894|       |  }
  895|       |
  896|       |  template <typename RhsPacketType>
  897|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar* b, RhsPacketType& dest) const {
  898|       |    loadRhs(b, dest);
  899|       |  }
  900|       |
  901|       |  EIGEN_STRONG_INLINE void updateRhs(const RhsScalar*, RhsPacketx4&) const {}
  902|       |
  903|       |  EIGEN_STRONG_INLINE void loadLhs(const LhsScalar* a, LhsPacket& dest) const { dest = ploaddup<LhsPacket>(a); }
  904|       |
  905|       |  EIGEN_STRONG_INLINE void loadRhsQuad(const RhsScalar* b, RhsPacket& dest) const { dest = ploadquad<RhsPacket>(b); }
  906|       |
  907|       |  template <typename LhsPacketType>
  908|       |  EIGEN_STRONG_INLINE void loadLhsUnaligned(const LhsScalar* a, LhsPacketType& dest) const {
  909|       |    dest = ploaddup<LhsPacketType>(a);
  910|       |  }
  911|       |
  912|       |  template <typename LhsPacketType, typename RhsPacketType, typename AccPacketType, typename LaneIdType>
  913|       |  EIGEN_STRONG_INLINE void madd(const LhsPacketType& a, const RhsPacketType& b, AccPacketType& c, RhsPacketType& tmp,
  914|       |                                const LaneIdType&) const {
  915|       |    madd_impl(a, b, c, tmp, std::conditional_t<Vectorizable, true_type, false_type>());
  916|       |  }
  917|       |
  918|       |  template <typename LhsPacketType, typename RhsPacketType, typename AccPacketType>
  919|       |  EIGEN_STRONG_INLINE void madd_impl(const LhsPacketType& a, const RhsPacketType& b, AccPacketType& c,
  920|       |                                     RhsPacketType& tmp, const true_type&) const {
  921|       |#ifdef EIGEN_HAS_SINGLE_INSTRUCTION_MADD
  922|       |    EIGEN_UNUSED_VARIABLE(tmp);
  923|       |    c.v = pmadd(a, b.v, c.v);
  924|       |#else
  925|       |    tmp = b;
  926|       |    tmp.v = pmul(a, tmp.v);
  927|       |    c = padd(c, tmp);
  928|       |#endif
  929|       |  }
  930|       |
  931|       |  EIGEN_STRONG_INLINE void madd_impl(const LhsScalar& a, const RhsScalar& b, ResScalar& c, RhsScalar& /*tmp*/,
  932|       |                                     const false_type&) const {
  933|       |    c += a * b;
  934|       |  }
  935|       |
  936|       |  template <typename LhsPacketType, typename AccPacketType, typename LaneIdType>
  937|       |  EIGEN_STRONG_INLINE void madd(const LhsPacketType& a, const RhsPacketx4& b, AccPacketType& c, RhsPacket& tmp,
  938|       |                                const LaneIdType& lane) const {
  939|       |    madd(a, b.get(lane), c, tmp, lane);
  940|       |  }
  941|       |
  942|       |  template <typename ResPacketType, typename AccPacketType>
  943|       |  EIGEN_STRONG_INLINE void acc(const AccPacketType& c, const ResPacketType& alpha, ResPacketType& r) const {
  944|       |    conj_helper<ResPacketType, ResPacketType, false, ConjRhs> cj;
  945|       |    r = cj.pmadd(alpha, c, r);
  946|       |  }
  947|       |
  948|       | protected:
  949|       |};
  950|       |
  951|       |/* optimized General packed Block * packed Panel product kernel
  952|       | *
  953|       | * Mixing type logic: C += A * B
  954|       | *  |  A  |  B  | comments
  955|       | *  |real |cplx | no vectorization yet, would require to pack A with duplication
  956|       | *  |cplx |real | easy vectorization
  957|       | */
  958|       |template <typename LhsScalar, typename RhsScalar, typename Index, typename DataMapper, int mr, int nr,
  959|       |          bool ConjugateLhs, bool ConjugateRhs>
  960|       |struct gebp_kernel {
  961|       |  typedef gebp_traits<LhsScalar, RhsScalar, ConjugateLhs, ConjugateRhs, Architecture::Target> Traits;
  962|       |  typedef gebp_traits<LhsScalar, RhsScalar, ConjugateLhs, ConjugateRhs, Architecture::Target, GEBPPacketHalf>
  963|       |      HalfTraits;
  964|       |  typedef gebp_traits<LhsScalar, RhsScalar, ConjugateLhs, ConjugateRhs, Architecture::Target, GEBPPacketQuarter>
  965|       |      QuarterTraits;
  966|       |
  967|       |  typedef typename Traits::ResScalar ResScalar;
  968|       |  typedef typename Traits::LhsPacket LhsPacket;
  969|       |  typedef typename Traits::RhsPacket RhsPacket;
  970|       |  typedef typename Traits::ResPacket ResPacket;
  971|       |  typedef typename Traits::AccPacket AccPacket;
  972|       |  typedef typename Traits::RhsPacketx4 RhsPacketx4;
  973|       |
  974|       |  typedef typename RhsPanelHelper<RhsPacket, RhsPacketx4, 15>::type RhsPanel15;
  975|       |  typedef typename RhsPanelHelper<RhsPacket, RhsPacketx4, 27>::type RhsPanel27;
  976|       |
  977|       |  typedef gebp_traits<RhsScalar, LhsScalar, ConjugateRhs, ConjugateLhs, Architecture::Target> SwappedTraits;
  978|       |
  979|       |  typedef typename SwappedTraits::ResScalar SResScalar;
  980|       |  typedef typename SwappedTraits::LhsPacket SLhsPacket;
  981|       |  typedef typename SwappedTraits::RhsPacket SRhsPacket;
  982|       |  typedef typename SwappedTraits::ResPacket SResPacket;
  983|       |  typedef typename SwappedTraits::AccPacket SAccPacket;
  984|       |
  985|       |  typedef typename HalfTraits::LhsPacket LhsPacketHalf;
  986|       |  typedef typename HalfTraits::RhsPacket RhsPacketHalf;
  987|       |  typedef typename HalfTraits::ResPacket ResPacketHalf;
  988|       |  typedef typename HalfTraits::AccPacket AccPacketHalf;
  989|       |
  990|       |  typedef typename QuarterTraits::LhsPacket LhsPacketQuarter;
  991|       |  typedef typename QuarterTraits::RhsPacket RhsPacketQuarter;
  992|       |  typedef typename QuarterTraits::ResPacket ResPacketQuarter;
  993|       |  typedef typename QuarterTraits::AccPacket AccPacketQuarter;
  994|       |
  995|       |  typedef typename DataMapper::LinearMapper LinearMapper;
  996|       |
  997|       |  enum {
  998|       |    Vectorizable = Traits::Vectorizable,
  999|       |    LhsProgress = Traits::LhsProgress,
 1000|       |    LhsProgressHalf = HalfTraits::LhsProgress,
 1001|       |    LhsProgressQuarter = QuarterTraits::LhsProgress,
 1002|       |    RhsProgress = Traits::RhsProgress,
 1003|       |    RhsProgressHalf = HalfTraits::RhsProgress,
 1004|       |    RhsProgressQuarter = QuarterTraits::RhsProgress,
 1005|       |    ResPacketSize = Traits::ResPacketSize
 1006|       |  };
 1007|       |
 1008|       |  EIGEN_DONT_INLINE void operator()(const DataMapper& res, const LhsScalar* blockA, const RhsScalar* blockB, Index rows,
 1009|       |                                    Index depth, Index cols, ResScalar alpha, Index strideA = -1, Index strideB = -1,
 1010|       |                                    Index offsetA = 0, Index offsetB = 0);
 1011|       |};
 1012|       |
 1013|       |template <typename LhsScalar, typename RhsScalar, typename Index, typename DataMapper, int mr, int nr,
 1014|       |          bool ConjugateLhs, bool ConjugateRhs,
 1015|       |          int SwappedLhsProgress =
 1016|       |              gebp_traits<RhsScalar, LhsScalar, ConjugateRhs, ConjugateLhs, Architecture::Target>::LhsProgress>
 1017|       |struct last_row_process_16_packets {
 1018|       |  typedef gebp_traits<LhsScalar, RhsScalar, ConjugateLhs, ConjugateRhs, Architecture::Target> Traits;
 1019|       |  typedef gebp_traits<RhsScalar, LhsScalar, ConjugateRhs, ConjugateLhs, Architecture::Target> SwappedTraits;
 1020|       |
 1021|       |  typedef typename Traits::ResScalar ResScalar;
 1022|       |  typedef typename SwappedTraits::LhsPacket SLhsPacket;
 1023|       |  typedef typename SwappedTraits::RhsPacket SRhsPacket;
 1024|       |  typedef typename SwappedTraits::ResPacket SResPacket;
 1025|       |  typedef typename SwappedTraits::AccPacket SAccPacket;
 1026|       |
 1027|       |  EIGEN_STRONG_INLINE void operator()(const DataMapper& res, SwappedTraits& straits, const LhsScalar* blA,
 1028|       |                                      const RhsScalar* blB, Index depth, const Index endk, Index i, Index j2,
 1029|       |                                      ResScalar alpha, SAccPacket& C0) {
 1030|       |    EIGEN_UNUSED_VARIABLE(res);
 1031|       |    EIGEN_UNUSED_VARIABLE(straits);
 1032|       |    EIGEN_UNUSED_VARIABLE(blA);
 1033|       |    EIGEN_UNUSED_VARIABLE(blB);
 1034|       |    EIGEN_UNUSED_VARIABLE(depth);
 1035|       |    EIGEN_UNUSED_VARIABLE(endk);
 1036|       |    EIGEN_UNUSED_VARIABLE(i);
 1037|       |    EIGEN_UNUSED_VARIABLE(j2);
 1038|       |    EIGEN_UNUSED_VARIABLE(alpha);
 1039|       |    EIGEN_UNUSED_VARIABLE(C0);
 1040|       |  }
 1041|       |};
 1042|       |
 1043|       |template <typename LhsScalar, typename RhsScalar, typename Index, typename DataMapper, int mr, int nr,
 1044|       |          bool ConjugateLhs, bool ConjugateRhs>
 1045|       |struct last_row_process_16_packets<LhsScalar, RhsScalar, Index, DataMapper, mr, nr, ConjugateLhs, ConjugateRhs, 16> {
 1046|       |  typedef gebp_traits<LhsScalar, RhsScalar, ConjugateLhs, ConjugateRhs, Architecture::Target> Traits;
 1047|       |  typedef gebp_traits<RhsScalar, LhsScalar, ConjugateRhs, ConjugateLhs, Architecture::Target> SwappedTraits;
 1048|       |
 1049|       |  typedef typename Traits::ResScalar ResScalar;
 1050|       |  typedef typename SwappedTraits::LhsPacket SLhsPacket;
 1051|       |  typedef typename SwappedTraits::RhsPacket SRhsPacket;
 1052|       |  typedef typename SwappedTraits::ResPacket SResPacket;
 1053|       |  typedef typename SwappedTraits::AccPacket SAccPacket;
 1054|       |
 1055|       |  EIGEN_STRONG_INLINE void operator()(const DataMapper& res, SwappedTraits& straits, const LhsScalar* blA,
 1056|       |                                      const RhsScalar* blB, Index depth, const Index endk, Index i, Index j2,
 1057|       |                                      ResScalar alpha, SAccPacket& C0) {
 1058|       |    typedef typename unpacket_traits<typename unpacket_traits<SResPacket>::half>::half SResPacketQuarter;
 1059|       |    typedef typename unpacket_traits<typename unpacket_traits<SLhsPacket>::half>::half SLhsPacketQuarter;
 1060|       |    typedef typename unpacket_traits<typename unpacket_traits<SRhsPacket>::half>::half SRhsPacketQuarter;
 1061|       |    typedef typename unpacket_traits<typename unpacket_traits<SAccPacket>::half>::half SAccPacketQuarter;
 1062|       |
 1063|       |    SResPacketQuarter R = res.template gatherPacket<SResPacketQuarter>(i, j2);
 1064|       |    SResPacketQuarter alphav = pset1<SResPacketQuarter>(alpha);
 1065|       |
 1066|       |    if (depth - endk > 0) {
 1067|       |      // We have to handle the last row(s) of the rhs, which
 1068|       |      // correspond to a half-packet
 1069|       |      SAccPacketQuarter c0 = predux_half_dowto4(predux_half_dowto4(C0));
 1070|       |
 1071|       |      for (Index kk = endk; kk < depth; kk++) {
 1072|       |        SLhsPacketQuarter a0;
 1073|       |        SRhsPacketQuarter b0;
 1074|       |        straits.loadLhsUnaligned(blB, a0);
 1075|       |        straits.loadRhs(blA, b0);
 1076|       |        straits.madd(a0, b0, c0, b0, fix<0>);
 1077|       |        blB += SwappedTraits::LhsProgress / 4;
 1078|       |        blA += 1;
 1079|       |      }
 1080|       |      straits.acc(c0, alphav, R);
 1081|       |    } else {
 1082|       |      straits.acc(predux_half_dowto4(predux_half_dowto4(C0)), alphav, R);
 1083|       |    }
 1084|       |    res.scatterPacket(i, j2, R);
 1085|       |  }
 1086|       |};
 1087|       |
 1088|       |template <int nr, Index LhsProgress, Index RhsProgress, typename LhsScalar, typename RhsScalar, typename ResScalar,
 1089|       |          typename AccPacket, typename LhsPacket, typename RhsPacket, typename ResPacket, typename GEBPTraits,
 1090|       |          typename LinearMapper, typename DataMapper>
 1091|       |struct lhs_process_one_packet {
 1092|       |  typedef typename GEBPTraits::RhsPacketx4 RhsPacketx4;
 1093|       |
 1094|       |  EIGEN_STRONG_INLINE void peeled_kc_onestep(Index K, const LhsScalar* blA, const RhsScalar* blB, GEBPTraits traits,
 1095|       |                                             LhsPacket* A0, RhsPacketx4* rhs_panel, RhsPacket* T0, AccPacket* C0,
 1096|       |                                             AccPacket* C1, AccPacket* C2, AccPacket* C3) {
 1097|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 1X4");
 1098|       |    EIGEN_ASM_COMMENT("Note: these asm comments work around bug 935!");
 1099|       |    traits.loadLhs(&blA[(0 + 1 * K) * LhsProgress], *A0);
 1100|       |    traits.loadRhs(&blB[(0 + 4 * K) * RhsProgress], *rhs_panel);
 1101|       |    traits.madd(*A0, *rhs_panel, *C0, *T0, fix<0>);
 1102|       |    traits.madd(*A0, *rhs_panel, *C1, *T0, fix<1>);
 1103|       |    traits.madd(*A0, *rhs_panel, *C2, *T0, fix<2>);
 1104|       |    traits.madd(*A0, *rhs_panel, *C3, *T0, fix<3>);
 1105|       |#if EIGEN_GNUC_STRICT_AT_LEAST(6, 0, 0) && defined(EIGEN_VECTORIZE_SSE) && !(EIGEN_COMP_LCC)
 1106|       |    __asm__("" : "+x,m"(*A0));
 1107|       |#endif
 1108|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 1X4");
 1109|       |  }
 1110|       |
 1111|       |  EIGEN_STRONG_INLINE void operator()(const DataMapper& res, const LhsScalar* blockA, const RhsScalar* blockB,
 1112|       |                                      ResScalar alpha, Index peelStart, Index peelEnd, Index strideA, Index strideB,
 1113|       |                                      Index offsetA, Index offsetB, int prefetch_res_offset, Index peeled_kc, Index pk,
 1114|       |                                      Index cols, Index depth, Index packet_cols4) {
 1115|       |    GEBPTraits traits;
 1116|       |    Index packet_cols8 = nr >= 8 ? (cols / 8) * 8 : 0;
 1117|       |    // loops on each largest micro horizontal panel of lhs
 1118|       |    // (LhsProgress x depth)
 1119|       |    for (Index i = peelStart; i < peelEnd; i += LhsProgress) {
 1120|       |#if EIGEN_ARCH_ARM64 || EIGEN_ARCH_LOONGARCH64
 1121|       |      EIGEN_IF_CONSTEXPR(nr >= 8) {
 1122|       |        for (Index j2 = 0; j2 < packet_cols8; j2 += 8) {
 1123|       |          const LhsScalar* blA = &blockA[i * strideA + offsetA * (LhsProgress)];
 1124|       |          prefetch(&blA[0]);
 1125|       |
 1126|       |          // gets res block as register
 1127|       |          AccPacket C0, C1, C2, C3, C4, C5, C6, C7;
 1128|       |          traits.initAcc(C0);
 1129|       |          traits.initAcc(C1);
 1130|       |          traits.initAcc(C2);
 1131|       |          traits.initAcc(C3);
 1132|       |          traits.initAcc(C4);
 1133|       |          traits.initAcc(C5);
 1134|       |          traits.initAcc(C6);
 1135|       |          traits.initAcc(C7);
 1136|       |
 1137|       |          LinearMapper r0 = res.getLinearMapper(i, j2 + 0);
 1138|       |          LinearMapper r1 = res.getLinearMapper(i, j2 + 1);
 1139|       |          LinearMapper r2 = res.getLinearMapper(i, j2 + 2);
 1140|       |          LinearMapper r3 = res.getLinearMapper(i, j2 + 3);
 1141|       |          LinearMapper r4 = res.getLinearMapper(i, j2 + 4);
 1142|       |          LinearMapper r5 = res.getLinearMapper(i, j2 + 5);
 1143|       |          LinearMapper r6 = res.getLinearMapper(i, j2 + 6);
 1144|       |          LinearMapper r7 = res.getLinearMapper(i, j2 + 7);
 1145|       |          r0.prefetch(prefetch_res_offset);
 1146|       |          r1.prefetch(prefetch_res_offset);
 1147|       |          r2.prefetch(prefetch_res_offset);
 1148|       |          r3.prefetch(prefetch_res_offset);
 1149|       |          r4.prefetch(prefetch_res_offset);
 1150|       |          r5.prefetch(prefetch_res_offset);
 1151|       |          r6.prefetch(prefetch_res_offset);
 1152|       |          r7.prefetch(prefetch_res_offset);
 1153|       |          const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 8];
 1154|       |          prefetch(&blB[0]);
 1155|       |
 1156|       |          LhsPacket A0;
 1157|       |          for (Index k = 0; k < peeled_kc; k += pk) {
 1158|       |            RhsPacketx4 rhs_panel;
 1159|       |            RhsPacket T0;
 1160|       |#define EIGEN_GEBGP_ONESTEP(K)                                    \
 1161|       |  do {                                                            \
 1162|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 1pX8");    \
 1163|       |    traits.loadLhs(&blA[(0 + 1 * K) * LhsProgress], A0);          \
 1164|       |    traits.loadRhs(&blB[(0 + 8 * K) * RhsProgress], rhs_panel);   \
 1165|       |    traits.madd(A0, rhs_panel, C0, T0, fix<0>);                   \
 1166|       |    traits.updateRhs(&blB[(1 + 8 * K) * RhsProgress], rhs_panel); \
 1167|       |    traits.madd(A0, rhs_panel, C1, T0, fix<1>);                   \
 1168|       |    traits.updateRhs(&blB[(2 + 8 * K) * RhsProgress], rhs_panel); \
 1169|       |    traits.madd(A0, rhs_panel, C2, T0, fix<2>);                   \
 1170|       |    traits.updateRhs(&blB[(3 + 8 * K) * RhsProgress], rhs_panel); \
 1171|       |    traits.madd(A0, rhs_panel, C3, T0, fix<3>);                   \
 1172|       |    traits.loadRhs(&blB[(4 + 8 * K) * RhsProgress], rhs_panel);   \
 1173|       |    traits.madd(A0, rhs_panel, C4, T0, fix<0>);                   \
 1174|       |    traits.updateRhs(&blB[(5 + 8 * K) * RhsProgress], rhs_panel); \
 1175|       |    traits.madd(A0, rhs_panel, C5, T0, fix<1>);                   \
 1176|       |    traits.updateRhs(&blB[(6 + 8 * K) * RhsProgress], rhs_panel); \
 1177|       |    traits.madd(A0, rhs_panel, C6, T0, fix<2>);                   \
 1178|       |    traits.updateRhs(&blB[(7 + 8 * K) * RhsProgress], rhs_panel); \
 1179|       |    traits.madd(A0, rhs_panel, C7, T0, fix<3>);                   \
 1180|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 1pX8");      \
 1181|       |  } while (false)
 1182|       |
 1183|       |            EIGEN_ASM_COMMENT("begin gebp micro kernel 1pX8");
 1184|       |
 1185|       |            EIGEN_GEBGP_ONESTEP(0);
 1186|       |            EIGEN_GEBGP_ONESTEP(1);
 1187|       |            EIGEN_GEBGP_ONESTEP(2);
 1188|       |            EIGEN_GEBGP_ONESTEP(3);
 1189|       |            EIGEN_GEBGP_ONESTEP(4);
 1190|       |            EIGEN_GEBGP_ONESTEP(5);
 1191|       |            EIGEN_GEBGP_ONESTEP(6);
 1192|       |            EIGEN_GEBGP_ONESTEP(7);
 1193|       |
 1194|       |            blB += pk * 8 * RhsProgress;
 1195|       |            blA += pk * (1 * LhsProgress);
 1196|       |
 1197|       |            EIGEN_ASM_COMMENT("end gebp micro kernel 1pX8");
 1198|       |          }
 1199|       |          // process remaining peeled loop
 1200|       |          for (Index k = peeled_kc; k < depth; k++) {
 1201|       |            RhsPacketx4 rhs_panel;
 1202|       |            RhsPacket T0;
 1203|       |            EIGEN_GEBGP_ONESTEP(0);
 1204|       |            blB += 8 * RhsProgress;
 1205|       |            blA += 1 * LhsProgress;
 1206|       |          }
 1207|       |
 1208|       |#undef EIGEN_GEBGP_ONESTEP
 1209|       |
 1210|       |          ResPacket R0, R1;
 1211|       |          ResPacket alphav = pset1<ResPacket>(alpha);
 1212|       |
 1213|       |          R0 = r0.template loadPacket<ResPacket>(0);
 1214|       |          R1 = r1.template loadPacket<ResPacket>(0);
 1215|       |          traits.acc(C0, alphav, R0);
 1216|       |          traits.acc(C1, alphav, R1);
 1217|       |          r0.storePacket(0, R0);
 1218|       |          r1.storePacket(0, R1);
 1219|       |
 1220|       |          R0 = r2.template loadPacket<ResPacket>(0);
 1221|       |          R1 = r3.template loadPacket<ResPacket>(0);
 1222|       |          traits.acc(C2, alphav, R0);
 1223|       |          traits.acc(C3, alphav, R1);
 1224|       |          r2.storePacket(0, R0);
 1225|       |          r3.storePacket(0, R1);
 1226|       |
 1227|       |          R0 = r4.template loadPacket<ResPacket>(0);
 1228|       |          R1 = r5.template loadPacket<ResPacket>(0);
 1229|       |          traits.acc(C4, alphav, R0);
 1230|       |          traits.acc(C5, alphav, R1);
 1231|       |          r4.storePacket(0, R0);
 1232|       |          r5.storePacket(0, R1);
 1233|       |
 1234|       |          R0 = r6.template loadPacket<ResPacket>(0);
 1235|       |          R1 = r7.template loadPacket<ResPacket>(0);
 1236|       |          traits.acc(C6, alphav, R0);
 1237|       |          traits.acc(C7, alphav, R1);
 1238|       |          r6.storePacket(0, R0);
 1239|       |          r7.storePacket(0, R1);
 1240|       |        }
 1241|       |      }
 1242|       |#endif
 1243|       |
 1244|       |      // loops on each largest micro vertical panel of rhs (depth * nr)
 1245|       |      for (Index j2 = packet_cols8; j2 < packet_cols4; j2 += 4) {
 1246|       |        // We select a LhsProgress x nr micro block of res
 1247|       |        // which is entirely stored into 1 x nr registers.
 1248|       |
 1249|       |        const LhsScalar* blA = &blockA[i * strideA + offsetA * (LhsProgress)];
 1250|       |        prefetch(&blA[0]);
 1251|       |
 1252|       |        // gets res block as register
 1253|       |        AccPacket C0, C1, C2, C3;
 1254|       |        traits.initAcc(C0);
 1255|       |        traits.initAcc(C1);
 1256|       |        traits.initAcc(C2);
 1257|       |        traits.initAcc(C3);
 1258|       |        // To improve instruction pipelining, let's double the accumulation registers:
 1259|       |        //  even k will accumulate in C*, while odd k will accumulate in D*.
 1260|       |        // This trick is crucial to get good performance with FMA, otherwise it is
 1261|       |        // actually faster to perform separated MUL+ADD because of a naturally
 1262|       |        // better instruction-level parallelism.
 1263|       |        AccPacket D0, D1, D2, D3;
 1264|       |        traits.initAcc(D0);
 1265|       |        traits.initAcc(D1);
 1266|       |        traits.initAcc(D2);
 1267|       |        traits.initAcc(D3);
 1268|       |
 1269|       |        LinearMapper r0 = res.getLinearMapper(i, j2 + 0);
 1270|       |        LinearMapper r1 = res.getLinearMapper(i, j2 + 1);
 1271|       |        LinearMapper r2 = res.getLinearMapper(i, j2 + 2);
 1272|       |        LinearMapper r3 = res.getLinearMapper(i, j2 + 3);
 1273|       |
 1274|       |        r0.prefetch(prefetch_res_offset);
 1275|       |        r1.prefetch(prefetch_res_offset);
 1276|       |        r2.prefetch(prefetch_res_offset);
 1277|       |        r3.prefetch(prefetch_res_offset);
 1278|       |
 1279|       |        // performs "inner" products
 1280|       |        const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 4];
 1281|       |        prefetch(&blB[0]);
 1282|       |        LhsPacket A0, A1;
 1283|       |
 1284|       |        for (Index k = 0; k < peeled_kc; k += pk) {
 1285|       |          EIGEN_ASM_COMMENT("begin gebp micro kernel 1/half/quarterX4");
 1286|       |          RhsPacketx4 rhs_panel;
 1287|       |          RhsPacket T0;
 1288|       |
 1289|       |          internal::prefetch(blB + (48 + 0));
 1290|       |          peeled_kc_onestep(0, blA, blB, traits, &A0, &rhs_panel, &T0, &C0, &C1, &C2, &C3);
 1291|       |          peeled_kc_onestep(1, blA, blB, traits, &A1, &rhs_panel, &T0, &D0, &D1, &D2, &D3);
 1292|       |          peeled_kc_onestep(2, blA, blB, traits, &A0, &rhs_panel, &T0, &C0, &C1, &C2, &C3);
 1293|       |          peeled_kc_onestep(3, blA, blB, traits, &A1, &rhs_panel, &T0, &D0, &D1, &D2, &D3);
 1294|       |          internal::prefetch(blB + (48 + 16));
 1295|       |          peeled_kc_onestep(4, blA, blB, traits, &A0, &rhs_panel, &T0, &C0, &C1, &C2, &C3);
 1296|       |          peeled_kc_onestep(5, blA, blB, traits, &A1, &rhs_panel, &T0, &D0, &D1, &D2, &D3);
 1297|       |          peeled_kc_onestep(6, blA, blB, traits, &A0, &rhs_panel, &T0, &C0, &C1, &C2, &C3);
 1298|       |          peeled_kc_onestep(7, blA, blB, traits, &A1, &rhs_panel, &T0, &D0, &D1, &D2, &D3);
 1299|       |
 1300|       |          blB += pk * 4 * RhsProgress;
 1301|       |          blA += pk * LhsProgress;
 1302|       |
 1303|       |          EIGEN_ASM_COMMENT("end gebp micro kernel 1/half/quarterX4");
 1304|       |        }
 1305|       |        C0 = padd(C0, D0);
 1306|       |        C1 = padd(C1, D1);
 1307|       |        C2 = padd(C2, D2);
 1308|       |        C3 = padd(C3, D3);
 1309|       |
 1310|       |        // process remaining peeled loop
 1311|       |        for (Index k = peeled_kc; k < depth; k++) {
 1312|       |          RhsPacketx4 rhs_panel;
 1313|       |          RhsPacket T0;
 1314|       |          peeled_kc_onestep(0, blA, blB, traits, &A0, &rhs_panel, &T0, &C0, &C1, &C2, &C3);
 1315|       |          blB += 4 * RhsProgress;
 1316|       |          blA += LhsProgress;
 1317|       |        }
 1318|       |
 1319|       |        ResPacket R0, R1;
 1320|       |        ResPacket alphav = pset1<ResPacket>(alpha);
 1321|       |
 1322|       |        R0 = r0.template loadPacket<ResPacket>(0);
 1323|       |        R1 = r1.template loadPacket<ResPacket>(0);
 1324|       |        traits.acc(C0, alphav, R0);
 1325|       |        traits.acc(C1, alphav, R1);
 1326|       |        r0.storePacket(0, R0);
 1327|       |        r1.storePacket(0, R1);
 1328|       |
 1329|       |        R0 = r2.template loadPacket<ResPacket>(0);
 1330|       |        R1 = r3.template loadPacket<ResPacket>(0);
 1331|       |        traits.acc(C2, alphav, R0);
 1332|       |        traits.acc(C3, alphav, R1);
 1333|       |        r2.storePacket(0, R0);
 1334|       |        r3.storePacket(0, R1);
 1335|       |      }
 1336|       |
 1337|       |      // Deal with remaining columns of the rhs
 1338|       |      for (Index j2 = packet_cols4; j2 < cols; j2++) {
 1339|       |        // One column at a time
 1340|       |        const LhsScalar* blA = &blockA[i * strideA + offsetA * (LhsProgress)];
 1341|       |        prefetch(&blA[0]);
 1342|       |
 1343|       |        // gets res block as register
 1344|       |        AccPacket C0;
 1345|       |        traits.initAcc(C0);
 1346|       |
 1347|       |        LinearMapper r0 = res.getLinearMapper(i, j2);
 1348|       |
 1349|       |        // performs "inner" products
 1350|       |        const RhsScalar* blB = &blockB[j2 * strideB + offsetB];
 1351|       |        LhsPacket A0;
 1352|       |
 1353|       |        for (Index k = 0; k < peeled_kc; k += pk) {
 1354|       |          EIGEN_ASM_COMMENT("begin gebp micro kernel 1/half/quarterX1");
 1355|       |          RhsPacket B_0;
 1356|       |
 1357|       |#define EIGEN_GEBGP_ONESTEP(K)                                             \
 1358|       |  do {                                                                     \
 1359|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 1/half/quarterX1"); \
 1360|       |    EIGEN_ASM_COMMENT("Note: these asm comments work around bug 935!");    \
 1361|       |    /* FIXME: why unaligned???? */                                         \
 1362|       |    traits.loadLhsUnaligned(&blA[(0 + 1 * K) * LhsProgress], A0);          \
 1363|       |    traits.loadRhs(&blB[(0 + K) * RhsProgress], B_0);                      \
 1364|       |    traits.madd(A0, B_0, C0, B_0, fix<0>);                                 \
 1365|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 1/half/quarterX1");   \
 1366|       |  } while (false);
 1367|       |
 1368|       |          EIGEN_GEBGP_ONESTEP(0);
 1369|       |          EIGEN_GEBGP_ONESTEP(1);
 1370|       |          EIGEN_GEBGP_ONESTEP(2);
 1371|       |          EIGEN_GEBGP_ONESTEP(3);
 1372|       |          EIGEN_GEBGP_ONESTEP(4);
 1373|       |          EIGEN_GEBGP_ONESTEP(5);
 1374|       |          EIGEN_GEBGP_ONESTEP(6);
 1375|       |          EIGEN_GEBGP_ONESTEP(7);
 1376|       |
 1377|       |          blB += pk * RhsProgress;
 1378|       |          blA += pk * LhsProgress;
 1379|       |
 1380|       |          EIGEN_ASM_COMMENT("end gebp micro kernel 1/half/quarterX1");
 1381|       |        }
 1382|       |
 1383|       |        // process remaining peeled loop
 1384|       |        for (Index k = peeled_kc; k < depth; k++) {
 1385|       |          RhsPacket B_0;
 1386|       |          EIGEN_GEBGP_ONESTEP(0);
 1387|       |          blB += RhsProgress;
 1388|       |          blA += LhsProgress;
 1389|       |        }
 1390|       |#undef EIGEN_GEBGP_ONESTEP
 1391|       |        ResPacket R0;
 1392|       |        ResPacket alphav = pset1<ResPacket>(alpha);
 1393|       |        R0 = r0.template loadPacket<ResPacket>(0);
 1394|       |        traits.acc(C0, alphav, R0);
 1395|       |        r0.storePacket(0, R0);
 1396|       |      }
 1397|       |    }
 1398|       |  }
 1399|       |};
 1400|       |
 1401|       |template <int nr, Index LhsProgress, Index RhsProgress, typename LhsScalar, typename RhsScalar, typename ResScalar,
 1402|       |          typename AccPacket, typename LhsPacket, typename RhsPacket, typename ResPacket, typename GEBPTraits,
 1403|       |          typename LinearMapper, typename DataMapper>
 1404|       |struct lhs_process_fraction_of_packet
 1405|       |    : lhs_process_one_packet<nr, LhsProgress, RhsProgress, LhsScalar, RhsScalar, ResScalar, AccPacket, LhsPacket,
 1406|       |                             RhsPacket, ResPacket, GEBPTraits, LinearMapper, DataMapper> {
 1407|       |  EIGEN_STRONG_INLINE void peeled_kc_onestep(Index K, const LhsScalar* blA, const RhsScalar* blB, GEBPTraits traits,
 1408|       |                                             LhsPacket* A0, RhsPacket* B_0, RhsPacket* B1, RhsPacket* B2, RhsPacket* B3,
 1409|       |                                             AccPacket* C0, AccPacket* C1, AccPacket* C2, AccPacket* C3) {
 1410|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 1X4");
 1411|       |    EIGEN_ASM_COMMENT("Note: these asm comments work around bug 935!");
 1412|       |    traits.loadLhsUnaligned(&blA[(0 + 1 * K) * (LhsProgress)], *A0);
 1413|       |    traits.broadcastRhs(&blB[(0 + 4 * K) * RhsProgress], *B_0, *B1, *B2, *B3);
 1414|       |    traits.madd(*A0, *B_0, *C0, *B_0);
 1415|       |    traits.madd(*A0, *B1, *C1, *B1);
 1416|       |    traits.madd(*A0, *B2, *C2, *B2);
 1417|       |    traits.madd(*A0, *B3, *C3, *B3);
 1418|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 1X4");
 1419|       |  }
 1420|       |};
 1421|       |
 1422|       |template <typename LhsScalar, typename RhsScalar, typename Index, typename DataMapper, int mr, int nr,
 1423|       |          bool ConjugateLhs, bool ConjugateRhs>
 1424|       |EIGEN_DONT_INLINE void gebp_kernel<LhsScalar, RhsScalar, Index, DataMapper, mr, nr, ConjugateLhs,
 1425|       |                                   ConjugateRhs>::operator()(const DataMapper& res, const LhsScalar* blockA,
 1426|       |                                                             const RhsScalar* blockB, Index rows, Index depth,
 1427|       |                                                             Index cols, ResScalar alpha, Index strideA, Index strideB,
 1428|       |                                                             Index offsetA, Index offsetB) {
 1429|       |  Traits traits;
 1430|       |  SwappedTraits straits;
 1431|       |
 1432|       |  if (strideA == -1) strideA = depth;
 1433|       |  if (strideB == -1) strideB = depth;
 1434|       |  conj_helper<LhsScalar, RhsScalar, ConjugateLhs, ConjugateRhs> cj;
 1435|       |  Index packet_cols4 = nr >= 4 ? (cols / 4) * 4 : 0;
 1436|       |  Index packet_cols8 = nr >= 8 ? (cols / 8) * 8 : 0;
 1437|       |  const Index peeled_mc3 = mr >= 3 * Traits::LhsProgress ? (rows / (3 * LhsProgress)) * (3 * LhsProgress) : 0;
 1438|       |  const Index peeled_mc2 =
 1439|       |      mr >= 2 * Traits::LhsProgress ? peeled_mc3 + ((rows - peeled_mc3) / (2 * LhsProgress)) * (2 * LhsProgress) : 0;
 1440|       |  const Index peeled_mc1 =
 1441|       |      mr >= 1 * Traits::LhsProgress ? peeled_mc2 + ((rows - peeled_mc2) / (1 * LhsProgress)) * (1 * LhsProgress) : 0;
 1442|       |  const Index peeled_mc_half =
 1443|       |      mr >= LhsProgressHalf ? peeled_mc1 + ((rows - peeled_mc1) / (LhsProgressHalf)) * (LhsProgressHalf) : 0;
 1444|       |  const Index peeled_mc_quarter =
 1445|       |      mr >= LhsProgressQuarter
 1446|       |          ? peeled_mc_half + ((rows - peeled_mc_half) / (LhsProgressQuarter)) * (LhsProgressQuarter)
 1447|       |          : 0;
 1448|       |  enum { pk = 8 };  // NOTE Such a large peeling factor is important for large matrices (~ +5% when >1000 on Haswell)
 1449|       |  const Index peeled_kc = depth & ~(pk - 1);
 1450|       |  const int prefetch_res_offset = 32 / sizeof(ResScalar);
 1451|       |  //     const Index depth2     = depth & ~1;
 1452|       |
 1453|       |  //---------- Process 3 * LhsProgress rows at once ----------
 1454|       |  // This corresponds to 3*LhsProgress x nr register blocks.
 1455|       |  // Usually, make sense only with FMA
 1456|       |  if (mr >= 3 * Traits::LhsProgress) {
 1457|       |    // Here, the general idea is to loop on each largest micro horizontal panel of the lhs (3*Traits::LhsProgress x
 1458|       |    // depth) and on each largest micro vertical panel of the rhs (depth * nr). Blocking sizes, i.e., 'depth' has been
 1459|       |    // computed so that the micro horizontal panel of the lhs fit in L1. However, if depth is too small, we can extend
 1460|       |    // the number of rows of these horizontal panels. This actual number of rows is computed as follow:
 1461|       |    const Index l1 = defaultL1CacheSize;  // in Bytes, TODO, l1 should be passed to this function.
 1462|       |    // The max(1, ...) here is needed because we may be using blocking params larger than what our known l1 cache size
 1463|       |    // suggests we should be using: either because our known l1 cache size is inaccurate (e.g. on Android, we can only
 1464|       |    // guess), or because we are testing specific blocking sizes.
 1465|       |    const Index actual_panel_rows =
 1466|       |        (3 * LhsProgress) * std::max<Index>(1, ((l1 - sizeof(ResScalar) * mr * nr - depth * nr * sizeof(RhsScalar)) /
 1467|       |                                                (depth * sizeof(LhsScalar) * 3 * LhsProgress)));
 1468|       |    for (Index i1 = 0; i1 < peeled_mc3; i1 += actual_panel_rows) {
 1469|       |      const Index actual_panel_end = (std::min)(i1 + actual_panel_rows, peeled_mc3);
 1470|       |#if EIGEN_ARCH_ARM64 || EIGEN_ARCH_LOONGARCH64
 1471|       |      EIGEN_IF_CONSTEXPR(nr >= 8) {
 1472|       |        for (Index j2 = 0; j2 < packet_cols8; j2 += 8) {
 1473|       |          for (Index i = i1; i < actual_panel_end; i += 3 * LhsProgress) {
 1474|       |            const LhsScalar* blA = &blockA[i * strideA + offsetA * (3 * LhsProgress)];
 1475|       |            prefetch(&blA[0]);
 1476|       |            // gets res block as register
 1477|       |            AccPacket C0, C1, C2, C3, C4, C5, C6, C7, C8, C9, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C20,
 1478|       |                C21, C22, C23;
 1479|       |            traits.initAcc(C0);
 1480|       |            traits.initAcc(C1);
 1481|       |            traits.initAcc(C2);
 1482|       |            traits.initAcc(C3);
 1483|       |            traits.initAcc(C4);
 1484|       |            traits.initAcc(C5);
 1485|       |            traits.initAcc(C6);
 1486|       |            traits.initAcc(C7);
 1487|       |            traits.initAcc(C8);
 1488|       |            traits.initAcc(C9);
 1489|       |            traits.initAcc(C10);
 1490|       |            traits.initAcc(C11);
 1491|       |            traits.initAcc(C12);
 1492|       |            traits.initAcc(C13);
 1493|       |            traits.initAcc(C14);
 1494|       |            traits.initAcc(C15);
 1495|       |            traits.initAcc(C16);
 1496|       |            traits.initAcc(C17);
 1497|       |            traits.initAcc(C18);
 1498|       |            traits.initAcc(C19);
 1499|       |            traits.initAcc(C20);
 1500|       |            traits.initAcc(C21);
 1501|       |            traits.initAcc(C22);
 1502|       |            traits.initAcc(C23);
 1503|       |
 1504|       |            LinearMapper r0 = res.getLinearMapper(i, j2 + 0);
 1505|       |            LinearMapper r1 = res.getLinearMapper(i, j2 + 1);
 1506|       |            LinearMapper r2 = res.getLinearMapper(i, j2 + 2);
 1507|       |            LinearMapper r3 = res.getLinearMapper(i, j2 + 3);
 1508|       |            LinearMapper r4 = res.getLinearMapper(i, j2 + 4);
 1509|       |            LinearMapper r5 = res.getLinearMapper(i, j2 + 5);
 1510|       |            LinearMapper r6 = res.getLinearMapper(i, j2 + 6);
 1511|       |            LinearMapper r7 = res.getLinearMapper(i, j2 + 7);
 1512|       |
 1513|       |            r0.prefetch(0);
 1514|       |            r1.prefetch(0);
 1515|       |            r2.prefetch(0);
 1516|       |            r3.prefetch(0);
 1517|       |            r4.prefetch(0);
 1518|       |            r5.prefetch(0);
 1519|       |            r6.prefetch(0);
 1520|       |            r7.prefetch(0);
 1521|       |
 1522|       |            // performs "inner" products
 1523|       |            const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 8];
 1524|       |            prefetch(&blB[0]);
 1525|       |            LhsPacket A0, A1;
 1526|       |            for (Index k = 0; k < peeled_kc; k += pk) {
 1527|       |              EIGEN_ASM_COMMENT("begin gebp micro kernel 3pX8");
 1528|       |              // 27 registers are taken (24 for acc, 3 for lhs).
 1529|       |              RhsPanel27 rhs_panel;
 1530|       |              RhsPacket T0;
 1531|       |              LhsPacket A2;
 1532|       |#if EIGEN_ARCH_ARM64 && defined(EIGEN_VECTORIZE_NEON) && EIGEN_GNUC_STRICT_LESS_THAN(9, 0, 0)
 1533|       |// see http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1633
 1534|       |// without this workaround A0, A1, and A2 are loaded in the same register,
 1535|       |// which is not good for pipelining
 1536|       |#define EIGEN_GEBP_3Px8_REGISTER_ALLOC_WORKAROUND __asm__("" : "+w,m"(A0), "+w,m"(A1), "+w,m"(A2));
 1537|       |#else
 1538|       |#define EIGEN_GEBP_3Px8_REGISTER_ALLOC_WORKAROUND
 1539|       |#endif
 1540|       |
 1541|       |#define EIGEN_GEBP_ONESTEP(K)                                                                                     \
 1542|       |  do {                                                                                                            \
 1543|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 3pX8");                                                    \
 1544|       |    traits.loadLhs(&blA[(0 + 3 * K) * LhsProgress], A0);                                                          \
 1545|       |    traits.loadLhs(&blA[(1 + 3 * K) * LhsProgress], A1);                                                          \
 1546|       |    traits.loadLhs(&blA[(2 + 3 * K) * LhsProgress], A2);                                                          \
 1547|       |    EIGEN_GEBP_3Px8_REGISTER_ALLOC_WORKAROUND traits.loadRhs(blB + (0 + 8 * K) * Traits::RhsProgress, rhs_panel); \
 1548|       |    traits.madd(A0, rhs_panel, C0, T0, fix<0>);                                                                   \
 1549|       |    traits.madd(A1, rhs_panel, C8, T0, fix<0>);                                                                   \
 1550|       |    traits.madd(A2, rhs_panel, C16, T0, fix<0>);                                                                  \
 1551|       |    traits.updateRhs(blB + (1 + 8 * K) * Traits::RhsProgress, rhs_panel);                                         \
 1552|       |    traits.madd(A0, rhs_panel, C1, T0, fix<1>);                                                                   \
 1553|       |    traits.madd(A1, rhs_panel, C9, T0, fix<1>);                                                                   \
 1554|       |    traits.madd(A2, rhs_panel, C17, T0, fix<1>);                                                                  \
 1555|       |    traits.updateRhs(blB + (2 + 8 * K) * Traits::RhsProgress, rhs_panel);                                         \
 1556|       |    traits.madd(A0, rhs_panel, C2, T0, fix<2>);                                                                   \
 1557|       |    traits.madd(A1, rhs_panel, C10, T0, fix<2>);                                                                  \
 1558|       |    traits.madd(A2, rhs_panel, C18, T0, fix<2>);                                                                  \
 1559|       |    traits.updateRhs(blB + (3 + 8 * K) * Traits::RhsProgress, rhs_panel);                                         \
 1560|       |    traits.madd(A0, rhs_panel, C3, T0, fix<3>);                                                                   \
 1561|       |    traits.madd(A1, rhs_panel, C11, T0, fix<3>);                                                                  \
 1562|       |    traits.madd(A2, rhs_panel, C19, T0, fix<3>);                                                                  \
 1563|       |    traits.loadRhs(blB + (4 + 8 * K) * Traits::RhsProgress, rhs_panel);                                           \
 1564|       |    traits.madd(A0, rhs_panel, C4, T0, fix<0>);                                                                   \
 1565|       |    traits.madd(A1, rhs_panel, C12, T0, fix<0>);                                                                  \
 1566|       |    traits.madd(A2, rhs_panel, C20, T0, fix<0>);                                                                  \
 1567|       |    traits.updateRhs(blB + (5 + 8 * K) * Traits::RhsProgress, rhs_panel);                                         \
 1568|       |    traits.madd(A0, rhs_panel, C5, T0, fix<1>);                                                                   \
 1569|       |    traits.madd(A1, rhs_panel, C13, T0, fix<1>);                                                                  \
 1570|       |    traits.madd(A2, rhs_panel, C21, T0, fix<1>);                                                                  \
 1571|       |    traits.updateRhs(blB + (6 + 8 * K) * Traits::RhsProgress, rhs_panel);                                         \
 1572|       |    traits.madd(A0, rhs_panel, C6, T0, fix<2>);                                                                   \
 1573|       |    traits.madd(A1, rhs_panel, C14, T0, fix<2>);                                                                  \
 1574|       |    traits.madd(A2, rhs_panel, C22, T0, fix<2>);                                                                  \
 1575|       |    traits.updateRhs(blB + (7 + 8 * K) * Traits::RhsProgress, rhs_panel);                                         \
 1576|       |    traits.madd(A0, rhs_panel, C7, T0, fix<3>);                                                                   \
 1577|       |    traits.madd(A1, rhs_panel, C15, T0, fix<3>);                                                                  \
 1578|       |    traits.madd(A2, rhs_panel, C23, T0, fix<3>);                                                                  \
 1579|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 3pX8");                                                      \
 1580|       |  } while (false)
 1581|       |
 1582|       |              EIGEN_GEBP_ONESTEP(0);
 1583|       |              EIGEN_GEBP_ONESTEP(1);
 1584|       |              EIGEN_GEBP_ONESTEP(2);
 1585|       |              EIGEN_GEBP_ONESTEP(3);
 1586|       |              EIGEN_GEBP_ONESTEP(4);
 1587|       |              EIGEN_GEBP_ONESTEP(5);
 1588|       |              EIGEN_GEBP_ONESTEP(6);
 1589|       |              EIGEN_GEBP_ONESTEP(7);
 1590|       |
 1591|       |              blB += pk * 8 * RhsProgress;
 1592|       |              blA += pk * 3 * Traits::LhsProgress;
 1593|       |              EIGEN_ASM_COMMENT("end gebp micro kernel 3pX8");
 1594|       |            }
 1595|       |
 1596|       |            // process remaining peeled loop
 1597|       |            for (Index k = peeled_kc; k < depth; k++) {
 1598|       |              RhsPanel27 rhs_panel;
 1599|       |              RhsPacket T0;
 1600|       |              LhsPacket A2;
 1601|       |              EIGEN_GEBP_ONESTEP(0);
 1602|       |              blB += 8 * RhsProgress;
 1603|       |              blA += 3 * Traits::LhsProgress;
 1604|       |            }
 1605|       |
 1606|       |#undef EIGEN_GEBP_ONESTEP
 1607|       |
 1608|       |            ResPacket R0, R1, R2;
 1609|       |            ResPacket alphav = pset1<ResPacket>(alpha);
 1610|       |
 1611|       |            R0 = r0.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1612|       |            R1 = r0.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1613|       |            R2 = r0.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1614|       |            traits.acc(C0, alphav, R0);
 1615|       |            traits.acc(C8, alphav, R1);
 1616|       |            traits.acc(C16, alphav, R2);
 1617|       |            r0.storePacket(0 * Traits::ResPacketSize, R0);
 1618|       |            r0.storePacket(1 * Traits::ResPacketSize, R1);
 1619|       |            r0.storePacket(2 * Traits::ResPacketSize, R2);
 1620|       |
 1621|       |            R0 = r1.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1622|       |            R1 = r1.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1623|       |            R2 = r1.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1624|       |            traits.acc(C1, alphav, R0);
 1625|       |            traits.acc(C9, alphav, R1);
 1626|       |            traits.acc(C17, alphav, R2);
 1627|       |            r1.storePacket(0 * Traits::ResPacketSize, R0);
 1628|       |            r1.storePacket(1 * Traits::ResPacketSize, R1);
 1629|       |            r1.storePacket(2 * Traits::ResPacketSize, R2);
 1630|       |
 1631|       |            R0 = r2.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1632|       |            R1 = r2.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1633|       |            R2 = r2.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1634|       |            traits.acc(C2, alphav, R0);
 1635|       |            traits.acc(C10, alphav, R1);
 1636|       |            traits.acc(C18, alphav, R2);
 1637|       |            r2.storePacket(0 * Traits::ResPacketSize, R0);
 1638|       |            r2.storePacket(1 * Traits::ResPacketSize, R1);
 1639|       |            r2.storePacket(2 * Traits::ResPacketSize, R2);
 1640|       |
 1641|       |            R0 = r3.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1642|       |            R1 = r3.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1643|       |            R2 = r3.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1644|       |            traits.acc(C3, alphav, R0);
 1645|       |            traits.acc(C11, alphav, R1);
 1646|       |            traits.acc(C19, alphav, R2);
 1647|       |            r3.storePacket(0 * Traits::ResPacketSize, R0);
 1648|       |            r3.storePacket(1 * Traits::ResPacketSize, R1);
 1649|       |            r3.storePacket(2 * Traits::ResPacketSize, R2);
 1650|       |
 1651|       |            R0 = r4.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1652|       |            R1 = r4.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1653|       |            R2 = r4.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1654|       |            traits.acc(C4, alphav, R0);
 1655|       |            traits.acc(C12, alphav, R1);
 1656|       |            traits.acc(C20, alphav, R2);
 1657|       |            r4.storePacket(0 * Traits::ResPacketSize, R0);
 1658|       |            r4.storePacket(1 * Traits::ResPacketSize, R1);
 1659|       |            r4.storePacket(2 * Traits::ResPacketSize, R2);
 1660|       |
 1661|       |            R0 = r5.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1662|       |            R1 = r5.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1663|       |            R2 = r5.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1664|       |            traits.acc(C5, alphav, R0);
 1665|       |            traits.acc(C13, alphav, R1);
 1666|       |            traits.acc(C21, alphav, R2);
 1667|       |            r5.storePacket(0 * Traits::ResPacketSize, R0);
 1668|       |            r5.storePacket(1 * Traits::ResPacketSize, R1);
 1669|       |            r5.storePacket(2 * Traits::ResPacketSize, R2);
 1670|       |
 1671|       |            R0 = r6.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1672|       |            R1 = r6.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1673|       |            R2 = r6.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1674|       |            traits.acc(C6, alphav, R0);
 1675|       |            traits.acc(C14, alphav, R1);
 1676|       |            traits.acc(C22, alphav, R2);
 1677|       |            r6.storePacket(0 * Traits::ResPacketSize, R0);
 1678|       |            r6.storePacket(1 * Traits::ResPacketSize, R1);
 1679|       |            r6.storePacket(2 * Traits::ResPacketSize, R2);
 1680|       |
 1681|       |            R0 = r7.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1682|       |            R1 = r7.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1683|       |            R2 = r7.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1684|       |            traits.acc(C7, alphav, R0);
 1685|       |            traits.acc(C15, alphav, R1);
 1686|       |            traits.acc(C23, alphav, R2);
 1687|       |            r7.storePacket(0 * Traits::ResPacketSize, R0);
 1688|       |            r7.storePacket(1 * Traits::ResPacketSize, R1);
 1689|       |            r7.storePacket(2 * Traits::ResPacketSize, R2);
 1690|       |          }
 1691|       |        }
 1692|       |      }
 1693|       |#endif
 1694|       |      for (Index j2 = packet_cols8; j2 < packet_cols4; j2 += 4) {
 1695|       |        for (Index i = i1; i < actual_panel_end; i += 3 * LhsProgress) {
 1696|       |          // We selected a 3*Traits::LhsProgress x nr micro block of res which is entirely
 1697|       |          // stored into 3 x nr registers.
 1698|       |
 1699|       |          const LhsScalar* blA = &blockA[i * strideA + offsetA * (3 * LhsProgress)];
 1700|       |          prefetch(&blA[0]);
 1701|       |
 1702|       |          // gets res block as register
 1703|       |          AccPacket C0, C1, C2, C3, C4, C5, C6, C7, C8, C9, C10, C11;
 1704|       |          traits.initAcc(C0);
 1705|       |          traits.initAcc(C1);
 1706|       |          traits.initAcc(C2);
 1707|       |          traits.initAcc(C3);
 1708|       |          traits.initAcc(C4);
 1709|       |          traits.initAcc(C5);
 1710|       |          traits.initAcc(C6);
 1711|       |          traits.initAcc(C7);
 1712|       |          traits.initAcc(C8);
 1713|       |          traits.initAcc(C9);
 1714|       |          traits.initAcc(C10);
 1715|       |          traits.initAcc(C11);
 1716|       |
 1717|       |          LinearMapper r0 = res.getLinearMapper(i, j2 + 0);
 1718|       |          LinearMapper r1 = res.getLinearMapper(i, j2 + 1);
 1719|       |          LinearMapper r2 = res.getLinearMapper(i, j2 + 2);
 1720|       |          LinearMapper r3 = res.getLinearMapper(i, j2 + 3);
 1721|       |
 1722|       |          r0.prefetch(0);
 1723|       |          r1.prefetch(0);
 1724|       |          r2.prefetch(0);
 1725|       |          r3.prefetch(0);
 1726|       |
 1727|       |          // performs "inner" products
 1728|       |          const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 4];
 1729|       |          prefetch(&blB[0]);
 1730|       |          LhsPacket A0, A1;
 1731|       |
 1732|       |          for (Index k = 0; k < peeled_kc; k += pk) {
 1733|       |            EIGEN_ASM_COMMENT("begin gebp micro kernel 3pX4");
 1734|       |            // 15 registers are taken (12 for acc, 3 for lhs).
 1735|       |            RhsPanel15 rhs_panel;
 1736|       |            RhsPacket T0;
 1737|       |            LhsPacket A2;
 1738|       |#if EIGEN_ARCH_ARM64 && defined(EIGEN_VECTORIZE_NEON) && EIGEN_GNUC_STRICT_LESS_THAN(9, 0, 0)
 1739|       |// see http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1633
 1740|       |// without this workaround A0, A1, and A2 are loaded in the same register,
 1741|       |// which is not good for pipelining
 1742|       |#define EIGEN_GEBP_3PX4_REGISTER_ALLOC_WORKAROUND __asm__("" : "+w,m"(A0), "+w,m"(A1), "+w,m"(A2));
 1743|       |#else
 1744|       |#define EIGEN_GEBP_3PX4_REGISTER_ALLOC_WORKAROUND
 1745|       |#endif
 1746|       |#define EIGEN_GEBP_ONESTEP(K)                                             \
 1747|       |  do {                                                                    \
 1748|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 3pX4");            \
 1749|       |    EIGEN_ASM_COMMENT("Note: these asm comments work around bug 935!");   \
 1750|       |    internal::prefetch(blA + (3 * K + 16) * LhsProgress);                 \
 1751|       |    if (EIGEN_ARCH_ARM || EIGEN_ARCH_MIPS) {                              \
 1752|       |      internal::prefetch(blB + (4 * K + 16) * RhsProgress);               \
 1753|       |    } /* Bug 953 */                                                       \
 1754|       |    traits.loadLhs(&blA[(0 + 3 * K) * LhsProgress], A0);                  \
 1755|       |    traits.loadLhs(&blA[(1 + 3 * K) * LhsProgress], A1);                  \
 1756|       |    traits.loadLhs(&blA[(2 + 3 * K) * LhsProgress], A2);                  \
 1757|       |    EIGEN_GEBP_3PX4_REGISTER_ALLOC_WORKAROUND                             \
 1758|       |    traits.loadRhs(blB + (0 + 4 * K) * Traits::RhsProgress, rhs_panel);   \
 1759|       |    traits.madd(A0, rhs_panel, C0, T0, fix<0>);                           \
 1760|       |    traits.madd(A1, rhs_panel, C4, T0, fix<0>);                           \
 1761|       |    traits.madd(A2, rhs_panel, C8, T0, fix<0>);                           \
 1762|       |    traits.updateRhs(blB + (1 + 4 * K) * Traits::RhsProgress, rhs_panel); \
 1763|       |    traits.madd(A0, rhs_panel, C1, T0, fix<1>);                           \
 1764|       |    traits.madd(A1, rhs_panel, C5, T0, fix<1>);                           \
 1765|       |    traits.madd(A2, rhs_panel, C9, T0, fix<1>);                           \
 1766|       |    traits.updateRhs(blB + (2 + 4 * K) * Traits::RhsProgress, rhs_panel); \
 1767|       |    traits.madd(A0, rhs_panel, C2, T0, fix<2>);                           \
 1768|       |    traits.madd(A1, rhs_panel, C6, T0, fix<2>);                           \
 1769|       |    traits.madd(A2, rhs_panel, C10, T0, fix<2>);                          \
 1770|       |    traits.updateRhs(blB + (3 + 4 * K) * Traits::RhsProgress, rhs_panel); \
 1771|       |    traits.madd(A0, rhs_panel, C3, T0, fix<3>);                           \
 1772|       |    traits.madd(A1, rhs_panel, C7, T0, fix<3>);                           \
 1773|       |    traits.madd(A2, rhs_panel, C11, T0, fix<3>);                          \
 1774|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 3pX4");              \
 1775|       |  } while (false)
 1776|       |
 1777|       |            internal::prefetch(blB);
 1778|       |            EIGEN_GEBP_ONESTEP(0);
 1779|       |            EIGEN_GEBP_ONESTEP(1);
 1780|       |            EIGEN_GEBP_ONESTEP(2);
 1781|       |            EIGEN_GEBP_ONESTEP(3);
 1782|       |            EIGEN_GEBP_ONESTEP(4);
 1783|       |            EIGEN_GEBP_ONESTEP(5);
 1784|       |            EIGEN_GEBP_ONESTEP(6);
 1785|       |            EIGEN_GEBP_ONESTEP(7);
 1786|       |
 1787|       |            blB += pk * 4 * RhsProgress;
 1788|       |            blA += pk * 3 * Traits::LhsProgress;
 1789|       |
 1790|       |            EIGEN_ASM_COMMENT("end gebp micro kernel 3pX4");
 1791|       |          }
 1792|       |          // process remaining peeled loop
 1793|       |          for (Index k = peeled_kc; k < depth; k++) {
 1794|       |            RhsPanel15 rhs_panel;
 1795|       |            RhsPacket T0;
 1796|       |            LhsPacket A2;
 1797|       |            EIGEN_GEBP_ONESTEP(0);
 1798|       |            blB += 4 * RhsProgress;
 1799|       |            blA += 3 * Traits::LhsProgress;
 1800|       |          }
 1801|       |
 1802|       |#undef EIGEN_GEBP_ONESTEP
 1803|       |
 1804|       |          ResPacket R0, R1, R2;
 1805|       |          ResPacket alphav = pset1<ResPacket>(alpha);
 1806|       |
 1807|       |          R0 = r0.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1808|       |          R1 = r0.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1809|       |          R2 = r0.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1810|       |          traits.acc(C0, alphav, R0);
 1811|       |          traits.acc(C4, alphav, R1);
 1812|       |          traits.acc(C8, alphav, R2);
 1813|       |          r0.storePacket(0 * Traits::ResPacketSize, R0);
 1814|       |          r0.storePacket(1 * Traits::ResPacketSize, R1);
 1815|       |          r0.storePacket(2 * Traits::ResPacketSize, R2);
 1816|       |
 1817|       |          R0 = r1.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1818|       |          R1 = r1.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1819|       |          R2 = r1.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1820|       |          traits.acc(C1, alphav, R0);
 1821|       |          traits.acc(C5, alphav, R1);
 1822|       |          traits.acc(C9, alphav, R2);
 1823|       |          r1.storePacket(0 * Traits::ResPacketSize, R0);
 1824|       |          r1.storePacket(1 * Traits::ResPacketSize, R1);
 1825|       |          r1.storePacket(2 * Traits::ResPacketSize, R2);
 1826|       |
 1827|       |          R0 = r2.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1828|       |          R1 = r2.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1829|       |          R2 = r2.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1830|       |          traits.acc(C2, alphav, R0);
 1831|       |          traits.acc(C6, alphav, R1);
 1832|       |          traits.acc(C10, alphav, R2);
 1833|       |          r2.storePacket(0 * Traits::ResPacketSize, R0);
 1834|       |          r2.storePacket(1 * Traits::ResPacketSize, R1);
 1835|       |          r2.storePacket(2 * Traits::ResPacketSize, R2);
 1836|       |
 1837|       |          R0 = r3.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1838|       |          R1 = r3.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1839|       |          R2 = r3.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1840|       |          traits.acc(C3, alphav, R0);
 1841|       |          traits.acc(C7, alphav, R1);
 1842|       |          traits.acc(C11, alphav, R2);
 1843|       |          r3.storePacket(0 * Traits::ResPacketSize, R0);
 1844|       |          r3.storePacket(1 * Traits::ResPacketSize, R1);
 1845|       |          r3.storePacket(2 * Traits::ResPacketSize, R2);
 1846|       |        }
 1847|       |      }
 1848|       |
 1849|       |      // Deal with remaining columns of the rhs
 1850|       |      for (Index j2 = packet_cols4; j2 < cols; j2++) {
 1851|       |        for (Index i = i1; i < actual_panel_end; i += 3 * LhsProgress) {
 1852|       |          // One column at a time
 1853|       |          const LhsScalar* blA = &blockA[i * strideA + offsetA * (3 * Traits::LhsProgress)];
 1854|       |          prefetch(&blA[0]);
 1855|       |
 1856|       |          // gets res block as register
 1857|       |          AccPacket C0, C4, C8;
 1858|       |          traits.initAcc(C0);
 1859|       |          traits.initAcc(C4);
 1860|       |          traits.initAcc(C8);
 1861|       |
 1862|       |          LinearMapper r0 = res.getLinearMapper(i, j2);
 1863|       |          r0.prefetch(0);
 1864|       |
 1865|       |          // performs "inner" products
 1866|       |          const RhsScalar* blB = &blockB[j2 * strideB + offsetB];
 1867|       |          LhsPacket A0, A1, A2;
 1868|       |
 1869|       |          for (Index k = 0; k < peeled_kc; k += pk) {
 1870|       |            EIGEN_ASM_COMMENT("begin gebp micro kernel 3pX1");
 1871|       |            RhsPacket B_0;
 1872|       |#define EIGEN_GEBGP_ONESTEP(K)                                          \
 1873|       |  do {                                                                  \
 1874|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 3pX1");          \
 1875|       |    EIGEN_ASM_COMMENT("Note: these asm comments work around bug 935!"); \
 1876|       |    traits.loadLhs(&blA[(0 + 3 * K) * LhsProgress], A0);                \
 1877|       |    traits.loadLhs(&blA[(1 + 3 * K) * LhsProgress], A1);                \
 1878|       |    traits.loadLhs(&blA[(2 + 3 * K) * LhsProgress], A2);                \
 1879|       |    traits.loadRhs(&blB[(0 + K) * RhsProgress], B_0);                   \
 1880|       |    traits.madd(A0, B_0, C0, B_0, fix<0>);                              \
 1881|       |    traits.madd(A1, B_0, C4, B_0, fix<0>);                              \
 1882|       |    traits.madd(A2, B_0, C8, B_0, fix<0>);                              \
 1883|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 3pX1");            \
 1884|       |  } while (false)
 1885|       |
 1886|       |            EIGEN_GEBGP_ONESTEP(0);
 1887|       |            EIGEN_GEBGP_ONESTEP(1);
 1888|       |            EIGEN_GEBGP_ONESTEP(2);
 1889|       |            EIGEN_GEBGP_ONESTEP(3);
 1890|       |            EIGEN_GEBGP_ONESTEP(4);
 1891|       |            EIGEN_GEBGP_ONESTEP(5);
 1892|       |            EIGEN_GEBGP_ONESTEP(6);
 1893|       |            EIGEN_GEBGP_ONESTEP(7);
 1894|       |
 1895|       |            blB += int(pk) * int(RhsProgress);
 1896|       |            blA += int(pk) * 3 * int(Traits::LhsProgress);
 1897|       |
 1898|       |            EIGEN_ASM_COMMENT("end gebp micro kernel 3pX1");
 1899|       |          }
 1900|       |
 1901|       |          // process remaining peeled loop
 1902|       |          for (Index k = peeled_kc; k < depth; k++) {
 1903|       |            RhsPacket B_0;
 1904|       |            EIGEN_GEBGP_ONESTEP(0);
 1905|       |            blB += RhsProgress;
 1906|       |            blA += 3 * Traits::LhsProgress;
 1907|       |          }
 1908|       |#undef EIGEN_GEBGP_ONESTEP
 1909|       |          ResPacket R0, R1, R2;
 1910|       |          ResPacket alphav = pset1<ResPacket>(alpha);
 1911|       |
 1912|       |          R0 = r0.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 1913|       |          R1 = r0.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 1914|       |          R2 = r0.template loadPacket<ResPacket>(2 * Traits::ResPacketSize);
 1915|       |          traits.acc(C0, alphav, R0);
 1916|       |          traits.acc(C4, alphav, R1);
 1917|       |          traits.acc(C8, alphav, R2);
 1918|       |          r0.storePacket(0 * Traits::ResPacketSize, R0);
 1919|       |          r0.storePacket(1 * Traits::ResPacketSize, R1);
 1920|       |          r0.storePacket(2 * Traits::ResPacketSize, R2);
 1921|       |        }
 1922|       |      }
 1923|       |    }
 1924|       |  }
 1925|       |
 1926|       |  //---------- Process 2 * LhsProgress rows at once ----------
 1927|       |  if (mr >= 2 * Traits::LhsProgress) {
 1928|       |    const Index l1 = defaultL1CacheSize;  // in Bytes, TODO, l1 should be passed to this function.
 1929|       |    // The max(1, ...) here is needed because we may be using blocking params larger than what our known l1 cache size
 1930|       |    // suggests we should be using: either because our known l1 cache size is inaccurate (e.g. on Android, we can only
 1931|       |    // guess), or because we are testing specific blocking sizes.
 1932|       |    Index actual_panel_rows =
 1933|       |        (2 * LhsProgress) * std::max<Index>(1, ((l1 - sizeof(ResScalar) * mr * nr - depth * nr * sizeof(RhsScalar)) /
 1934|       |                                                (depth * sizeof(LhsScalar) * 2 * LhsProgress)));
 1935|       |
 1936|       |    for (Index i1 = peeled_mc3; i1 < peeled_mc2; i1 += actual_panel_rows) {
 1937|       |      Index actual_panel_end = (std::min)(i1 + actual_panel_rows, peeled_mc2);
 1938|       |#if EIGEN_ARCH_ARM64 || EIGEN_ARCH_LOONGARCH64
 1939|       |      EIGEN_IF_CONSTEXPR(nr >= 8) {
 1940|       |        for (Index j2 = 0; j2 < packet_cols8; j2 += 8) {
 1941|       |          for (Index i = i1; i < actual_panel_end; i += 2 * LhsProgress) {
 1942|       |            const LhsScalar* blA = &blockA[i * strideA + offsetA * (2 * Traits::LhsProgress)];
 1943|       |            prefetch(&blA[0]);
 1944|       |
 1945|       |            AccPacket C0, C1, C2, C3, C4, C5, C6, C7, C8, C9, C10, C11, C12, C13, C14, C15;
 1946|       |            traits.initAcc(C0);
 1947|       |            traits.initAcc(C1);
 1948|       |            traits.initAcc(C2);
 1949|       |            traits.initAcc(C3);
 1950|       |            traits.initAcc(C4);
 1951|       |            traits.initAcc(C5);
 1952|       |            traits.initAcc(C6);
 1953|       |            traits.initAcc(C7);
 1954|       |            traits.initAcc(C8);
 1955|       |            traits.initAcc(C9);
 1956|       |            traits.initAcc(C10);
 1957|       |            traits.initAcc(C11);
 1958|       |            traits.initAcc(C12);
 1959|       |            traits.initAcc(C13);
 1960|       |            traits.initAcc(C14);
 1961|       |            traits.initAcc(C15);
 1962|       |
 1963|       |            LinearMapper r0 = res.getLinearMapper(i, j2 + 0);
 1964|       |            LinearMapper r1 = res.getLinearMapper(i, j2 + 1);
 1965|       |            LinearMapper r2 = res.getLinearMapper(i, j2 + 2);
 1966|       |            LinearMapper r3 = res.getLinearMapper(i, j2 + 3);
 1967|       |            LinearMapper r4 = res.getLinearMapper(i, j2 + 4);
 1968|       |            LinearMapper r5 = res.getLinearMapper(i, j2 + 5);
 1969|       |            LinearMapper r6 = res.getLinearMapper(i, j2 + 6);
 1970|       |            LinearMapper r7 = res.getLinearMapper(i, j2 + 7);
 1971|       |            r0.prefetch(prefetch_res_offset);
 1972|       |            r1.prefetch(prefetch_res_offset);
 1973|       |            r2.prefetch(prefetch_res_offset);
 1974|       |            r3.prefetch(prefetch_res_offset);
 1975|       |            r4.prefetch(prefetch_res_offset);
 1976|       |            r5.prefetch(prefetch_res_offset);
 1977|       |            r6.prefetch(prefetch_res_offset);
 1978|       |            r7.prefetch(prefetch_res_offset);
 1979|       |
 1980|       |            const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 8];
 1981|       |            prefetch(&blB[0]);
 1982|       |            LhsPacket A0, A1;
 1983|       |            for (Index k = 0; k < peeled_kc; k += pk) {
 1984|       |              RhsPacketx4 rhs_panel;
 1985|       |              RhsPacket T0;
 1986|       |// NOTE: the begin/end asm comments below work around bug 935!
 1987|       |// but they are not enough for gcc>=6 without FMA (bug 1637)
 1988|       |#if EIGEN_GNUC_STRICT_AT_LEAST(6, 0, 0) && defined(EIGEN_VECTORIZE_SSE)
 1989|       |#define EIGEN_GEBP_2Px8_SPILLING_WORKAROUND __asm__("" : [a0] "+x,m"(A0), [a1] "+x,m"(A1));
 1990|       |#else
 1991|       |#define EIGEN_GEBP_2Px8_SPILLING_WORKAROUND
 1992|       |#endif
 1993|       |#define EIGEN_GEBGP_ONESTEP(K)                                                                   \
 1994|       |  do {                                                                                           \
 1995|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 2pX8");                                   \
 1996|       |    traits.loadLhs(&blA[(0 + 2 * K) * LhsProgress], A0);                                         \
 1997|       |    traits.loadLhs(&blA[(1 + 2 * K) * LhsProgress], A1);                                         \
 1998|       |    traits.loadRhs(&blB[(0 + 8 * K) * RhsProgress], rhs_panel);                                  \
 1999|       |    traits.madd(A0, rhs_panel, C0, T0, fix<0>);                                                  \
 2000|       |    traits.madd(A1, rhs_panel, C8, T0, fix<0>);                                                  \
 2001|       |    traits.updateRhs(&blB[(1 + 8 * K) * RhsProgress], rhs_panel);                                \
 2002|       |    traits.madd(A0, rhs_panel, C1, T0, fix<1>);                                                  \
 2003|       |    traits.madd(A1, rhs_panel, C9, T0, fix<1>);                                                  \
 2004|       |    traits.updateRhs(&blB[(2 + 8 * K) * RhsProgress], rhs_panel);                                \
 2005|       |    traits.madd(A0, rhs_panel, C2, T0, fix<2>);                                                  \
 2006|       |    traits.madd(A1, rhs_panel, C10, T0, fix<2>);                                                 \
 2007|       |    traits.updateRhs(&blB[(3 + 8 * K) * RhsProgress], rhs_panel);                                \
 2008|       |    traits.madd(A0, rhs_panel, C3, T0, fix<3>);                                                  \
 2009|       |    traits.madd(A1, rhs_panel, C11, T0, fix<3>);                                                 \
 2010|       |    traits.loadRhs(&blB[(4 + 8 * K) * RhsProgress], rhs_panel);                                  \
 2011|       |    traits.madd(A0, rhs_panel, C4, T0, fix<0>);                                                  \
 2012|       |    traits.madd(A1, rhs_panel, C12, T0, fix<0>);                                                 \
 2013|       |    traits.updateRhs(&blB[(5 + 8 * K) * RhsProgress], rhs_panel);                                \
 2014|       |    traits.madd(A0, rhs_panel, C5, T0, fix<1>);                                                  \
 2015|       |    traits.madd(A1, rhs_panel, C13, T0, fix<1>);                                                 \
 2016|       |    traits.updateRhs(&blB[(6 + 8 * K) * RhsProgress], rhs_panel);                                \
 2017|       |    traits.madd(A0, rhs_panel, C6, T0, fix<2>);                                                  \
 2018|       |    traits.madd(A1, rhs_panel, C14, T0, fix<2>);                                                 \
 2019|       |    traits.updateRhs(&blB[(7 + 8 * K) * RhsProgress], rhs_panel);                                \
 2020|       |    traits.madd(A0, rhs_panel, C7, T0, fix<3>);                                                  \
 2021|       |    traits.madd(A1, rhs_panel, C15, T0, fix<3>);                                                 \
 2022|       |    EIGEN_GEBP_2Px8_SPILLING_WORKAROUND EIGEN_ASM_COMMENT("end step of gebp micro kernel 2pX8"); \
 2023|       |  } while (false)
 2024|       |
 2025|       |              EIGEN_ASM_COMMENT("begin gebp micro kernel 2pX8");
 2026|       |
 2027|       |              EIGEN_GEBGP_ONESTEP(0);
 2028|       |              EIGEN_GEBGP_ONESTEP(1);
 2029|       |              EIGEN_GEBGP_ONESTEP(2);
 2030|       |              EIGEN_GEBGP_ONESTEP(3);
 2031|       |              EIGEN_GEBGP_ONESTEP(4);
 2032|       |              EIGEN_GEBGP_ONESTEP(5);
 2033|       |              EIGEN_GEBGP_ONESTEP(6);
 2034|       |              EIGEN_GEBGP_ONESTEP(7);
 2035|       |
 2036|       |              blB += pk * 8 * RhsProgress;
 2037|       |              blA += pk * (2 * Traits::LhsProgress);
 2038|       |
 2039|       |              EIGEN_ASM_COMMENT("end gebp micro kernel 2pX8");
 2040|       |            }
 2041|       |            // process remaining peeled loop
 2042|       |            for (Index k = peeled_kc; k < depth; k++) {
 2043|       |              RhsPacketx4 rhs_panel;
 2044|       |              RhsPacket T0;
 2045|       |              EIGEN_GEBGP_ONESTEP(0);
 2046|       |              blB += 8 * RhsProgress;
 2047|       |              blA += 2 * Traits::LhsProgress;
 2048|       |            }
 2049|       |
 2050|       |#undef EIGEN_GEBGP_ONESTEP
 2051|       |
 2052|       |            ResPacket R0, R1, R2, R3;
 2053|       |            ResPacket alphav = pset1<ResPacket>(alpha);
 2054|       |
 2055|       |            R0 = r0.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2056|       |            R1 = r0.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2057|       |            R2 = r1.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2058|       |            R3 = r1.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2059|       |            traits.acc(C0, alphav, R0);
 2060|       |            traits.acc(C8, alphav, R1);
 2061|       |            traits.acc(C1, alphav, R2);
 2062|       |            traits.acc(C9, alphav, R3);
 2063|       |            r0.storePacket(0 * Traits::ResPacketSize, R0);
 2064|       |            r0.storePacket(1 * Traits::ResPacketSize, R1);
 2065|       |            r1.storePacket(0 * Traits::ResPacketSize, R2);
 2066|       |            r1.storePacket(1 * Traits::ResPacketSize, R3);
 2067|       |
 2068|       |            R0 = r2.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2069|       |            R1 = r2.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2070|       |            R2 = r3.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2071|       |            R3 = r3.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2072|       |            traits.acc(C2, alphav, R0);
 2073|       |            traits.acc(C10, alphav, R1);
 2074|       |            traits.acc(C3, alphav, R2);
 2075|       |            traits.acc(C11, alphav, R3);
 2076|       |            r2.storePacket(0 * Traits::ResPacketSize, R0);
 2077|       |            r2.storePacket(1 * Traits::ResPacketSize, R1);
 2078|       |            r3.storePacket(0 * Traits::ResPacketSize, R2);
 2079|       |            r3.storePacket(1 * Traits::ResPacketSize, R3);
 2080|       |
 2081|       |            R0 = r4.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2082|       |            R1 = r4.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2083|       |            R2 = r5.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2084|       |            R3 = r5.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2085|       |            traits.acc(C4, alphav, R0);
 2086|       |            traits.acc(C12, alphav, R1);
 2087|       |            traits.acc(C5, alphav, R2);
 2088|       |            traits.acc(C13, alphav, R3);
 2089|       |            r4.storePacket(0 * Traits::ResPacketSize, R0);
 2090|       |            r4.storePacket(1 * Traits::ResPacketSize, R1);
 2091|       |            r5.storePacket(0 * Traits::ResPacketSize, R2);
 2092|       |            r5.storePacket(1 * Traits::ResPacketSize, R3);
 2093|       |
 2094|       |            R0 = r6.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2095|       |            R1 = r6.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2096|       |            R2 = r7.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2097|       |            R3 = r7.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2098|       |            traits.acc(C6, alphav, R0);
 2099|       |            traits.acc(C14, alphav, R1);
 2100|       |            traits.acc(C7, alphav, R2);
 2101|       |            traits.acc(C15, alphav, R3);
 2102|       |            r6.storePacket(0 * Traits::ResPacketSize, R0);
 2103|       |            r6.storePacket(1 * Traits::ResPacketSize, R1);
 2104|       |            r7.storePacket(0 * Traits::ResPacketSize, R2);
 2105|       |            r7.storePacket(1 * Traits::ResPacketSize, R3);
 2106|       |          }
 2107|       |        }
 2108|       |      }
 2109|       |#endif
 2110|       |      for (Index j2 = packet_cols8; j2 < packet_cols4; j2 += 4) {
 2111|       |        for (Index i = i1; i < actual_panel_end; i += 2 * LhsProgress) {
 2112|       |          // We selected a 2*Traits::LhsProgress x nr micro block of res which is entirely
 2113|       |          // stored into 2 x nr registers.
 2114|       |
 2115|       |          const LhsScalar* blA = &blockA[i * strideA + offsetA * (2 * Traits::LhsProgress)];
 2116|       |          prefetch(&blA[0]);
 2117|       |
 2118|       |          // gets res block as register
 2119|       |          AccPacket C0, C1, C2, C3, C4, C5, C6, C7;
 2120|       |          traits.initAcc(C0);
 2121|       |          traits.initAcc(C1);
 2122|       |          traits.initAcc(C2);
 2123|       |          traits.initAcc(C3);
 2124|       |          traits.initAcc(C4);
 2125|       |          traits.initAcc(C5);
 2126|       |          traits.initAcc(C6);
 2127|       |          traits.initAcc(C7);
 2128|       |
 2129|       |          LinearMapper r0 = res.getLinearMapper(i, j2 + 0);
 2130|       |          LinearMapper r1 = res.getLinearMapper(i, j2 + 1);
 2131|       |          LinearMapper r2 = res.getLinearMapper(i, j2 + 2);
 2132|       |          LinearMapper r3 = res.getLinearMapper(i, j2 + 3);
 2133|       |
 2134|       |          r0.prefetch(prefetch_res_offset);
 2135|       |          r1.prefetch(prefetch_res_offset);
 2136|       |          r2.prefetch(prefetch_res_offset);
 2137|       |          r3.prefetch(prefetch_res_offset);
 2138|       |
 2139|       |          // performs "inner" products
 2140|       |          const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 4];
 2141|       |          prefetch(&blB[0]);
 2142|       |          LhsPacket A0, A1;
 2143|       |
 2144|       |          for (Index k = 0; k < peeled_kc; k += pk) {
 2145|       |            EIGEN_ASM_COMMENT("begin gebp micro kernel 2pX4");
 2146|       |            RhsPacketx4 rhs_panel;
 2147|       |            RhsPacket T0;
 2148|       |
 2149|       |// NOTE: the begin/end asm comments below work around bug 935!
 2150|       |// but they are not enough for gcc>=6 without FMA (bug 1637)
 2151|       |#if EIGEN_GNUC_STRICT_AT_LEAST(6, 0, 0) && defined(EIGEN_VECTORIZE_SSE) && !(EIGEN_COMP_LCC)
 2152|       |#define EIGEN_GEBP_2PX4_SPILLING_WORKAROUND __asm__("" : [a0] "+x,m"(A0), [a1] "+x,m"(A1));
 2153|       |#else
 2154|       |#define EIGEN_GEBP_2PX4_SPILLING_WORKAROUND
 2155|       |#endif
 2156|       |#define EIGEN_GEBGP_ONESTEP(K)                                  \
 2157|       |  do {                                                          \
 2158|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 2pX4");  \
 2159|       |    traits.loadLhs(&blA[(0 + 2 * K) * LhsProgress], A0);        \
 2160|       |    traits.loadLhs(&blA[(1 + 2 * K) * LhsProgress], A1);        \
 2161|       |    traits.loadRhs(&blB[(0 + 4 * K) * RhsProgress], rhs_panel); \
 2162|       |    traits.madd(A0, rhs_panel, C0, T0, fix<0>);                 \
 2163|       |    traits.madd(A1, rhs_panel, C4, T0, fix<0>);                 \
 2164|       |    traits.madd(A0, rhs_panel, C1, T0, fix<1>);                 \
 2165|       |    traits.madd(A1, rhs_panel, C5, T0, fix<1>);                 \
 2166|       |    traits.madd(A0, rhs_panel, C2, T0, fix<2>);                 \
 2167|       |    traits.madd(A1, rhs_panel, C6, T0, fix<2>);                 \
 2168|       |    traits.madd(A0, rhs_panel, C3, T0, fix<3>);                 \
 2169|       |    traits.madd(A1, rhs_panel, C7, T0, fix<3>);                 \
 2170|       |    EIGEN_GEBP_2PX4_SPILLING_WORKAROUND                         \
 2171|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 2pX4");    \
 2172|       |  } while (false)
 2173|       |
 2174|       |            internal::prefetch(blB + (48 + 0));
 2175|       |            EIGEN_GEBGP_ONESTEP(0);
 2176|       |            EIGEN_GEBGP_ONESTEP(1);
 2177|       |            EIGEN_GEBGP_ONESTEP(2);
 2178|       |            EIGEN_GEBGP_ONESTEP(3);
 2179|       |            internal::prefetch(blB + (48 + 16));
 2180|       |            EIGEN_GEBGP_ONESTEP(4);
 2181|       |            EIGEN_GEBGP_ONESTEP(5);
 2182|       |            EIGEN_GEBGP_ONESTEP(6);
 2183|       |            EIGEN_GEBGP_ONESTEP(7);
 2184|       |
 2185|       |            blB += pk * 4 * RhsProgress;
 2186|       |            blA += pk * (2 * Traits::LhsProgress);
 2187|       |
 2188|       |            EIGEN_ASM_COMMENT("end gebp micro kernel 2pX4");
 2189|       |          }
 2190|       |          // process remaining peeled loop
 2191|       |          for (Index k = peeled_kc; k < depth; k++) {
 2192|       |            RhsPacketx4 rhs_panel;
 2193|       |            RhsPacket T0;
 2194|       |            EIGEN_GEBGP_ONESTEP(0);
 2195|       |            blB += 4 * RhsProgress;
 2196|       |            blA += 2 * Traits::LhsProgress;
 2197|       |          }
 2198|       |#undef EIGEN_GEBGP_ONESTEP
 2199|       |
 2200|       |          ResPacket R0, R1, R2, R3;
 2201|       |          ResPacket alphav = pset1<ResPacket>(alpha);
 2202|       |
 2203|       |          R0 = r0.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2204|       |          R1 = r0.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2205|       |          R2 = r1.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2206|       |          R3 = r1.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2207|       |          traits.acc(C0, alphav, R0);
 2208|       |          traits.acc(C4, alphav, R1);
 2209|       |          traits.acc(C1, alphav, R2);
 2210|       |          traits.acc(C5, alphav, R3);
 2211|       |          r0.storePacket(0 * Traits::ResPacketSize, R0);
 2212|       |          r0.storePacket(1 * Traits::ResPacketSize, R1);
 2213|       |          r1.storePacket(0 * Traits::ResPacketSize, R2);
 2214|       |          r1.storePacket(1 * Traits::ResPacketSize, R3);
 2215|       |
 2216|       |          R0 = r2.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2217|       |          R1 = r2.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2218|       |          R2 = r3.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2219|       |          R3 = r3.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2220|       |          traits.acc(C2, alphav, R0);
 2221|       |          traits.acc(C6, alphav, R1);
 2222|       |          traits.acc(C3, alphav, R2);
 2223|       |          traits.acc(C7, alphav, R3);
 2224|       |          r2.storePacket(0 * Traits::ResPacketSize, R0);
 2225|       |          r2.storePacket(1 * Traits::ResPacketSize, R1);
 2226|       |          r3.storePacket(0 * Traits::ResPacketSize, R2);
 2227|       |          r3.storePacket(1 * Traits::ResPacketSize, R3);
 2228|       |        }
 2229|       |      }
 2230|       |
 2231|       |      // Deal with remaining columns of the rhs
 2232|       |      for (Index j2 = packet_cols4; j2 < cols; j2++) {
 2233|       |        for (Index i = i1; i < actual_panel_end; i += 2 * LhsProgress) {
 2234|       |          // One column at a time
 2235|       |          const LhsScalar* blA = &blockA[i * strideA + offsetA * (2 * Traits::LhsProgress)];
 2236|       |          prefetch(&blA[0]);
 2237|       |
 2238|       |          // gets res block as register
 2239|       |          AccPacket C0, C4;
 2240|       |          traits.initAcc(C0);
 2241|       |          traits.initAcc(C4);
 2242|       |
 2243|       |          LinearMapper r0 = res.getLinearMapper(i, j2);
 2244|       |          r0.prefetch(prefetch_res_offset);
 2245|       |
 2246|       |          // performs "inner" products
 2247|       |          const RhsScalar* blB = &blockB[j2 * strideB + offsetB];
 2248|       |          LhsPacket A0, A1;
 2249|       |
 2250|       |          for (Index k = 0; k < peeled_kc; k += pk) {
 2251|       |            EIGEN_ASM_COMMENT("begin gebp micro kernel 2pX1");
 2252|       |            RhsPacket B_0, B1;
 2253|       |
 2254|       |#define EIGEN_GEBGP_ONESTEP(K)                                          \
 2255|       |  do {                                                                  \
 2256|       |    EIGEN_ASM_COMMENT("begin step of gebp micro kernel 2pX1");          \
 2257|       |    EIGEN_ASM_COMMENT("Note: these asm comments work around bug 935!"); \
 2258|       |    traits.loadLhs(&blA[(0 + 2 * K) * LhsProgress], A0);                \
 2259|       |    traits.loadLhs(&blA[(1 + 2 * K) * LhsProgress], A1);                \
 2260|       |    traits.loadRhs(&blB[(0 + K) * RhsProgress], B_0);                   \
 2261|       |    traits.madd(A0, B_0, C0, B1, fix<0>);                               \
 2262|       |    traits.madd(A1, B_0, C4, B_0, fix<0>);                              \
 2263|       |    EIGEN_ASM_COMMENT("end step of gebp micro kernel 2pX1");            \
 2264|       |  } while (false)
 2265|       |
 2266|       |            EIGEN_GEBGP_ONESTEP(0);
 2267|       |            EIGEN_GEBGP_ONESTEP(1);
 2268|       |            EIGEN_GEBGP_ONESTEP(2);
 2269|       |            EIGEN_GEBGP_ONESTEP(3);
 2270|       |            EIGEN_GEBGP_ONESTEP(4);
 2271|       |            EIGEN_GEBGP_ONESTEP(5);
 2272|       |            EIGEN_GEBGP_ONESTEP(6);
 2273|       |            EIGEN_GEBGP_ONESTEP(7);
 2274|       |
 2275|       |            blB += int(pk) * int(RhsProgress);
 2276|       |            blA += int(pk) * 2 * int(Traits::LhsProgress);
 2277|       |
 2278|       |            EIGEN_ASM_COMMENT("end gebp micro kernel 2pX1");
 2279|       |          }
 2280|       |
 2281|       |          // process remaining peeled loop
 2282|       |          for (Index k = peeled_kc; k < depth; k++) {
 2283|       |            RhsPacket B_0, B1;
 2284|       |            EIGEN_GEBGP_ONESTEP(0);
 2285|       |            blB += RhsProgress;
 2286|       |            blA += 2 * Traits::LhsProgress;
 2287|       |          }
 2288|       |#undef EIGEN_GEBGP_ONESTEP
 2289|       |          ResPacket R0, R1;
 2290|       |          ResPacket alphav = pset1<ResPacket>(alpha);
 2291|       |
 2292|       |          R0 = r0.template loadPacket<ResPacket>(0 * Traits::ResPacketSize);
 2293|       |          R1 = r0.template loadPacket<ResPacket>(1 * Traits::ResPacketSize);
 2294|       |          traits.acc(C0, alphav, R0);
 2295|       |          traits.acc(C4, alphav, R1);
 2296|       |          r0.storePacket(0 * Traits::ResPacketSize, R0);
 2297|       |          r0.storePacket(1 * Traits::ResPacketSize, R1);
 2298|       |        }
 2299|       |      }
 2300|       |    }
 2301|       |  }
 2302|       |  //---------- Process 1 * LhsProgress rows at once ----------
 2303|       |  if (mr >= 1 * Traits::LhsProgress) {
 2304|       |    lhs_process_one_packet<nr, LhsProgress, RhsProgress, LhsScalar, RhsScalar, ResScalar, AccPacket, LhsPacket,
 2305|       |                           RhsPacket, ResPacket, Traits, LinearMapper, DataMapper>
 2306|       |        p;
 2307|       |    p(res, blockA, blockB, alpha, peeled_mc2, peeled_mc1, strideA, strideB, offsetA, offsetB, prefetch_res_offset,
 2308|       |      peeled_kc, pk, cols, depth, packet_cols4);
 2309|       |  }
 2310|       |  //---------- Process LhsProgressHalf rows at once ----------
 2311|       |  if ((LhsProgressHalf < LhsProgress) && mr >= LhsProgressHalf) {
 2312|       |    lhs_process_fraction_of_packet<nr, LhsProgressHalf, RhsProgressHalf, LhsScalar, RhsScalar, ResScalar, AccPacketHalf,
 2313|       |                                   LhsPacketHalf, RhsPacketHalf, ResPacketHalf, HalfTraits, LinearMapper, DataMapper>
 2314|       |        p;
 2315|       |    p(res, blockA, blockB, alpha, peeled_mc1, peeled_mc_half, strideA, strideB, offsetA, offsetB, prefetch_res_offset,
 2316|       |      peeled_kc, pk, cols, depth, packet_cols4);
 2317|       |  }
 2318|       |  //---------- Process LhsProgressQuarter rows at once ----------
 2319|       |  if ((LhsProgressQuarter < LhsProgressHalf) && mr >= LhsProgressQuarter) {
 2320|       |    lhs_process_fraction_of_packet<nr, LhsProgressQuarter, RhsProgressQuarter, LhsScalar, RhsScalar, ResScalar,
 2321|       |                                   AccPacketQuarter, LhsPacketQuarter, RhsPacketQuarter, ResPacketQuarter,
 2322|       |                                   QuarterTraits, LinearMapper, DataMapper>
 2323|       |        p;
 2324|       |    p(res, blockA, blockB, alpha, peeled_mc_half, peeled_mc_quarter, strideA, strideB, offsetA, offsetB,
 2325|       |      prefetch_res_offset, peeled_kc, pk, cols, depth, packet_cols4);
 2326|       |  }
 2327|       |  //---------- Process remaining rows, 1 at once ----------
 2328|       |  if (peeled_mc_quarter < rows) {
 2329|       |#if EIGEN_ARCH_ARM64 || EIGEN_ARCH_LOONGARCH64
 2330|       |    EIGEN_IF_CONSTEXPR(nr >= 8) {
 2331|       |      // loop on each panel of the rhs
 2332|       |      for (Index j2 = 0; j2 < packet_cols8; j2 += 8) {
 2333|       |        // loop on each row of the lhs (1*LhsProgress x depth)
 2334|       |        for (Index i = peeled_mc_quarter; i < rows; i += 1) {
 2335|       |          const LhsScalar* blA = &blockA[i * strideA + offsetA];
 2336|       |          prefetch(&blA[0]);
 2337|       |          // gets a 1 x 1 res block as registers
 2338|       |          ResScalar C0(0), C1(0), C2(0), C3(0), C4(0), C5(0), C6(0), C7(0);
 2339|       |          const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 8];
 2340|       |          for (Index k = 0; k < depth; k++) {
 2341|       |            LhsScalar A0 = blA[k];
 2342|       |            RhsScalar B_0;
 2343|       |
 2344|       |            B_0 = blB[0];
 2345|       |            C0 = cj.pmadd(A0, B_0, C0);
 2346|       |
 2347|       |            B_0 = blB[1];
 2348|       |            C1 = cj.pmadd(A0, B_0, C1);
 2349|       |
 2350|       |            B_0 = blB[2];
 2351|       |            C2 = cj.pmadd(A0, B_0, C2);
 2352|       |
 2353|       |            B_0 = blB[3];
 2354|       |            C3 = cj.pmadd(A0, B_0, C3);
 2355|       |
 2356|       |            B_0 = blB[4];
 2357|       |            C4 = cj.pmadd(A0, B_0, C4);
 2358|       |
 2359|       |            B_0 = blB[5];
 2360|       |            C5 = cj.pmadd(A0, B_0, C5);
 2361|       |
 2362|       |            B_0 = blB[6];
 2363|       |            C6 = cj.pmadd(A0, B_0, C6);
 2364|       |
 2365|       |            B_0 = blB[7];
 2366|       |            C7 = cj.pmadd(A0, B_0, C7);
 2367|       |
 2368|       |            blB += 8;
 2369|       |          }
 2370|       |          res(i, j2 + 0) += alpha * C0;
 2371|       |          res(i, j2 + 1) += alpha * C1;
 2372|       |          res(i, j2 + 2) += alpha * C2;
 2373|       |          res(i, j2 + 3) += alpha * C3;
 2374|       |          res(i, j2 + 4) += alpha * C4;
 2375|       |          res(i, j2 + 5) += alpha * C5;
 2376|       |          res(i, j2 + 6) += alpha * C6;
 2377|       |          res(i, j2 + 7) += alpha * C7;
 2378|       |        }
 2379|       |      }
 2380|       |    }
 2381|       |#endif
 2382|       |
 2383|       |    for (Index j2 = packet_cols8; j2 < packet_cols4; j2 += 4) {
 2384|       |      // loop on each row of the lhs (1*LhsProgress x depth)
 2385|       |      for (Index i = peeled_mc_quarter; i < rows; i += 1) {
 2386|       |        const LhsScalar* blA = &blockA[i * strideA + offsetA];
 2387|       |        prefetch(&blA[0]);
 2388|       |        const RhsScalar* blB = &blockB[j2 * strideB + offsetB * 4];
 2389|       |
 2390|       |        // If LhsProgress is 8 or 16, it assumes that there is a
 2391|       |        // half or quarter packet, respectively, of the same size as
 2392|       |        // nr (which is currently 4) for the return type.
 2393|       |        const int SResPacketHalfSize = unpacket_traits<typename unpacket_traits<SResPacket>::half>::size;
 2394|       |        const int SResPacketQuarterSize =
 2395|       |            unpacket_traits<typename unpacket_traits<typename unpacket_traits<SResPacket>::half>::half>::size;
 2396|       |        // The following code assumes we can load SRhsPacket in such a way that
 2397|       |        // it multiplies blocks of 4 elements in SLhsPacket.  This is not the
 2398|       |        // case for some customized kernels (i.e. NEON fp16).  If the assumption
 2399|       |        // fails, drop down to the scalar path.
 2400|       |        constexpr bool kCanLoadSRhsQuad =
 2401|       |            (unpacket_traits<SLhsPacket>::size < 4) ||
 2402|       |            (unpacket_traits<SRhsPacket>::size % ((std::max<int>)(unpacket_traits<SLhsPacket>::size, 4) / 4)) == 0;
 2403|       |        if (kCanLoadSRhsQuad && (SwappedTraits::LhsProgress % 4) == 0 && (SwappedTraits::LhsProgress <= 16) &&
 2404|       |            (SwappedTraits::LhsProgress != 8 || SResPacketHalfSize == nr) &&
 2405|       |            (SwappedTraits::LhsProgress != 16 || SResPacketQuarterSize == nr)) {
 2406|       |          SAccPacket C0, C1, C2, C3;
 2407|       |          straits.initAcc(C0);
 2408|       |          straits.initAcc(C1);
 2409|       |          straits.initAcc(C2);
 2410|       |          straits.initAcc(C3);
 2411|       |
 2412|       |          const Index spk = (std::max)(1, SwappedTraits::LhsProgress / 4);
 2413|       |          const Index endk = (depth / spk) * spk;
 2414|       |          const Index endk4 = (depth / (spk * 4)) * (spk * 4);
 2415|       |
 2416|       |          Index k = 0;
 2417|       |          for (; k < endk4; k += 4 * spk) {
 2418|       |            SLhsPacket A0, A1;
 2419|       |            SRhsPacket B_0, B_1;
 2420|       |
 2421|       |            straits.loadLhsUnaligned(blB + 0 * SwappedTraits::LhsProgress, A0);
 2422|       |            straits.loadLhsUnaligned(blB + 1 * SwappedTraits::LhsProgress, A1);
 2423|       |
 2424|       |            straits.loadRhsQuad(blA + 0 * spk, B_0);
 2425|       |            straits.loadRhsQuad(blA + 1 * spk, B_1);
 2426|       |            straits.madd(A0, B_0, C0, B_0, fix<0>);
 2427|       |            straits.madd(A1, B_1, C1, B_1, fix<0>);
 2428|       |
 2429|       |            straits.loadLhsUnaligned(blB + 2 * SwappedTraits::LhsProgress, A0);
 2430|       |            straits.loadLhsUnaligned(blB + 3 * SwappedTraits::LhsProgress, A1);
 2431|       |            straits.loadRhsQuad(blA + 2 * spk, B_0);
 2432|       |            straits.loadRhsQuad(blA + 3 * spk, B_1);
 2433|       |            straits.madd(A0, B_0, C2, B_0, fix<0>);
 2434|       |            straits.madd(A1, B_1, C3, B_1, fix<0>);
 2435|       |
 2436|       |            blB += 4 * SwappedTraits::LhsProgress;
 2437|       |            blA += 4 * spk;
 2438|       |          }
 2439|       |          C0 = padd(padd(C0, C1), padd(C2, C3));
 2440|       |          for (; k < endk; k += spk) {
 2441|       |            SLhsPacket A0;
 2442|       |            SRhsPacket B_0;
 2443|       |
 2444|       |            straits.loadLhsUnaligned(blB, A0);
 2445|       |            straits.loadRhsQuad(blA, B_0);
 2446|       |            straits.madd(A0, B_0, C0, B_0, fix<0>);
 2447|       |
 2448|       |            blB += SwappedTraits::LhsProgress;
 2449|       |            blA += spk;
 2450|       |          }
 2451|       |          if (SwappedTraits::LhsProgress == 8) {
 2452|       |            // Special case where we have to first reduce the accumulation register C0
 2453|       |            typedef std::conditional_t<SwappedTraits::LhsProgress >= 8, typename unpacket_traits<SResPacket>::half,
 2454|       |                                       SResPacket>
 2455|       |                SResPacketHalf;
 2456|       |            typedef std::conditional_t<SwappedTraits::LhsProgress >= 8, typename unpacket_traits<SLhsPacket>::half,
 2457|       |                                       SLhsPacket>
 2458|       |                SLhsPacketHalf;
 2459|       |            typedef std::conditional_t<SwappedTraits::LhsProgress >= 8, typename unpacket_traits<SRhsPacket>::half,
 2460|       |                                       SRhsPacket>
 2461|       |                SRhsPacketHalf;
 2462|       |            typedef std::conditional_t<SwappedTraits::LhsProgress >= 8, typename unpacket_traits<SAccPacket>::half,
 2463|       |                                       SAccPacket>
 2464|       |                SAccPacketHalf;
 2465|       |
 2466|       |            SResPacketHalf R = res.template gatherPacket<SResPacketHalf>(i, j2);
 2467|       |            SResPacketHalf alphav = pset1<SResPacketHalf>(alpha);
 2468|       |
 2469|       |            if (depth - endk > 0) {
 2470|       |              // We have to handle the last row of the rhs which corresponds to a half-packet
 2471|       |              SLhsPacketHalf a0;
 2472|       |              SRhsPacketHalf b0;
 2473|       |              straits.loadLhsUnaligned(blB, a0);
 2474|       |              straits.loadRhs(blA, b0);
 2475|       |              SAccPacketHalf c0 = predux_half_dowto4(C0);
 2476|       |              straits.madd(a0, b0, c0, b0, fix<0>);
 2477|       |              straits.acc(c0, alphav, R);
 2478|       |            } else {
 2479|       |              straits.acc(predux_half_dowto4(C0), alphav, R);
 2480|       |            }
 2481|       |            res.scatterPacket(i, j2, R);
 2482|       |          } else if (SwappedTraits::LhsProgress == 16) {
 2483|       |            // Special case where we have to first reduce the
 2484|       |            // accumulation register C0. We specialize the block in
 2485|       |            // template form, so that LhsProgress < 16 paths don't
 2486|       |            // fail to compile
 2487|       |            last_row_process_16_packets<LhsScalar, RhsScalar, Index, DataMapper, mr, nr, ConjugateLhs, ConjugateRhs> p;
 2488|       |            p(res, straits, blA, blB, depth, endk, i, j2, alpha, C0);
 2489|       |          } else {
 2490|       |            SResPacket R = res.template gatherPacket<SResPacket>(i, j2);
 2491|       |            SResPacket alphav = pset1<SResPacket>(alpha);
 2492|       |            straits.acc(C0, alphav, R);
 2493|       |            res.scatterPacket(i, j2, R);
 2494|       |          }
 2495|       |        } else  // scalar path
 2496|       |        {
 2497|       |          // get a 1 x 4 res block as registers
 2498|       |          ResScalar C0(0), C1(0), C2(0), C3(0);
 2499|       |
 2500|       |          for (Index k = 0; k < depth; k++) {
 2501|       |            LhsScalar A0;
 2502|       |            RhsScalar B_0, B_1;
 2503|       |
 2504|       |            A0 = blA[k];
 2505|       |
 2506|       |            B_0 = blB[0];
 2507|       |            B_1 = blB[1];
 2508|       |            C0 = cj.pmadd(A0, B_0, C0);
 2509|       |            C1 = cj.pmadd(A0, B_1, C1);
 2510|       |
 2511|       |            B_0 = blB[2];
 2512|       |            B_1 = blB[3];
 2513|       |            C2 = cj.pmadd(A0, B_0, C2);
 2514|       |            C3 = cj.pmadd(A0, B_1, C3);
 2515|       |
 2516|       |            blB += 4;
 2517|       |          }
 2518|       |          res(i, j2 + 0) += alpha * C0;
 2519|       |          res(i, j2 + 1) += alpha * C1;
 2520|       |          res(i, j2 + 2) += alpha * C2;
 2521|       |          res(i, j2 + 3) += alpha * C3;
 2522|       |        }
 2523|       |      }
 2524|       |    }
 2525|       |    // remaining columns
 2526|       |    for (Index j2 = packet_cols4; j2 < cols; j2++) {
 2527|       |      // loop on each row of the lhs (1*LhsProgress x depth)
 2528|       |      for (Index i = peeled_mc_quarter; i < rows; i += 1) {
 2529|       |        const LhsScalar* blA = &blockA[i * strideA + offsetA];
 2530|       |        prefetch(&blA[0]);
 2531|       |        // gets a 1 x 1 res block as registers
 2532|       |        ResScalar C0(0);
 2533|       |        const RhsScalar* blB = &blockB[j2 * strideB + offsetB];
 2534|       |        for (Index k = 0; k < depth; k++) {
 2535|       |          LhsScalar A0 = blA[k];
 2536|       |          RhsScalar B_0 = blB[k];
 2537|       |          C0 = cj.pmadd(A0, B_0, C0);
 2538|       |        }
 2539|       |        res(i, j2) += alpha * C0;
 2540|       |      }
 2541|       |    }
 2542|       |  }
 2543|       |}
 2544|       |
 2545|       |// pack a block of the lhs
 2546|       |// The traversal is as follow (mr==4):
 2547|       |//   0  4  8 12 ...
 2548|       |//   1  5  9 13 ...
 2549|       |//   2  6 10 14 ...
 2550|       |//   3  7 11 15 ...
 2551|       |//
 2552|       |//  16 20 24 28 ...
 2553|       |//  17 21 25 29 ...
 2554|       |//  18 22 26 30 ...
 2555|       |//  19 23 27 31 ...
 2556|       |//
 2557|       |//  32 33 34 35 ...
 2558|       |//  36 36 38 39 ...
 2559|       |template <typename Scalar, typename Index, typename DataMapper, int Pack1, int Pack2, typename Packet, bool Conjugate,
 2560|       |          bool PanelMode>
 2561|       |struct gemm_pack_lhs<Scalar, Index, DataMapper, Pack1, Pack2, Packet, ColMajor, Conjugate, PanelMode> {
 2562|       |  typedef typename DataMapper::LinearMapper LinearMapper;
 2563|       |  EIGEN_DONT_INLINE void operator()(Scalar* blockA, const DataMapper& lhs, Index depth, Index rows, Index stride = 0,
 2564|       |                                    Index offset = 0);
 2565|       |};
 2566|       |
 2567|       |template <typename Scalar, typename Index, typename DataMapper, int Pack1, int Pack2, typename Packet, bool Conjugate,
 2568|       |          bool PanelMode>
 2569|       |EIGEN_DONT_INLINE void gemm_pack_lhs<Scalar, Index, DataMapper, Pack1, Pack2, Packet, ColMajor, Conjugate,
 2570|       |                                     PanelMode>::operator()(Scalar* blockA, const DataMapper& lhs, Index depth,
 2571|       |                                                            Index rows, Index stride, Index offset) {
 2572|       |  typedef typename unpacket_traits<Packet>::half HalfPacket;
 2573|       |  typedef typename unpacket_traits<typename unpacket_traits<Packet>::half>::half QuarterPacket;
 2574|       |  enum {
 2575|       |    PacketSize = unpacket_traits<Packet>::size,
 2576|       |    HalfPacketSize = unpacket_traits<HalfPacket>::size,
 2577|       |    QuarterPacketSize = unpacket_traits<QuarterPacket>::size,
 2578|       |    HasHalf = (int)HalfPacketSize < (int)PacketSize,
 2579|       |    HasQuarter = (int)QuarterPacketSize < (int)HalfPacketSize
 2580|       |  };
 2581|       |
 2582|       |  EIGEN_ASM_COMMENT("EIGEN PRODUCT PACK LHS");
 2583|       |  EIGEN_UNUSED_VARIABLE(stride);
 2584|       |  EIGEN_UNUSED_VARIABLE(offset);
 2585|       |  eigen_assert(((!PanelMode) && stride == 0 && offset == 0) || (PanelMode && stride >= depth && offset <= stride));
 2586|       |  eigen_assert(((Pack1 % PacketSize) == 0 && Pack1 <= 4 * PacketSize) || (Pack1 <= 4));
 2587|       |  conj_if<NumTraits<Scalar>::IsComplex && Conjugate> cj;
 2588|       |  Index count = 0;
 2589|       |
 2590|       |  const Index peeled_mc3 = Pack1 >= 3 * PacketSize ? (rows / (3 * PacketSize)) * (3 * PacketSize) : 0;
 2591|       |  const Index peeled_mc2 =
 2592|       |      Pack1 >= 2 * PacketSize ? peeled_mc3 + ((rows - peeled_mc3) / (2 * PacketSize)) * (2 * PacketSize) : 0;
 2593|       |  const Index peeled_mc1 =
 2594|       |      Pack1 >= 1 * PacketSize ? peeled_mc2 + ((rows - peeled_mc2) / (1 * PacketSize)) * (1 * PacketSize) : 0;
 2595|       |  const Index peeled_mc_half =
 2596|       |      Pack1 >= HalfPacketSize ? peeled_mc1 + ((rows - peeled_mc1) / (HalfPacketSize)) * (HalfPacketSize) : 0;
 2597|       |  const Index peeled_mc_quarter = Pack1 >= QuarterPacketSize ? (rows / (QuarterPacketSize)) * (QuarterPacketSize) : 0;
 2598|       |  const Index last_lhs_progress = rows > peeled_mc_quarter ? (rows - peeled_mc_quarter) & ~1 : 0;
 2599|       |  const Index peeled_mc0 = Pack2 >= PacketSize              ? peeled_mc_quarter
 2600|       |                           : Pack2 > 1 && last_lhs_progress ? (rows / last_lhs_progress) * last_lhs_progress
 2601|       |                                                            : 0;
 2602|       |
 2603|       |  Index i = 0;
 2604|       |
 2605|       |  // Pack 3 packets
 2606|       |  if (Pack1 >= 3 * PacketSize) {
 2607|       |    for (; i < peeled_mc3; i += 3 * PacketSize) {
 2608|       |      if (PanelMode) count += (3 * PacketSize) * offset;
 2609|       |
 2610|       |      for (Index k = 0; k < depth; k++) {
 2611|       |        Packet A, B, C;
 2612|       |        A = lhs.template loadPacket<Packet>(i + 0 * PacketSize, k);
 2613|       |        B = lhs.template loadPacket<Packet>(i + 1 * PacketSize, k);
 2614|       |        C = lhs.template loadPacket<Packet>(i + 2 * PacketSize, k);
 2615|       |        pstore(blockA + count, cj.pconj(A));
 2616|       |        count += PacketSize;
 2617|       |        pstore(blockA + count, cj.pconj(B));
 2618|       |        count += PacketSize;
 2619|       |        pstore(blockA + count, cj.pconj(C));
 2620|       |        count += PacketSize;
 2621|       |      }
 2622|       |      if (PanelMode) count += (3 * PacketSize) * (stride - offset - depth);
 2623|       |    }
 2624|       |  }
 2625|       |  // Pack 2 packets
 2626|       |  if (Pack1 >= 2 * PacketSize) {
 2627|       |    for (; i < peeled_mc2; i += 2 * PacketSize) {
 2628|       |      if (PanelMode) count += (2 * PacketSize) * offset;
 2629|       |
 2630|       |      for (Index k = 0; k < depth; k++) {
 2631|       |        Packet A, B;
 2632|       |        A = lhs.template loadPacket<Packet>(i + 0 * PacketSize, k);
 2633|       |        B = lhs.template loadPacket<Packet>(i + 1 * PacketSize, k);
 2634|       |        pstore(blockA + count, cj.pconj(A));
 2635|       |        count += PacketSize;
 2636|       |        pstore(blockA + count, cj.pconj(B));
 2637|       |        count += PacketSize;
 2638|       |      }
 2639|       |      if (PanelMode) count += (2 * PacketSize) * (stride - offset - depth);
 2640|       |    }
 2641|       |  }
 2642|       |  // Pack 1 packets
 2643|       |  if (Pack1 >= 1 * PacketSize) {
 2644|       |    for (; i < peeled_mc1; i += 1 * PacketSize) {
 2645|       |      if (PanelMode) count += (1 * PacketSize) * offset;
 2646|       |
 2647|       |      for (Index k = 0; k < depth; k++) {
 2648|       |        Packet A;
 2649|       |        A = lhs.template loadPacket<Packet>(i + 0 * PacketSize, k);
 2650|       |        pstore(blockA + count, cj.pconj(A));
 2651|       |        count += PacketSize;
 2652|       |      }
 2653|       |      if (PanelMode) count += (1 * PacketSize) * (stride - offset - depth);
 2654|       |    }
 2655|       |  }
 2656|       |  // Pack half packets
 2657|       |  if (HasHalf && Pack1 >= HalfPacketSize) {
 2658|       |    for (; i < peeled_mc_half; i += HalfPacketSize) {
 2659|       |      if (PanelMode) count += (HalfPacketSize)*offset;
 2660|       |
 2661|       |      for (Index k = 0; k < depth; k++) {
 2662|       |        HalfPacket A;
 2663|       |        A = lhs.template loadPacket<HalfPacket>(i + 0 * (HalfPacketSize), k);
 2664|       |        pstoreu(blockA + count, cj.pconj(A));
 2665|       |        count += HalfPacketSize;
 2666|       |      }
 2667|       |      if (PanelMode) count += (HalfPacketSize) * (stride - offset - depth);
 2668|       |    }
 2669|       |  }
 2670|       |  // Pack quarter packets
 2671|       |  if (HasQuarter && Pack1 >= QuarterPacketSize) {
 2672|       |    for (; i < peeled_mc_quarter; i += QuarterPacketSize) {
 2673|       |      if (PanelMode) count += (QuarterPacketSize)*offset;
 2674|       |
 2675|       |      for (Index k = 0; k < depth; k++) {
 2676|       |        QuarterPacket A;
 2677|       |        A = lhs.template loadPacket<QuarterPacket>(i + 0 * (QuarterPacketSize), k);
 2678|       |        pstoreu(blockA + count, cj.pconj(A));
 2679|       |        count += QuarterPacketSize;
 2680|       |      }
 2681|       |      if (PanelMode) count += (QuarterPacketSize) * (stride - offset - depth);
 2682|       |    }
 2683|       |  }
 2684|       |  // Pack2 may be *smaller* than PacketSize—that happens for
 2685|       |  // products like real * complex, where we have to go half the
 2686|       |  // progress on the lhs in order to duplicate those operands to
 2687|       |  // address both real & imaginary parts on the rhs. This portion will
 2688|       |  // pack those half ones until they match the number expected on the
 2689|       |  // last peeling loop at this point (for the rhs).
 2690|       |  if (Pack2 < PacketSize && Pack2 > 1) {
 2691|       |    for (; i < peeled_mc0; i += last_lhs_progress) {
 2692|       |      if (PanelMode) count += last_lhs_progress * offset;
 2693|       |
 2694|       |      for (Index k = 0; k < depth; k++)
 2695|       |        for (Index w = 0; w < last_lhs_progress; w++) blockA[count++] = cj(lhs(i + w, k));
 2696|       |
 2697|       |      if (PanelMode) count += last_lhs_progress * (stride - offset - depth);
 2698|       |    }
 2699|       |  }
 2700|       |  // Pack scalars
 2701|       |  for (; i < rows; i++) {
 2702|       |    if (PanelMode) count += offset;
 2703|       |    for (Index k = 0; k < depth; k++) blockA[count++] = cj(lhs(i, k));
 2704|       |    if (PanelMode) count += (stride - offset - depth);
 2705|       |  }
 2706|       |}
 2707|       |
 2708|       |template <typename Scalar, typename Index, typename DataMapper, int Pack1, int Pack2, typename Packet, bool Conjugate,
 2709|       |          bool PanelMode>
 2710|       |struct gemm_pack_lhs<Scalar, Index, DataMapper, Pack1, Pack2, Packet, RowMajor, Conjugate, PanelMode> {
 2711|       |  typedef typename DataMapper::LinearMapper LinearMapper;
 2712|       |  EIGEN_DONT_INLINE void operator()(Scalar* blockA, const DataMapper& lhs, Index depth, Index rows, Index stride = 0,
 2713|       |                                    Index offset = 0);
 2714|       |};
 2715|       |
 2716|       |template <typename Scalar, typename Index, typename DataMapper, int Pack1, int Pack2, typename Packet, bool Conjugate,
 2717|       |          bool PanelMode>
 2718|       |EIGEN_DONT_INLINE void gemm_pack_lhs<Scalar, Index, DataMapper, Pack1, Pack2, Packet, RowMajor, Conjugate,
 2719|       |                                     PanelMode>::operator()(Scalar* blockA, const DataMapper& lhs, Index depth,
 2720|       |                                                            Index rows, Index stride, Index offset) {
 2721|       |  typedef typename unpacket_traits<Packet>::half HalfPacket;
 2722|       |  typedef typename unpacket_traits<typename unpacket_traits<Packet>::half>::half QuarterPacket;
 2723|       |  enum {
 2724|       |    PacketSize = unpacket_traits<Packet>::size,
 2725|       |    HalfPacketSize = unpacket_traits<HalfPacket>::size,
 2726|       |    QuarterPacketSize = unpacket_traits<QuarterPacket>::size,
 2727|       |    HasHalf = (int)HalfPacketSize < (int)PacketSize,
 2728|       |    HasQuarter = (int)QuarterPacketSize < (int)HalfPacketSize
 2729|       |  };
 2730|       |
 2731|       |  EIGEN_ASM_COMMENT("EIGEN PRODUCT PACK LHS");
 2732|       |  EIGEN_UNUSED_VARIABLE(stride);
 2733|       |  EIGEN_UNUSED_VARIABLE(offset);
 2734|       |  eigen_assert(((!PanelMode) && stride == 0 && offset == 0) || (PanelMode && stride >= depth && offset <= stride));
 2735|       |  conj_if<NumTraits<Scalar>::IsComplex && Conjugate> cj;
 2736|       |  Index count = 0;
 2737|       |  bool gone_half = false, gone_quarter = false, gone_last = false;
 2738|       |
 2739|       |  Index i = 0;
 2740|       |  Index pack = Pack1;
 2741|       |  Index psize = PacketSize;
 2742|       |  while (pack > 0) {
 2743|       |    Index remaining_rows = rows - i;
 2744|       |    Index peeled_mc = gone_last ? Pack2 > 1 ? (rows / pack) * pack : 0 : i + (remaining_rows / pack) * pack;
 2745|       |    Index starting_pos = i;
 2746|       |    for (; i < peeled_mc; i += pack) {
 2747|       |      if (PanelMode) count += pack * offset;
 2748|       |
 2749|       |      Index k = 0;
 2750|       |      if (pack >= psize && psize >= QuarterPacketSize) {
 2751|       |        const Index peeled_k = (depth / psize) * psize;
 2752|       |        for (; k < peeled_k; k += psize) {
 2753|       |          for (Index m = 0; m < pack; m += psize) {
 2754|       |            if (psize == PacketSize) {
 2755|       |              PacketBlock<Packet> kernel;
 2756|       |              for (Index p = 0; p < psize; ++p) kernel.packet[p] = lhs.template loadPacket<Packet>(i + p + m, k);
 2757|       |              ptranspose(kernel);
 2758|       |              for (Index p = 0; p < psize; ++p) pstore(blockA + count + m + (pack)*p, cj.pconj(kernel.packet[p]));
 2759|       |            } else if (HasHalf && psize == HalfPacketSize) {
 2760|       |              gone_half = true;
 2761|       |              PacketBlock<HalfPacket> kernel_half;
 2762|       |              for (Index p = 0; p < psize; ++p)
 2763|       |                kernel_half.packet[p] = lhs.template loadPacket<HalfPacket>(i + p + m, k);
 2764|       |              ptranspose(kernel_half);
 2765|       |              for (Index p = 0; p < psize; ++p) pstore(blockA + count + m + (pack)*p, cj.pconj(kernel_half.packet[p]));
 2766|       |            } else if (HasQuarter && psize == QuarterPacketSize) {
 2767|       |              gone_quarter = true;
 2768|       |              PacketBlock<QuarterPacket> kernel_quarter;
 2769|       |              for (Index p = 0; p < psize; ++p)
 2770|       |                kernel_quarter.packet[p] = lhs.template loadPacket<QuarterPacket>(i + p + m, k);
 2771|       |              ptranspose(kernel_quarter);
 2772|       |              for (Index p = 0; p < psize; ++p)
 2773|       |                pstore(blockA + count + m + (pack)*p, cj.pconj(kernel_quarter.packet[p]));
 2774|       |            }
 2775|       |          }
 2776|       |          count += psize * pack;
 2777|       |        }
 2778|       |      }
 2779|       |
 2780|       |      for (; k < depth; k++) {
 2781|       |        Index w = 0;
 2782|       |        for (; w < pack - 3; w += 4) {
 2783|       |          Scalar a(cj(lhs(i + w + 0, k))), b(cj(lhs(i + w + 1, k))), c(cj(lhs(i + w + 2, k))), d(cj(lhs(i + w + 3, k)));
 2784|       |          blockA[count++] = a;
 2785|       |          blockA[count++] = b;
 2786|       |          blockA[count++] = c;
 2787|       |          blockA[count++] = d;
 2788|       |        }
 2789|       |        if (pack % 4)
 2790|       |          for (; w < pack; ++w) blockA[count++] = cj(lhs(i + w, k));
 2791|       |      }
 2792|       |
 2793|       |      if (PanelMode) count += pack * (stride - offset - depth);
 2794|       |    }
 2795|       |
 2796|       |    pack -= psize;
 2797|       |    Index left = rows - i;
 2798|       |    if (pack <= 0) {
 2799|       |      if (!gone_last && (starting_pos == i || left >= psize / 2 || left >= psize / 4) &&
 2800|       |          ((psize / 2 == HalfPacketSize && HasHalf && !gone_half) ||
 2801|       |           (psize / 2 == QuarterPacketSize && HasQuarter && !gone_quarter))) {
 2802|       |        psize /= 2;
 2803|       |        pack = psize;
 2804|       |        continue;
 2805|       |      }
 2806|       |      // Pack2 may be *smaller* than PacketSize—that happens for
 2807|       |      // products like real * complex, where we have to go half the
 2808|       |      // progress on the lhs in order to duplicate those operands to
 2809|       |      // address both real & imaginary parts on the rhs. This portion will
 2810|       |      // pack those half ones until they match the number expected on the
 2811|       |      // last peeling loop at this point (for the rhs).
 2812|       |      if (Pack2 < PacketSize && !gone_last) {
 2813|       |        gone_last = true;
 2814|       |        psize = pack = left & ~1;
 2815|       |      }
 2816|       |    }
 2817|       |  }
 2818|       |
 2819|       |  for (; i < rows; i++) {
 2820|       |    if (PanelMode) count += offset;
 2821|       |    for (Index k = 0; k < depth; k++) blockA[count++] = cj(lhs(i, k));
 2822|       |    if (PanelMode) count += (stride - offset - depth);
 2823|       |  }
 2824|       |}
 2825|       |
 2826|       |// copy a complete panel of the rhs
 2827|       |// this version is optimized for column major matrices
 2828|       |// The traversal order is as follow: (nr==4):
 2829|       |//  0  1  2  3   12 13 14 15   24 27
 2830|       |//  4  5  6  7   16 17 18 19   25 28
 2831|       |//  8  9 10 11   20 21 22 23   26 29
 2832|       |//  .  .  .  .    .  .  .  .    .  .
 2833|       |template <typename Scalar, typename Index, typename DataMapper, int nr, bool Conjugate, bool PanelMode>
 2834|       |struct gemm_pack_rhs<Scalar, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode> {
 2835|       |  typedef typename packet_traits<Scalar>::type Packet;
 2836|       |  typedef typename DataMapper::LinearMapper LinearMapper;
 2837|       |  enum { PacketSize = packet_traits<Scalar>::size };
 2838|       |  EIGEN_DONT_INLINE void operator()(Scalar* blockB, const DataMapper& rhs, Index depth, Index cols, Index stride = 0,
 2839|       |                                    Index offset = 0);
 2840|       |};
 2841|       |
 2842|       |template <typename Scalar, typename Index, typename DataMapper, int nr, bool Conjugate, bool PanelMode>
 2843|       |EIGEN_DONT_INLINE void gemm_pack_rhs<Scalar, Index, DataMapper, nr, ColMajor, Conjugate, PanelMode>::operator()(
 2844|       |    Scalar* blockB, const DataMapper& rhs, Index depth, Index cols, Index stride, Index offset) {
 2845|       |  EIGEN_ASM_COMMENT("EIGEN PRODUCT PACK RHS COLMAJOR");
 2846|       |  EIGEN_UNUSED_VARIABLE(stride);
 2847|       |  EIGEN_UNUSED_VARIABLE(offset);
 2848|       |  eigen_assert(((!PanelMode) && stride == 0 && offset == 0) || (PanelMode && stride >= depth && offset <= stride));
 2849|       |  conj_if<NumTraits<Scalar>::IsComplex && Conjugate> cj;
 2850|       |  Index packet_cols8 = nr >= 8 ? (cols / 8) * 8 : 0;
 2851|       |  Index packet_cols4 = nr >= 4 ? (cols / 4) * 4 : 0;
 2852|       |  Index count = 0;
 2853|       |  const Index peeled_k = (depth / PacketSize) * PacketSize;
 2854|       |
 2855|       |#if EIGEN_ARCH_ARM64 || EIGEN_ARCH_LOONGARCH64
 2856|       |  EIGEN_IF_CONSTEXPR(nr >= 8) {
 2857|       |    for (Index j2 = 0; j2 < packet_cols8; j2 += 8) {
 2858|       |      // skip what we have before
 2859|       |      if (PanelMode) count += 8 * offset;
 2860|       |      const LinearMapper dm0 = rhs.getLinearMapper(0, j2 + 0);
 2861|       |      const LinearMapper dm1 = rhs.getLinearMapper(0, j2 + 1);
 2862|       |      const LinearMapper dm2 = rhs.getLinearMapper(0, j2 + 2);
 2863|       |      const LinearMapper dm3 = rhs.getLinearMapper(0, j2 + 3);
 2864|       |      const LinearMapper dm4 = rhs.getLinearMapper(0, j2 + 4);
 2865|       |      const LinearMapper dm5 = rhs.getLinearMapper(0, j2 + 5);
 2866|       |      const LinearMapper dm6 = rhs.getLinearMapper(0, j2 + 6);
 2867|       |      const LinearMapper dm7 = rhs.getLinearMapper(0, j2 + 7);
 2868|       |      Index k = 0;
 2869|       |      if (PacketSize % 2 == 0 && PacketSize <= 8)  // 2 4 8
 2870|       |      {
 2871|       |        for (; k < peeled_k; k += PacketSize) {
 2872|       |          if (PacketSize == 2) {
 2873|       |            PacketBlock<Packet, PacketSize == 2 ? 2 : PacketSize> kernel0, kernel1, kernel2, kernel3;
 2874|       |            kernel0.packet[0 % PacketSize] = dm0.template loadPacket<Packet>(k);
 2875|       |            kernel0.packet[1 % PacketSize] = dm1.template loadPacket<Packet>(k);
 2876|       |            kernel1.packet[0 % PacketSize] = dm2.template loadPacket<Packet>(k);
 2877|       |            kernel1.packet[1 % PacketSize] = dm3.template loadPacket<Packet>(k);
 2878|       |            kernel2.packet[0 % PacketSize] = dm4.template loadPacket<Packet>(k);
 2879|       |            kernel2.packet[1 % PacketSize] = dm5.template loadPacket<Packet>(k);
 2880|       |            kernel3.packet[0 % PacketSize] = dm6.template loadPacket<Packet>(k);
 2881|       |            kernel3.packet[1 % PacketSize] = dm7.template loadPacket<Packet>(k);
 2882|       |            ptranspose(kernel0);
 2883|       |            ptranspose(kernel1);
 2884|       |            ptranspose(kernel2);
 2885|       |            ptranspose(kernel3);
 2886|       |
 2887|       |            pstoreu(blockB + count + 0 * PacketSize, cj.pconj(kernel0.packet[0 % PacketSize]));
 2888|       |            pstoreu(blockB + count + 1 * PacketSize, cj.pconj(kernel1.packet[0 % PacketSize]));
 2889|       |            pstoreu(blockB + count + 2 * PacketSize, cj.pconj(kernel2.packet[0 % PacketSize]));
 2890|       |            pstoreu(blockB + count + 3 * PacketSize, cj.pconj(kernel3.packet[0 % PacketSize]));
 2891|       |
 2892|       |            pstoreu(blockB + count + 4 * PacketSize, cj.pconj(kernel0.packet[1 % PacketSize]));
 2893|       |            pstoreu(blockB + count + 5 * PacketSize, cj.pconj(kernel1.packet[1 % PacketSize]));
 2894|       |            pstoreu(blockB + count + 6 * PacketSize, cj.pconj(kernel2.packet[1 % PacketSize]));
 2895|       |            pstoreu(blockB + count + 7 * PacketSize, cj.pconj(kernel3.packet[1 % PacketSize]));
 2896|       |            count += 8 * PacketSize;
 2897|       |          } else if (PacketSize == 4) {
 2898|       |            PacketBlock<Packet, PacketSize == 4 ? 4 : PacketSize> kernel0, kernel1;
 2899|       |
 2900|       |            kernel0.packet[0 % PacketSize] = dm0.template loadPacket<Packet>(k);
 2901|       |            kernel0.packet[1 % PacketSize] = dm1.template loadPacket<Packet>(k);
 2902|       |            kernel0.packet[2 % PacketSize] = dm2.template loadPacket<Packet>(k);
 2903|       |            kernel0.packet[3 % PacketSize] = dm3.template loadPacket<Packet>(k);
 2904|       |            kernel1.packet[0 % PacketSize] = dm4.template loadPacket<Packet>(k);
 2905|       |            kernel1.packet[1 % PacketSize] = dm5.template loadPacket<Packet>(k);
 2906|       |            kernel1.packet[2 % PacketSize] = dm6.template loadPacket<Packet>(k);
 2907|       |            kernel1.packet[3 % PacketSize] = dm7.template loadPacket<Packet>(k);
 2908|       |            ptranspose(kernel0);
 2909|       |            ptranspose(kernel1);
 2910|       |
 2911|       |            pstoreu(blockB + count + 0 * PacketSize, cj.pconj(kernel0.packet[0 % PacketSize]));
 2912|       |            pstoreu(blockB + count + 1 * PacketSize, cj.pconj(kernel1.packet[0 % PacketSize]));
 2913|       |            pstoreu(blockB + count + 2 * PacketSize, cj.pconj(kernel0.packet[1 % PacketSize]));
 2914|       |            pstoreu(blockB + count + 3 * PacketSize, cj.pconj(kernel1.packet[1 % PacketSize]));
 2915|       |            pstoreu(blockB + count + 4 * PacketSize, cj.pconj(kernel0.packet[2 % PacketSize]));
 2916|       |            pstoreu(blockB + count + 5 * PacketSize, cj.pconj(kernel1.packet[2 % PacketSize]));
 2917|       |            pstoreu(blockB + count + 6 * PacketSize, cj.pconj(kernel0.packet[3 % PacketSize]));
 2918|       |            pstoreu(blockB + count + 7 * PacketSize, cj.pconj(kernel1.packet[3 % PacketSize]));
 2919|       |            count += 8 * PacketSize;
 2920|       |          } else if (PacketSize == 8) {
 2921|       |            PacketBlock<Packet, PacketSize == 8 ? 8 : PacketSize> kernel0;
 2922|       |
 2923|       |            kernel0.packet[0 % PacketSize] = dm0.template loadPacket<Packet>(k);
 2924|       |            kernel0.packet[1 % PacketSize] = dm1.template loadPacket<Packet>(k);
 2925|       |            kernel0.packet[2 % PacketSize] = dm2.template loadPacket<Packet>(k);
 2926|       |            kernel0.packet[3 % PacketSize] = dm3.template loadPacket<Packet>(k);
 2927|       |            kernel0.packet[4 % PacketSize] = dm4.template loadPacket<Packet>(k);
 2928|       |            kernel0.packet[5 % PacketSize] = dm5.template loadPacket<Packet>(k);
 2929|       |            kernel0.packet[6 % PacketSize] = dm6.template loadPacket<Packet>(k);
 2930|       |            kernel0.packet[7 % PacketSize] = dm7.template loadPacket<Packet>(k);
 2931|       |            ptranspose(kernel0);
 2932|       |
 2933|       |            pstoreu(blockB + count + 0 * PacketSize, cj.pconj(kernel0.packet[0 % PacketSize]));
 2934|       |            pstoreu(blockB + count + 1 * PacketSize, cj.pconj(kernel0.packet[1 % PacketSize]));
 2935|       |            pstoreu(blockB + count + 2 * PacketSize, cj.pconj(kernel0.packet[2 % PacketSize]));
 2936|       |            pstoreu(blockB + count + 3 * PacketSize, cj.pconj(kernel0.packet[3 % PacketSize]));
 2937|       |            pstoreu(blockB + count + 4 * PacketSize, cj.pconj(kernel0.packet[4 % PacketSize]));
 2938|       |            pstoreu(blockB + count + 5 * PacketSize, cj.pconj(kernel0.packet[5 % PacketSize]));
 2939|       |            pstoreu(blockB + count + 6 * PacketSize, cj.pconj(kernel0.packet[6 % PacketSize]));
 2940|       |            pstoreu(blockB + count + 7 * PacketSize, cj.pconj(kernel0.packet[7 % PacketSize]));
 2941|       |            count += 8 * PacketSize;
 2942|       |          }
 2943|       |        }
 2944|       |      }
 2945|       |
 2946|       |      for (; k < depth; k++) {
 2947|       |        blockB[count + 0] = cj(dm0(k));
 2948|       |        blockB[count + 1] = cj(dm1(k));
 2949|       |        blockB[count + 2] = cj(dm2(k));
 2950|       |        blockB[count + 3] = cj(dm3(k));
 2951|       |        blockB[count + 4] = cj(dm4(k));
 2952|       |        blockB[count + 5] = cj(dm5(k));
 2953|       |        blockB[count + 6] = cj(dm6(k));
 2954|       |        blockB[count + 7] = cj(dm7(k));
 2955|       |        count += 8;
 2956|       |      }
 2957|       |      // skip what we have after
 2958|       |      if (PanelMode) count += 8 * (stride - offset - depth);
 2959|       |    }
 2960|       |  }
 2961|       |#endif
 2962|       |
 2963|       |  EIGEN_IF_CONSTEXPR(nr >= 4) {
 2964|       |    for (Index j2 = packet_cols8; j2 < packet_cols4; j2 += 4) {
 2965|       |      // skip what we have before
 2966|       |      if (PanelMode) count += 4 * offset;
 2967|       |      const LinearMapper dm0 = rhs.getLinearMapper(0, j2 + 0);
 2968|       |      const LinearMapper dm1 = rhs.getLinearMapper(0, j2 + 1);
 2969|       |      const LinearMapper dm2 = rhs.getLinearMapper(0, j2 + 2);
 2970|       |      const LinearMapper dm3 = rhs.getLinearMapper(0, j2 + 3);
 2971|       |
 2972|       |      Index k = 0;
 2973|       |      if ((PacketSize % 4) == 0)  // TODO enable vectorized transposition for PacketSize==2 ??
 2974|       |      {
 2975|       |        for (; k < peeled_k; k += PacketSize) {
 2976|       |          PacketBlock<Packet, (PacketSize % 4) == 0 ? 4 : PacketSize> kernel;
 2977|       |          kernel.packet[0] = dm0.template loadPacket<Packet>(k);
 2978|       |          kernel.packet[1 % PacketSize] = dm1.template loadPacket<Packet>(k);
 2979|       |          kernel.packet[2 % PacketSize] = dm2.template loadPacket<Packet>(k);
 2980|       |          kernel.packet[3 % PacketSize] = dm3.template loadPacket<Packet>(k);
 2981|       |          ptranspose(kernel);
 2982|       |          pstoreu(blockB + count + 0 * PacketSize, cj.pconj(kernel.packet[0]));
 2983|       |          pstoreu(blockB + count + 1 * PacketSize, cj.pconj(kernel.packet[1 % PacketSize]));
 2984|       |          pstoreu(blockB + count + 2 * PacketSize, cj.pconj(kernel.packet[2 % PacketSize]));
 2985|       |          pstoreu(blockB + count + 3 * PacketSize, cj.pconj(kernel.packet[3 % PacketSize]));
 2986|       |          count += 4 * PacketSize;
 2987|       |        }
 2988|       |      }
 2989|       |      for (; k < depth; k++) {
 2990|       |        blockB[count + 0] = cj(dm0(k));
 2991|       |        blockB[count + 1] = cj(dm1(k));
 2992|       |        blockB[count + 2] = cj(dm2(k));
 2993|       |        blockB[count + 3] = cj(dm3(k));
 2994|       |        count += 4;
 2995|       |      }
 2996|       |      // skip what we have after
 2997|       |      if (PanelMode) count += 4 * (stride - offset - depth);
 2998|       |    }
 2999|       |  }
 3000|       |
 3001|       |  // copy the remaining columns one at a time (nr==1)
 3002|       |  for (Index j2 = packet_cols4; j2 < cols; ++j2) {
 3003|       |    if (PanelMode) count += offset;
 3004|       |    const LinearMapper dm0 = rhs.getLinearMapper(0, j2);
 3005|       |    for (Index k = 0; k < depth; k++) {
 3006|       |      blockB[count] = cj(dm0(k));
 3007|       |      count += 1;
 3008|       |    }
 3009|       |    if (PanelMode) count += (stride - offset - depth);
 3010|       |  }
 3011|       |}
 3012|       |
 3013|       |// this version is optimized for row major matrices
 3014|       |template <typename Scalar, typename Index, typename DataMapper, int nr, bool Conjugate, bool PanelMode>
 3015|       |struct gemm_pack_rhs<Scalar, Index, DataMapper, nr, RowMajor, Conjugate, PanelMode> {
 3016|       |  typedef typename packet_traits<Scalar>::type Packet;
 3017|       |  typedef typename unpacket_traits<Packet>::half HalfPacket;
 3018|       |  typedef typename unpacket_traits<typename unpacket_traits<Packet>::half>::half QuarterPacket;
 3019|       |  typedef typename DataMapper::LinearMapper LinearMapper;
 3020|       |  enum {
 3021|       |    PacketSize = packet_traits<Scalar>::size,
 3022|       |    HalfPacketSize = unpacket_traits<HalfPacket>::size,
 3023|       |    QuarterPacketSize = unpacket_traits<QuarterPacket>::size
 3024|       |  };
 3025|       |  EIGEN_DONT_INLINE void operator()(Scalar* blockB, const DataMapper& rhs, Index depth, Index cols, Index stride = 0,
 3026|       |                                    Index offset = 0) {
 3027|       |    EIGEN_ASM_COMMENT("EIGEN PRODUCT PACK RHS ROWMAJOR");
 3028|       |    EIGEN_UNUSED_VARIABLE(stride);
 3029|       |    EIGEN_UNUSED_VARIABLE(offset);
 3030|       |    eigen_assert(((!PanelMode) && stride == 0 && offset == 0) || (PanelMode && stride >= depth && offset <= stride));
 3031|       |    const bool HasHalf = (int)HalfPacketSize < (int)PacketSize;
 3032|       |    const bool HasQuarter = (int)QuarterPacketSize < (int)HalfPacketSize;
 3033|       |    conj_if<NumTraits<Scalar>::IsComplex && Conjugate> cj;
 3034|       |    Index packet_cols8 = nr >= 8 ? (cols / 8) * 8 : 0;
 3035|       |    Index packet_cols4 = nr >= 4 ? (cols / 4) * 4 : 0;
 3036|       |    Index count = 0;
 3037|       |
 3038|       |#if EIGEN_ARCH_ARM64 || EIGEN_ARCH_LOONGARCH64
 3039|       |    EIGEN_IF_CONSTEXPR(nr >= 8) {
 3040|       |      for (Index j2 = 0; j2 < packet_cols8; j2 += 8) {
 3041|       |        // skip what we have before
 3042|       |        if (PanelMode) count += 8 * offset;
 3043|       |        for (Index k = 0; k < depth; k++) {
 3044|       |          if (PacketSize == 8) {
 3045|       |            Packet A = rhs.template loadPacket<Packet>(k, j2);
 3046|       |            pstoreu(blockB + count, cj.pconj(A));
 3047|       |            count += PacketSize;
 3048|       |          } else if (PacketSize == 4) {
 3049|       |            Packet A = rhs.template loadPacket<Packet>(k, j2);
 3050|       |            Packet B = rhs.template loadPacket<Packet>(k, j2 + 4);
 3051|       |            pstoreu(blockB + count, cj.pconj(A));
 3052|       |            pstoreu(blockB + count + PacketSize, cj.pconj(B));
 3053|       |            count += 2 * PacketSize;
 3054|       |          } else {
 3055|       |            const LinearMapper dm0 = rhs.getLinearMapper(k, j2);
 3056|       |            blockB[count + 0] = cj(dm0(0));
 3057|       |            blockB[count + 1] = cj(dm0(1));
 3058|       |            blockB[count + 2] = cj(dm0(2));
 3059|       |            blockB[count + 3] = cj(dm0(3));
 3060|       |            blockB[count + 4] = cj(dm0(4));
 3061|       |            blockB[count + 5] = cj(dm0(5));
 3062|       |            blockB[count + 6] = cj(dm0(6));
 3063|       |            blockB[count + 7] = cj(dm0(7));
 3064|       |            count += 8;
 3065|       |          }
 3066|       |        }
 3067|       |        // skip what we have after
 3068|       |        if (PanelMode) count += 8 * (stride - offset - depth);
 3069|       |      }
 3070|       |    }
 3071|       |#endif
 3072|       |
 3073|       |    if (nr >= 4) {
 3074|       |      for (Index j2 = packet_cols8; j2 < packet_cols4; j2 += 4) {
 3075|       |        // skip what we have before
 3076|       |        if (PanelMode) count += 4 * offset;
 3077|       |        for (Index k = 0; k < depth; k++) {
 3078|       |          if (PacketSize == 4) {
 3079|       |            Packet A = rhs.template loadPacket<Packet>(k, j2);
 3080|       |            pstoreu(blockB + count, cj.pconj(A));
 3081|       |            count += PacketSize;
 3082|       |          } else if (HasHalf && HalfPacketSize == 4) {
 3083|       |            HalfPacket A = rhs.template loadPacket<HalfPacket>(k, j2);
 3084|       |            pstoreu(blockB + count, cj.pconj(A));
 3085|       |            count += HalfPacketSize;
 3086|       |          } else if (HasQuarter && QuarterPacketSize == 4) {
 3087|       |            QuarterPacket A = rhs.template loadPacket<QuarterPacket>(k, j2);
 3088|       |            pstoreu(blockB + count, cj.pconj(A));
 3089|       |            count += QuarterPacketSize;
 3090|       |          } else {
 3091|       |            const LinearMapper dm0 = rhs.getLinearMapper(k, j2);
 3092|       |            blockB[count + 0] = cj(dm0(0));
 3093|       |            blockB[count + 1] = cj(dm0(1));
 3094|       |            blockB[count + 2] = cj(dm0(2));
 3095|       |            blockB[count + 3] = cj(dm0(3));
 3096|       |            count += 4;
 3097|       |          }
 3098|       |        }
 3099|       |        // skip what we have after
 3100|       |        if (PanelMode) count += 4 * (stride - offset - depth);
 3101|       |      }
 3102|       |    }
 3103|       |    // copy the remaining columns one at a time (nr==1)
 3104|       |    for (Index j2 = packet_cols4; j2 < cols; ++j2) {
 3105|       |      if (PanelMode) count += offset;
 3106|       |      for (Index k = 0; k < depth; k++) {
 3107|       |        blockB[count] = cj(rhs(k, j2));
 3108|       |        count += 1;
 3109|       |      }
 3110|       |      if (PanelMode) count += stride - offset - depth;
 3111|       |    }
 3112|       |  }
 3113|       |};
 3114|       |
 3115|       |}  // end namespace internal
 3116|       |
 3117|       |/** \returns the currently set level 1 cpu cache size (in bytes) used to estimate the ideal blocking size parameters.
 3118|       | * \sa setCpuCacheSize */
 3119|      0|inline std::ptrdiff_t l1CacheSize() {
 3120|      0|  std::ptrdiff_t l1, l2, l3;
 3121|      0|  internal::manage_caching_sizes(GetAction, &l1, &l2, &l3);
 3122|      0|  return l1;
 3123|      0|}
 3124|       |
 3125|       |/** \returns the currently set level 2 cpu cache size (in bytes) used to estimate the ideal blocking size parameters.
 3126|       | * \sa setCpuCacheSize */
 3127|      0|inline std::ptrdiff_t l2CacheSize() {
 3128|      0|  std::ptrdiff_t l1, l2, l3;
 3129|      0|  internal::manage_caching_sizes(GetAction, &l1, &l2, &l3);
 3130|      0|  return l2;
 3131|      0|}
 3132|       |
 3133|       |/** \returns the currently set level 3 cpu cache size (in bytes) used to estimate the ideal blocking size parameters.
 3134|       | * \sa setCpuCacheSize */
 3135|      0|inline std::ptrdiff_t l3CacheSize() {
 3136|      0|  std::ptrdiff_t l1, l2, l3;
 3137|      0|  internal::manage_caching_sizes(GetAction, &l1, &l2, &l3);
 3138|      0|  return l3;
 3139|      0|}
 3140|       |
 3141|       |/** Set the cpu L1 and L2 cache sizes (in bytes).
 3142|       | * These values are use to adjust the size of the blocks
 3143|       | * for the algorithms working per blocks.
 3144|       | *
 3145|       | * \sa computeProductBlockingSizes */
 3146|      0|inline void setCpuCacheSizes(std::ptrdiff_t l1, std::ptrdiff_t l2, std::ptrdiff_t l3) {
 3147|      0|  internal::manage_caching_sizes(SetAction, &l1, &l2, &l3);
 3148|      0|}
 3149|       |
 3150|       |}  // end namespace Eigen
 3151|       |
 3152|       |#endif  // EIGEN_GENERAL_BLOCK_PANEL_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/products/Parallelizer.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2010 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_PARALLELIZER_H
   11|       |#define EIGEN_PARALLELIZER_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |// Note that in the following, there are 3 different uses of the concept
   17|       |// "number of threads":
   18|       |//  1. Max number of threads used by OpenMP or ThreadPool.
   19|       |//     * For OpenMP this is typically the value set by the OMP_NUM_THREADS
   20|       |//       environment variable, or by a call to omp_set_num_threads() prior to
   21|       |//       calling Eigen.
   22|       |//     * For ThreadPool, this is the number of threads in the ThreadPool.
   23|       |//  2. Max number of threads currently allowed to be used by parallel Eigen
   24|       |//     operations. This is set by setNbThreads(), and cannot exceed the value
   25|       |//     in 1.
   26|       |//  3. The actual number of threads used for a given parallel Eigen operation.
   27|       |//     This is typically computed on the fly using a cost model and cannot exceed
   28|       |//     the value in 2.
   29|       |//     * For OpenMP, this is typically the number of threads specified in individual
   30|       |//       "omp parallel" pragmas associated with an Eigen operation.
   31|       |//     * For ThreadPool, it is the number of concurrent tasks scheduled in the
   32|       |//       threadpool for a given Eigen operation. Notice that since the threadpool
   33|       |//       uses task stealing, there is no way to limit the number of concurrently
   34|       |//       executing tasks to below the number in 1. except by limiting the total
   35|       |//       number of tasks in flight.
   36|       |
   37|       |#if defined(EIGEN_HAS_OPENMP) && defined(EIGEN_GEMM_THREADPOOL)
   38|       |#error "EIGEN_HAS_OPENMP and EIGEN_GEMM_THREADPOOL may not both be defined."
   39|       |#endif
   40|       |
   41|       |namespace Eigen {
   42|       |
   43|       |namespace internal {
   44|       |inline void manage_multi_threading(Action action, int* v);
   45|       |}
   46|       |
   47|       |// Public APIs.
   48|       |
   49|       |/** Must be call first when calling Eigen from multiple threads */
   50|      0|EIGEN_DEPRECATED inline void initParallel() {}
   51|       |
   52|       |/** \returns the max number of threads reserved for Eigen
   53|       | * \sa setNbThreads */
   54|      0|inline int nbThreads() {
   55|      0|  int ret;
   56|      0|  internal::manage_multi_threading(GetAction, &ret);
   57|      0|  return ret;
   58|      0|}
   59|       |
   60|       |/** Sets the max number of threads reserved for Eigen
   61|       | * \sa nbThreads */
   62|      0|inline void setNbThreads(int v) { internal::manage_multi_threading(SetAction, &v); }
   63|       |
   64|       |#ifdef EIGEN_GEMM_THREADPOOL
   65|       |// Sets the ThreadPool used by Eigen parallel Gemm.
   66|       |//
   67|       |// NOTICE: This function has a known race condition with
   68|       |// parallelize_gemm below, and should not be called while
   69|       |// an instance of that function is running.
   70|       |//
   71|       |// TODO(rmlarsen): Make the device API available instead of
   72|       |// storing a local static pointer variable to avoid this issue.
   73|       |inline ThreadPool* setGemmThreadPool(ThreadPool* new_pool) {
   74|       |  static ThreadPool* pool = nullptr;
   75|       |  if (new_pool != nullptr) {
   76|       |    // This will wait for work in all threads in *pool to finish,
   77|       |    // then destroy the old ThreadPool, and then replace it with new_pool.
   78|       |    pool = new_pool;
   79|       |    // Reset the number of threads to the number of threads on the new pool.
   80|       |    setNbThreads(pool->NumThreads());
   81|       |  }
   82|       |  return pool;
   83|       |}
   84|       |
   85|       |// Gets the ThreadPool used by Eigen parallel Gemm.
   86|       |inline ThreadPool* getGemmThreadPool() { return setGemmThreadPool(nullptr); }
   87|       |#endif
   88|       |
   89|       |namespace internal {
   90|       |
   91|       |// Implementation.
   92|       |
   93|       |#if defined(EIGEN_USE_BLAS) || (!defined(EIGEN_HAS_OPENMP) && !defined(EIGEN_GEMM_THREADPOOL))
   94|       |
   95|      0|inline void manage_multi_threading(Action action, int* v) {
   96|      0|  if (action == SetAction) {
   97|      0|    eigen_internal_assert(v != nullptr);
   98|      0|  } else if (action == GetAction) {
   99|      0|    eigen_internal_assert(v != nullptr);
  100|      0|    *v = 1;
  101|      0|  } else {
  102|      0|    eigen_internal_assert(false);
  103|      0|  }
  104|      0|}
  105|       |template <typename Index>
  106|       |struct GemmParallelInfo {};
  107|       |template <bool Condition, typename Functor, typename Index>
  108|       |EIGEN_STRONG_INLINE void parallelize_gemm(const Functor& func, Index rows, Index cols, Index /*unused*/,
  109|       |                                          bool /*unused*/) {
  110|       |  func(0, rows, 0, cols);
  111|       |}
  112|       |
  113|       |#else
  114|       |
  115|       |template <typename Index>
  116|       |struct GemmParallelTaskInfo {
  117|       |  GemmParallelTaskInfo() : sync(-1), users(0), lhs_start(0), lhs_length(0) {}
  118|       |  std::atomic<Index> sync;
  119|       |  std::atomic<int> users;
  120|       |  Index lhs_start;
  121|       |  Index lhs_length;
  122|       |};
  123|       |
  124|       |template <typename Index>
  125|       |struct GemmParallelInfo {
  126|       |  const int logical_thread_id;
  127|       |  const int num_threads;
  128|       |  GemmParallelTaskInfo<Index>* task_info;
  129|       |
  130|       |  GemmParallelInfo(int logical_thread_id_, int num_threads_, GemmParallelTaskInfo<Index>* task_info_)
  131|       |      : logical_thread_id(logical_thread_id_), num_threads(num_threads_), task_info(task_info_) {}
  132|       |};
  133|       |
  134|       |inline void manage_multi_threading(Action action, int* v) {
  135|       |  static int m_maxThreads = -1;
  136|       |  if (action == SetAction) {
  137|       |    eigen_internal_assert(v != nullptr);
  138|       |#if defined(EIGEN_HAS_OPENMP)
  139|       |    // Calling action == SetAction and *v = 0 means
  140|       |    // restoring m_maxThreads to the maximum number of threads specified
  141|       |    // for OpenMP.
  142|       |    eigen_internal_assert(*v >= 0);
  143|       |    int omp_threads = omp_get_max_threads();
  144|       |    m_maxThreads = (*v == 0 ? omp_threads : std::min(*v, omp_threads));
  145|       |#elif defined(EIGEN_GEMM_THREADPOOL)
  146|       |    // Calling action == SetAction and *v = 0 means
  147|       |    // restoring m_maxThreads to the number of threads in the ThreadPool,
  148|       |    // which defaults to 1 if no pool was provided.
  149|       |    eigen_internal_assert(*v >= 0);
  150|       |    ThreadPool* pool = getGemmThreadPool();
  151|       |    int pool_threads = pool != nullptr ? pool->NumThreads() : 1;
  152|       |    m_maxThreads = (*v == 0 ? pool_threads : numext::mini(pool_threads, *v));
  153|       |#endif
  154|       |  } else if (action == GetAction) {
  155|       |    eigen_internal_assert(v != nullptr);
  156|       |#if defined(EIGEN_HAS_OPENMP)
  157|       |    if (m_maxThreads > 0)
  158|       |      *v = m_maxThreads;
  159|       |    else
  160|       |      *v = omp_get_max_threads();
  161|       |#else
  162|       |    *v = m_maxThreads;
  163|       |#endif
  164|       |  } else {
  165|       |    eigen_internal_assert(false);
  166|       |  }
  167|       |}
  168|       |
  169|       |template <bool Condition, typename Functor, typename Index>
  170|       |EIGEN_STRONG_INLINE void parallelize_gemm(const Functor& func, Index rows, Index cols, Index depth, bool transpose) {
  171|       |  // Dynamically check whether we should even try to execute in parallel.
  172|       |  // The conditions are:
  173|       |  // - the max number of threads we can create is greater than 1
  174|       |  // - we are not already in a parallel code
  175|       |  // - the sizes are large enough
  176|       |
  177|       |  // compute the maximal number of threads from the size of the product:
  178|       |  // This first heuristic takes into account that the product kernel is fully optimized when working with nr columns at
  179|       |  // once.
  180|       |  Index size = transpose ? rows : cols;
  181|       |  Index pb_max_threads = std::max<Index>(1, size / Functor::Traits::nr);
  182|       |
  183|       |  // compute the maximal number of threads from the total amount of work:
  184|       |  double work = static_cast<double>(rows) * static_cast<double>(cols) * static_cast<double>(depth);
  185|       |  double kMinTaskSize = 50000;  // FIXME improve this heuristic.
  186|       |  pb_max_threads = std::max<Index>(1, std::min<Index>(pb_max_threads, static_cast<Index>(work / kMinTaskSize)));
  187|       |
  188|       |  // compute the number of threads we are going to use
  189|       |  int threads = std::min<int>(nbThreads(), static_cast<int>(pb_max_threads));
  190|       |
  191|       |  // if multi-threading is explicitly disabled, not useful, or if we already are
  192|       |  // inside a parallel session, then abort multi-threading
  193|       |  bool dont_parallelize = (!Condition) || (threads <= 1);
  194|       |#if defined(EIGEN_HAS_OPENMP)
  195|       |  // don't parallelize if we are executing in a parallel context already.
  196|       |  dont_parallelize |= omp_get_num_threads() > 1;
  197|       |#elif defined(EIGEN_GEMM_THREADPOOL)
  198|       |  // don't parallelize if we have a trivial threadpool or the current thread id
  199|       |  // is != -1, indicating that we are already executing on a thread inside the pool.
  200|       |  // In other words, we do not allow nested parallelism, since this would lead to
  201|       |  // deadlocks due to the workstealing nature of the threadpool.
  202|       |  ThreadPool* pool = getGemmThreadPool();
  203|       |  dont_parallelize |= (pool == nullptr || pool->CurrentThreadId() != -1);
  204|       |#endif
  205|       |  if (dont_parallelize) return func(0, rows, 0, cols);
  206|       |
  207|       |  func.initParallelSession(threads);
  208|       |
  209|       |  if (transpose) std::swap(rows, cols);
  210|       |
  211|       |  ei_declare_aligned_stack_constructed_variable(GemmParallelTaskInfo<Index>, task_info, threads, 0);
  212|       |
  213|       |#if defined(EIGEN_HAS_OPENMP)
  214|       |#pragma omp parallel num_threads(threads)
  215|       |  {
  216|       |    Index i = omp_get_thread_num();
  217|       |    // Note that the actual number of threads might be lower than the number of
  218|       |    // requested ones
  219|       |    Index actual_threads = omp_get_num_threads();
  220|       |    GemmParallelInfo<Index> info(i, static_cast<int>(actual_threads), task_info);
  221|       |
  222|       |    Index blockCols = (cols / actual_threads) & ~Index(0x3);
  223|       |    Index blockRows = (rows / actual_threads);
  224|       |    blockRows = (blockRows / Functor::Traits::mr) * Functor::Traits::mr;
  225|       |
  226|       |    Index r0 = i * blockRows;
  227|       |    Index actualBlockRows = (i + 1 == actual_threads) ? rows - r0 : blockRows;
  228|       |
  229|       |    Index c0 = i * blockCols;
  230|       |    Index actualBlockCols = (i + 1 == actual_threads) ? cols - c0 : blockCols;
  231|       |
  232|       |    info.task_info[i].lhs_start = r0;
  233|       |    info.task_info[i].lhs_length = actualBlockRows;
  234|       |
  235|       |    if (transpose)
  236|       |      func(c0, actualBlockCols, 0, rows, &info);
  237|       |    else
  238|       |      func(0, rows, c0, actualBlockCols, &info);
  239|       |  }
  240|       |
  241|       |#elif defined(EIGEN_GEMM_THREADPOOL)
  242|       |  Barrier barrier(threads);
  243|       |  auto task = [=, &func, &barrier, &task_info](int i) {
  244|       |    Index actual_threads = threads;
  245|       |    GemmParallelInfo<Index> info(i, static_cast<int>(actual_threads), task_info);
  246|       |    Index blockCols = (cols / actual_threads) & ~Index(0x3);
  247|       |    Index blockRows = (rows / actual_threads);
  248|       |    blockRows = (blockRows / Functor::Traits::mr) * Functor::Traits::mr;
  249|       |
  250|       |    Index r0 = i * blockRows;
  251|       |    Index actualBlockRows = (i + 1 == actual_threads) ? rows - r0 : blockRows;
  252|       |
  253|       |    Index c0 = i * blockCols;
  254|       |    Index actualBlockCols = (i + 1 == actual_threads) ? cols - c0 : blockCols;
  255|       |
  256|       |    info.task_info[i].lhs_start = r0;
  257|       |    info.task_info[i].lhs_length = actualBlockRows;
  258|       |
  259|       |    if (transpose)
  260|       |      func(c0, actualBlockCols, 0, rows, &info);
  261|       |    else
  262|       |      func(0, rows, c0, actualBlockCols, &info);
  263|       |
  264|       |    barrier.Notify();
  265|       |  };
  266|       |  // Notice that we do not schedule more than "threads" tasks, which allows us to
  267|       |  // limit number of running threads, even if the threadpool itself was constructed
  268|       |  // with a larger number of threads.
  269|       |  for (int i = 0; i < threads - 1; ++i) {
  270|       |    pool->Schedule([=, task = std::move(task)] { task(i); });
  271|       |  }
  272|       |  task(threads - 1);
  273|       |  barrier.Wait();
  274|       |#endif
  275|       |}
  276|       |
  277|       |#endif
  278|       |
  279|       |}  // end namespace internal
  280|       |}  // end namespace Eigen
  281|       |
  282|       |#endif  // EIGEN_PARALLELIZER_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/Assert.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2022, The Eigen authors.
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_CORE_UTIL_ASSERT_H
   11|       |#define EIGEN_CORE_UTIL_ASSERT_H
   12|       |
   13|       |// Eigen custom assert function.
   14|       |//
   15|       |// The combination of Eigen's relative includes and cassert's `assert` function
   16|       |// (or any usage of the __FILE__ macro) can lead to ODR issues:
   17|       |// a header included using different relative paths in two different TUs will
   18|       |// have two different token-for-token definitions, since __FILE__ is expanded
   19|       |// as an in-line string with different values.  Normally this would be
   20|       |// harmless - the linker would just choose one definition. However, it breaks
   21|       |// with C++20 modules when functions in different modules have different
   22|       |// definitions.
   23|       |//
   24|       |// To get around this, we need to use __builtin_FILE() when available, which is
   25|       |// considered a single token, and thus satisfies the ODR.
   26|       |
   27|       |// Only define eigen_plain_assert if we are debugging, and either
   28|       |//  - we are not compiling for GPU, or
   29|       |//  - gpu debugging is enabled.
   30|       |#if !defined(EIGEN_NO_DEBUG) && (!defined(EIGEN_GPU_COMPILE_PHASE) || !defined(EIGEN_NO_DEBUG_GPU))
   31|       |
   32|       |#include <cassert>
   33|       |
   34|       |#ifndef EIGEN_USE_CUSTOM_PLAIN_ASSERT
   35|       |// Disable new custom asserts by default for now.
   36|       |#define EIGEN_USE_CUSTOM_PLAIN_ASSERT 0
   37|       |#endif
   38|       |
   39|       |#if EIGEN_USE_CUSTOM_PLAIN_ASSERT
   40|       |
   41|       |#ifndef EIGEN_HAS_BUILTIN_FILE
   42|       |// Clang can check if __builtin_FILE() is supported.
   43|       |// GCC > 5, MSVC 2019 14.26 (1926) all have __builtin_FILE().
   44|       |//
   45|       |// For NVCC, it's more complicated.  Through trial-and-error:
   46|       |//   - nvcc+gcc supports __builtin_FILE() on host, and on device after CUDA 11.
   47|       |//   - nvcc+msvc supports __builtin_FILE() only after CUDA 11.
   48|       |#if (EIGEN_HAS_BUILTIN(__builtin_FILE) && (EIGEN_COMP_CLANG || !defined(EIGEN_CUDA_ARCH))) ||            \
   49|       |    (EIGEN_GNUC_STRICT_AT_LEAST(5, 0, 0) && (EIGEN_COMP_NVCC >= 110000 || !defined(EIGEN_CUDA_ARCH))) || \
   50|       |    (EIGEN_COMP_MSVC >= 1926 && (!EIGEN_COMP_NVCC || EIGEN_COMP_NVCC >= 110000))
   51|       |#define EIGEN_HAS_BUILTIN_FILE 1
   52|       |#else
   53|       |#define EIGEN_HAS_BUILTIN_FILE 0
   54|       |#endif
   55|       |#endif  // EIGEN_HAS_BUILTIN_FILE
   56|       |
   57|       |#if EIGEN_HAS_BUILTIN_FILE
   58|       |#define EIGEN_BUILTIN_FILE __builtin_FILE()
   59|       |#define EIGEN_BUILTIN_LINE __builtin_LINE()
   60|       |#else
   61|       |// Default (potentially unsafe) values.
   62|       |#define EIGEN_BUILTIN_FILE __FILE__
   63|       |#define EIGEN_BUILTIN_LINE __LINE__
   64|       |#endif
   65|       |
   66|       |// Use __PRETTY_FUNCTION__ when available, since it is more descriptive, as
   67|       |// __builtin_FUNCTION() only returns the undecorated function name.
   68|       |// This should still be okay ODR-wise since it is a compiler-specific fixed
   69|       |// value.  Mixing compilers will likely lead to ODR violations anyways.
   70|       |#if EIGEN_COMP_MSVC
   71|       |#define EIGEN_BUILTIN_FUNCTION __FUNCSIG__
   72|       |#elif EIGEN_COMP_GNUC
   73|       |#define EIGEN_BUILTIN_FUNCTION __PRETTY_FUNCTION__
   74|       |#else
   75|       |#define EIGEN_BUILTIN_FUNCTION __func__
   76|       |#endif
   77|       |
   78|       |namespace Eigen {
   79|       |namespace internal {
   80|       |
   81|       |// Generic default assert handler.
   82|       |template <typename EnableIf = void, typename... EmptyArgs>
   83|       |struct assert_handler_impl {
   84|       |  EIGEN_DEVICE_FUNC EIGEN_DONT_INLINE static inline void run(const char* expression, const char* file, unsigned line,
   85|       |                                                             const char* function) {
   86|       |#ifdef EIGEN_GPU_COMPILE_PHASE
   87|       |    // GPU device code doesn't allow stderr or abort, so use printf and raise an
   88|       |    // illegal instruction exception to trigger a kernel failure.
   89|       |#ifndef EIGEN_NO_IO
   90|       |    printf("Assertion failed at %s:%u in %s: %s\n", file == nullptr ? "<file>" : file, line,
   91|       |           function == nullptr ? "<function>" : function, expression);
   92|       |#endif
   93|       |    __trap();
   94|       |
   95|       |#else  // EIGEN_GPU_COMPILE_PHASE
   96|       |
   97|       |    // Print to stderr and abort, as specified in <cassert>.
   98|       |#ifndef EIGEN_NO_IO
   99|       |    fprintf(stderr, "Assertion failed at %s:%u in %s: %s\n", file == nullptr ? "<file>" : file, line,
  100|       |            function == nullptr ? "<function>" : function, expression);
  101|       |#endif
  102|       |    std::abort();
  103|       |
  104|       |#endif  // EIGEN_GPU_COMPILE_PHASE
  105|       |  }
  106|       |};
  107|       |
  108|       |// Use POSIX __assert_fail handler when available.
  109|       |//
  110|       |// This allows us to integrate with systems that have custom handlers.
  111|       |//
  112|       |// NOTE: this handler is not always available on all POSIX systems (otherwise
  113|       |// we could simply test for __unix__ or similar).  The handler function name
  114|       |// seems to depend on the specific toolchain implementation, and differs between
  115|       |// compilers, platforms, OSes, etc.  Hence, we detect support via SFINAE.
  116|       |template <typename... EmptyArgs>
  117|       |struct assert_handler_impl<void_t<decltype(__assert_fail((const char*)nullptr,         // expression
  118|       |                                                         (const char*)nullptr,         // file
  119|       |                                                         0,                            // line
  120|       |                                                         (const char*)nullptr,         // function
  121|       |                                                         std::declval<EmptyArgs>()...  // Empty substitution required
  122|       |                                                                                       // for SFINAE.
  123|       |                                                         ))>,
  124|       |                           EmptyArgs...> {
  125|       |  EIGEN_DEVICE_FUNC EIGEN_DONT_INLINE static inline void run(const char* expression, const char* file, unsigned line,
  126|       |                                                             const char* function) {
  127|       |    // GCC requires this call to be dependent on the template parameters.
  128|       |    __assert_fail(expression, file, line, function, std::declval<EmptyArgs>()...);
  129|       |  }
  130|       |};
  131|       |
  132|       |EIGEN_DEVICE_FUNC EIGEN_DONT_INLINE inline void __assert_handler(const char* expression, const char* file,
  133|       |                                                                 unsigned line, const char* function) {
  134|       |  assert_handler_impl<>::run(expression, file, line, function);
  135|       |}
  136|       |
  137|       |}  // namespace internal
  138|       |}  // namespace Eigen
  139|       |
  140|       |#define eigen_plain_assert(expression)                                                                                \
  141|       |  (EIGEN_PREDICT_FALSE(!(expression)) ? Eigen::internal::__assert_handler(#expression, EIGEN_BUILTIN_FILE,            \
  142|       |                                                                          EIGEN_BUILTIN_LINE, EIGEN_BUILTIN_FUNCTION) \
  143|       |                                      : (void)0)
  144|       |
  145|       |#else  // EIGEN_USE_CUSTOM_PLAIN_ASSERT
  146|       |
  147|       |// Use regular assert.
  148|      0|#define eigen_plain_assert(condition) assert(condition)
  149|       |
  150|       |#endif  // EIGEN_USE_CUSTOM_PLAIN_ASSERT
  151|       |
  152|       |#else  // EIGEN_NO_DEBUG
  153|       |
  154|       |#define eigen_plain_assert(condition) ((void)0)
  155|       |
  156|       |#endif  // EIGEN_NO_DEBUG
  157|       |
  158|       |#endif  // EIGEN_CORE_UTIL_ASSERT_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/ConfigureVectorization.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2018 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |// Copyright (C) 2020, Arm Limited and Contributors
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_CONFIGURE_VECTORIZATION_H
   12|       |#define EIGEN_CONFIGURE_VECTORIZATION_H
   13|       |
   14|       |//------------------------------------------------------------------------------------------
   15|       |// Static and dynamic alignment control
   16|       |//
   17|       |// The main purpose of this section is to define EIGEN_MAX_ALIGN_BYTES and EIGEN_MAX_STATIC_ALIGN_BYTES
   18|       |// as the maximal boundary in bytes on which dynamically and statically allocated data may be alignment respectively.
   19|       |// The values of EIGEN_MAX_ALIGN_BYTES and EIGEN_MAX_STATIC_ALIGN_BYTES can be specified by the user. If not,
   20|       |// a default value is automatically computed based on architecture, compiler, and OS.
   21|       |//
   22|       |// This section also defines macros EIGEN_ALIGN_TO_BOUNDARY(N) and the shortcuts EIGEN_ALIGN{8,16,32,_MAX}
   23|       |// to be used to declare statically aligned buffers.
   24|       |//------------------------------------------------------------------------------------------
   25|       |
   26|       |/* EIGEN_ALIGN_TO_BOUNDARY(n) forces data to be n-byte aligned. This is used to satisfy SIMD requirements.
   27|       | * However, we do that EVEN if vectorization (EIGEN_VECTORIZE) is disabled,
   28|       | * so that vectorization doesn't affect binary compatibility.
   29|       | *
   30|       | * If we made alignment depend on whether or not EIGEN_VECTORIZE is defined, it would be impossible to link
   31|       | * vectorized and non-vectorized code.
   32|       | */
   33|       |#if (defined EIGEN_CUDACC)
   34|       |#define EIGEN_ALIGN_TO_BOUNDARY(n) __align__(n)
   35|       |#define EIGEN_ALIGNOF(x) __alignof(x)
   36|       |#else
   37|       |#define EIGEN_ALIGN_TO_BOUNDARY(n) alignas(n)
   38|       |#define EIGEN_ALIGNOF(x) alignof(x)
   39|       |#endif
   40|       |
   41|       |// Align to the boundary that avoids false sharing.
   42|       |// https://en.cppreference.com/w/cpp/thread/hardware_destructive_interference_size
   43|       |#ifdef __cpp_lib_hardware_interference_size
   44|       |#include <new>
   45|       |#define EIGEN_ALIGN_TO_AVOID_FALSE_SHARING EIGEN_ALIGN_TO_BOUNDARY(std::hardware_destructive_interference_size)
   46|       |#else
   47|       |// Overalign for the cache line size of 128 bytes (Apple M1)
   48|       |#define EIGEN_ALIGN_TO_AVOID_FALSE_SHARING EIGEN_ALIGN_TO_BOUNDARY(128)
   49|       |#endif
   50|       |
   51|       |// If the user explicitly disable vectorization, then we also disable alignment
   52|       |#if defined(EIGEN_DONT_VECTORIZE)
   53|       |#if defined(EIGEN_GPUCC)
   54|       |// GPU code is always vectorized and requires memory alignment for
   55|       |// statically allocated buffers.
   56|       |#define EIGEN_IDEAL_MAX_ALIGN_BYTES 16
   57|       |#else
   58|       |#define EIGEN_IDEAL_MAX_ALIGN_BYTES 0
   59|       |#endif
   60|       |#elif defined(__AVX512F__)
   61|       |// 64 bytes static alignment is preferred only if really required
   62|       |#define EIGEN_IDEAL_MAX_ALIGN_BYTES 64
   63|       |#elif defined(__AVX__)
   64|       |// 32 bytes static alignment is preferred only if really required
   65|       |#define EIGEN_IDEAL_MAX_ALIGN_BYTES 32
   66|       |#elif defined __HVX__ && (__HVX_LENGTH__ == 128)
   67|       |#define EIGEN_IDEAL_MAX_ALIGN_BYTES 128
   68|       |#else
   69|       |#define EIGEN_IDEAL_MAX_ALIGN_BYTES 16
   70|       |#endif
   71|       |
   72|       |// EIGEN_MIN_ALIGN_BYTES defines the minimal value for which the notion of explicit alignment makes sense
   73|       |#define EIGEN_MIN_ALIGN_BYTES 16
   74|       |
   75|       |// Defined the boundary (in bytes) on which the data needs to be aligned. Note
   76|       |// that unless EIGEN_ALIGN is defined and not equal to 0, the data may not be
   77|       |// aligned at all regardless of the value of this #define.
   78|       |
   79|       |#if (defined(EIGEN_DONT_ALIGN_STATICALLY) || defined(EIGEN_DONT_ALIGN)) && defined(EIGEN_MAX_STATIC_ALIGN_BYTES) && \
   80|       |    EIGEN_MAX_STATIC_ALIGN_BYTES > 0
   81|       |#error EIGEN_MAX_STATIC_ALIGN_BYTES and EIGEN_DONT_ALIGN[_STATICALLY] are both defined with EIGEN_MAX_STATIC_ALIGN_BYTES!=0. Use EIGEN_MAX_STATIC_ALIGN_BYTES=0 as a synonym of EIGEN_DONT_ALIGN_STATICALLY.
   82|       |#endif
   83|       |
   84|       |// EIGEN_DONT_ALIGN_STATICALLY and EIGEN_DONT_ALIGN are deprecated
   85|       |// They imply EIGEN_MAX_STATIC_ALIGN_BYTES=0
   86|       |#if defined(EIGEN_DONT_ALIGN_STATICALLY) || defined(EIGEN_DONT_ALIGN)
   87|       |#ifdef EIGEN_MAX_STATIC_ALIGN_BYTES
   88|       |#undef EIGEN_MAX_STATIC_ALIGN_BYTES
   89|       |#endif
   90|       |#define EIGEN_MAX_STATIC_ALIGN_BYTES 0
   91|       |#endif
   92|       |
   93|       |#ifndef EIGEN_MAX_STATIC_ALIGN_BYTES
   94|       |
   95|       |// Try to automatically guess what is the best default value for EIGEN_MAX_STATIC_ALIGN_BYTES
   96|       |
   97|       |// 16 byte alignment is only useful for vectorization. Since it affects the ABI, we need to enable
   98|       |// 16 byte alignment on all platforms where vectorization might be enabled. In theory we could always
   99|       |// enable alignment, but it can be a cause of problems on some platforms, so we just disable it in
  100|       |// certain common platform (compiler+architecture combinations) to avoid these problems.
  101|       |// Only static alignment is really problematic (relies on nonstandard compiler extensions),
  102|       |// try to keep heap alignment even when we have to disable static alignment.
  103|       |#if EIGEN_COMP_GNUC && !(EIGEN_ARCH_i386_OR_x86_64 || EIGEN_ARCH_ARM_OR_ARM64 || EIGEN_ARCH_PPC || EIGEN_ARCH_IA64 || \
  104|       |                         EIGEN_ARCH_MIPS || EIGEN_ARCH_LOONGARCH64)
  105|       |#define EIGEN_GCC_AND_ARCH_DOESNT_WANT_STACK_ALIGNMENT 1
  106|       |#else
  107|       |#define EIGEN_GCC_AND_ARCH_DOESNT_WANT_STACK_ALIGNMENT 0
  108|       |#endif
  109|       |
  110|       |// static alignment is completely disabled with GCC 3, Sun Studio, and QCC/QNX
  111|       |#if !EIGEN_GCC_AND_ARCH_DOESNT_WANT_STACK_ALIGNMENT && !EIGEN_COMP_SUNCC && !EIGEN_OS_QNX
  112|       |#define EIGEN_ARCH_WANTS_STACK_ALIGNMENT 1
  113|       |#else
  114|       |#define EIGEN_ARCH_WANTS_STACK_ALIGNMENT 0
  115|       |#endif
  116|       |
  117|       |#if EIGEN_ARCH_WANTS_STACK_ALIGNMENT
  118|       |#define EIGEN_MAX_STATIC_ALIGN_BYTES EIGEN_IDEAL_MAX_ALIGN_BYTES
  119|       |#else
  120|       |#define EIGEN_MAX_STATIC_ALIGN_BYTES 0
  121|       |#endif
  122|       |
  123|       |#endif
  124|       |
  125|       |// If EIGEN_MAX_ALIGN_BYTES is defined, then it is considered as an upper bound for EIGEN_MAX_STATIC_ALIGN_BYTES
  126|       |#if defined(EIGEN_MAX_ALIGN_BYTES) && EIGEN_MAX_ALIGN_BYTES < EIGEN_MAX_STATIC_ALIGN_BYTES
  127|       |#undef EIGEN_MAX_STATIC_ALIGN_BYTES
  128|       |#define EIGEN_MAX_STATIC_ALIGN_BYTES EIGEN_MAX_ALIGN_BYTES
  129|       |#endif
  130|       |
  131|       |#if EIGEN_MAX_STATIC_ALIGN_BYTES == 0 && !defined(EIGEN_DISABLE_UNALIGNED_ARRAY_ASSERT)
  132|       |#define EIGEN_DISABLE_UNALIGNED_ARRAY_ASSERT
  133|       |#endif
  134|       |
  135|       |// At this stage, EIGEN_MAX_STATIC_ALIGN_BYTES>0 is the true test whether we want to align arrays on the stack or not.
  136|       |// It takes into account both the user choice to explicitly enable/disable alignment (by setting
  137|       |// EIGEN_MAX_STATIC_ALIGN_BYTES) and the architecture config (EIGEN_ARCH_WANTS_STACK_ALIGNMENT). Henceforth, only
  138|       |// EIGEN_MAX_STATIC_ALIGN_BYTES should be used.
  139|       |
  140|       |// Shortcuts to EIGEN_ALIGN_TO_BOUNDARY
  141|       |#define EIGEN_ALIGN8 EIGEN_ALIGN_TO_BOUNDARY(8)
  142|       |#define EIGEN_ALIGN16 EIGEN_ALIGN_TO_BOUNDARY(16)
  143|       |#define EIGEN_ALIGN32 EIGEN_ALIGN_TO_BOUNDARY(32)
  144|       |#define EIGEN_ALIGN64 EIGEN_ALIGN_TO_BOUNDARY(64)
  145|       |#if EIGEN_MAX_STATIC_ALIGN_BYTES > 0
  146|       |#define EIGEN_ALIGN_MAX EIGEN_ALIGN_TO_BOUNDARY(EIGEN_MAX_STATIC_ALIGN_BYTES)
  147|       |#else
  148|       |#define EIGEN_ALIGN_MAX
  149|       |#endif
  150|       |
  151|       |// Dynamic alignment control
  152|       |
  153|       |#if defined(EIGEN_DONT_ALIGN) && defined(EIGEN_MAX_ALIGN_BYTES) && EIGEN_MAX_ALIGN_BYTES > 0
  154|       |#error EIGEN_MAX_ALIGN_BYTES and EIGEN_DONT_ALIGN are both defined with EIGEN_MAX_ALIGN_BYTES!=0. Use EIGEN_MAX_ALIGN_BYTES=0 as a synonym of EIGEN_DONT_ALIGN.
  155|       |#endif
  156|       |
  157|       |#ifdef EIGEN_DONT_ALIGN
  158|       |#ifdef EIGEN_MAX_ALIGN_BYTES
  159|       |#undef EIGEN_MAX_ALIGN_BYTES
  160|       |#endif
  161|       |#define EIGEN_MAX_ALIGN_BYTES 0
  162|       |#elif !defined(EIGEN_MAX_ALIGN_BYTES)
  163|       |#define EIGEN_MAX_ALIGN_BYTES EIGEN_IDEAL_MAX_ALIGN_BYTES
  164|       |#endif
  165|       |
  166|       |#if EIGEN_IDEAL_MAX_ALIGN_BYTES > EIGEN_MAX_ALIGN_BYTES
  167|       |#define EIGEN_DEFAULT_ALIGN_BYTES EIGEN_IDEAL_MAX_ALIGN_BYTES
  168|       |#else
  169|       |#define EIGEN_DEFAULT_ALIGN_BYTES EIGEN_MAX_ALIGN_BYTES
  170|       |#endif
  171|       |
  172|       |#ifndef EIGEN_UNALIGNED_VECTORIZE
  173|       |#define EIGEN_UNALIGNED_VECTORIZE 1
  174|       |#endif
  175|       |
  176|       |//----------------------------------------------------------------------
  177|       |
  178|       |// if alignment is disabled, then disable vectorization. Note: EIGEN_MAX_ALIGN_BYTES is the proper check, it takes into
  179|       |// account both the user's will (EIGEN_MAX_ALIGN_BYTES,EIGEN_DONT_ALIGN) and our own platform checks
  180|       |#if EIGEN_MAX_ALIGN_BYTES == 0
  181|       |#ifndef EIGEN_DONT_VECTORIZE
  182|       |#define EIGEN_DONT_VECTORIZE
  183|       |#endif
  184|       |#endif
  185|       |
  186|       |// The following (except #include <malloc.h> and _M_IX86_FP ??) can likely be
  187|       |// removed as gcc 4.1 and msvc 2008 are not supported anyways.
  188|       |#if EIGEN_COMP_MSVC
  189|       |#include <malloc.h>  // for _aligned_malloc -- need it regardless of whether vectorization is enabled
  190|       |// a user reported that in 64-bit mode, MSVC doesn't care to define _M_IX86_FP.
  191|       |#if (defined(_M_IX86_FP) && (_M_IX86_FP >= 2)) || EIGEN_ARCH_x86_64
  192|       |#define EIGEN_SSE2_ON_MSVC_2008_OR_LATER
  193|       |#endif
  194|       |#else
  195|       |#if defined(__SSE2__)
  196|       |#define EIGEN_SSE2_ON_NON_MSVC
  197|       |#endif
  198|       |#endif
  199|       |
  200|       |#if !(defined(EIGEN_DONT_VECTORIZE) || defined(EIGEN_GPUCC))
  201|       |
  202|       |#if defined(EIGEN_SSE2_ON_NON_MSVC) || defined(EIGEN_SSE2_ON_MSVC_2008_OR_LATER)
  203|       |
  204|       |// Defines symbols for compile-time detection of which instructions are
  205|       |// used.
  206|       |// EIGEN_VECTORIZE_YY is defined if and only if the instruction set YY is used
  207|       |#define EIGEN_VECTORIZE
  208|       |#define EIGEN_VECTORIZE_SSE
  209|       |#define EIGEN_VECTORIZE_SSE2
  210|       |
  211|       |// Detect sse3/ssse3/sse4:
  212|       |// gcc and icc defines __SSE3__, ...
  213|       |// there is no way to know about this on msvc. You can define EIGEN_VECTORIZE_SSE* if you
  214|       |// want to force the use of those instructions with msvc.
  215|       |#ifdef __SSE3__
  216|       |#define EIGEN_VECTORIZE_SSE3
  217|       |#endif
  218|       |#ifdef __SSSE3__
  219|       |#define EIGEN_VECTORIZE_SSSE3
  220|       |#endif
  221|       |#ifdef __SSE4_1__
  222|       |#define EIGEN_VECTORIZE_SSE4_1
  223|       |#endif
  224|       |#ifdef __SSE4_2__
  225|       |#define EIGEN_VECTORIZE_SSE4_2
  226|       |#endif
  227|       |#ifdef __AVX__
  228|       |#ifndef EIGEN_USE_SYCL
  229|       |#define EIGEN_VECTORIZE_AVX
  230|       |#endif
  231|       |#define EIGEN_VECTORIZE_SSE3
  232|       |#define EIGEN_VECTORIZE_SSSE3
  233|       |#define EIGEN_VECTORIZE_SSE4_1
  234|       |#define EIGEN_VECTORIZE_SSE4_2
  235|       |#endif
  236|       |#ifdef __AVX2__
  237|       |#ifndef EIGEN_USE_SYCL
  238|       |#define EIGEN_VECTORIZE_AVX2
  239|       |#define EIGEN_VECTORIZE_AVX
  240|       |#endif
  241|       |#define EIGEN_VECTORIZE_SSE3
  242|       |#define EIGEN_VECTORIZE_SSSE3
  243|       |#define EIGEN_VECTORIZE_SSE4_1
  244|       |#define EIGEN_VECTORIZE_SSE4_2
  245|       |#endif
  246|       |#if defined(__FMA__) || (EIGEN_COMP_MSVC && defined(__AVX2__))
  247|       |// MSVC does not expose a switch dedicated for FMA
  248|       |// For MSVC, AVX2 => FMA
  249|       |#define EIGEN_VECTORIZE_FMA
  250|       |#endif
  251|       |#if defined(__AVX512F__)
  252|       |#ifndef EIGEN_VECTORIZE_FMA
  253|       |#if EIGEN_COMP_GNUC
  254|       |#error Please add -mfma to your compiler flags: compiling with -mavx512f alone without SSE/AVX FMA is not supported (bug 1638).
  255|       |#else
  256|       |#error Please enable FMA in your compiler flags (e.g. -mfma): compiling with AVX512 alone without SSE/AVX FMA is not supported (bug 1638).
  257|       |#endif
  258|       |#endif
  259|       |#ifndef EIGEN_USE_SYCL
  260|       |#define EIGEN_VECTORIZE_AVX512
  261|       |#define EIGEN_VECTORIZE_AVX2
  262|       |#define EIGEN_VECTORIZE_AVX
  263|       |#endif
  264|       |#define EIGEN_VECTORIZE_FMA
  265|       |#define EIGEN_VECTORIZE_SSE3
  266|       |#define EIGEN_VECTORIZE_SSSE3
  267|       |#define EIGEN_VECTORIZE_SSE4_1
  268|       |#define EIGEN_VECTORIZE_SSE4_2
  269|       |#ifndef EIGEN_USE_SYCL
  270|       |#ifdef __AVX512DQ__
  271|       |#define EIGEN_VECTORIZE_AVX512DQ
  272|       |#endif
  273|       |#ifdef __AVX512ER__
  274|       |#define EIGEN_VECTORIZE_AVX512ER
  275|       |#endif
  276|       |#ifdef __AVX512BF16__
  277|       |#define EIGEN_VECTORIZE_AVX512BF16
  278|       |#endif
  279|       |#ifdef __AVX512VL__
  280|       |#define EIGEN_VECTORIZE_AVX512VL
  281|       |#endif
  282|       |#ifdef __AVX512FP16__
  283|       |#ifdef __AVX512VL__
  284|       |#define EIGEN_VECTORIZE_AVX512FP16
  285|       |#else
  286|       |#if EIGEN_COMP_GNUC
  287|       |#error Please add -mavx512vl to your compiler flags: compiling with -mavx512fp16 alone without AVX512-VL is not supported.
  288|       |#else
  289|       |#error Please enable AVX512-VL in your compiler flags (e.g. -mavx512vl): compiling with AVX512-FP16 alone without AVX512-VL is not supported.
  290|       |#endif
  291|       |#endif
  292|       |#endif
  293|       |#endif
  294|       |#endif
  295|       |
  296|       |// Disable AVX support on broken xcode versions
  297|       |#if (EIGEN_COMP_CLANGAPPLE == 11000033) && (__MAC_OS_X_VERSION_MIN_REQUIRED == 101500)
  298|       |// A nasty bug in the clang compiler shipped with xcode in a common compilation situation
  299|       |// when XCode 11.0 and Mac deployment target macOS 10.15 is https://trac.macports.org/ticket/58776#no1
  300|       |#ifdef EIGEN_VECTORIZE_AVX
  301|       |#undef EIGEN_VECTORIZE_AVX
  302|       |#warning \
  303|       |    "Disabling AVX support: clang compiler shipped with XCode 11.[012] generates broken assembly with -macosx-version-min=10.15 and AVX enabled. "
  304|       |#ifdef EIGEN_VECTORIZE_AVX2
  305|       |#undef EIGEN_VECTORIZE_AVX2
  306|       |#endif
  307|       |#ifdef EIGEN_VECTORIZE_FMA
  308|       |#undef EIGEN_VECTORIZE_FMA
  309|       |#endif
  310|       |#ifdef EIGEN_VECTORIZE_AVX512
  311|       |#undef EIGEN_VECTORIZE_AVX512
  312|       |#endif
  313|       |#ifdef EIGEN_VECTORIZE_AVX512DQ
  314|       |#undef EIGEN_VECTORIZE_AVX512DQ
  315|       |#endif
  316|       |#ifdef EIGEN_VECTORIZE_AVX512ER
  317|       |#undef EIGEN_VECTORIZE_AVX512ER
  318|       |#endif
  319|       |#endif
  320|       |// NOTE: Confirmed test failures in XCode 11.0, and XCode 11.2 with  -macosx-version-min=10.15 and AVX
  321|       |// NOTE using -macosx-version-min=10.15 with Xcode 11.0 results in runtime segmentation faults in many tests, 11.2
  322|       |// produce core dumps in 3 tests NOTE using -macosx-version-min=10.14 produces functioning and passing tests in all
  323|       |// cases NOTE __clang_version__ "11.0.0 (clang-1100.0.33.8)"  XCode 11.0 <- Produces many segfault and core dumping
  324|       |// tests
  325|       |//                                                                    with  -macosx-version-min=10.15 and AVX
  326|       |// NOTE __clang_version__ "11.0.0 (clang-1100.0.33.12)" XCode 11.2 <- Produces 3 core dumping tests with
  327|       |//                                                                    -macosx-version-min=10.15 and AVX
  328|       |#endif
  329|       |
  330|       |// include files
  331|       |
  332|       |// This extern "C" works around a MINGW-w64 compilation issue
  333|       |// https://sourceforge.net/tracker/index.php?func=detail&aid=3018394&group_id=202880&atid=983354
  334|       |// In essence, intrin.h is included by windows.h and also declares intrinsics (just as emmintrin.h etc. below do).
  335|       |// However, intrin.h uses an extern "C" declaration, and g++ thus complains of duplicate declarations
  336|       |// with conflicting linkage.  The linkage for intrinsics doesn't matter, but at that stage the compiler doesn't know;
  337|       |// so, to avoid compile errors when windows.h is included after Eigen/Core, ensure intrinsics are extern "C" here too.
  338|       |// notice that since these are C headers, the extern "C" is theoretically needed anyways.
  339|       |extern "C" {
  340|       |// In theory we should only include immintrin.h and not the other *mmintrin.h header files directly.
  341|       |// Doing so triggers some issues with ICC. However old gcc versions seems to not have this file, thus:
  342|       |#if EIGEN_COMP_ICC >= 1110 || EIGEN_COMP_EMSCRIPTEN
  343|       |#include <immintrin.h>
  344|       |#else
  345|       |#include <mmintrin.h>
  346|       |#include <emmintrin.h>
  347|       |#include <xmmintrin.h>
  348|       |#ifdef EIGEN_VECTORIZE_SSE3
  349|       |#include <pmmintrin.h>
  350|       |#endif
  351|       |#ifdef EIGEN_VECTORIZE_SSSE3
  352|       |#include <tmmintrin.h>
  353|       |#endif
  354|       |#ifdef EIGEN_VECTORIZE_SSE4_1
  355|       |#include <smmintrin.h>
  356|       |#endif
  357|       |#ifdef EIGEN_VECTORIZE_SSE4_2
  358|       |#include <nmmintrin.h>
  359|       |#endif
  360|       |#if defined(EIGEN_VECTORIZE_AVX) || defined(EIGEN_VECTORIZE_AVX512)
  361|       |#include <immintrin.h>
  362|       |#endif
  363|       |#endif
  364|       |}  // end extern "C"
  365|       |
  366|       |#elif defined(__VSX__) && !defined(__APPLE__)
  367|       |
  368|       |#define EIGEN_VECTORIZE
  369|       |#define EIGEN_VECTORIZE_VSX 1
  370|       |#define EIGEN_VECTORIZE_FMA
  371|       |#include <altivec.h>
  372|       |// We need to #undef all these ugly tokens defined in <altivec.h>
  373|       |// => use __vector instead of vector
  374|       |#undef bool
  375|       |#undef vector
  376|       |#undef pixel
  377|       |
  378|       |#elif defined __ALTIVEC__
  379|       |
  380|       |#define EIGEN_VECTORIZE
  381|       |#define EIGEN_VECTORIZE_ALTIVEC
  382|       |#define EIGEN_VECTORIZE_FMA
  383|       |#include <altivec.h>
  384|       |// We need to #undef all these ugly tokens defined in <altivec.h>
  385|       |// => use __vector instead of vector
  386|       |#undef bool
  387|       |#undef vector
  388|       |#undef pixel
  389|       |
  390|       |#elif ((defined __ARM_NEON) || (defined __ARM_NEON__)) && !(defined EIGEN_ARM64_USE_SVE)
  391|       |
  392|       |#define EIGEN_VECTORIZE
  393|       |#define EIGEN_VECTORIZE_NEON
  394|       |#include <arm_neon.h>
  395|       |
  396|       |// We currently require SVE to be enabled explicitly via EIGEN_ARM64_USE_SVE and
  397|       |// will not select the backend automatically
  398|       |#elif (defined __ARM_FEATURE_SVE) && (defined EIGEN_ARM64_USE_SVE)
  399|       |
  400|       |#define EIGEN_VECTORIZE
  401|       |#define EIGEN_VECTORIZE_SVE
  402|       |#include <arm_sve.h>
  403|       |
  404|       |// Since we depend on knowing SVE vector lengths at compile-time, we need
  405|       |// to ensure a fixed lengths is set
  406|       |#if defined __ARM_FEATURE_SVE_BITS
  407|       |#define EIGEN_ARM64_SVE_VL __ARM_FEATURE_SVE_BITS
  408|       |#else
  409|       |#error "Eigen requires a fixed SVE lector length but EIGEN_ARM64_SVE_VL is not set."
  410|       |#endif
  411|       |
  412|       |#elif (defined __s390x__ && defined __VEC__)
  413|       |
  414|       |#define EIGEN_VECTORIZE
  415|       |#define EIGEN_VECTORIZE_ZVECTOR
  416|       |#include <vecintrin.h>
  417|       |
  418|       |#elif defined __mips_msa
  419|       |
  420|       |// Limit MSA optimizations to little-endian CPUs for now.
  421|       |// TODO: Perhaps, eventually support MSA optimizations on big-endian CPUs?
  422|       |#if defined(__BYTE_ORDER__) && (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__)
  423|       |#if defined(__LP64__)
  424|       |#define EIGEN_MIPS_64
  425|       |#else
  426|       |#define EIGEN_MIPS_32
  427|       |#endif
  428|       |#define EIGEN_VECTORIZE
  429|       |#define EIGEN_VECTORIZE_MSA
  430|       |#include <msa.h>
  431|       |#endif
  432|       |
  433|       |#elif (defined __loongarch64 && defined __loongarch_sx)
  434|       |
  435|       |#define EIGEN_VECTORIZE
  436|       |#define EIGEN_VECTORIZE_LSX
  437|       |#include <lsxintrin.h>
  438|       |
  439|       |#elif defined __HVX__ && (__HVX_LENGTH__ == 128)
  440|       |
  441|       |#define EIGEN_VECTORIZE
  442|       |#define EIGEN_VECTORIZE_HVX
  443|       |#include <hexagon_types.h>
  444|       |
  445|       |#endif
  446|       |#endif
  447|       |
  448|       |// Following the Arm ACLE arm_neon.h should also include arm_fp16.h but not all
  449|       |// compilers seem to follow this. We therefore include it explicitly.
  450|       |// See also: https://bugs.llvm.org/show_bug.cgi?id=47955
  451|       |#if defined(EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC)
  452|       |#include <arm_fp16.h>
  453|       |#endif
  454|       |
  455|       |// Enable FMA for ARM.
  456|       |#if defined(__ARM_FEATURE_FMA)
  457|       |#define EIGEN_VECTORIZE_FMA
  458|       |#endif
  459|       |
  460|       |#if defined(__F16C__) && !defined(EIGEN_GPUCC) && (!EIGEN_COMP_CLANG_STRICT || EIGEN_CLANG_STRICT_AT_LEAST(3, 8, 0))
  461|       |// We can use the optimized fp16 to float and float to fp16 conversion routines
  462|       |#define EIGEN_HAS_FP16_C
  463|       |
  464|       |#if EIGEN_COMP_GNUC
  465|       |// Make sure immintrin.h is included, even if e.g. vectorization is
  466|       |// explicitly disabled (see also issue #2395).
  467|       |// Note that FP16C intrinsics for gcc and clang are included by immintrin.h,
  468|       |// as opposed to emmintrin.h as suggested by Intel:
  469|       |// https://software.intel.com/sites/landingpage/IntrinsicsGuide/#othertechs=FP16C&expand=1711
  470|       |#include <immintrin.h>
  471|       |#endif
  472|       |#endif
  473|       |
  474|       |#if defined EIGEN_CUDACC
  475|       |#define EIGEN_VECTORIZE_GPU
  476|       |#include <vector_types.h>
  477|       |#if EIGEN_CUDA_SDK_VER >= 70500
  478|       |#define EIGEN_HAS_CUDA_FP16
  479|       |#endif
  480|       |#endif
  481|       |
  482|       |#if defined(EIGEN_HAS_CUDA_FP16)
  483|       |#include <cuda_runtime_api.h>
  484|       |#include <cuda_fp16.h>
  485|       |#endif
  486|       |
  487|       |#if defined(EIGEN_HIPCC)
  488|       |#define EIGEN_VECTORIZE_GPU
  489|       |#include <hip/hip_vector_types.h>
  490|       |#define EIGEN_HAS_HIP_FP16
  491|       |#include <hip/hip_fp16.h>
  492|       |#define EIGEN_HAS_HIP_BF16
  493|       |#include <hip/hip_bfloat16.h>
  494|       |#endif
  495|       |
  496|       |/** \brief Namespace containing all symbols from the %Eigen library. */
  497|       |// IWYU pragma: private
  498|       |#include "../InternalHeaderCheck.h"
  499|       |
  500|       |namespace Eigen {
  501|       |
  502|      0|inline static const char *SimdInstructionSetsInUse(void) {
  503|      0|#if defined(EIGEN_VECTORIZE_AVX512)
  504|      0|  return "AVX512, FMA, AVX2, AVX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2";
  505|      0|#elif defined(EIGEN_VECTORIZE_AVX)
  506|      0|  return "AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2";
  507|      0|#elif defined(EIGEN_VECTORIZE_SSE4_2)
  508|      0|  return "SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2";
  509|      0|#elif defined(EIGEN_VECTORIZE_SSE4_1)
  510|      0|  return "SSE, SSE2, SSE3, SSSE3, SSE4.1";
  511|      0|#elif defined(EIGEN_VECTORIZE_SSSE3)
  512|      0|  return "SSE, SSE2, SSE3, SSSE3";
  513|      0|#elif defined(EIGEN_VECTORIZE_SSE3)
  514|      0|  return "SSE, SSE2, SSE3";
  515|      0|#elif defined(EIGEN_VECTORIZE_SSE2)
  516|      0|  return "SSE, SSE2";
  517|      0|#elif defined(EIGEN_VECTORIZE_ALTIVEC)
  518|      0|  return "AltiVec";
  519|      0|#elif defined(EIGEN_VECTORIZE_VSX)
  520|      0|  return "VSX";
  521|      0|#elif defined(EIGEN_VECTORIZE_NEON)
  522|      0|  return "ARM NEON";
  523|      0|#elif defined(EIGEN_VECTORIZE_SVE)
  524|      0|  return "ARM SVE";
  525|      0|#elif defined(EIGEN_VECTORIZE_ZVECTOR)
  526|      0|  return "S390X ZVECTOR";
  527|      0|#elif defined(EIGEN_VECTORIZE_MSA)
  528|      0|  return "MIPS MSA";
  529|      0|#elif defined(EIGEN_VECTORIZE_LSX)
  530|      0|  return "LOONGARCH64 LSX";
  531|      0|#else
  532|      0|  return "None";
  533|      0|#endif
  534|      0|}
  535|       |
  536|       |}  // end namespace Eigen
  537|       |
  538|       |#endif  // EIGEN_CONFIGURE_VECTORIZATION_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/Constants.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2015 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |// Copyright (C) 2007-2009 Benoit Jacob <jacob.benoit.1@gmail.com>
    6|       |// Copyright (C) 2020, Arm Limited and Contributors
    7|       |//
    8|       |// This Source Code Form is subject to the terms of the Mozilla
    9|       |// Public License v. 2.0. If a copy of the MPL was not distributed
   10|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   11|       |
   12|       |#ifndef EIGEN_CONSTANTS_H
   13|       |#define EIGEN_CONSTANTS_H
   14|       |
   15|       |// IWYU pragma: private
   16|       |#include "../InternalHeaderCheck.h"
   17|       |
   18|       |namespace Eigen {
   19|       |
   20|       |/** This value means that a positive quantity (e.g., a size) is not known at compile-time, and that instead the value is
   21|       | * stored in some runtime variable.
   22|       | *
   23|       | * Changing the value of Dynamic breaks the ABI, as Dynamic is often used as a template parameter for Matrix.
   24|       | */
   25|       |const int Dynamic = -1;
   26|       |
   27|       |/** This value means that a signed quantity (e.g., a signed index) is not known at compile-time, and that instead its
   28|       | * value has to be specified at runtime.
   29|       | */
   30|       |const int DynamicIndex = 0xffffff;
   31|       |
   32|       |/** This value means that the requested value is not defined.
   33|       | */
   34|       |const int Undefined = 0xfffffe;
   35|       |
   36|       |/** This value means +Infinity; it is currently used only as the p parameter to MatrixBase::lpNorm<int>().
   37|       | * The value Infinity there means the L-infinity norm.
   38|       | */
   39|       |const int Infinity = -1;
   40|       |
   41|       |/** This value means that the cost to evaluate an expression coefficient is either very expensive or
   42|       | * cannot be known at compile time.
   43|       | *
   44|       | * This value has to be positive to (1) simplify cost computation, and (2) allow to distinguish between a very expensive
   45|       | * and very very expensive expressions. It thus must also be large enough to make sure unrolling won't happen and that
   46|       | * sub expressions will be evaluated, but not too large to avoid overflow.
   47|       | */
   48|       |const int HugeCost = 10000;
   49|       |
   50|       |/** \defgroup flags Flags
   51|       | * \ingroup Core_Module
   52|       | *
   53|       | * These are the possible bits which can be OR'ed to constitute the flags of a matrix or
   54|       | * expression.
   55|       | *
   56|       | * It is important to note that these flags are a purely compile-time notion. They are a compile-time property of
   57|       | * an expression type, implemented as enum's. They are not stored in memory at runtime, and they do not incur any
   58|       | * runtime overhead.
   59|       | *
   60|       | * \sa MatrixBase::Flags
   61|       | */
   62|       |
   63|       |/** \ingroup flags
   64|       | *
   65|       | * for a matrix, this means that the storage order is row-major.
   66|       | * If this bit is not set, the storage order is column-major.
   67|       | * For an expression, this determines the storage order of
   68|       | * the matrix created by evaluation of that expression.
   69|       | * \sa \blank  \ref TopicStorageOrders */
   70|       |const unsigned int RowMajorBit = 0x1;
   71|       |
   72|       |/** \ingroup flags
   73|       | * means the expression should be evaluated by the calling expression */
   74|       |const unsigned int EvalBeforeNestingBit = 0x2;
   75|       |
   76|       |/** \ingroup flags
   77|       | * \deprecated
   78|       | * means the expression should be evaluated before any assignment */
   79|       |EIGEN_DEPRECATED const unsigned int EvalBeforeAssigningBit = 0x4;  // FIXME deprecated
   80|       |
   81|       |/** \ingroup flags
   82|       | *
   83|       | * Short version: means the expression might be vectorized
   84|       | *
   85|       | * Long version: means that the coefficients can be handled by packets
   86|       | * and start at a memory location whose alignment meets the requirements
   87|       | * of the present CPU architecture for optimized packet access. In the fixed-size
   88|       | * case, there is the additional condition that it be possible to access all the
   89|       | * coefficients by packets (this implies the requirement that the size be a multiple of 16 bytes,
   90|       | * and that any nontrivial strides don't break the alignment). In the dynamic-size case,
   91|       | * there is no such condition on the total size and strides, so it might not be possible to access
   92|       | * all coeffs by packets.
   93|       | *
   94|       | * \note This bit can be set regardless of whether vectorization is actually enabled.
   95|       | *       To check for actual vectorizability, see \a ActualPacketAccessBit.
   96|       | */
   97|       |const unsigned int PacketAccessBit = 0x8;
   98|       |
   99|       |#ifdef EIGEN_VECTORIZE
  100|       |/** \ingroup flags
  101|       | *
  102|       | * If vectorization is enabled (EIGEN_VECTORIZE is defined) this constant
  103|       | * is set to the value \a PacketAccessBit.
  104|       | *
  105|       | * If vectorization is not enabled (EIGEN_VECTORIZE is not defined) this constant
  106|       | * is set to the value 0.
  107|       | */
  108|       |const unsigned int ActualPacketAccessBit = PacketAccessBit;
  109|       |#else
  110|       |const unsigned int ActualPacketAccessBit = 0x0;
  111|       |#endif
  112|       |
  113|       |/** \ingroup flags
  114|       | *
  115|       | * Short version: means the expression can be seen as 1D vector.
  116|       | *
  117|       | * Long version: means that one can access the coefficients
  118|       | * of this expression by coeff(int), and coeffRef(int) in the case of a lvalue expression. These
  119|       | * index-based access methods are guaranteed
  120|       | * to not have to do any runtime computation of a (row, col)-pair from the index, so that it
  121|       | * is guaranteed that whenever it is available, index-based access is at least as fast as
  122|       | * (row,col)-based access. Expressions for which that isn't possible don't have the LinearAccessBit.
  123|       | *
  124|       | * If both PacketAccessBit and LinearAccessBit are set, then the
  125|       | * packets of this expression can be accessed by packet(int), and writePacket(int) in the case of a
  126|       | * lvalue expression.
  127|       | *
  128|       | * Typically, all vector expressions have the LinearAccessBit, but there is one exception:
  129|       | * Product expressions don't have it, because it would be troublesome for vectorization, even when the
  130|       | * Product is a vector expression. Thus, vector Product expressions allow index-based coefficient access but
  131|       | * not index-based packet access, so they don't have the LinearAccessBit.
  132|       | */
  133|       |const unsigned int LinearAccessBit = 0x10;
  134|       |
  135|       |/** \ingroup flags
  136|       | *
  137|       | * Means the expression has a coeffRef() method, i.e. is writable as its individual coefficients are directly
  138|       | * addressable. This rules out read-only expressions.
  139|       | *
  140|       | * Note that DirectAccessBit and LvalueBit are mutually orthogonal, as there are examples of expression having one but
  141|       | * not the other: \li writable expressions that don't have a very simple memory layout as a strided array, have
  142|       | * LvalueBit but not DirectAccessBit \li Map-to-const expressions, for example Map<const Matrix>, have DirectAccessBit
  143|       | * but not LvalueBit
  144|       | *
  145|       | * Expressions having LvalueBit also have their coeff() method returning a const reference instead of returning a new
  146|       | * value.
  147|       | */
  148|       |const unsigned int LvalueBit = 0x20;
  149|       |
  150|       |/** \ingroup flags
  151|       | *
  152|       | * Means that the underlying array of coefficients can be directly accessed as a plain strided array. The memory layout
  153|       | * of the array of coefficients must be exactly the natural one suggested by rows(), cols(),
  154|       | * outerStride(), innerStride(), and the RowMajorBit. This rules out expressions such as Diagonal, whose coefficients,
  155|       | * though referenceable, do not have such a regular memory layout.
  156|       | *
  157|       | * See the comment on LvalueBit for an explanation of how LvalueBit and DirectAccessBit are mutually orthogonal.
  158|       | */
  159|       |const unsigned int DirectAccessBit = 0x40;
  160|       |
  161|       |/** \deprecated \ingroup flags
  162|       | *
  163|       | * means the first coefficient packet is guaranteed to be aligned.
  164|       | * An expression cannot have the AlignedBit without the PacketAccessBit flag.
  165|       | * In other words, this means we are allow to perform an aligned packet access to the first element regardless
  166|       | * of the expression kind:
  167|       | * \code
  168|       | * expression.packet<Aligned>(0);
  169|       | * \endcode
  170|       | */
  171|       |EIGEN_DEPRECATED const unsigned int AlignedBit = 0x80;
  172|       |
  173|       |const unsigned int NestByRefBit = 0x100;
  174|       |
  175|       |/** \ingroup flags
  176|       | *
  177|       | * for an expression, this means that the storage order
  178|       | * can be either row-major or column-major.
  179|       | * The precise choice will be decided at evaluation time or when
  180|       | * combined with other expressions.
  181|       | * \sa \blank  \ref RowMajorBit, \ref TopicStorageOrders */
  182|       |const unsigned int NoPreferredStorageOrderBit = 0x200;
  183|       |
  184|       |/** \ingroup flags
  185|       |  *
  186|       |  * Means that the underlying coefficients can be accessed through pointers to the sparse (un)compressed storage format,
  187|       |  * that is, the expression provides:
  188|       |  * \code
  189|       |    inline const Scalar* valuePtr() const;
  190|       |    inline const Index* innerIndexPtr() const;
  191|       |    inline const Index* outerIndexPtr() const;
  192|       |    inline const Index* innerNonZeroPtr() const;
  193|       |    \endcode
  194|       |  */
  195|       |const unsigned int CompressedAccessBit = 0x400;
  196|       |
  197|       |// list of flags that are inherited by default
  198|       |const unsigned int HereditaryBits = RowMajorBit | EvalBeforeNestingBit;
  199|       |
  200|       |/** \defgroup enums Enumerations
  201|       | * \ingroup Core_Module
  202|       | *
  203|       | * Various enumerations used in %Eigen. Many of these are used as template parameters.
  204|       | */
  205|       |
  206|       |/** \ingroup enums
  207|       | * Enum containing possible values for the \c Mode or \c UpLo parameter of
  208|       | * MatrixBase::selfadjointView() and MatrixBase::triangularView(), and selfadjoint solvers. */
  209|       |enum UpLoType {
  210|       |  /** View matrix as a lower triangular matrix. */
  211|       |  Lower = 0x1,
  212|       |  /** View matrix as an upper triangular matrix. */
  213|       |  Upper = 0x2,
  214|       |  /** %Matrix has ones on the diagonal; to be used in combination with #Lower or #Upper. */
  215|       |  UnitDiag = 0x4,
  216|       |  /** %Matrix has zeros on the diagonal; to be used in combination with #Lower or #Upper. */
  217|       |  ZeroDiag = 0x8,
  218|       |  /** View matrix as a lower triangular matrix with ones on the diagonal. */
  219|       |  UnitLower = UnitDiag | Lower,
  220|       |  /** View matrix as an upper triangular matrix with ones on the diagonal. */
  221|       |  UnitUpper = UnitDiag | Upper,
  222|       |  /** View matrix as a lower triangular matrix with zeros on the diagonal. */
  223|       |  StrictlyLower = ZeroDiag | Lower,
  224|       |  /** View matrix as an upper triangular matrix with zeros on the diagonal. */
  225|       |  StrictlyUpper = ZeroDiag | Upper,
  226|       |  /** Used in BandMatrix and SelfAdjointView to indicate that the matrix is self-adjoint. */
  227|       |  SelfAdjoint = 0x10,
  228|       |  /** Used to support symmetric, non-selfadjoint, complex matrices. */
  229|       |  Symmetric = 0x20
  230|       |};
  231|       |
  232|       |/** \ingroup enums
  233|       | * Enum for indicating whether a buffer is aligned or not. */
  234|       |enum AlignmentType {
  235|       |  Unaligned = 0,    /**< Data pointer has no specific alignment. */
  236|       |  Aligned8 = 8,     /**< Data pointer is aligned on a 8 bytes boundary. */
  237|       |  Aligned16 = 16,   /**< Data pointer is aligned on a 16 bytes boundary. */
  238|       |  Aligned32 = 32,   /**< Data pointer is aligned on a 32 bytes boundary. */
  239|       |  Aligned64 = 64,   /**< Data pointer is aligned on a 64 bytes boundary. */
  240|       |  Aligned128 = 128, /**< Data pointer is aligned on a 128 bytes boundary. */
  241|       |  AlignedMask = 255,
  242|       |  Aligned = 16, /**< \deprecated Synonym for Aligned16. */
  243|       |#if EIGEN_MAX_ALIGN_BYTES == 128
  244|       |  AlignedMax = Aligned128
  245|       |#elif EIGEN_MAX_ALIGN_BYTES == 64
  246|       |  AlignedMax = Aligned64
  247|       |#elif EIGEN_MAX_ALIGN_BYTES == 32
  248|       |  AlignedMax = Aligned32
  249|       |#elif EIGEN_MAX_ALIGN_BYTES == 16
  250|       |  AlignedMax = Aligned16
  251|       |#elif EIGEN_MAX_ALIGN_BYTES == 8
  252|       |  AlignedMax = Aligned8
  253|       |#elif EIGEN_MAX_ALIGN_BYTES == 0
  254|       |  AlignedMax = Unaligned
  255|       |#else
  256|       |#error Invalid value for EIGEN_MAX_ALIGN_BYTES
  257|       |#endif
  258|       |};
  259|       |
  260|       |/** \ingroup enums
  261|       | * Enum containing possible values for the \p Direction parameter of
  262|       | * Reverse, PartialReduxExpr and VectorwiseOp. */
  263|       |enum DirectionType {
  264|       |  /** For Reverse, all columns are reversed;
  265|       |   * for PartialReduxExpr and VectorwiseOp, act on columns. */
  266|       |  Vertical,
  267|       |  /** For Reverse, all rows are reversed;
  268|       |   * for PartialReduxExpr and VectorwiseOp, act on rows. */
  269|       |  Horizontal,
  270|       |  /** For Reverse, both rows and columns are reversed;
  271|       |   * not used for PartialReduxExpr and VectorwiseOp. */
  272|       |  BothDirections
  273|       |};
  274|       |
  275|       |/** \internal \ingroup enums
  276|       | * Enum to specify how to traverse the entries of a matrix. */
  277|       |enum TraversalType {
  278|       |  /** \internal Default traversal, no vectorization, no index-based access */
  279|       |  DefaultTraversal,
  280|       |  /** \internal No vectorization, use index-based access to have only one for loop instead of 2 nested loops */
  281|       |  LinearTraversal,
  282|       |  /** \internal Equivalent to a slice vectorization for fixed-size matrices having good alignment
  283|       |   * and good size */
  284|       |  InnerVectorizedTraversal,
  285|       |  /** \internal Vectorization path using a single loop plus scalar loops for the
  286|       |   * unaligned boundaries */
  287|       |  LinearVectorizedTraversal,
  288|       |  /** \internal Generic vectorization path using one vectorized loop per row/column with some
  289|       |   * scalar loops to handle the unaligned boundaries */
  290|       |  SliceVectorizedTraversal,
  291|       |  /** \internal Special case to properly handle incompatible scalar types or other defecting cases*/
  292|       |  InvalidTraversal,
  293|       |  /** \internal Evaluate all entries at once */
  294|       |  AllAtOnceTraversal
  295|       |};
  296|       |
  297|       |/** \internal \ingroup enums
  298|       | * Enum to specify whether to unroll loops when traversing over the entries of a matrix. */
  299|       |enum UnrollingType {
  300|       |  /** \internal Do not unroll loops. */
  301|       |  NoUnrolling,
  302|       |  /** \internal Unroll only the inner loop, but not the outer loop. */
  303|       |  InnerUnrolling,
  304|       |  /** \internal Unroll both the inner and the outer loop. If there is only one loop,
  305|       |   * because linear traversal is used, then unroll that loop. */
  306|       |  CompleteUnrolling
  307|       |};
  308|       |
  309|       |/** \internal \ingroup enums
  310|       | * Enum to specify whether to use the default (built-in) implementation or the specialization. */
  311|       |enum SpecializedType { Specialized, BuiltIn };
  312|       |
  313|       |/** \ingroup enums
  314|       | * Enum containing possible values for the \p Options_ template parameter of
  315|       | * Matrix, Array and BandMatrix. */
  316|       |enum StorageOptions {
  317|       |  /** Storage order is column major (see \ref TopicStorageOrders). */
  318|       |  ColMajor = 0,
  319|       |  /** Storage order is row major (see \ref TopicStorageOrders). */
  320|       |  RowMajor = 0x1,  // it is only a coincidence that this is equal to RowMajorBit -- don't rely on that
  321|       |  /** Align the matrix itself if it is vectorizable fixed-size */
  322|       |  AutoAlign = 0,
  323|       |  /** Don't require alignment for the matrix itself (the array of coefficients, if dynamically allocated, may still be requested to be aligned) */ // FIXME --- clarify the situation
  324|       |  DontAlign = 0x2
  325|       |};
  326|       |
  327|       |/** \ingroup enums
  328|       | * Enum for specifying whether to apply or solve on the left or right. */
  329|       |enum SideType {
  330|       |  /** Apply transformation on the left. */
  331|       |  OnTheLeft = 1,
  332|       |  /** Apply transformation on the right. */
  333|       |  OnTheRight = 2
  334|       |};
  335|       |
  336|       |/** \ingroup enums
  337|       | * Enum for specifying NaN-propagation behavior, e.g. for coeff-wise min/max. */
  338|       |enum NaNPropagationOptions {
  339|       |  /**  Implementation defined behavior if NaNs are present. */
  340|       |  PropagateFast = 0,
  341|       |  /**  Always propagate NaNs. */
  342|       |  PropagateNaN,
  343|       |  /**  Always propagate not-NaNs. */
  344|       |  PropagateNumbers
  345|       |};
  346|       |
  347|       |/* the following used to be written as:
  348|       | *
  349|       | *   struct NoChange_t {};
  350|       | *   namespace {
  351|       | *     EIGEN_UNUSED NoChange_t NoChange;
  352|       | *   }
  353|       | *
  354|       | * on the ground that it feels dangerous to disambiguate overloaded functions on enum/integer types.
  355|       | * However, this leads to "variable declared but never referenced" warnings on Intel Composer XE,
  356|       | * and we do not know how to get rid of them (bug 450).
  357|       | */
  358|       |
  359|       |enum NoChange_t { NoChange };
  360|       |enum Sequential_t { Sequential };
  361|       |enum Default_t { Default };
  362|       |
  363|       |/** \internal \ingroup enums
  364|       | * Used in AmbiVector. */
  365|       |enum AmbiVectorMode { IsDense = 0, IsSparse };
  366|       |
  367|       |/** \ingroup enums
  368|       | * Used as template parameter in DenseCoeffBase and MapBase to indicate
  369|       | * which accessors should be provided. */
  370|       |enum AccessorLevels {
  371|       |  /** Read-only access via a member function. */
  372|       |  ReadOnlyAccessors,
  373|       |  /** Read/write access via member functions. */
  374|       |  WriteAccessors,
  375|       |  /** Direct read-only access to the coefficients. */
  376|       |  DirectAccessors,
  377|       |  /** Direct read/write access to the coefficients. */
  378|       |  DirectWriteAccessors
  379|       |};
  380|       |
  381|       |/** \ingroup enums
  382|       | * Enum with options to give to various decompositions. */
  383|       |enum DecompositionOptions {
  384|       |  /** \internal Not used (meant for LDLT?). */
  385|       |  Pivoting = 0x01,
  386|       |  /** \internal Not used (meant for LDLT?). */
  387|       |  NoPivoting = 0x02,
  388|       |  /** Used in JacobiSVD to indicate that the square matrix U is to be computed. */
  389|       |  ComputeFullU = 0x04,
  390|       |  /** Used in JacobiSVD to indicate that the thin matrix U is to be computed. */
  391|       |  ComputeThinU = 0x08,
  392|       |  /** Used in JacobiSVD to indicate that the square matrix V is to be computed. */
  393|       |  ComputeFullV = 0x10,
  394|       |  /** Used in JacobiSVD to indicate that the thin matrix V is to be computed. */
  395|       |  ComputeThinV = 0x20,
  396|       |  /** Used in SelfAdjointEigenSolver and GeneralizedSelfAdjointEigenSolver to specify
  397|       |   * that only the eigenvalues are to be computed and not the eigenvectors. */
  398|       |  EigenvaluesOnly = 0x40,
  399|       |  /** Used in SelfAdjointEigenSolver and GeneralizedSelfAdjointEigenSolver to specify
  400|       |   * that both the eigenvalues and the eigenvectors are to be computed. */
  401|       |  ComputeEigenvectors = 0x80,
  402|       |  /** \internal */
  403|       |  EigVecMask = EigenvaluesOnly | ComputeEigenvectors,
  404|       |  /** Used in GeneralizedSelfAdjointEigenSolver to indicate that it should
  405|       |   * solve the generalized eigenproblem \f$ Ax = \lambda B x \f$. */
  406|       |  Ax_lBx = 0x100,
  407|       |  /** Used in GeneralizedSelfAdjointEigenSolver to indicate that it should
  408|       |   * solve the generalized eigenproblem \f$ ABx = \lambda x \f$. */
  409|       |  ABx_lx = 0x200,
  410|       |  /** Used in GeneralizedSelfAdjointEigenSolver to indicate that it should
  411|       |   * solve the generalized eigenproblem \f$ BAx = \lambda x \f$. */
  412|       |  BAx_lx = 0x400,
  413|       |  /** \internal */
  414|       |  GenEigMask = Ax_lBx | ABx_lx | BAx_lx
  415|       |};
  416|       |
  417|       |/** \ingroup enums
  418|       | * Possible values for the \p QRPreconditioner template parameter of JacobiSVD. */
  419|       |enum QRPreconditioners {
  420|       |  /** Use a QR decomposition with column pivoting as the first step. */
  421|       |  ColPivHouseholderQRPreconditioner = 0x0,
  422|       |  /** Do not specify what is to be done if the SVD of a non-square matrix is asked for. */
  423|       |  NoQRPreconditioner = 0x40,
  424|       |  /** Use a QR decomposition without pivoting as the first step. */
  425|       |  HouseholderQRPreconditioner = 0x80,
  426|       |  /** Use a QR decomposition with full pivoting as the first step. */
  427|       |  FullPivHouseholderQRPreconditioner = 0xC0,
  428|       |  /** Used to disable the QR Preconditioner in BDCSVD. */
  429|       |  DisableQRDecomposition = NoQRPreconditioner
  430|       |};
  431|       |
  432|       |#ifdef Success
  433|       |#error The preprocessor symbol 'Success' is defined, possibly by the X11 header file X.h
  434|       |#endif
  435|       |
  436|       |/** \ingroup enums
  437|       | * Enum for reporting the status of a computation. */
  438|       |enum ComputationInfo {
  439|       |  /** Computation was successful. */
  440|       |  Success = 0,
  441|       |  /** The provided data did not satisfy the prerequisites. */
  442|       |  NumericalIssue = 1,
  443|       |  /** Iterative procedure did not converge. */
  444|       |  NoConvergence = 2,
  445|       |  /** The inputs are invalid, or the algorithm has been improperly called.
  446|       |   * When assertions are enabled, such errors trigger an assert. */
  447|       |  InvalidInput = 3
  448|       |};
  449|       |
  450|       |/** \ingroup enums
  451|       | * Enum used to specify how a particular transformation is stored in a matrix.
  452|       | * \sa Transform, Hyperplane::transform(). */
  453|       |enum TransformTraits {
  454|       |  /** Transformation is an isometry. */
  455|       |  Isometry = 0x1,
  456|       |  /** Transformation is an affine transformation stored as a (Dim+1)^2 matrix whose last row is
  457|       |   * assumed to be [0 ... 0 1]. */
  458|       |  Affine = 0x2,
  459|       |  /** Transformation is an affine transformation stored as a (Dim) x (Dim+1) matrix. */
  460|       |  AffineCompact = 0x10 | Affine,
  461|       |  /** Transformation is a general projective transformation stored as a (Dim+1)^2 matrix. */
  462|       |  Projective = 0x20
  463|       |};
  464|       |
  465|       |/** \internal \ingroup enums
  466|       | * Enum used to choose between implementation depending on the computer architecture. */
  467|       |namespace Architecture {
  468|       |enum Type {
  469|       |  Generic = 0x0,
  470|       |  SSE = 0x1,
  471|       |  AltiVec = 0x2,
  472|       |  VSX = 0x3,
  473|       |  NEON = 0x4,
  474|       |  MSA = 0x5,
  475|       |  SVE = 0x6,
  476|       |  HVX = 0x7,
  477|       |  LSX = 0x8,
  478|       |#if defined EIGEN_VECTORIZE_SSE
  479|       |  Target = SSE
  480|       |#elif defined EIGEN_VECTORIZE_ALTIVEC
  481|       |  Target = AltiVec
  482|       |#elif defined EIGEN_VECTORIZE_VSX
  483|       |  Target = VSX
  484|       |#elif defined EIGEN_VECTORIZE_NEON
  485|       |  Target = NEON
  486|       |#elif defined EIGEN_VECTORIZE_SVE
  487|       |  Target = SVE
  488|       |#elif defined EIGEN_VECTORIZE_MSA
  489|       |  Target = MSA
  490|       |#elif defined EIGEN_VECTORIZE_HVX
  491|       |  Target = HVX
  492|       |#elif defined EIGEN_VECTORIZE_LSX
  493|       |  Target = LSX
  494|       |#else
  495|       |  Target = Generic
  496|       |#endif
  497|       |};
  498|       |}  // namespace Architecture
  499|       |
  500|       |/** \internal \ingroup enums
  501|       | * Enum used as template parameter in Product and product evaluators. */
  502|       |enum ProductImplType {
  503|       |  DefaultProduct = 0,
  504|       |  LazyProduct,
  505|       |  AliasFreeProduct,
  506|       |  CoeffBasedProductMode,
  507|       |  LazyCoeffBasedProductMode,
  508|       |  OuterProduct,
  509|       |  InnerProduct,
  510|       |  GemvProduct,
  511|       |  GemmProduct
  512|       |};
  513|       |
  514|       |/** \internal \ingroup enums
  515|       | * Enum used in experimental parallel implementation. */
  516|       |enum Action { GetAction, SetAction };
  517|       |
  518|       |/** The type used to identify a dense storage. */
  519|       |struct Dense {};
  520|       |
  521|       |/** The type used to identify a general sparse storage. */
  522|       |struct Sparse {};
  523|       |
  524|       |/** The type used to identify a general solver (factored) storage. */
  525|       |struct SolverStorage {};
  526|       |
  527|       |/** The type used to identify a permutation storage. */
  528|       |struct PermutationStorage {};
  529|       |
  530|       |/** The type used to identify a permutation storage. */
  531|       |struct TranspositionsStorage {};
  532|       |
  533|       |/** The type used to identify a matrix expression */
  534|       |struct MatrixXpr {};
  535|       |
  536|       |/** The type used to identify an array expression */
  537|       |struct ArrayXpr {};
  538|       |
  539|       |// An evaluator must define its shape. By default, it can be one of the following:
  540|       |struct DenseShape {
  541|      0|  static std::string debugName() { return "DenseShape"; }
  542|       |};
  543|       |struct SolverShape {
  544|      0|  static std::string debugName() { return "SolverShape"; }
  545|       |};
  546|       |struct HomogeneousShape {
  547|      0|  static std::string debugName() { return "HomogeneousShape"; }
  548|       |};
  549|       |struct DiagonalShape {
  550|      0|  static std::string debugName() { return "DiagonalShape"; }
  551|       |};
  552|       |struct SkewSymmetricShape {
  553|      0|  static std::string debugName() { return "SkewSymmetricShape"; }
  554|       |};
  555|       |struct BandShape {
  556|      0|  static std::string debugName() { return "BandShape"; }
  557|       |};
  558|       |struct TriangularShape {
  559|      0|  static std::string debugName() { return "TriangularShape"; }
  560|       |};
  561|       |struct SelfAdjointShape {
  562|      0|  static std::string debugName() { return "SelfAdjointShape"; }
  563|       |};
  564|       |struct PermutationShape {
  565|      0|  static std::string debugName() { return "PermutationShape"; }
  566|       |};
  567|       |struct TranspositionsShape {
  568|      0|  static std::string debugName() { return "TranspositionsShape"; }
  569|       |};
  570|       |struct SparseShape {
  571|      0|  static std::string debugName() { return "SparseShape"; }
  572|       |};
  573|       |
  574|       |namespace internal {
  575|       |
  576|       |// random access iterators based on coeff*() accessors.
  577|       |struct IndexBased {};
  578|       |
  579|       |// evaluator based on iterators to access coefficients.
  580|       |struct IteratorBased {};
  581|       |
  582|       |/** \internal
  583|       | * Constants for comparison functors
  584|       | */
  585|       |enum ComparisonName : unsigned int {
  586|       |  cmp_EQ = 0,
  587|       |  cmp_LT = 1,
  588|       |  cmp_LE = 2,
  589|       |  cmp_UNORD = 3,
  590|       |  cmp_NEQ = 4,
  591|       |  cmp_GT = 5,
  592|       |  cmp_GE = 6
  593|       |};
  594|       |}  // end namespace internal
  595|       |
  596|       |}  // end namespace Eigen
  597|       |
  598|       |#endif  // EIGEN_CONSTANTS_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/IntegralConstant.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2017 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_INTEGRAL_CONSTANT_H
   11|       |#define EIGEN_INTEGRAL_CONSTANT_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |namespace internal {
   19|       |
   20|       |template <int N>
   21|       |class FixedInt;
   22|       |template <int N>
   23|       |class VariableAndFixedInt;
   24|       |
   25|       |/** \internal
   26|       | * \class FixedInt
   27|       | *
   28|       | * This class embeds a compile-time integer \c N.
   29|       | *
   30|       | * It is similar to c++11 std::integral_constant<int,N> but with some additional features
   31|       | * such as:
   32|       | *  - implicit conversion to int
   33|       | *  - arithmetic and some bitwise operators: -, +, *, /, %, &, |
   34|       | *  - c++98/14 compatibility with fix<N> and fix<N>() syntax to define integral constants.
   35|       | *
   36|       | * It is strongly discouraged to directly deal with this class FixedInt. Instances are expected to
   37|       | * be created by the user using Eigen::fix<N> or Eigen::fix<N>().
   38|       | * \code
   39|       | * internal::cleanup_index_type<T>::type
   40|       | * internal::cleanup_index_type<T,DynamicKey>::type
   41|       | * \endcode
   42|       | * where T can a FixedInt<N>, a pointer to function FixedInt<N> (*)(), or numerous other integer-like representations.
   43|       | * \c DynamicKey is either Dynamic (default) or DynamicIndex and used to identify true compile-time values.
   44|       | *
   45|       | * For convenience, you can extract the compile-time value \c N in a generic way using the following helper:
   46|       | * \code
   47|       | * internal::get_fixed_value<T,DefaultVal>::value
   48|       | * \endcode
   49|       | * that will give you \c N if T equals FixedInt<N> or FixedInt<N> (*)(), and \c DefaultVal if T does not embed any
   50|       | * compile-time value (e.g., T==int).
   51|       | *
   52|       | * \sa fix<N>, class VariableAndFixedInt
   53|       | */
   54|       |template <int N>
   55|       |class FixedInt {
   56|       | public:
   57|       |  static constexpr int value = N;
   58|       |  constexpr operator int() const { return N; }
   59|       |
   60|       |  constexpr FixedInt() = default;
   61|       |  constexpr FixedInt(std::integral_constant<int, N>) {}
   62|       |
   63|       |  constexpr FixedInt(VariableAndFixedInt<N> other) {
   64|       |#ifndef EIGEN_INTERNAL_DEBUGGING
   65|       |    EIGEN_UNUSED_VARIABLE(other);
   66|       |#endif
   67|       |    eigen_internal_assert(int(other) == N);
   68|       |  }
   69|       |
   70|       |  constexpr FixedInt<-N> operator-() const { return FixedInt<-N>(); }
   71|       |
   72|       |  template <int M>
   73|       |  constexpr FixedInt<N + M> operator+(FixedInt<M>) const {
   74|       |    return FixedInt<N + M>();
   75|       |  }
   76|       |
   77|       |  template <int M>
   78|       |  constexpr FixedInt<N - M> operator-(FixedInt<M>) const {
   79|       |    return FixedInt<N - M>();
   80|       |  }
   81|       |
   82|       |  template <int M>
   83|       |  constexpr FixedInt<N * M> operator*(FixedInt<M>) const {
   84|       |    return FixedInt<N * M>();
   85|       |  }
   86|       |
   87|       |  template <int M>
   88|       |  constexpr FixedInt<N / M> operator/(FixedInt<M>) const {
   89|       |    return FixedInt<N / M>();
   90|       |  }
   91|       |
   92|       |  template <int M>
   93|       |  constexpr FixedInt<N % M> operator%(FixedInt<M>) const {
   94|       |    return FixedInt<N % M>();
   95|       |  }
   96|       |
   97|       |  template <int M>
   98|       |  constexpr FixedInt<N | M> operator|(FixedInt<M>) const {
   99|       |    return FixedInt<N | M>();
  100|       |  }
  101|       |
  102|       |  template <int M>
  103|       |  constexpr FixedInt<N & M> operator&(FixedInt<M>) const {
  104|       |    return FixedInt<N & M>();
  105|       |  }
  106|       |
  107|       |  // Needed in C++14 to allow fix<N>():
  108|      0|  constexpr FixedInt operator()() const { return *this; }
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal8FixedIntILin1EEclEv
  ------------------
  | Unexecuted instantiation: _ZNK5Eigen8internal8FixedIntILi1EEclEv
  ------------------
  109|       |
  110|       |  constexpr VariableAndFixedInt<N> operator()(int val) const { return VariableAndFixedInt<N>(val); }
  111|       |};
  112|       |
  113|       |/** \internal
  114|       | * \class VariableAndFixedInt
  115|       | *
  116|       | * This class embeds both a compile-time integer \c N and a runtime integer.
  117|       | * Both values are supposed to be equal unless the compile-time value \c N has a special
  118|       | * value meaning that the runtime-value should be used. Depending on the context, this special
  119|       | * value can be either Eigen::Dynamic (for positive quantities) or Eigen::DynamicIndex (for
  120|       | * quantities that can be negative).
  121|       | *
  122|       | * It is the return-type of the function Eigen::fix<N>(int), and most of the time this is the only
  123|       | * way it is used. It is strongly discouraged to directly deal with instances of VariableAndFixedInt.
  124|       | * Indeed, in order to write generic code, it is the responsibility of the callee to properly convert
  125|       | * it to either a true compile-time quantity (i.e. a FixedInt<N>), or to a runtime quantity (e.g., an Index)
  126|       | * using the following generic helper:
  127|       | * \code
  128|       | * internal::cleanup_index_type<T>::type
  129|       | * internal::cleanup_index_type<T,DynamicKey>::type
  130|       | * \endcode
  131|       | * where T can be a template instantiation of VariableAndFixedInt or numerous other integer-like representations.
  132|       | * \c DynamicKey is either Dynamic (default) or DynamicIndex and used to identify true compile-time values.
  133|       | *
  134|       | * For convenience, you can also extract the compile-time value \c N using the following helper:
  135|       | * \code
  136|       | * internal::get_fixed_value<T,DefaultVal>::value
  137|       | * \endcode
  138|       | * that will give you \c N if T equals VariableAndFixedInt<N>, and \c DefaultVal if T does not embed any compile-time
  139|       | * value (e.g., T==int).
  140|       | *
  141|       | * \sa fix<N>(int), class FixedInt
  142|       | */
  143|       |template <int N>
  144|       |class VariableAndFixedInt {
  145|       | public:
  146|       |  static const int value = N;
  147|       |  operator int() const { return m_value; }
  148|       |  VariableAndFixedInt(int val) { m_value = val; }
  149|       |
  150|       | protected:
  151|       |  int m_value;
  152|       |};
  153|       |
  154|       |template <typename T, int Default = Dynamic>
  155|       |struct get_fixed_value {
  156|       |  static const int value = Default;
  157|       |};
  158|       |
  159|       |template <int N, int Default>
  160|       |struct get_fixed_value<FixedInt<N>, Default> {
  161|       |  static const int value = N;
  162|       |};
  163|       |
  164|       |template <int N, int Default>
  165|       |struct get_fixed_value<VariableAndFixedInt<N>, Default> {
  166|       |  static const int value = N;
  167|       |};
  168|       |
  169|       |template <typename T, int N, int Default>
  170|       |struct get_fixed_value<variable_if_dynamic<T, N>, Default> {
  171|       |  static const int value = N;
  172|       |};
  173|       |
  174|       |template <typename T>
  175|       |EIGEN_DEVICE_FUNC Index get_runtime_value(const T &x) {
  176|       |  return x;
  177|       |}
  178|       |
  179|       |// Cleanup integer/FixedInt/VariableAndFixedInt/etc types:
  180|       |
  181|       |// By default, no cleanup:
  182|       |template <typename T, int DynamicKey = Dynamic, typename EnableIf = void>
  183|       |struct cleanup_index_type {
  184|       |  typedef T type;
  185|       |};
  186|       |
  187|       |// Convert any integral type (e.g., short, int, unsigned int, etc.) to Eigen::Index
  188|       |template <typename T, int DynamicKey>
  189|       |struct cleanup_index_type<T, DynamicKey, std::enable_if_t<internal::is_integral<T>::value>> {
  190|       |  typedef Index type;
  191|       |};
  192|       |
  193|       |// If VariableAndFixedInt does not match DynamicKey, then we turn it to a pure compile-time value:
  194|       |template <int N, int DynamicKey>
  195|       |struct cleanup_index_type<VariableAndFixedInt<N>, DynamicKey> {
  196|       |  typedef FixedInt<N> type;
  197|       |};
  198|       |// If VariableAndFixedInt matches DynamicKey, then we turn it to a pure runtime-value (aka Index):
  199|       |template <int DynamicKey>
  200|       |struct cleanup_index_type<VariableAndFixedInt<DynamicKey>, DynamicKey> {
  201|       |  typedef Index type;
  202|       |};
  203|       |
  204|       |template <int N, int DynamicKey>
  205|       |struct cleanup_index_type<std::integral_constant<int, N>, DynamicKey> {
  206|       |  typedef FixedInt<N> type;
  207|       |};
  208|       |
  209|       |}  // end namespace internal
  210|       |
  211|       |#ifndef EIGEN_PARSED_BY_DOXYGEN
  212|       |
  213|       |template <int N>
  214|       |constexpr internal::FixedInt<N> fix{};
  215|       |
  216|       |#else  // EIGEN_PARSED_BY_DOXYGEN
  217|       |
  218|       |/** \var fix<N>()
  219|       | * \ingroup Core_Module
  220|       | *
  221|       | * This \em identifier permits to construct an object embedding a compile-time integer \c N.
  222|       | *
  223|       | * \tparam N the compile-time integer value
  224|       | *
  225|       | * It is typically used in conjunction with the Eigen::seq and Eigen::seqN functions to pass compile-time values to
  226|       | * them: \code seqN(10,fix<4>,fix<-3>)   // <=> [10 7 4 1] \endcode
  227|       | *
  228|       | * See also the function fix(int) to pass both a compile-time and runtime value.
  229|       | *
  230|       | * In c++14, it is implemented as:
  231|       | * \code
  232|       | * template<int N> static const internal::FixedInt<N> fix{};
  233|       | * \endcode
  234|       | * where internal::FixedInt<N> is an internal template class similar to
  235|       | * <a href="http://en.cppreference.com/w/cpp/types/integral_constant">\c std::integral_constant </a><tt> <int,N> </tt>
  236|       | * Here, \c fix<N> is thus an object of type \c internal::FixedInt<N>.
  237|       | *
  238|       | * \sa fix<N>(int), seq, seqN
  239|       | */
  240|       |template <int N>
  241|       |static const auto fix();
  242|       |
  243|       |/** \fn fix<N>(int)
  244|       | * \ingroup Core_Module
  245|       | *
  246|       | * This function returns an object embedding both a compile-time integer \c N, and a fallback runtime value \a val.
  247|       | *
  248|       | * \tparam N the compile-time integer value
  249|       | * \param  val the fallback runtime integer value
  250|       | *
  251|       | * This function is a more general version of the \ref fix identifier/function that can be used in template code
  252|       | * where the compile-time value could turn out to actually mean "undefined at compile-time". For positive integers
  253|       | * such as a size or a dimension, this case is identified by Eigen::Dynamic, whereas runtime signed integers
  254|       | * (e.g., an increment/stride) are identified as Eigen::DynamicIndex. In such a case, the runtime value \a val
  255|       | * will be used as a fallback.
  256|       | *
  257|       | * A typical use case would be:
  258|       | * \code
  259|       | * template<typename Derived> void foo(const MatrixBase<Derived> &mat) {
  260|       | *   const int N = Derived::RowsAtCompileTime==Dynamic ? Dynamic : Derived::RowsAtCompileTime/2;
  261|       | *   const int n = mat.rows()/2;
  262|       | *   ... mat( seqN(0,fix<N>(n) ) ...;
  263|       | * }
  264|       | * \endcode
  265|       | * In this example, the function Eigen::seqN knows that the second argument is expected to be a size.
  266|       | * If the passed compile-time value N equals Eigen::Dynamic, then the proxy object returned by fix will be dismissed,
  267|       | * and converted to an Eigen::Index of value \c n. Otherwise, the runtime-value \c n will be dismissed, and the
  268|       | * returned ArithmeticSequence will be of the exact same type as <tt> seqN(0,fix<N>) </tt>.
  269|       | *
  270|       | * \sa fix, seqN, class ArithmeticSequence
  271|       | */
  272|       |template <int N>
  273|       |static const auto fix(int val);
  274|       |
  275|       |#endif  // EIGEN_PARSED_BY_DOXYGEN
  276|       |
  277|       |}  // end namespace Eigen
  278|       |
  279|       |#endif  // EIGEN_INTEGRAL_CONSTANT_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/Macros.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2015 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |// Copyright (C) 2006-2008 Benoit Jacob <jacob.benoit.1@gmail.com>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_MACROS_H
   12|       |#define EIGEN_MACROS_H
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |//------------------------------------------------------------------------------------------
   17|       |// Eigen version and basic defaults
   18|       |//------------------------------------------------------------------------------------------
   19|       |
   20|       |#define EIGEN_WORLD_VERSION 3
   21|       |#define EIGEN_MAJOR_VERSION 4
   22|       |#define EIGEN_MINOR_VERSION 90
   23|       |
   24|       |#define EIGEN_VERSION_AT_LEAST(x, y, z) \
   25|       |  (EIGEN_WORLD_VERSION > x ||           \
   26|       |   (EIGEN_WORLD_VERSION >= x && (EIGEN_MAJOR_VERSION > y || (EIGEN_MAJOR_VERSION >= y && EIGEN_MINOR_VERSION >= z))))
   27|       |
   28|       |#ifdef EIGEN_DEFAULT_TO_ROW_MAJOR
   29|       |#define EIGEN_DEFAULT_MATRIX_STORAGE_ORDER_OPTION Eigen::RowMajor
   30|       |#else
   31|       |#define EIGEN_DEFAULT_MATRIX_STORAGE_ORDER_OPTION Eigen::ColMajor
   32|       |#endif
   33|       |
   34|       |#ifndef EIGEN_DEFAULT_DENSE_INDEX_TYPE
   35|       |#define EIGEN_DEFAULT_DENSE_INDEX_TYPE std::ptrdiff_t
   36|       |#endif
   37|       |
   38|       |// Upperbound on the C++ version to use.
   39|       |// Expected values are 03, 11, 14, 17, etc.
   40|       |// By default, let's use an arbitrarily large C++ version.
   41|       |#ifndef EIGEN_MAX_CPP_VER
   42|       |#define EIGEN_MAX_CPP_VER 99
   43|       |#endif
   44|       |
   45|       |/** Allows to disable some optimizations which might affect the accuracy of the result.
   46|       | * Such optimization are enabled by default, and set EIGEN_FAST_MATH to 0 to disable them.
   47|       | * They currently include:
   48|       | *   - single precision ArrayBase::sin() and ArrayBase::cos() for SSE and AVX vectorization.
   49|       | */
   50|       |#ifndef EIGEN_FAST_MATH
   51|       |#define EIGEN_FAST_MATH 1
   52|       |#endif
   53|       |
   54|       |#ifndef EIGEN_STACK_ALLOCATION_LIMIT
   55|       |// 131072 == 128 KB
   56|       |#define EIGEN_STACK_ALLOCATION_LIMIT 131072
   57|       |#endif
   58|       |
   59|       |//------------------------------------------------------------------------------------------
   60|       |// Compiler identification, EIGEN_COMP_*
   61|       |//------------------------------------------------------------------------------------------
   62|       |
   63|       |/// \internal EIGEN_COMP_GNUC set to version (e.g., 951 for GCC 9.5.1) for all compilers compatible with GCC
   64|       |#ifdef __GNUC__
   65|       |#define EIGEN_COMP_GNUC (__GNUC__ * 100 + __GNUC_MINOR__ * 10 + __GNUC_PATCHLEVEL__)
   66|       |#else
   67|       |#define EIGEN_COMP_GNUC 0
   68|       |#endif
   69|       |
   70|       |/// \internal EIGEN_COMP_CLANG set to version (e.g., 372 for clang 3.7.2) if the compiler is clang
   71|       |#if defined(__clang__)
   72|       |#define EIGEN_COMP_CLANG (__clang_major__ * 100 + __clang_minor__ * 10 + __clang_patchlevel__)
   73|       |#else
   74|       |#define EIGEN_COMP_CLANG 0
   75|       |#endif
   76|       |
   77|       |/// \internal EIGEN_COMP_CLANGAPPLE set to the version number (e.g. 9000000 for AppleClang 9.0) if the compiler is
   78|       |/// AppleClang
   79|       |#if defined(__clang__) && defined(__apple_build_version__)
   80|       |#define EIGEN_COMP_CLANGAPPLE __apple_build_version__
   81|       |#else
   82|       |#define EIGEN_COMP_CLANGAPPLE 0
   83|       |#endif
   84|       |
   85|       |/// \internal EIGEN_COMP_CASTXML set to 1 if being preprocessed by CastXML
   86|       |#if defined(__castxml__)
   87|       |#define EIGEN_COMP_CASTXML 1
   88|       |#else
   89|       |#define EIGEN_COMP_CASTXML 0
   90|       |#endif
   91|       |
   92|       |/// \internal EIGEN_COMP_LLVM set to 1 if the compiler backend is llvm
   93|       |#if defined(__llvm__)
   94|       |#define EIGEN_COMP_LLVM 1
   95|       |#else
   96|       |#define EIGEN_COMP_LLVM 0
   97|       |#endif
   98|       |
   99|       |/// \internal EIGEN_COMP_ICC set to __INTEL_COMPILER if the compiler is Intel icc compiler, 0 otherwise
  100|       |#if defined(__INTEL_COMPILER)
  101|       |#define EIGEN_COMP_ICC __INTEL_COMPILER
  102|       |#else
  103|       |#define EIGEN_COMP_ICC 0
  104|       |#endif
  105|       |
  106|       |/// \internal EIGEN_COMP_CLANGICC set to __INTEL_CLANG_COMPILER if the compiler is Intel icx compiler, 0 otherwise
  107|       |#if defined(__INTEL_CLANG_COMPILER)
  108|       |#define EIGEN_COMP_CLANGICC __INTEL_CLANG_COMPILER
  109|       |#else
  110|       |#define EIGEN_COMP_CLANGICC 0
  111|       |#endif
  112|       |
  113|       |/// \internal EIGEN_COMP_MINGW set to 1 if the compiler is mingw
  114|       |#if defined(__MINGW32__)
  115|       |#define EIGEN_COMP_MINGW 1
  116|       |#else
  117|       |#define EIGEN_COMP_MINGW 0
  118|       |#endif
  119|       |
  120|       |/// \internal EIGEN_COMP_SUNCC set to 1 if the compiler is Solaris Studio
  121|       |#if defined(__SUNPRO_CC)
  122|       |#define EIGEN_COMP_SUNCC 1
  123|       |#else
  124|       |#define EIGEN_COMP_SUNCC 0
  125|       |#endif
  126|       |
  127|       |/// \internal EIGEN_COMP_MSVC set to _MSC_VER if the compiler is Microsoft Visual C++, 0 otherwise.
  128|       |#if defined(_MSC_VER)
  129|       |#define EIGEN_COMP_MSVC _MSC_VER
  130|       |#else
  131|       |#define EIGEN_COMP_MSVC 0
  132|       |#endif
  133|       |
  134|       |#if defined(__NVCC__)
  135|       |#if defined(__CUDACC_VER_MAJOR__) && (__CUDACC_VER_MAJOR__ >= 9)
  136|       |#define EIGEN_COMP_NVCC ((__CUDACC_VER_MAJOR__ * 10000) + (__CUDACC_VER_MINOR__ * 100))
  137|       |#elif defined(__CUDACC_VER__)
  138|       |#define EIGEN_COMP_NVCC __CUDACC_VER__
  139|       |#else
  140|       |#error "NVCC did not define compiler version."
  141|       |#endif
  142|       |#else
  143|       |#define EIGEN_COMP_NVCC 0
  144|       |#endif
  145|       |
  146|       |// For the record, here is a table summarizing the possible values for EIGEN_COMP_MSVC:
  147|       |//  name        ver   MSC_VER
  148|       |//  2015        14      1900
  149|       |//  "15"        15      1900
  150|       |//  2017-14.1   15.0    1910
  151|       |//  2017-14.11  15.3    1911
  152|       |//  2017-14.12  15.5    1912
  153|       |//  2017-14.13  15.6    1913
  154|       |//  2017-14.14  15.7    1914
  155|       |//  2017        15.8    1915
  156|       |//  2017        15.9    1916
  157|       |//  2019 RTW    16.0    1920
  158|       |
  159|       |/// \internal EIGEN_COMP_MSVC_LANG set to _MSVC_LANG if the compiler is Microsoft Visual C++, 0 otherwise.
  160|       |#if defined(_MSVC_LANG)
  161|       |#define EIGEN_COMP_MSVC_LANG _MSVC_LANG
  162|       |#else
  163|       |#define EIGEN_COMP_MSVC_LANG 0
  164|       |#endif
  165|       |
  166|       |// For the record, here is a table summarizing the possible values for EIGEN_COMP_MSVC_LANG:
  167|       |// MSVC option                          Standard  MSVC_LANG
  168|       |// /std:c++14 (default as of VS 2019)   C++14     201402L
  169|       |// /std:c++17                           C++17     201703L
  170|       |// /std:c++latest                       >C++17    >201703L
  171|       |
  172|       |/// \internal EIGEN_COMP_MSVC_STRICT set to 1 if the compiler is really Microsoft Visual C++ and not ,e.g., ICC or
  173|       |/// clang-cl
  174|       |#if EIGEN_COMP_MSVC && !(EIGEN_COMP_ICC || EIGEN_COMP_LLVM || EIGEN_COMP_CLANG)
  175|       |#define EIGEN_COMP_MSVC_STRICT _MSC_VER
  176|       |#else
  177|       |#define EIGEN_COMP_MSVC_STRICT 0
  178|       |#endif
  179|       |
  180|       |/// \internal EIGEN_COMP_IBM set to xlc version if the compiler is IBM XL C++
  181|       |// XLC   version
  182|       |// 3.1   0x0301
  183|       |// 4.5   0x0405
  184|       |// 5.0   0x0500
  185|       |// 12.1  0x0C01
  186|       |#if defined(__IBMCPP__) || defined(__xlc__) || defined(__ibmxl__)
  187|       |#define EIGEN_COMP_IBM __xlC__
  188|       |#else
  189|       |#define EIGEN_COMP_IBM 0
  190|       |#endif
  191|       |
  192|       |/// \internal EIGEN_COMP_PGI set to PGI version if the compiler is Portland Group Compiler
  193|       |#if defined(__PGI)
  194|       |#define EIGEN_COMP_PGI (__PGIC__ * 100 + __PGIC_MINOR__)
  195|       |#else
  196|       |#define EIGEN_COMP_PGI 0
  197|       |#endif
  198|       |
  199|       |/// \internal EIGEN_COMP_NVHPC set to NVHPC version if the compiler is nvc++
  200|       |#if defined(__NVCOMPILER)
  201|       |#define EIGEN_COMP_NVHPC (__NVCOMPILER_MAJOR__ * 100 + __NVCOMPILER_MINOR__)
  202|       |#else
  203|       |#define EIGEN_COMP_NVHPC 0
  204|       |#endif
  205|       |
  206|       |/// \internal EIGEN_COMP_ARM set to 1 if the compiler is ARM Compiler
  207|       |#if defined(__CC_ARM) || defined(__ARMCC_VERSION)
  208|       |#define EIGEN_COMP_ARM 1
  209|       |#else
  210|       |#define EIGEN_COMP_ARM 0
  211|       |#endif
  212|       |
  213|       |/// \internal EIGEN_COMP_EMSCRIPTEN set to 1 if the compiler is Emscripten Compiler
  214|       |#if defined(__EMSCRIPTEN__)
  215|       |#define EIGEN_COMP_EMSCRIPTEN 1
  216|       |#else
  217|       |#define EIGEN_COMP_EMSCRIPTEN 0
  218|       |#endif
  219|       |
  220|       |/// \internal EIGEN_COMP_FCC set to FCC version if the compiler is Fujitsu Compiler (traditional mode)
  221|       |/// \note The Fujitsu C/C++ compiler uses the traditional mode based
  222|       |/// on EDG g++ 6.1 by default or if invoked with the -Nnoclang flag
  223|       |#if defined(__FUJITSU)
  224|       |#define EIGEN_COMP_FCC (__FCC_major__ * 100 + __FCC_minor__ * 10 + __FCC_patchlevel__)
  225|       |#else
  226|       |#define EIGEN_COMP_FCC 0
  227|       |#endif
  228|       |
  229|       |/// \internal EIGEN_COMP_CLANGFCC set to FCC version if the compiler is Fujitsu Compiler (Clang mode)
  230|       |/// \note The Fujitsu C/C++ compiler uses the non-traditional mode
  231|       |/// based on Clang 7.1.0 if invoked with the -Nclang flag
  232|       |#if defined(__CLANG_FUJITSU)
  233|       |#define EIGEN_COMP_CLANGFCC (__FCC_major__ * 100 + __FCC_minor__ * 10 + __FCC_patchlevel__)
  234|       |#else
  235|       |#define EIGEN_COMP_CLANGFCC 0
  236|       |#endif
  237|       |
  238|       |/// \internal EIGEN_COMP_CPE set to CPE version if the compiler is HPE Cray Compiler (GCC based)
  239|       |/// \note This is the SVE-enabled C/C++ compiler from the HPE Cray
  240|       |/// Programming Environment (CPE) based on Cray GCC 8.1
  241|       |#if defined(_CRAYC) && !defined(__clang__)
  242|       |#define EIGEN_COMP_CPE (_RELEASE_MAJOR * 100 + _RELEASE_MINOR * 10 + _RELEASE_PATCHLEVEL)
  243|       |#else
  244|       |#define EIGEN_COMP_CPE 0
  245|       |#endif
  246|       |
  247|       |/// \internal EIGEN_COMP_CLANGCPE set to CPE version if the compiler is HPE Cray Compiler (Clang based)
  248|       |/// \note This is the C/C++ compiler from the HPE Cray Programming
  249|       |/// Environment (CPE) based on Cray Clang 11.0 without SVE-support
  250|       |#if defined(_CRAYC) && defined(__clang__)
  251|       |#define EIGEN_COMP_CLANGCPE (_RELEASE_MAJOR * 100 + _RELEASE_MINOR * 10 + _RELEASE_PATCHLEVEL)
  252|       |#else
  253|       |#define EIGEN_COMP_CLANGCPE 0
  254|       |#endif
  255|       |
  256|       |/// \internal EIGEN_COMP_LCC set to 1 if the compiler is MCST-LCC (MCST eLbrus Compiler Collection)
  257|       |#if defined(__LCC__) && defined(__MCST__)
  258|       |#define EIGEN_COMP_LCC (__LCC__ * 100 + __LCC_MINOR__)
  259|       |#else
  260|       |#define EIGEN_COMP_LCC 0
  261|       |#endif
  262|       |
  263|       |/// \internal EIGEN_COMP_GNUC_STRICT set to 1 if the compiler is really GCC and not a compatible compiler (e.g., ICC,
  264|       |/// clang, mingw, etc.)
  265|       |#if EIGEN_COMP_GNUC &&                                                                                      \
  266|       |    !(EIGEN_COMP_CLANG || EIGEN_COMP_ICC || EIGEN_COMP_CLANGICC || EIGEN_COMP_MINGW || EIGEN_COMP_PGI ||    \
  267|       |      EIGEN_COMP_IBM || EIGEN_COMP_ARM || EIGEN_COMP_EMSCRIPTEN || EIGEN_COMP_FCC || EIGEN_COMP_CLANGFCC || \
  268|       |      EIGEN_COMP_CPE || EIGEN_COMP_CLANGCPE || EIGEN_COMP_LCC)
  269|       |#define EIGEN_COMP_GNUC_STRICT 1
  270|       |#else
  271|       |#define EIGEN_COMP_GNUC_STRICT 0
  272|       |#endif
  273|       |
  274|       |// GCC, and compilers that pretend to be it, have different version schemes, so this only makes sense to use with the
  275|       |// real GCC.
  276|       |#if EIGEN_COMP_GNUC_STRICT
  277|       |#define EIGEN_GNUC_STRICT_AT_LEAST(x, y, z)                   \
  278|       |  ((__GNUC__ > x) || (__GNUC__ == x && __GNUC_MINOR__ > y) || \
  279|       |   (__GNUC__ == x && __GNUC_MINOR__ == y && __GNUC_PATCHLEVEL__ >= z))
  280|       |#define EIGEN_GNUC_STRICT_LESS_THAN(x, y, z)                  \
  281|       |  ((__GNUC__ < x) || (__GNUC__ == x && __GNUC_MINOR__ < y) || \
  282|       |   (__GNUC__ == x && __GNUC_MINOR__ == y && __GNUC_PATCHLEVEL__ < z))
  283|       |#else
  284|       |#define EIGEN_GNUC_STRICT_AT_LEAST(x, y, z) 0
  285|       |#define EIGEN_GNUC_STRICT_LESS_THAN(x, y, z) 0
  286|       |#endif
  287|       |
  288|       |/// \internal EIGEN_COMP_CLANG_STRICT set to 1 if the compiler is really Clang and not a compatible compiler (e.g.,
  289|       |/// AppleClang, etc.)
  290|       |#if EIGEN_COMP_CLANG && !(EIGEN_COMP_CLANGAPPLE || EIGEN_COMP_CLANGICC || EIGEN_COMP_CLANGFCC || EIGEN_COMP_CLANGCPE)
  291|       |#define EIGEN_COMP_CLANG_STRICT 1
  292|       |#else
  293|       |#define EIGEN_COMP_CLANG_STRICT 0
  294|       |#endif
  295|       |
  296|       |// Clang, and compilers forked from it, have different version schemes, so this only makes sense to use with the real
  297|       |// Clang.
  298|       |#if EIGEN_COMP_CLANG_STRICT
  299|       |#define EIGEN_CLANG_STRICT_AT_LEAST(x, y, z)                                 \
  300|       |  ((__clang_major__ > x) || (__clang_major__ == x && __clang_minor__ > y) || \
  301|       |   (__clang_major__ == x && __clang_minor__ == y && __clang_patchlevel__ >= z))
  302|       |#define EIGEN_CLANG_STRICT_LESS_THAN(x, y, z)                                \
  303|       |  ((__clang_major__ < x) || (__clang_major__ == x && __clang_minor__ < y) || \
  304|       |   (__clang_major__ == x && __clang_minor__ == y && __clang_patchlevel__ < z))
  305|       |#else
  306|       |#define EIGEN_CLANG_STRICT_AT_LEAST(x, y, z) 0
  307|       |#define EIGEN_CLANG_STRICT_LESS_THAN(x, y, z) 0
  308|       |#endif
  309|       |
  310|       |//------------------------------------------------------------------------------------------
  311|       |// Architecture identification, EIGEN_ARCH_*
  312|       |//------------------------------------------------------------------------------------------
  313|       |
  314|       |#if defined(__x86_64__) || (defined(_M_X64) && !defined(_M_ARM64EC)) || defined(__amd64)
  315|       |#define EIGEN_ARCH_x86_64 1
  316|       |#else
  317|       |#define EIGEN_ARCH_x86_64 0
  318|       |#endif
  319|       |
  320|       |#if defined(__i386__) || defined(_M_IX86) || defined(_X86_) || defined(__i386)
  321|       |#define EIGEN_ARCH_i386 1
  322|       |#else
  323|       |#define EIGEN_ARCH_i386 0
  324|       |#endif
  325|       |
  326|       |#if EIGEN_ARCH_x86_64 || EIGEN_ARCH_i386
  327|       |#define EIGEN_ARCH_i386_OR_x86_64 1
  328|       |#else
  329|       |#define EIGEN_ARCH_i386_OR_x86_64 0
  330|       |#endif
  331|       |
  332|       |/// \internal EIGEN_ARCH_ARM set to 1 if the architecture is ARM
  333|       |#if defined(__arm__)
  334|       |#define EIGEN_ARCH_ARM 1
  335|       |#else
  336|       |#define EIGEN_ARCH_ARM 0
  337|       |#endif
  338|       |
  339|       |/// \internal EIGEN_ARCH_ARM64 set to 1 if the architecture is ARM64
  340|       |#if defined(__aarch64__) || defined(_M_ARM64) || defined(_M_ARM64EC)
  341|       |#define EIGEN_ARCH_ARM64 1
  342|       |#else
  343|       |#define EIGEN_ARCH_ARM64 0
  344|       |#endif
  345|       |
  346|       |/// \internal EIGEN_ARCH_ARM_OR_ARM64 set to 1 if the architecture is ARM or ARM64
  347|       |#if EIGEN_ARCH_ARM || EIGEN_ARCH_ARM64
  348|       |#define EIGEN_ARCH_ARM_OR_ARM64 1
  349|       |#else
  350|       |#define EIGEN_ARCH_ARM_OR_ARM64 0
  351|       |#endif
  352|       |
  353|       |/// \internal EIGEN_ARCH_ARMV8 set to 1 if the architecture is armv8 or greater.
  354|       |#if EIGEN_ARCH_ARM_OR_ARM64 && defined(__ARM_ARCH) && __ARM_ARCH >= 8
  355|       |#define EIGEN_ARCH_ARMV8 1
  356|       |#else
  357|       |#define EIGEN_ARCH_ARMV8 0
  358|       |#endif
  359|       |
  360|       |/// \internal EIGEN_HAS_ARM64_FP16 set to 1 if the architecture provides an IEEE
  361|       |/// compliant Arm fp16 type
  362|       |#if EIGEN_ARCH_ARM_OR_ARM64
  363|       |#ifndef EIGEN_HAS_ARM64_FP16
  364|       |#if defined(__ARM_FP16_FORMAT_IEEE)
  365|       |#define EIGEN_HAS_ARM64_FP16 1
  366|       |#else
  367|       |#define EIGEN_HAS_ARM64_FP16 0
  368|       |#endif
  369|       |#endif
  370|       |#endif
  371|       |
  372|       |/// \internal EIGEN_ARCH_MIPS set to 1 if the architecture is MIPS
  373|       |#if defined(__mips__) || defined(__mips)
  374|       |#define EIGEN_ARCH_MIPS 1
  375|       |#else
  376|       |#define EIGEN_ARCH_MIPS 0
  377|       |#endif
  378|       |
  379|       |/// \internal EIGEN_ARCH_LOONGARCH64 set to 1 if the architecture is LOONGARCH64
  380|       |#if defined(__loongarch64)
  381|       |#define EIGEN_ARCH_LOONGARCH64 1
  382|       |#else
  383|       |#define EIGEN_ARCH_LOONGARCH64 0
  384|       |#endif
  385|       |
  386|       |/// \internal EIGEN_ARCH_SPARC set to 1 if the architecture is SPARC
  387|       |#if defined(__sparc__) || defined(__sparc)
  388|       |#define EIGEN_ARCH_SPARC 1
  389|       |#else
  390|       |#define EIGEN_ARCH_SPARC 0
  391|       |#endif
  392|       |
  393|       |/// \internal EIGEN_ARCH_IA64 set to 1 if the architecture is Intel Itanium
  394|       |#if defined(__ia64__)
  395|       |#define EIGEN_ARCH_IA64 1
  396|       |#else
  397|       |#define EIGEN_ARCH_IA64 0
  398|       |#endif
  399|       |
  400|       |/// \internal EIGEN_ARCH_PPC set to 1 if the architecture is PowerPC
  401|       |#if defined(__powerpc__) || defined(__ppc__) || defined(_M_PPC) || defined(__POWERPC__)
  402|       |#define EIGEN_ARCH_PPC 1
  403|       |#else
  404|       |#define EIGEN_ARCH_PPC 0
  405|       |#endif
  406|       |
  407|       |//------------------------------------------------------------------------------------------
  408|       |// Operating system identification, EIGEN_OS_*
  409|       |//------------------------------------------------------------------------------------------
  410|       |
  411|       |/// \internal EIGEN_OS_UNIX set to 1 if the OS is a unix variant
  412|       |#if defined(__unix__) || defined(__unix)
  413|       |#define EIGEN_OS_UNIX 1
  414|       |#else
  415|       |#define EIGEN_OS_UNIX 0
  416|       |#endif
  417|       |
  418|       |/// \internal EIGEN_OS_LINUX set to 1 if the OS is based on Linux kernel
  419|       |#if defined(__linux__)
  420|       |#define EIGEN_OS_LINUX 1
  421|       |#else
  422|       |#define EIGEN_OS_LINUX 0
  423|       |#endif
  424|       |
  425|       |/// \internal EIGEN_OS_ANDROID set to 1 if the OS is Android
  426|       |// note: ANDROID is defined when using ndk_build, __ANDROID__ is defined when using a standalone toolchain.
  427|       |#if defined(__ANDROID__) || defined(ANDROID)
  428|       |#define EIGEN_OS_ANDROID 1
  429|       |#else
  430|       |#define EIGEN_OS_ANDROID 0
  431|       |#endif
  432|       |
  433|       |/// \internal EIGEN_OS_GNULINUX set to 1 if the OS is GNU Linux and not Linux-based OS (e.g., not android)
  434|       |#if defined(__gnu_linux__) && !(EIGEN_OS_ANDROID)
  435|       |#define EIGEN_OS_GNULINUX 1
  436|       |#else
  437|       |#define EIGEN_OS_GNULINUX 0
  438|       |#endif
  439|       |
  440|       |/// \internal EIGEN_OS_BSD set to 1 if the OS is a BSD variant
  441|       |#if defined(__FreeBSD__) || defined(__NetBSD__) || defined(__OpenBSD__) || defined(__bsdi__) || defined(__DragonFly__)
  442|       |#define EIGEN_OS_BSD 1
  443|       |#else
  444|       |#define EIGEN_OS_BSD 0
  445|       |#endif
  446|       |
  447|       |/// \internal EIGEN_OS_MAC set to 1 if the OS is MacOS
  448|       |#if defined(__APPLE__)
  449|       |#define EIGEN_OS_MAC 1
  450|       |#else
  451|       |#define EIGEN_OS_MAC 0
  452|       |#endif
  453|       |
  454|       |/// \internal EIGEN_OS_QNX set to 1 if the OS is QNX
  455|       |#if defined(__QNX__)
  456|       |#define EIGEN_OS_QNX 1
  457|       |#else
  458|       |#define EIGEN_OS_QNX 0
  459|       |#endif
  460|       |
  461|       |/// \internal EIGEN_OS_WIN set to 1 if the OS is Windows based
  462|       |#if defined(_WIN32)
  463|       |#define EIGEN_OS_WIN 1
  464|       |#else
  465|       |#define EIGEN_OS_WIN 0
  466|       |#endif
  467|       |
  468|       |/// \internal EIGEN_OS_WIN64 set to 1 if the OS is Windows 64bits
  469|       |#if defined(_WIN64)
  470|       |#define EIGEN_OS_WIN64 1
  471|       |#else
  472|       |#define EIGEN_OS_WIN64 0
  473|       |#endif
  474|       |
  475|       |/// \internal EIGEN_OS_WINCE set to 1 if the OS is Windows CE
  476|       |#if defined(_WIN32_WCE)
  477|       |#define EIGEN_OS_WINCE 1
  478|       |#else
  479|       |#define EIGEN_OS_WINCE 0
  480|       |#endif
  481|       |
  482|       |/// \internal EIGEN_OS_CYGWIN set to 1 if the OS is Windows/Cygwin
  483|       |#if defined(__CYGWIN__)
  484|       |#define EIGEN_OS_CYGWIN 1
  485|       |#else
  486|       |#define EIGEN_OS_CYGWIN 0
  487|       |#endif
  488|       |
  489|       |/// \internal EIGEN_OS_WIN_STRICT set to 1 if the OS is really Windows and not some variants
  490|       |#if EIGEN_OS_WIN && !(EIGEN_OS_WINCE || EIGEN_OS_CYGWIN)
  491|       |#define EIGEN_OS_WIN_STRICT 1
  492|       |#else
  493|       |#define EIGEN_OS_WIN_STRICT 0
  494|       |#endif
  495|       |
  496|       |/// \internal EIGEN_OS_SUN set to __SUNPRO_C if the OS is SUN
  497|       |// compiler  solaris   __SUNPRO_C
  498|       |// version   studio
  499|       |// 5.7       10        0x570
  500|       |// 5.8       11        0x580
  501|       |// 5.9       12        0x590
  502|       |// 5.10	     12.1      0x5100
  503|       |// 5.11	     12.2      0x5110
  504|       |// 5.12	     12.3      0x5120
  505|       |#if (defined(sun) || defined(__sun)) && !(defined(__SVR4) || defined(__svr4__))
  506|       |#define EIGEN_OS_SUN __SUNPRO_C
  507|       |#else
  508|       |#define EIGEN_OS_SUN 0
  509|       |#endif
  510|       |
  511|       |/// \internal EIGEN_OS_SOLARIS set to 1 if the OS is Solaris
  512|       |#if (defined(sun) || defined(__sun)) && (defined(__SVR4) || defined(__svr4__))
  513|       |#define EIGEN_OS_SOLARIS 1
  514|       |#else
  515|       |#define EIGEN_OS_SOLARIS 0
  516|       |#endif
  517|       |
  518|       |//------------------------------------------------------------------------------------------
  519|       |// Detect GPU compilers and architectures
  520|       |//------------------------------------------------------------------------------------------
  521|       |
  522|       |// NVCC is not supported as the target platform for HIPCC
  523|       |// Note that this also makes EIGEN_CUDACC and EIGEN_HIPCC mutually exclusive
  524|       |#if defined(__NVCC__) && defined(__HIPCC__)
  525|       |#error "NVCC as the target platform for HIPCC is currently not supported."
  526|       |#endif
  527|       |
  528|       |#if defined(__CUDACC__) && !defined(EIGEN_NO_CUDA) && !defined(__SYCL_DEVICE_ONLY__)
  529|       |// Means the compiler is either nvcc or clang with CUDA enabled
  530|       |#define EIGEN_CUDACC __CUDACC__
  531|       |#endif
  532|       |
  533|       |#if defined(__CUDA_ARCH__) && !defined(EIGEN_NO_CUDA) && !defined(__SYCL_DEVICE_ONLY__)
  534|       |// Means we are generating code for the device
  535|       |#define EIGEN_CUDA_ARCH __CUDA_ARCH__
  536|       |#endif
  537|       |
  538|       |#if defined(EIGEN_CUDACC)
  539|       |#include <cuda.h>
  540|       |#define EIGEN_CUDA_SDK_VER (CUDA_VERSION * 10)
  541|       |#else
  542|       |#define EIGEN_CUDA_SDK_VER 0
  543|       |#endif
  544|       |
  545|       |#if defined(__HIPCC__) && !defined(EIGEN_NO_HIP) && !defined(__SYCL_DEVICE_ONLY__)
  546|       |// Means the compiler is HIPCC (analogous to EIGEN_CUDACC, but for HIP)
  547|       |#define EIGEN_HIPCC __HIPCC__
  548|       |
  549|       |// We need to include hip_runtime.h here because it pulls in
  550|       |// ++ hip_common.h which contains the define for  __HIP_DEVICE_COMPILE__
  551|       |// ++ host_defines.h which contains the defines for the __host__ and __device__ macros
  552|       |#include <hip/hip_runtime.h>
  553|       |
  554|       |#if defined(__HIP_DEVICE_COMPILE__) && !defined(__SYCL_DEVICE_ONLY__)
  555|       |// analogous to EIGEN_CUDA_ARCH, but for HIP
  556|       |#define EIGEN_HIP_DEVICE_COMPILE __HIP_DEVICE_COMPILE__
  557|       |#endif
  558|       |
  559|       |// For HIP (ROCm 3.5 and higher), we need to explicitly set the launch_bounds attribute
  560|       |// value to 1024. The compiler assigns a default value of 256 when the attribute is not
  561|       |// specified. This results in failures on the HIP platform, for cases when a GPU kernel
  562|       |// without an explicit launch_bounds attribute is called with a threads_per_block value
  563|       |// greater than 256.
  564|       |//
  565|       |// This is a regression in functioanlity and is expected to be fixed within the next
  566|       |// couple of ROCm releases (compiler will go back to using 1024 value as the default)
  567|       |//
  568|       |// In the meantime, we will use a "only enabled for HIP" macro to set the launch_bounds
  569|       |// attribute.
  570|       |
  571|       |#define EIGEN_HIP_LAUNCH_BOUNDS_1024 __launch_bounds__(1024)
  572|       |
  573|       |#endif
  574|       |
  575|       |#if !defined(EIGEN_HIP_LAUNCH_BOUNDS_1024)
  576|       |#define EIGEN_HIP_LAUNCH_BOUNDS_1024
  577|       |#endif  // !defined(EIGEN_HIP_LAUNCH_BOUNDS_1024)
  578|       |
  579|       |// Unify CUDA/HIPCC
  580|       |
  581|       |#if defined(EIGEN_CUDACC) || defined(EIGEN_HIPCC)
  582|       |//
  583|       |// If either EIGEN_CUDACC or EIGEN_HIPCC is defined, then define EIGEN_GPUCC
  584|       |//
  585|       |#define EIGEN_GPUCC
  586|       |//
  587|       |// EIGEN_HIPCC implies the HIP compiler and is used to tweak Eigen code for use in HIP kernels
  588|       |// EIGEN_CUDACC implies the CUDA compiler and is used to tweak Eigen code for use in CUDA kernels
  589|       |//
  590|       |// In most cases the same tweaks are required to the Eigen code to enable in both the HIP and CUDA kernels.
  591|       |// For those cases, the corresponding code should be guarded with
  592|       |//      #if defined(EIGEN_GPUCC)
  593|       |// instead of
  594|       |//      #if defined(EIGEN_CUDACC) || defined(EIGEN_HIPCC)
  595|       |//
  596|       |// For cases where the tweak is specific to HIP, the code should be guarded with
  597|       |//      #if defined(EIGEN_HIPCC)
  598|       |//
  599|       |// For cases where the tweak is specific to CUDA, the code should be guarded with
  600|       |//      #if defined(EIGEN_CUDACC)
  601|       |//
  602|       |#endif
  603|       |
  604|       |#if defined(EIGEN_CUDA_ARCH) || defined(EIGEN_HIP_DEVICE_COMPILE)
  605|       |//
  606|       |// If either EIGEN_CUDA_ARCH or EIGEN_HIP_DEVICE_COMPILE is defined, then define EIGEN_GPU_COMPILE_PHASE
  607|       |//
  608|       |#define EIGEN_GPU_COMPILE_PHASE
  609|       |//
  610|       |// GPU compilers (HIPCC, NVCC) typically do two passes over the source code,
  611|       |//   + one to compile the source for the "host" (ie CPU)
  612|       |//   + another to compile the source for the "device" (ie. GPU)
  613|       |//
  614|       |// Code that needs to enabled only during the either the "host" or "device" compilation phase
  615|       |// needs to be guarded with a macro that indicates the current compilation phase
  616|       |//
  617|       |// EIGEN_HIP_DEVICE_COMPILE implies the device compilation phase in HIP
  618|       |// EIGEN_CUDA_ARCH implies the device compilation phase in CUDA
  619|       |//
  620|       |// In most cases, the "host" / "device" specific code is the same for both HIP and CUDA
  621|       |// For those cases, the code should be guarded with
  622|       |//       #if defined(EIGEN_GPU_COMPILE_PHASE)
  623|       |// instead of
  624|       |//       #if defined(EIGEN_CUDA_ARCH) || defined(EIGEN_HIP_DEVICE_COMPILE)
  625|       |//
  626|       |// For cases where the tweak is specific to HIP, the code should be guarded with
  627|       |//      #if defined(EIGEN_HIP_DEVICE_COMPILE)
  628|       |//
  629|       |// For cases where the tweak is specific to CUDA, the code should be guarded with
  630|       |//      #if defined(EIGEN_CUDA_ARCH)
  631|       |//
  632|       |#endif
  633|       |
  634|       |/// \internal EIGEN_HAS_ARM64_FP16_VECTOR_ARITHMETIC set to 1 if the architecture
  635|       |/// supports Neon vector intrinsics for fp16.
  636|       |#if EIGEN_ARCH_ARM_OR_ARM64
  637|       |#ifndef EIGEN_HAS_ARM64_FP16_VECTOR_ARITHMETIC
  638|       |// Clang only supports FP16 on aarch64, and not all intrinsics are available
  639|       |// on A32 anyways even in GCC (e.g. vdiv_f16, vsqrt_f16).
  640|       |#if EIGEN_ARCH_ARM64 && defined(__ARM_FEATURE_FP16_VECTOR_ARITHMETIC) && !defined(EIGEN_GPU_COMPILE_PHASE)
  641|       |#define EIGEN_HAS_ARM64_FP16_VECTOR_ARITHMETIC 1
  642|       |#else
  643|       |#define EIGEN_HAS_ARM64_FP16_VECTOR_ARITHMETIC 0
  644|       |#endif
  645|       |#endif
  646|       |#endif
  647|       |
  648|       |/// \internal EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC set to 1 if the architecture
  649|       |/// supports Neon scalar intrinsics for fp16.
  650|       |#if EIGEN_ARCH_ARM_OR_ARM64
  651|       |#ifndef EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC
  652|       |// Clang only supports FP16 on aarch64, and not all intrinsics are available
  653|       |// on A32 anyways, even in GCC (e.g. vceqh_f16).
  654|       |#if EIGEN_ARCH_ARM64 && defined(__ARM_FEATURE_FP16_SCALAR_ARITHMETIC) && !defined(EIGEN_GPU_COMPILE_PHASE)
  655|       |#define EIGEN_HAS_ARM64_FP16_SCALAR_ARITHMETIC 1
  656|       |#endif
  657|       |#endif
  658|       |#endif
  659|       |
  660|       |#if defined(EIGEN_USE_SYCL) && defined(__SYCL_DEVICE_ONLY__)
  661|       |// EIGEN_USE_SYCL is a user-defined macro while __SYCL_DEVICE_ONLY__ is a compiler-defined macro.
  662|       |// In most cases we want to check if both macros are defined which can be done using the define below.
  663|       |#define SYCL_DEVICE_ONLY
  664|       |#endif
  665|       |
  666|       |//------------------------------------------------------------------------------------------
  667|       |// Detect Compiler/Architecture/OS specific features
  668|       |//------------------------------------------------------------------------------------------
  669|       |
  670|       |// Cross compiler wrapper around LLVM's __has_builtin
  671|       |#ifdef __has_builtin
  672|       |#define EIGEN_HAS_BUILTIN(x) __has_builtin(x)
  673|       |#else
  674|       |#define EIGEN_HAS_BUILTIN(x) 0
  675|       |#endif
  676|       |
  677|       |// A Clang feature extension to determine compiler features.
  678|       |// We use it to determine 'cxx_rvalue_references'
  679|       |#ifndef __has_feature
  680|       |#define __has_feature(x) 0
  681|       |#endif
  682|       |
  683|       |// The macro EIGEN_CPLUSPLUS is a replacement for __cplusplus/_MSVC_LANG that
  684|       |// works for both platforms, indicating the C++ standard version number.
  685|       |//
  686|       |// With MSVC, without defining /Zc:__cplusplus, the __cplusplus macro will
  687|       |// report 199711L regardless of the language standard specified via /std.
  688|       |// We need to rely on _MSVC_LANG instead, which is only available after
  689|       |// VS2015.3.
  690|       |#if EIGEN_COMP_MSVC_LANG > 0
  691|       |#define EIGEN_CPLUSPLUS EIGEN_COMP_MSVC_LANG
  692|       |#elif EIGEN_COMP_MSVC >= 1900
  693|       |#define EIGEN_CPLUSPLUS 201103L
  694|       |#elif defined(__cplusplus)
  695|       |#define EIGEN_CPLUSPLUS __cplusplus
  696|       |#else
  697|       |#define EIGEN_CPLUSPLUS 0
  698|       |#endif
  699|       |
  700|       |// The macro EIGEN_COMP_CXXVER defines the c++ version expected by the compiler.
  701|       |// For instance, if compiling with gcc and -std=c++17, then EIGEN_COMP_CXXVER
  702|       |// is defined to 17.
  703|       |#if EIGEN_CPLUSPLUS >= 202002L
  704|       |#define EIGEN_COMP_CXXVER 20
  705|       |#elif EIGEN_CPLUSPLUS >= 201703L
  706|       |#define EIGEN_COMP_CXXVER 17
  707|       |#elif EIGEN_CPLUSPLUS >= 201402L
  708|       |#define EIGEN_COMP_CXXVER 14
  709|       |#elif EIGEN_CPLUSPLUS >= 201103L
  710|       |#define EIGEN_COMP_CXXVER 11
  711|       |#else
  712|       |#define EIGEN_COMP_CXXVER 03
  713|       |#endif
  714|       |
  715|       |// The macros EIGEN_HAS_CXX?? defines a rough estimate of available c++ features
  716|       |// but in practice we should not rely on them but rather on the availability of
  717|       |// individual features as defined later.
  718|       |// This is why there is no EIGEN_HAS_CXX17.
  719|       |#if EIGEN_MAX_CPP_VER < 14 || EIGEN_COMP_CXXVER < 14 || (EIGEN_COMP_MSVC && EIGEN_COMP_MSVC < 1900) || \
  720|       |    (EIGEN_COMP_ICC && EIGEN_COMP_ICC < 1500) || (EIGEN_COMP_NVCC && EIGEN_COMP_NVCC < 80000) ||       \
  721|       |    (EIGEN_COMP_CLANG_STRICT && EIGEN_COMP_CLANG < 390) ||                                             \
  722|       |    (EIGEN_COMP_CLANGAPPLE && EIGEN_COMP_CLANGAPPLE < 9000000) || (EIGEN_COMP_GNUC_STRICT && EIGEN_COMP_GNUC < 510)
  723|       |#error Eigen requires at least c++14 support.
  724|       |#endif
  725|       |
  726|       |// Does the compiler support C99?
  727|       |// Need to include <cmath> to make sure _GLIBCXX_USE_C99 gets defined
  728|       |#include <cmath>
  729|       |#ifndef EIGEN_HAS_C99_MATH
  730|       |#if ((defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901)) ||                                          \
  731|       |     (defined(__GNUC__) && defined(_GLIBCXX_USE_C99)) || (defined(_LIBCPP_VERSION) && !defined(_MSC_VER)) || \
  732|       |     (EIGEN_COMP_MSVC) || defined(SYCL_DEVICE_ONLY))
  733|       |#define EIGEN_HAS_C99_MATH 1
  734|       |#else
  735|       |#define EIGEN_HAS_C99_MATH 0
  736|       |#endif
  737|       |#endif
  738|       |
  739|       |// Does the compiler support std::hash?
  740|       |#ifndef EIGEN_HAS_STD_HASH
  741|       |// The std::hash struct is defined in C++11 but is not labelled as a __device__
  742|       |// function and is not constexpr, so cannot be used on device.
  743|       |#if !defined(EIGEN_GPU_COMPILE_PHASE)
  744|       |#define EIGEN_HAS_STD_HASH 1
  745|       |#else
  746|       |#define EIGEN_HAS_STD_HASH 0
  747|       |#endif
  748|       |#endif  // EIGEN_HAS_STD_HASH
  749|       |
  750|       |#ifndef EIGEN_HAS_STD_INVOKE_RESULT
  751|       |#if EIGEN_MAX_CPP_VER >= 17 && EIGEN_COMP_CXXVER >= 17
  752|       |#define EIGEN_HAS_STD_INVOKE_RESULT 1
  753|       |#else
  754|       |#define EIGEN_HAS_STD_INVOKE_RESULT 0
  755|       |#endif
  756|       |#endif
  757|       |
  758|       |#define EIGEN_CONSTEXPR constexpr
  759|       |
  760|       |// NOTE: the required Apple's clang version is very conservative
  761|       |//       and it could be that XCode 9 works just fine.
  762|       |// NOTE: the MSVC version is based on https://en.cppreference.com/w/cpp/compiler_support
  763|       |//       and not tested.
  764|       |// NOTE: Intel C++ Compiler Classic (icc) Version 19.0 and later supports dynamic allocation
  765|       |//       for over-aligned data, but not in a manner that is compatible with Eigen.
  766|       |//       See https://gitlab.com/libeigen/eigen/-/issues/2575
  767|       |#ifndef EIGEN_HAS_CXX17_OVERALIGN
  768|       |#if EIGEN_MAX_CPP_VER >= 17 && EIGEN_COMP_CXXVER >= 17 &&                                                            \
  769|       |    ((EIGEN_COMP_MSVC >= 1912) || (EIGEN_GNUC_STRICT_AT_LEAST(7, 0, 0)) || (EIGEN_CLANG_STRICT_AT_LEAST(5, 0, 0)) || \
  770|       |     (EIGEN_COMP_CLANGAPPLE && EIGEN_COMP_CLANGAPPLE >= 10000000)) &&                                                \
  771|       |    !EIGEN_COMP_ICC
  772|       |#define EIGEN_HAS_CXX17_OVERALIGN 1
  773|       |#else
  774|       |#define EIGEN_HAS_CXX17_OVERALIGN 0
  775|       |#endif
  776|       |#endif
  777|       |
  778|       |#if defined(EIGEN_CUDACC)
  779|       |// While available already with c++11, this is useful mostly starting with c++14 and relaxed constexpr rules
  780|       |#if defined(__NVCC__)
  781|       |// nvcc considers constexpr functions as __host__ __device__ with the option --expt-relaxed-constexpr
  782|       |#ifdef __CUDACC_RELAXED_CONSTEXPR__
  783|       |#define EIGEN_CONSTEXPR_ARE_DEVICE_FUNC
  784|       |#endif
  785|       |#elif defined(__clang__) && defined(__CUDA__) && __has_feature(cxx_relaxed_constexpr)
  786|       |// clang++ always considers constexpr functions as implicitly __host__ __device__
  787|       |#define EIGEN_CONSTEXPR_ARE_DEVICE_FUNC
  788|       |#endif
  789|       |#endif
  790|       |
  791|       |// Does the compiler support the __int128 and __uint128_t extensions for 128-bit
  792|       |// integer arithmetic?
  793|       |//
  794|       |// Clang and GCC define __SIZEOF_INT128__ when these extensions are supported,
  795|       |// but we avoid using them in certain cases:
  796|       |//
  797|       |// * Building using Clang for Windows, where the Clang runtime library has
  798|       |//   128-bit support only on LP64 architectures, but Windows is LLP64.
  799|       |#ifndef EIGEN_HAS_BUILTIN_INT128
  800|       |#if defined(__SIZEOF_INT128__) && !(EIGEN_OS_WIN && EIGEN_COMP_CLANG)
  801|       |#define EIGEN_HAS_BUILTIN_INT128 1
  802|       |#else
  803|       |#define EIGEN_HAS_BUILTIN_INT128 0
  804|       |#endif
  805|       |#endif
  806|       |
  807|       |//------------------------------------------------------------------------------------------
  808|       |// Preprocessor programming helpers
  809|       |//------------------------------------------------------------------------------------------
  810|       |
  811|       |// This macro can be used to prevent from macro expansion, e.g.:
  812|       |//   std::max EIGEN_NOT_A_MACRO(a,b)
  813|       |#define EIGEN_NOT_A_MACRO
  814|       |
  815|       |#define EIGEN_DEBUG_VAR(x) std::cerr << #x << " = " << x << std::endl;
  816|       |
  817|       |// concatenate two tokens
  818|       |#define EIGEN_CAT2(a, b) a##b
  819|       |#define EIGEN_CAT(a, b) EIGEN_CAT2(a, b)
  820|       |
  821|       |#define EIGEN_COMMA ,
  822|       |
  823|       |// convert a token to a string
  824|      1|#define EIGEN_MAKESTRING2(a) #a
  825|      1|#define EIGEN_MAKESTRING(a) EIGEN_MAKESTRING2(a)
  826|       |
  827|       |// EIGEN_STRONG_INLINE is a stronger version of the inline, using __forceinline on MSVC,
  828|       |// but it still doesn't use GCC's always_inline. This is useful in (common) situations where MSVC needs forceinline
  829|       |// but GCC is still doing fine with just inline.
  830|       |#ifndef EIGEN_STRONG_INLINE
  831|       |#if (EIGEN_COMP_MSVC || EIGEN_COMP_ICC) && !defined(EIGEN_GPUCC)
  832|       |#define EIGEN_STRONG_INLINE __forceinline
  833|       |#else
  834|       |#define EIGEN_STRONG_INLINE inline
  835|       |#endif
  836|       |#endif
  837|       |
  838|       |// EIGEN_ALWAYS_INLINE is the strongest, it has the effect of making the function inline and adding every possible
  839|       |// attribute to maximize inlining. This should only be used when really necessary: in particular,
  840|       |// it uses __attribute__((always_inline)) on GCC, which most of the time is useless and can severely harm compile times.
  841|       |// FIXME with the always_inline attribute,
  842|       |#if EIGEN_COMP_GNUC && !defined(SYCL_DEVICE_ONLY)
  843|       |#define EIGEN_ALWAYS_INLINE __attribute__((always_inline)) inline
  844|       |#else
  845|       |#define EIGEN_ALWAYS_INLINE EIGEN_STRONG_INLINE
  846|       |#endif
  847|       |
  848|       |#if EIGEN_COMP_GNUC
  849|       |#define EIGEN_DONT_INLINE __attribute__((noinline))
  850|       |#elif EIGEN_COMP_MSVC
  851|       |#define EIGEN_DONT_INLINE __declspec(noinline)
  852|       |#else
  853|       |#define EIGEN_DONT_INLINE
  854|       |#endif
  855|       |
  856|       |#if EIGEN_COMP_GNUC
  857|       |#define EIGEN_PERMISSIVE_EXPR __extension__
  858|       |#else
  859|       |#define EIGEN_PERMISSIVE_EXPR
  860|       |#endif
  861|       |
  862|       |// GPU stuff
  863|       |
  864|       |// Disable some features when compiling with GPU compilers (SYCL/HIPCC)
  865|       |#if defined(SYCL_DEVICE_ONLY) || defined(EIGEN_HIP_DEVICE_COMPILE)
  866|       |// Do not try asserts on device code
  867|       |#ifndef EIGEN_NO_DEBUG
  868|       |#define EIGEN_NO_DEBUG
  869|       |#endif
  870|       |
  871|       |#ifdef EIGEN_INTERNAL_DEBUGGING
  872|       |#undef EIGEN_INTERNAL_DEBUGGING
  873|       |#endif
  874|       |#endif
  875|       |
  876|       |// No exceptions on device.
  877|       |#if defined(SYCL_DEVICE_ONLY) || defined(EIGEN_GPU_COMPILE_PHASE)
  878|       |#ifdef EIGEN_EXCEPTIONS
  879|       |#undef EIGEN_EXCEPTIONS
  880|       |#endif
  881|       |#endif
  882|       |
  883|       |#if defined(SYCL_DEVICE_ONLY)
  884|       |#ifndef EIGEN_DONT_VECTORIZE
  885|       |#define EIGEN_DONT_VECTORIZE
  886|       |#endif
  887|       |#define EIGEN_DEVICE_FUNC __attribute__((flatten)) __attribute__((always_inline))
  888|       |// All functions callable from CUDA/HIP code must be qualified with __device__
  889|       |#elif defined(EIGEN_GPUCC)
  890|       |#define EIGEN_DEVICE_FUNC __host__ __device__
  891|       |#else
  892|       |#define EIGEN_DEVICE_FUNC
  893|       |#endif
  894|       |
  895|       |// this macro allows to get rid of linking errors about multiply defined functions.
  896|       |//  - static is not very good because it prevents definitions from different object files to be merged.
  897|       |//           So static causes the resulting linked executable to be bloated with multiple copies of the same function.
  898|       |//  - inline is not perfect either as it unwantedly hints the compiler toward inlining the function.
  899|       |#define EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS EIGEN_DEVICE_FUNC
  900|       |#define EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS EIGEN_DEVICE_FUNC inline
  901|       |
  902|       |#ifdef NDEBUG
  903|       |#ifndef EIGEN_NO_DEBUG
  904|       |#define EIGEN_NO_DEBUG
  905|       |#endif
  906|       |#endif
  907|       |
  908|       |// eigen_assert can be overridden
  909|       |#ifndef eigen_assert
  910|       |#define eigen_assert(x) eigen_plain_assert(x)
  911|       |#endif
  912|       |
  913|       |#ifdef EIGEN_INTERNAL_DEBUGGING
  914|       |#define eigen_internal_assert(x) eigen_assert(x)
  915|       |#else
  916|       |#define eigen_internal_assert(x) ((void)0)
  917|       |#endif
  918|       |
  919|       |#if defined(EIGEN_NO_DEBUG) || (defined(EIGEN_GPU_COMPILE_PHASE) && defined(EIGEN_NO_DEBUG_GPU))
  920|       |#define EIGEN_ONLY_USED_FOR_DEBUG(x) EIGEN_UNUSED_VARIABLE(x)
  921|       |#else
  922|       |#define EIGEN_ONLY_USED_FOR_DEBUG(x)
  923|       |#endif
  924|       |
  925|       |#ifndef EIGEN_NO_DEPRECATED_WARNING
  926|       |#if EIGEN_COMP_GNUC
  927|       |#define EIGEN_DEPRECATED __attribute__((deprecated))
  928|       |#elif EIGEN_COMP_MSVC
  929|       |#define EIGEN_DEPRECATED __declspec(deprecated)
  930|       |#else
  931|       |#define EIGEN_DEPRECATED
  932|       |#endif
  933|       |#else
  934|       |#define EIGEN_DEPRECATED
  935|       |#endif
  936|       |
  937|       |#if EIGEN_COMP_GNUC
  938|       |#define EIGEN_UNUSED __attribute__((unused))
  939|       |#else
  940|       |#define EIGEN_UNUSED
  941|       |#endif
  942|       |
  943|       |#if EIGEN_COMP_GNUC
  944|       |#define EIGEN_PRAGMA(tokens) _Pragma(#tokens)
  945|       |#define EIGEN_DIAGNOSTICS(tokens) EIGEN_PRAGMA(GCC diagnostic tokens)
  946|       |#define EIGEN_DIAGNOSTICS_OFF(msc, gcc) EIGEN_DIAGNOSTICS(gcc)
  947|       |#elif EIGEN_COMP_MSVC
  948|       |#define EIGEN_PRAGMA(tokens) __pragma(tokens)
  949|       |#define EIGEN_DIAGNOSTICS(tokens) EIGEN_PRAGMA(warning(tokens))
  950|       |#define EIGEN_DIAGNOSTICS_OFF(msc, gcc) EIGEN_DIAGNOSTICS(msc)
  951|       |#else
  952|       |#define EIGEN_PRAGMA(tokens)
  953|       |#define EIGEN_DIAGNOSTICS(tokens)
  954|       |#define EIGEN_DIAGNOSTICS_OFF(msc, gcc)
  955|       |#endif
  956|       |
  957|       |#define EIGEN_DISABLE_DEPRECATED_WARNING EIGEN_DIAGNOSTICS_OFF(disable : 4996, ignored "-Wdeprecated-declarations")
  958|       |
  959|       |// Suppresses 'unused variable' warnings.
  960|       |namespace Eigen {
  961|       |namespace internal {
  962|       |template <typename T>
  963|      0|EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE constexpr void ignore_unused_variable(const T&) {}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal22ignore_unused_variableImEEvRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal22ignore_unused_variableIlEEvRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal22ignore_unused_variableIbEEvRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal22ignore_unused_variableIfEEvRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal22ignore_unused_variableIDv4_fEEvRKT_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal22ignore_unused_variableIDv2_dEEvRKT_
  ------------------
  964|       |}  // namespace internal
  965|       |}  // namespace Eigen
  966|       |#define EIGEN_UNUSED_VARIABLE(var) Eigen::internal::ignore_unused_variable(var);
  967|       |
  968|       |#if !defined(EIGEN_ASM_COMMENT)
  969|       |#if EIGEN_COMP_GNUC && (EIGEN_ARCH_i386_OR_x86_64 || EIGEN_ARCH_ARM_OR_ARM64)
  970|       |#define EIGEN_ASM_COMMENT(X) __asm__("#" X)
  971|       |#else
  972|       |#define EIGEN_ASM_COMMENT(X)
  973|       |#endif
  974|       |#endif
  975|       |
  976|       |// Acts as a barrier preventing operations involving `X` from crossing. This
  977|       |// occurs, for example, in the fast rounding trick where a magic constant is
  978|       |// added then subtracted, which is otherwise compiled away with -ffast-math.
  979|       |//
  980|       |// See bug 1674
  981|       |#if defined(EIGEN_GPU_COMPILE_PHASE)
  982|       |#define EIGEN_OPTIMIZATION_BARRIER(X)
  983|       |#endif
  984|       |
  985|       |#if !defined(EIGEN_OPTIMIZATION_BARRIER)
  986|       |#if EIGEN_COMP_GNUC
  987|       |   // According to https://gcc.gnu.org/onlinedocs/gcc/Constraints.html:
  988|       |//   X: Any operand whatsoever.
  989|       |//   r: A register operand is allowed provided that it is in a general
  990|       |//      register.
  991|       |//   g: Any register, memory or immediate integer operand is allowed, except
  992|       |//      for registers that are not general registers.
  993|       |//   w: (AArch32/AArch64) Floating point register, Advanced SIMD vector
  994|       |//      register or SVE vector register.
  995|       |//   x: (SSE) Any SSE register.
  996|       |//      (AArch64) Like w, but restricted to registers 0 to 15 inclusive.
  997|       |//   v: (PowerPC) An Altivec vector register.
  998|       |//   wa:(PowerPC) A VSX register.
  999|       |//
 1000|       |// "X" (uppercase) should work for all cases, though this seems to fail for
 1001|       |// some versions of GCC for arm/aarch64 with
 1002|       |//   "error: inconsistent operand constraints in an 'asm'"
 1003|       |// Clang x86_64/arm/aarch64 seems to require "g" to support both scalars and
 1004|       |// vectors, otherwise
 1005|       |//   "error: non-trivial scalar-to-vector conversion, possible invalid
 1006|       |//    constraint for vector type"
 1007|       |//
 1008|       |// GCC for ppc64le generates an internal compiler error with x/X/g.
 1009|       |// GCC for AVX generates an internal compiler error with X.
 1010|       |//
 1011|       |// Tested on icc/gcc/clang for sse, avx, avx2, avx512dq
 1012|       |//           gcc for arm, aarch64,
 1013|       |//           gcc for ppc64le,
 1014|       |// both vectors and scalars.
 1015|       |//
 1016|       |// Note that this is restricted to plain types - this will not work
 1017|       |// directly for std::complex<T>, Eigen::half, Eigen::bfloat16. For these,
 1018|       |// you will need to apply to the underlying POD type.
 1019|       |#if EIGEN_ARCH_PPC && EIGEN_COMP_GNUC_STRICT
 1020|       |   // This seems to be broken on clang. Packet4f is loaded into a single
 1021|       |//   register rather than a vector, zeroing out some entries. Integer
 1022|       |//   types also generate a compile error.
 1023|       |#if EIGEN_OS_MAC
 1024|       |   // General, Altivec for Apple (VSX were added in ISA v2.06):
 1025|       |#define EIGEN_OPTIMIZATION_BARRIER(X) __asm__("" : "+r,v"(X));
 1026|       |#else
 1027|       |   // General, Altivec, VSX otherwise:
 1028|       |#define EIGEN_OPTIMIZATION_BARRIER(X) __asm__("" : "+r,v,wa"(X));
 1029|       |#endif
 1030|       |#elif EIGEN_ARCH_ARM_OR_ARM64
 1031|       |#ifdef __ARM_FP
 1032|       |   // General, VFP or NEON.
 1033|       |// Clang doesn't like "r",
 1034|       |//    error: non-trivial scalar-to-vector conversion, possible invalid
 1035|       |//           constraint for vector typ
 1036|       |#define EIGEN_OPTIMIZATION_BARRIER(X) __asm__("" : "+g,w"(X));
 1037|       |#else
 1038|       |   // Arm without VFP or NEON.
 1039|       |// "w" constraint will not compile.
 1040|       |#define EIGEN_OPTIMIZATION_BARRIER(X) __asm__("" : "+g"(X));
 1041|       |#endif
 1042|       |#elif EIGEN_ARCH_i386_OR_x86_64
 1043|       |   // General, SSE.
 1044|       |#define EIGEN_OPTIMIZATION_BARRIER(X) __asm__("" : "+g,x"(X));
 1045|       |#else
 1046|       |   // Not implemented for other architectures.
 1047|       |#define EIGEN_OPTIMIZATION_BARRIER(X)
 1048|       |#endif
 1049|       |#else
 1050|       |   // Not implemented for other compilers.
 1051|       |#define EIGEN_OPTIMIZATION_BARRIER(X)
 1052|       |#endif
 1053|       |#endif
 1054|       |
 1055|       |#if EIGEN_COMP_MSVC
 1056|       |// NOTE MSVC often gives C4127 warnings with compiletime if statements. See bug 1362.
 1057|       |// This workaround is ugly, but it does the job.
 1058|       |#define EIGEN_CONST_CONDITIONAL(cond) (void)0, cond
 1059|       |#else
 1060|       |#define EIGEN_CONST_CONDITIONAL(cond) cond
 1061|       |#endif
 1062|       |
 1063|       |#ifdef EIGEN_DONT_USE_RESTRICT_KEYWORD
 1064|       |#define EIGEN_RESTRICT
 1065|       |#endif
 1066|       |#ifndef EIGEN_RESTRICT
 1067|       |#define EIGEN_RESTRICT __restrict
 1068|       |#endif
 1069|       |
 1070|       |#ifndef EIGEN_DEFAULT_IO_FORMAT
 1071|       |#ifdef EIGEN_MAKING_DOCS
 1072|       |// format used in Eigen's documentation
 1073|       |// needed to define it here as escaping characters in CMake add_definition's argument seems very problematic.
 1074|       |#define EIGEN_DEFAULT_IO_FORMAT Eigen::IOFormat(3, 0, " ", "\n", "", "")
 1075|       |#else
 1076|       |#define EIGEN_DEFAULT_IO_FORMAT Eigen::IOFormat()
 1077|       |#endif
 1078|       |#endif
 1079|       |
 1080|       |// just an empty macro !
 1081|       |#define EIGEN_EMPTY
 1082|       |
 1083|       |// When compiling CUDA/HIP device code with NVCC or HIPCC
 1084|       |// pull in math functions from the global namespace.
 1085|       |// In host mode, and when device code is compiled with clang,
 1086|       |// use the std versions.
 1087|       |#if (defined(EIGEN_CUDA_ARCH) && defined(__NVCC__)) || defined(EIGEN_HIP_DEVICE_COMPILE)
 1088|       |#define EIGEN_USING_STD(FUNC) using ::FUNC;
 1089|       |#else
 1090|      0|#define EIGEN_USING_STD(FUNC) using std::FUNC;
 1091|       |#endif
 1092|       |
 1093|       |#if EIGEN_COMP_CLANG  // workaround clang bug (see http://forum.kde.org/viewtopic.php?f=74&t=102653)
 1094|       |#define EIGEN_INHERIT_ASSIGNMENT_EQUAL_OPERATOR(Derived)                                           \
 1095|       |  using Base::operator=;                                                                           \
 1096|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Derived& operator=(const Derived& other) {                 \
 1097|       |    Base::operator=(other);                                                                        \
 1098|       |    return *this;                                                                                  \
 1099|       |  }                                                                                                \
 1100|       |  template <typename OtherDerived>                                                                 \
 1101|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Derived& operator=(const DenseBase<OtherDerived>& other) { \
 1102|       |    Base::operator=(other.derived());                                                              \
 1103|       |    return *this;                                                                                  \
 1104|       |  }
 1105|       |#else
 1106|       |#define EIGEN_INHERIT_ASSIGNMENT_EQUAL_OPERATOR(Derived)                           \
 1107|       |  using Base::operator=;                                                           \
 1108|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE Derived& operator=(const Derived& other) { \
 1109|       |    Base::operator=(other);                                                        \
 1110|       |    return *this;                                                                  \
 1111|       |  }
 1112|       |#endif
 1113|       |
 1114|       |/**
 1115|       | * \internal
 1116|       | * \brief Macro to explicitly define the default copy constructor.
 1117|       | * This is necessary, because the implicit definition is deprecated if the copy-assignment is overridden.
 1118|       | */
 1119|       |#define EIGEN_DEFAULT_COPY_CONSTRUCTOR(CLASS) EIGEN_DEVICE_FUNC CLASS(const CLASS&) = default;
 1120|       |
 1121|       |/** \internal
 1122|       | * \brief Macro to manually inherit assignment operators.
 1123|       | * This is necessary, because the implicitly defined assignment operator gets deleted when a custom operator= is
 1124|       | * defined. With C++11 or later this also default-implements the copy-constructor
 1125|       | */
 1126|       |#define EIGEN_INHERIT_ASSIGNMENT_OPERATORS(Derived) \
 1127|       |  EIGEN_INHERIT_ASSIGNMENT_EQUAL_OPERATOR(Derived)  \
 1128|       |  EIGEN_DEFAULT_COPY_CONSTRUCTOR(Derived)
 1129|       |
 1130|       |/** \internal
 1131|       | * \brief Macro to manually define default constructors and destructors.
 1132|       | * This is necessary when the copy constructor is re-defined.
 1133|       | * For empty helper classes this should usually be protected, to avoid accidentally creating empty objects.
 1134|       | *
 1135|       | * Hiding the default destructor lead to problems in C++03 mode together with boost::multiprecision
 1136|       | */
 1137|       |#define EIGEN_DEFAULT_EMPTY_CONSTRUCTOR_AND_DESTRUCTOR(Derived) \
 1138|       |  EIGEN_DEVICE_FUNC Derived() = default;                        \
 1139|       |  EIGEN_DEVICE_FUNC ~Derived() = default;
 1140|       |
 1141|       |/**
 1142|       | * Just a side note. Commenting within defines works only by documenting
 1143|       | * behind the object (via '!<'). Comments cannot be multi-line and thus
 1144|       | * we have these extra long lines. What is confusing doxygen over here is
 1145|       | * that we use '\' and basically have a bunch of typedefs with their
 1146|       | * documentation in a single line.
 1147|       | **/
 1148|       |
 1149|       |#define EIGEN_GENERIC_PUBLIC_INTERFACE(Derived)                                                                        \
 1150|       |  typedef typename Eigen::internal::traits<Derived>::Scalar                                                            \
 1151|       |      Scalar; /*!< \brief Numeric type, e.g. float, double, int or std::complex<float>. */                             \
 1152|       |  typedef typename Eigen::NumTraits<Scalar>::Real                                                                      \
 1153|       |      RealScalar; /*!< \brief The underlying numeric type for composed scalar types. \details In cases where Scalar is \
 1154|       |                     e.g. std::complex<T>, T were corresponding to RealScalar. */                                      \
 1155|       |  typedef typename Base::CoeffReturnType                                                                               \
 1156|       |      CoeffReturnType; /*!< \brief The return type for coefficient access. \details Depending on whether the object    \
 1157|       |                          allows direct coefficient access (e.g. for a MatrixXd), this type is either 'const Scalar&'  \
 1158|       |                          or simply 'Scalar' for objects that do not allow direct coefficient access. */               \
 1159|       |  typedef typename Eigen::internal::ref_selector<Derived>::type Nested;                                                \
 1160|       |  typedef typename Eigen::internal::traits<Derived>::StorageKind StorageKind;                                          \
 1161|       |  typedef typename Eigen::internal::traits<Derived>::StorageIndex StorageIndex;                                        \
 1162|       |  enum CompileTimeTraits {                                                                                             \
 1163|       |    RowsAtCompileTime = Eigen::internal::traits<Derived>::RowsAtCompileTime,                                           \
 1164|       |    ColsAtCompileTime = Eigen::internal::traits<Derived>::ColsAtCompileTime,                                           \
 1165|       |    Flags = Eigen::internal::traits<Derived>::Flags,                                                                   \
 1166|       |    SizeAtCompileTime = Base::SizeAtCompileTime,                                                                       \
 1167|       |    MaxSizeAtCompileTime = Base::MaxSizeAtCompileTime,                                                                 \
 1168|       |    IsVectorAtCompileTime = Base::IsVectorAtCompileTime                                                                \
 1169|       |  };                                                                                                                   \
 1170|       |  using Base::derived;                                                                                                 \
 1171|       |  using Base::const_cast_derived;
 1172|       |
 1173|       |// FIXME Maybe the EIGEN_DENSE_PUBLIC_INTERFACE could be removed as importing PacketScalar is rarely needed
 1174|       |#define EIGEN_DENSE_PUBLIC_INTERFACE(Derived) \
 1175|       |  EIGEN_GENERIC_PUBLIC_INTERFACE(Derived)     \
 1176|       |  typedef typename Base::PacketScalar PacketScalar;
 1177|       |
 1178|       |#if EIGEN_HAS_BUILTIN(__builtin_expect) || EIGEN_COMP_GNUC
 1179|       |#define EIGEN_PREDICT_FALSE(x) (__builtin_expect(x, false))
 1180|       |#define EIGEN_PREDICT_TRUE(x) (__builtin_expect(false || (x), true))
 1181|       |#else
 1182|       |#define EIGEN_PREDICT_FALSE(x) (x)
 1183|       |#define EIGEN_PREDICT_TRUE(x) (x)
 1184|       |#endif
 1185|       |
 1186|       |// the expression type of a standard coefficient wise binary operation
 1187|       |#define EIGEN_CWISE_BINARY_RETURN_TYPE(LHS, RHS, OPNAME)                                                       \
 1188|       |  CwiseBinaryOp<EIGEN_CAT(EIGEN_CAT(internal::scalar_, OPNAME), _op) < typename internal::traits<LHS>::Scalar, \
 1189|       |                typename internal::traits<RHS>::Scalar>,                                                       \
 1190|       |      const LHS, const RHS >
 1191|       |
 1192|       |#define EIGEN_MAKE_CWISE_BINARY_OP(METHOD, OPNAME)                                                                \
 1193|       |  template <typename OtherDerived>                                                                                \
 1194|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const EIGEN_CWISE_BINARY_RETURN_TYPE(                                     \
 1195|       |      Derived, OtherDerived, OPNAME)(METHOD)(const EIGEN_CURRENT_STORAGE_BASE_CLASS<OtherDerived>& other) const { \
 1196|       |    return EIGEN_CWISE_BINARY_RETURN_TYPE(Derived, OtherDerived, OPNAME)(derived(), other.derived());             \
 1197|       |  }
 1198|       |
 1199|       |#define EIGEN_SCALAR_BINARY_SUPPORTED(OPNAME, TYPEA, TYPEB)     \
 1200|       |  (Eigen::internal::has_ReturnType<Eigen::ScalarBinaryOpTraits< \
 1201|       |       TYPEA, TYPEB, EIGEN_CAT(EIGEN_CAT(Eigen::internal::scalar_, OPNAME), _op) < TYPEA, TYPEB> > > ::value)
 1202|       |
 1203|       |#define EIGEN_EXPR_BINARYOP_SCALAR_RETURN_TYPE(EXPR, SCALAR, OPNAME)                                            \
 1204|       |  CwiseBinaryOp<EIGEN_CAT(EIGEN_CAT(internal::scalar_, OPNAME), _op) < typename internal::traits<EXPR>::Scalar, \
 1205|       |                SCALAR>,                                                                                        \
 1206|       |      const EXPR, const typename internal::plain_constant_type<EXPR, SCALAR>::type >
 1207|       |
 1208|       |#define EIGEN_SCALAR_BINARYOP_EXPR_RETURN_TYPE(SCALAR, EXPR, OPNAME)           \
 1209|       |  CwiseBinaryOp<EIGEN_CAT(EIGEN_CAT(internal::scalar_, OPNAME), _op) < SCALAR, \
 1210|       |                typename internal::traits<EXPR>::Scalar>,                      \
 1211|       |      const typename internal::plain_constant_type<EXPR, SCALAR>::type, const EXPR >
 1212|       |
 1213|       |#define EIGEN_MAKE_SCALAR_BINARY_OP_ONTHERIGHT(METHOD, OPNAME)                                                       \
 1214|       |  template <typename T>                                                                                              \
 1215|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const EIGEN_EXPR_BINARYOP_SCALAR_RETURN_TYPE(                                \
 1216|       |      Derived,                                                                                                       \
 1217|       |      typename internal::promote_scalar_arg<Scalar EIGEN_COMMA T EIGEN_COMMA EIGEN_SCALAR_BINARY_SUPPORTED(          \
 1218|       |          OPNAME, Scalar, T)>::type,                                                                                 \
 1219|       |      OPNAME)(METHOD)(const T& scalar) const {                                                                       \
 1220|       |    typedef typename internal::promote_scalar_arg<Scalar, T, EIGEN_SCALAR_BINARY_SUPPORTED(OPNAME, Scalar, T)>::type \
 1221|       |        PromotedT;                                                                                                   \
 1222|       |    return EIGEN_EXPR_BINARYOP_SCALAR_RETURN_TYPE(Derived, PromotedT, OPNAME)(                                       \
 1223|       |        derived(), typename internal::plain_constant_type<Derived, PromotedT>::type(                                 \
 1224|       |                       derived().rows(), derived().cols(), internal::scalar_constant_op<PromotedT>(scalar)));        \
 1225|       |  }
 1226|       |
 1227|       |#define EIGEN_MAKE_SCALAR_BINARY_OP_ONTHELEFT(METHOD, OPNAME)                                                        \
 1228|       |  template <typename T>                                                                                              \
 1229|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE friend const EIGEN_SCALAR_BINARYOP_EXPR_RETURN_TYPE(                         \
 1230|       |      typename internal::promote_scalar_arg<Scalar EIGEN_COMMA T EIGEN_COMMA EIGEN_SCALAR_BINARY_SUPPORTED(          \
 1231|       |          OPNAME, T, Scalar)>::type,                                                                                 \
 1232|       |      Derived, OPNAME)(METHOD)(const T& scalar, const StorageBaseType& matrix) {                                     \
 1233|       |    typedef typename internal::promote_scalar_arg<Scalar, T, EIGEN_SCALAR_BINARY_SUPPORTED(OPNAME, T, Scalar)>::type \
 1234|       |        PromotedT;                                                                                                   \
 1235|       |    return EIGEN_SCALAR_BINARYOP_EXPR_RETURN_TYPE(PromotedT, Derived, OPNAME)(                                       \
 1236|       |        typename internal::plain_constant_type<Derived, PromotedT>::type(                                            \
 1237|       |            matrix.derived().rows(), matrix.derived().cols(), internal::scalar_constant_op<PromotedT>(scalar)),      \
 1238|       |        matrix.derived());                                                                                           \
 1239|       |  }
 1240|       |
 1241|       |#define EIGEN_MAKE_SCALAR_BINARY_OP(METHOD, OPNAME)     \
 1242|       |  EIGEN_MAKE_SCALAR_BINARY_OP_ONTHELEFT(METHOD, OPNAME) \
 1243|       |  EIGEN_MAKE_SCALAR_BINARY_OP_ONTHERIGHT(METHOD, OPNAME)
 1244|       |
 1245|       |#if (defined(_CPPUNWIND) || defined(__EXCEPTIONS)) && !defined(EIGEN_CUDA_ARCH) && !defined(EIGEN_EXCEPTIONS) && \
 1246|       |    !defined(EIGEN_USE_SYCL) && !defined(EIGEN_HIP_DEVICE_COMPILE)
 1247|       |#define EIGEN_EXCEPTIONS
 1248|       |#endif
 1249|       |
 1250|       |#ifdef EIGEN_EXCEPTIONS
 1251|      0|#define EIGEN_THROW_X(X) throw X
 1252|       |#define EIGEN_THROW throw
 1253|       |#define EIGEN_TRY try
 1254|       |#define EIGEN_CATCH(X) catch (X)
 1255|       |#else
 1256|       |#if defined(EIGEN_CUDA_ARCH)
 1257|       |#define EIGEN_THROW_X(X) asm("trap;")
 1258|       |#define EIGEN_THROW asm("trap;")
 1259|       |#elif defined(EIGEN_HIP_DEVICE_COMPILE)
 1260|       |#define EIGEN_THROW_X(X) asm("s_trap 0")
 1261|       |#define EIGEN_THROW asm("s_trap 0")
 1262|       |#else
 1263|       |#define EIGEN_THROW_X(X) std::abort()
 1264|       |#define EIGEN_THROW std::abort()
 1265|       |#endif
 1266|       |#define EIGEN_TRY if (true)
 1267|       |#define EIGEN_CATCH(X) else
 1268|       |#endif
 1269|       |
 1270|       |#define EIGEN_NOEXCEPT noexcept
 1271|       |#define EIGEN_NOEXCEPT_IF(x) noexcept(x)
 1272|       |#define EIGEN_NO_THROW noexcept(true)
 1273|       |#define EIGEN_EXCEPTION_SPEC(X) noexcept(false)
 1274|       |
 1275|       |// The all function is used to enable a variadic version of eigen_assert which can take a parameter pack as its input.
 1276|       |namespace Eigen {
 1277|       |namespace internal {
 1278|       |
 1279|      0|EIGEN_DEVICE_FUNC inline bool all() { return true; }
 1280|       |
 1281|       |template <typename T, typename... Ts>
 1282|       |EIGEN_DEVICE_FUNC bool all(T t, Ts... ts) {
 1283|       |  return t && all(ts...);
 1284|       |}
 1285|       |
 1286|       |}  // namespace internal
 1287|       |}  // namespace Eigen
 1288|       |
 1289|       |// provide override and final specifiers if they are available:
 1290|       |#define EIGEN_OVERRIDE override
 1291|       |#define EIGEN_FINAL final
 1292|       |
 1293|       |// Wrapping #pragma unroll in a macro since it is required for SYCL
 1294|       |#if defined(SYCL_DEVICE_ONLY)
 1295|       |#if defined(_MSC_VER)
 1296|       |#define EIGEN_UNROLL_LOOP __pragma(unroll)
 1297|       |#else
 1298|       |#define EIGEN_UNROLL_LOOP _Pragma("unroll")
 1299|       |#endif
 1300|       |#else
 1301|       |#define EIGEN_UNROLL_LOOP
 1302|       |#endif
 1303|       |
 1304|       |// Notice: Use this macro with caution. The code in the if body should still
 1305|       |// compile with C++14.
 1306|       |#if defined(EIGEN_HAS_CXX17_IFCONSTEXPR)
 1307|       |#define EIGEN_IF_CONSTEXPR(X) if constexpr (X)
 1308|       |#else
 1309|       |#define EIGEN_IF_CONSTEXPR(X) if (X)
 1310|       |#endif
 1311|       |
 1312|       |#endif  // EIGEN_MACROS_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/Memory.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2015 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |// Copyright (C) 2008-2009 Benoit Jacob <jacob.benoit.1@gmail.com>
    6|       |// Copyright (C) 2009 Kenneth Riddile <kfriddile@yahoo.com>
    7|       |// Copyright (C) 2010 Hauke Heibel <hauke.heibel@gmail.com>
    8|       |// Copyright (C) 2010 Thomas Capricelli <orzel@freehackers.org>
    9|       |// Copyright (C) 2013 Pavel Holoborodko <pavel@holoborodko.com>
   10|       |//
   11|       |// This Source Code Form is subject to the terms of the Mozilla
   12|       |// Public License v. 2.0. If a copy of the MPL was not distributed
   13|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   14|       |
   15|       |/*****************************************************************************
   16|       |*** Platform checks for aligned malloc functions                           ***
   17|       |*****************************************************************************/
   18|       |
   19|       |#ifndef EIGEN_MEMORY_H
   20|       |#define EIGEN_MEMORY_H
   21|       |
   22|       |#ifndef EIGEN_MALLOC_ALREADY_ALIGNED
   23|       |
   24|       |// Try to determine automatically if malloc is already aligned.
   25|       |
   26|       |// On 64-bit systems, glibc's malloc returns 16-byte-aligned pointers, see:
   27|       |//   http://www.gnu.org/s/libc/manual/html_node/Aligned-Memory-Blocks.html
   28|       |// This is true at least since glibc 2.8.
   29|       |// This leaves the question how to detect 64-bit. According to this document,
   30|       |//   http://gcc.fyxm.net/summit/2003/Porting%20to%2064%20bit.pdf
   31|       |// page 114, "[The] LP64 model [...] is used by all 64-bit UNIX ports" so it's indeed
   32|       |// quite safe, at least within the context of glibc, to equate 64-bit with LP64.
   33|       |#if defined(__GLIBC__) && ((__GLIBC__ >= 2 && __GLIBC_MINOR__ >= 8) || __GLIBC__ > 2) && defined(__LP64__) && \
   34|       |    !defined(__SANITIZE_ADDRESS__) && (EIGEN_DEFAULT_ALIGN_BYTES == 16)
   35|       |#define EIGEN_GLIBC_MALLOC_ALREADY_ALIGNED 1
   36|       |#else
   37|       |#define EIGEN_GLIBC_MALLOC_ALREADY_ALIGNED 0
   38|       |#endif
   39|       |
   40|       |// FreeBSD 6 seems to have 16-byte aligned malloc
   41|       |//   See http://svn.freebsd.org/viewvc/base/stable/6/lib/libc/stdlib/malloc.c?view=markup
   42|       |// FreeBSD 7 seems to have 16-byte aligned malloc except on ARM and MIPS architectures
   43|       |//   See http://svn.freebsd.org/viewvc/base/stable/7/lib/libc/stdlib/malloc.c?view=markup
   44|       |#if defined(__FreeBSD__) && !(EIGEN_ARCH_ARM || EIGEN_ARCH_MIPS) && (EIGEN_DEFAULT_ALIGN_BYTES == 16)
   45|       |#define EIGEN_FREEBSD_MALLOC_ALREADY_ALIGNED 1
   46|       |#else
   47|       |#define EIGEN_FREEBSD_MALLOC_ALREADY_ALIGNED 0
   48|       |#endif
   49|       |
   50|       |#if (EIGEN_OS_MAC && (EIGEN_DEFAULT_ALIGN_BYTES == 16)) || (EIGEN_OS_WIN64 && (EIGEN_DEFAULT_ALIGN_BYTES == 16)) || \
   51|       |    EIGEN_GLIBC_MALLOC_ALREADY_ALIGNED || EIGEN_FREEBSD_MALLOC_ALREADY_ALIGNED
   52|       |#define EIGEN_MALLOC_ALREADY_ALIGNED 1
   53|       |#else
   54|       |#define EIGEN_MALLOC_ALREADY_ALIGNED 0
   55|       |#endif
   56|       |
   57|       |#endif
   58|       |
   59|       |#ifndef EIGEN_MALLOC_CHECK_THREAD_LOCAL
   60|       |
   61|       |// Check whether we can use the thread_local keyword to allow or disallow
   62|       |// allocating memory with per-thread granularity, by means of the
   63|       |// set_is_malloc_allowed() function.
   64|       |#ifndef EIGEN_AVOID_THREAD_LOCAL
   65|       |
   66|       |#if ((EIGEN_COMP_GNUC) || __has_feature(cxx_thread_local) || EIGEN_COMP_MSVC >= 1900) && \
   67|       |    !defined(EIGEN_GPU_COMPILE_PHASE)
   68|       |#define EIGEN_MALLOC_CHECK_THREAD_LOCAL thread_local
   69|       |#else
   70|       |#define EIGEN_MALLOC_CHECK_THREAD_LOCAL
   71|       |#endif
   72|       |
   73|       |#else  // EIGEN_AVOID_THREAD_LOCAL
   74|       |#define EIGEN_MALLOC_CHECK_THREAD_LOCAL
   75|       |#endif  // EIGEN_AVOID_THREAD_LOCAL
   76|       |
   77|       |#endif
   78|       |
   79|       |// IWYU pragma: private
   80|       |#include "../InternalHeaderCheck.h"
   81|       |
   82|       |namespace Eigen {
   83|       |
   84|       |namespace internal {
   85|       |
   86|       |/*****************************************************************************
   87|       |*** Implementation of portable aligned versions of malloc/free/realloc     ***
   88|       |*****************************************************************************/
   89|       |
   90|       |#ifdef EIGEN_NO_MALLOC
   91|       |EIGEN_DEVICE_FUNC inline void check_that_malloc_is_allowed() {
   92|       |  eigen_assert(false && "heap allocation is forbidden (EIGEN_NO_MALLOC is defined)");
   93|       |}
   94|       |#elif defined EIGEN_RUNTIME_NO_MALLOC
   95|       |EIGEN_DEVICE_FUNC inline bool is_malloc_allowed_impl(bool update, bool new_value = false) {
   96|       |  EIGEN_MALLOC_CHECK_THREAD_LOCAL static bool value = true;
   97|       |  if (update == 1) value = new_value;
   98|       |  return value;
   99|       |}
  100|       |EIGEN_DEVICE_FUNC inline bool is_malloc_allowed() { return is_malloc_allowed_impl(false); }
  101|       |EIGEN_DEVICE_FUNC inline bool set_is_malloc_allowed(bool new_value) { return is_malloc_allowed_impl(true, new_value); }
  102|       |EIGEN_DEVICE_FUNC inline void check_that_malloc_is_allowed() {
  103|       |  eigen_assert(is_malloc_allowed() &&
  104|       |               "heap allocation is forbidden (EIGEN_RUNTIME_NO_MALLOC is defined and g_is_malloc_allowed is false)");
  105|       |}
  106|       |#else
  107|      0|EIGEN_DEVICE_FUNC inline void check_that_malloc_is_allowed() {}
  108|       |#endif
  109|       |
  110|      0|EIGEN_DEVICE_FUNC inline void throw_std_bad_alloc() {
  111|      0|#ifdef EIGEN_EXCEPTIONS
  112|      0|  throw std::bad_alloc();
  113|      0|#else
  114|      0|  std::size_t huge = static_cast<std::size_t>(-1);
  115|      0|#if defined(EIGEN_HIPCC)
  116|      0|  //
  117|      0|  // calls to "::operator new" are to be treated as opaque function calls (i.e no inlining),
  118|      0|  // and as a consequence the code in the #else block triggers the hipcc warning :
  119|      0|  // "no overloaded function has restriction specifiers that are compatible with the ambient context"
  120|      0|  //
  121|      0|  // "throw_std_bad_alloc" has the EIGEN_DEVICE_FUNC attribute, so it seems that hipcc expects
  122|      0|  // the same on "operator new"
  123|      0|  // Reverting code back to the old version in this #if block for the hipcc compiler
  124|      0|  //
  125|      0|  new int[huge];
  126|      0|#else
  127|      0|  void* unused = ::operator new(huge);
  128|      0|  EIGEN_UNUSED_VARIABLE(unused);
  129|      0|#endif
  130|      0|#endif
  131|      0|}
  132|       |
  133|       |/*****************************************************************************
  134|       |*** Implementation of handmade aligned functions                           ***
  135|       |*****************************************************************************/
  136|       |
  137|       |/* ----- Hand made implementations of aligned malloc/free and realloc ----- */
  138|       |
  139|       |/** \internal Like malloc, but the returned pointer is guaranteed to be aligned to `alignment`.
  140|       | * Fast, but wastes `alignment` additional bytes of memory. Does not throw any exception.
  141|       | */
  142|       |EIGEN_DEVICE_FUNC inline void* handmade_aligned_malloc(std::size_t size,
  143|      0|                                                       std::size_t alignment = EIGEN_DEFAULT_ALIGN_BYTES) {
  144|      0|  eigen_assert(alignment >= sizeof(void*) && alignment <= 128 && (alignment & (alignment - 1)) == 0 &&
  145|      0|               "Alignment must be at least sizeof(void*), less than or equal to 128, and a power of 2");
  146|      0|
  147|      0|  check_that_malloc_is_allowed();
  148|      0|  EIGEN_USING_STD(malloc)
  149|      0|  void* original = malloc(size + alignment);
  150|      0|  if (original == nullptr) return nullptr;
  151|      0|  uint8_t offset = static_cast<uint8_t>(alignment - (reinterpret_cast<std::size_t>(original) & (alignment - 1)));
  152|      0|  void* aligned = static_cast<void*>(static_cast<uint8_t*>(original) + offset);
  153|      0|  *(static_cast<uint8_t*>(aligned) - 1) = offset;
  154|      0|  return aligned;
  155|      0|}
  156|       |
  157|       |/** \internal Frees memory allocated with handmade_aligned_malloc */
  158|      0|EIGEN_DEVICE_FUNC inline void handmade_aligned_free(void* ptr) {
  159|      0|  if (ptr != nullptr) {
  160|      0|    uint8_t offset = static_cast<uint8_t>(*(static_cast<uint8_t*>(ptr) - 1));
  161|      0|    void* original = static_cast<void*>(static_cast<uint8_t*>(ptr) - offset);
  162|      0|
  163|      0|    check_that_malloc_is_allowed();
  164|      0|    EIGEN_USING_STD(free)
  165|      0|    free(original);
  166|      0|  }
  167|      0|}
  168|       |
  169|       |/** \internal
  170|       | * \brief Reallocates aligned memory.
  171|       | * Since we know that our handmade version is based on std::malloc
  172|       | * we can use std::realloc to implement efficient reallocation.
  173|       | */
  174|       |EIGEN_DEVICE_FUNC inline void* handmade_aligned_realloc(void* ptr, std::size_t new_size, std::size_t old_size,
  175|      0|                                                        std::size_t alignment = EIGEN_DEFAULT_ALIGN_BYTES) {
  176|      0|  if (ptr == nullptr) return handmade_aligned_malloc(new_size, alignment);
  177|      0|  uint8_t old_offset = *(static_cast<uint8_t*>(ptr) - 1);
  178|      0|  void* old_original = static_cast<uint8_t*>(ptr) - old_offset;
  179|      0|
  180|      0|  check_that_malloc_is_allowed();
  181|      0|  EIGEN_USING_STD(realloc)
  182|      0|  void* original = realloc(old_original, new_size + alignment);
  183|      0|  if (original == nullptr) return nullptr;
  184|      0|  if (original == old_original) return ptr;
  185|      0|  uint8_t offset = static_cast<uint8_t>(alignment - (reinterpret_cast<std::size_t>(original) & (alignment - 1)));
  186|      0|  void* aligned = static_cast<void*>(static_cast<uint8_t*>(original) + offset);
  187|      0|  if (offset != old_offset) {
  188|      0|    const void* src = static_cast<const void*>(static_cast<uint8_t*>(original) + old_offset);
  189|      0|    std::size_t count = (std::min)(new_size, old_size);
  190|      0|    std::memmove(aligned, src, count);
  191|      0|  }
  192|      0|  *(static_cast<uint8_t*>(aligned) - 1) = offset;
  193|      0|  return aligned;
  194|      0|}
  195|       |
  196|       |/** \internal Allocates \a size bytes. The returned pointer is guaranteed to have 16 or 32 bytes alignment depending on
  197|       | * the requirements. On allocation error, the returned pointer is null, and std::bad_alloc is thrown.
  198|       | */
  199|      0|EIGEN_DEVICE_FUNC inline void* aligned_malloc(std::size_t size) {
  200|      0|  if (size == 0) return nullptr;
  201|      0|
  202|      0|  void* result;
  203|      0|#if (EIGEN_DEFAULT_ALIGN_BYTES == 0) || EIGEN_MALLOC_ALREADY_ALIGNED
  204|      0|
  205|      0|  check_that_malloc_is_allowed();
  206|      0|  EIGEN_USING_STD(malloc)
  207|      0|  result = malloc(size);
  208|      0|
  209|      0|#if EIGEN_DEFAULT_ALIGN_BYTES == 16
  210|      0|  eigen_assert((size < 16 || (std::size_t(result) % 16) == 0) &&
  211|      0|               "System's malloc returned an unaligned pointer. Compile with EIGEN_MALLOC_ALREADY_ALIGNED=0 to fallback "
  212|      0|               "to handmade aligned memory allocator.");
  213|      0|#endif
  214|      0|#else
  215|      0|  result = handmade_aligned_malloc(size);
  216|      0|#endif
  217|      0|
  218|      0|  if (!result && size) throw_std_bad_alloc();
  219|      0|
  220|      0|  return result;
  221|      0|}
  222|       |
  223|       |/** \internal Frees memory allocated with aligned_malloc. */
  224|      0|EIGEN_DEVICE_FUNC inline void aligned_free(void* ptr) {
  225|      0|#if (EIGEN_DEFAULT_ALIGN_BYTES == 0) || EIGEN_MALLOC_ALREADY_ALIGNED
  226|      0|
  227|      0|  if (ptr != nullptr) {
  228|      0|    check_that_malloc_is_allowed();
  229|      0|    EIGEN_USING_STD(free)
  230|      0|    free(ptr);
  231|      0|  }
  232|      0|
  233|      0|#else
  234|      0|  handmade_aligned_free(ptr);
  235|      0|#endif
  236|      0|}
  237|       |
  238|       |/**
  239|       | * \internal
  240|       | * \brief Reallocates an aligned block of memory.
  241|       | * \throws std::bad_alloc on allocation failure
  242|       | */
  243|      0|EIGEN_DEVICE_FUNC inline void* aligned_realloc(void* ptr, std::size_t new_size, std::size_t old_size) {
  244|      0|  if (ptr == nullptr) return aligned_malloc(new_size);
  245|      0|  if (old_size == new_size) return ptr;
  246|      0|  if (new_size == 0) {
  247|      0|    aligned_free(ptr);
  248|      0|    return nullptr;
  249|      0|  }
  250|      0|
  251|      0|  void* result;
  252|      0|#if (EIGEN_DEFAULT_ALIGN_BYTES == 0) || EIGEN_MALLOC_ALREADY_ALIGNED
  253|      0|  EIGEN_UNUSED_VARIABLE(old_size)
  254|      0|
  255|      0|  check_that_malloc_is_allowed();
  256|      0|  EIGEN_USING_STD(realloc)
  257|      0|  result = realloc(ptr, new_size);
  258|      0|#else
  259|      0|  result = handmade_aligned_realloc(ptr, new_size, old_size);
  260|      0|#endif
  261|      0|
  262|      0|  if (!result && new_size) throw_std_bad_alloc();
  263|      0|
  264|      0|  return result;
  265|      0|}
  266|       |
  267|       |/*****************************************************************************
  268|       |*** Implementation of conditionally aligned functions                      ***
  269|       |*****************************************************************************/
  270|       |
  271|       |/** \internal Allocates \a size bytes. If Align is true, then the returned ptr is 16-byte-aligned.
  272|       | * On allocation error, the returned pointer is null, and a std::bad_alloc is thrown.
  273|       | */
  274|       |template <bool Align>
  275|       |EIGEN_DEVICE_FUNC inline void* conditional_aligned_malloc(std::size_t size) {
  276|       |  return aligned_malloc(size);
  277|       |}
  278|       |
  279|       |template <>
  280|      0|EIGEN_DEVICE_FUNC inline void* conditional_aligned_malloc<false>(std::size_t size) {
  281|      0|  if (size == 0) return nullptr;
  282|      0|
  283|      0|  check_that_malloc_is_allowed();
  284|      0|  EIGEN_USING_STD(malloc)
  285|      0|  void* result = malloc(size);
  286|      0|
  287|      0|  if (!result && size) throw_std_bad_alloc();
  288|      0|  return result;
  289|      0|}
  290|       |
  291|       |/** \internal Frees memory allocated with conditional_aligned_malloc */
  292|       |template <bool Align>
  293|       |EIGEN_DEVICE_FUNC inline void conditional_aligned_free(void* ptr) {
  294|       |  aligned_free(ptr);
  295|       |}
  296|       |
  297|       |template <>
  298|      0|EIGEN_DEVICE_FUNC inline void conditional_aligned_free<false>(void* ptr) {
  299|      0|  if (ptr != nullptr) {
  300|      0|    check_that_malloc_is_allowed();
  301|      0|    EIGEN_USING_STD(free)
  302|      0|    free(ptr);
  303|      0|  }
  304|      0|}
  305|       |
  306|       |template <bool Align>
  307|       |EIGEN_DEVICE_FUNC inline void* conditional_aligned_realloc(void* ptr, std::size_t new_size, std::size_t old_size) {
  308|       |  return aligned_realloc(ptr, new_size, old_size);
  309|       |}
  310|       |
  311|       |template <>
  312|       |EIGEN_DEVICE_FUNC inline void* conditional_aligned_realloc<false>(void* ptr, std::size_t new_size,
  313|      0|                                                                  std::size_t old_size) {
  314|      0|  if (ptr == nullptr) return conditional_aligned_malloc<false>(new_size);
  315|      0|  if (old_size == new_size) return ptr;
  316|      0|  if (new_size == 0) {
  317|      0|    conditional_aligned_free<false>(ptr);
  318|      0|    return nullptr;
  319|      0|  }
  320|      0|
  321|      0|  check_that_malloc_is_allowed();
  322|      0|  EIGEN_USING_STD(realloc)
  323|      0|  return realloc(ptr, new_size);
  324|      0|}
  325|       |
  326|       |/*****************************************************************************
  327|       |*** Construction/destruction of array elements                             ***
  328|       |*****************************************************************************/
  329|       |
  330|       |/** \internal Destructs the elements of an array.
  331|       | * The \a size parameters tells on how many objects to call the destructor of T.
  332|       | */
  333|       |template <typename T>
  334|       |EIGEN_DEVICE_FUNC inline void destruct_elements_of_array(T* ptr, std::size_t size) {
  335|       |  // always destruct an array starting from the end.
  336|       |  if (ptr)
  337|       |    while (size) ptr[--size].~T();
  338|       |}
  339|       |
  340|       |/** \internal Constructs the elements of an array.
  341|       | * The \a size parameter tells on how many objects to call the constructor of T.
  342|       | */
  343|       |template <typename T>
  344|       |EIGEN_DEVICE_FUNC inline T* default_construct_elements_of_array(T* ptr, std::size_t size) {
  345|       |  std::size_t i = 0;
  346|       |  EIGEN_TRY {
  347|       |    for (i = 0; i < size; ++i) ::new (ptr + i) T;
  348|       |  }
  349|       |  EIGEN_CATCH(...) {
  350|       |    destruct_elements_of_array(ptr, i);
  351|       |    EIGEN_THROW;
  352|       |  }
  353|       |  return ptr;
  354|       |}
  355|       |
  356|       |/** \internal Copy-constructs the elements of an array.
  357|       | * The \a size parameter tells on how many objects to copy.
  358|       | */
  359|       |template <typename T>
  360|       |EIGEN_DEVICE_FUNC inline T* copy_construct_elements_of_array(T* ptr, const T* src, std::size_t size) {
  361|       |  std::size_t i = 0;
  362|       |  EIGEN_TRY {
  363|       |    for (i = 0; i < size; ++i) ::new (ptr + i) T(*(src + i));
  364|       |  }
  365|       |  EIGEN_CATCH(...) {
  366|       |    destruct_elements_of_array(ptr, i);
  367|       |    EIGEN_THROW;
  368|       |  }
  369|       |  return ptr;
  370|       |}
  371|       |
  372|       |/** \internal Move-constructs the elements of an array.
  373|       | * The \a size parameter tells on how many objects to move.
  374|       | */
  375|       |template <typename T>
  376|       |EIGEN_DEVICE_FUNC inline T* move_construct_elements_of_array(T* ptr, T* src, std::size_t size) {
  377|       |  std::size_t i = 0;
  378|       |  EIGEN_TRY {
  379|       |    for (i = 0; i < size; ++i) ::new (ptr + i) T(std::move(*(src + i)));
  380|       |  }
  381|       |  EIGEN_CATCH(...) {
  382|       |    destruct_elements_of_array(ptr, i);
  383|       |    EIGEN_THROW;
  384|       |  }
  385|       |  return ptr;
  386|       |}
  387|       |
  388|       |/*****************************************************************************
  389|       |*** Implementation of aligned new/delete-like functions                    ***
  390|       |*****************************************************************************/
  391|       |
  392|       |template <typename T>
  393|       |EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE void check_size_for_overflow(std::size_t size) {
  394|       |  constexpr std::size_t max_elements = (std::numeric_limits<std::ptrdiff_t>::max)() / sizeof(T);
  395|       |  if (size > max_elements) throw_std_bad_alloc();
  396|       |}
  397|       |
  398|       |/** \internal Allocates \a size objects of type T. The returned pointer is guaranteed to have 16 bytes alignment.
  399|       | * On allocation error, the returned pointer is undefined, but a std::bad_alloc is thrown.
  400|       | * The default constructor of T is called.
  401|       | */
  402|       |template <typename T>
  403|       |EIGEN_DEVICE_FUNC inline T* aligned_new(std::size_t size) {
  404|       |  check_size_for_overflow<T>(size);
  405|       |  T* result = static_cast<T*>(aligned_malloc(sizeof(T) * size));
  406|       |  EIGEN_TRY { return default_construct_elements_of_array(result, size); }
  407|       |  EIGEN_CATCH(...) {
  408|       |    aligned_free(result);
  409|       |    EIGEN_THROW;
  410|       |  }
  411|       |  return result;
  412|       |}
  413|       |
  414|       |template <typename T, bool Align>
  415|       |EIGEN_DEVICE_FUNC inline T* conditional_aligned_new(std::size_t size) {
  416|       |  check_size_for_overflow<T>(size);
  417|       |  T* result = static_cast<T*>(conditional_aligned_malloc<Align>(sizeof(T) * size));
  418|       |  EIGEN_TRY { return default_construct_elements_of_array(result, size); }
  419|       |  EIGEN_CATCH(...) {
  420|       |    conditional_aligned_free<Align>(result);
  421|       |    EIGEN_THROW;
  422|       |  }
  423|       |  return result;
  424|       |}
  425|       |
  426|       |/** \internal Deletes objects constructed with aligned_new
  427|       | * The \a size parameters tells on how many objects to call the destructor of T.
  428|       | */
  429|       |template <typename T>
  430|       |EIGEN_DEVICE_FUNC inline void aligned_delete(T* ptr, std::size_t size) {
  431|       |  destruct_elements_of_array<T>(ptr, size);
  432|       |  aligned_free(ptr);
  433|       |}
  434|       |
  435|       |/** \internal Deletes objects constructed with conditional_aligned_new
  436|       | * The \a size parameters tells on how many objects to call the destructor of T.
  437|       | */
  438|       |template <typename T, bool Align>
  439|       |EIGEN_DEVICE_FUNC inline void conditional_aligned_delete(T* ptr, std::size_t size) {
  440|       |  destruct_elements_of_array<T>(ptr, size);
  441|       |  conditional_aligned_free<Align>(ptr);
  442|       |}
  443|       |
  444|       |template <typename T, bool Align>
  445|       |EIGEN_DEVICE_FUNC inline T* conditional_aligned_realloc_new(T* pts, std::size_t new_size, std::size_t old_size) {
  446|       |  check_size_for_overflow<T>(new_size);
  447|       |  check_size_for_overflow<T>(old_size);
  448|       |
  449|       |  // If elements need to be explicitly initialized, we cannot simply realloc
  450|       |  // (or memcpy) the memory block - each element needs to be reconstructed.
  451|       |  // Otherwise, objects that contain internal pointers like mpfr or
  452|       |  // AnnoyingScalar can be pointing to the wrong thing.
  453|       |  T* result = static_cast<T*>(conditional_aligned_malloc<Align>(sizeof(T) * new_size));
  454|       |  EIGEN_TRY {
  455|       |    // Move-construct initial elements.
  456|       |    std::size_t copy_size = (std::min)(old_size, new_size);
  457|       |    move_construct_elements_of_array(result, pts, copy_size);
  458|       |
  459|       |    // Default-construct remaining elements.
  460|       |    if (new_size > old_size) {
  461|       |      default_construct_elements_of_array(result + copy_size, new_size - old_size);
  462|       |    }
  463|       |
  464|       |    // Delete old elements.
  465|       |    conditional_aligned_delete<T, Align>(pts, old_size);
  466|       |  }
  467|       |  EIGEN_CATCH(...) {
  468|       |    conditional_aligned_free<Align>(result);
  469|       |    EIGEN_THROW;
  470|       |  }
  471|       |
  472|       |  return result;
  473|       |}
  474|       |
  475|       |template <typename T, bool Align>
  476|       |EIGEN_DEVICE_FUNC inline T* conditional_aligned_new_auto(std::size_t size) {
  477|       |  if (size == 0) return nullptr;  // short-cut. Also fixes Bug 884
  478|       |  check_size_for_overflow<T>(size);
  479|       |  T* result = static_cast<T*>(conditional_aligned_malloc<Align>(sizeof(T) * size));
  480|       |  if (NumTraits<T>::RequireInitialization) {
  481|       |    EIGEN_TRY { default_construct_elements_of_array(result, size); }
  482|       |    EIGEN_CATCH(...) {
  483|       |      conditional_aligned_free<Align>(result);
  484|       |      EIGEN_THROW;
  485|       |    }
  486|       |  }
  487|       |  return result;
  488|       |}
  489|       |
  490|       |template <typename T, bool Align>
  491|       |EIGEN_DEVICE_FUNC inline T* conditional_aligned_realloc_new_auto(T* pts, std::size_t new_size, std::size_t old_size) {
  492|       |  if (NumTraits<T>::RequireInitialization) {
  493|       |    return conditional_aligned_realloc_new<T, Align>(pts, new_size, old_size);
  494|       |  }
  495|       |
  496|       |  check_size_for_overflow<T>(new_size);
  497|       |  check_size_for_overflow<T>(old_size);
  498|       |  return static_cast<T*>(
  499|       |      conditional_aligned_realloc<Align>(static_cast<void*>(pts), sizeof(T) * new_size, sizeof(T) * old_size));
  500|       |}
  501|       |
  502|       |template <typename T, bool Align>
  503|       |EIGEN_DEVICE_FUNC inline void conditional_aligned_delete_auto(T* ptr, std::size_t size) {
  504|       |  if (NumTraits<T>::RequireInitialization) destruct_elements_of_array<T>(ptr, size);
  505|       |  conditional_aligned_free<Align>(ptr);
  506|       |}
  507|       |
  508|       |/****************************************************************************/
  509|       |
  510|       |/** \internal Returns the index of the first element of the array that is well aligned with respect to the requested \a
  511|       | * Alignment.
  512|       | *
  513|       | * \tparam Alignment requested alignment in Bytes.
  514|       | * \param array the address of the start of the array
  515|       | * \param size the size of the array
  516|       | *
  517|       | * \note If no element of the array is well aligned or the requested alignment is not a multiple of a scalar,
  518|       | * the size of the array is returned. For example with SSE, the requested alignment is typically 16-bytes. If
  519|       | * packet size for the given scalar type is 1, then everything is considered well-aligned.
  520|       | *
  521|       | * \note Otherwise, if the Alignment is larger that the scalar size, we rely on the assumptions that sizeof(Scalar) is a
  522|       | * power of 2. On the other hand, we do not assume that the array address is a multiple of sizeof(Scalar), as that fails
  523|       | * for example with Scalar=double on certain 32-bit platforms, see bug #79.
  524|       | *
  525|       | * There is also the variant first_aligned(const MatrixBase&) defined in DenseCoeffsBase.h.
  526|       | * \sa first_default_aligned()
  527|       | */
  528|       |template <int Alignment, typename Scalar, typename Index>
  529|       |EIGEN_DEVICE_FUNC inline Index first_aligned(const Scalar* array, Index size) {
  530|       |  const Index ScalarSize = sizeof(Scalar);
  531|       |  const Index AlignmentSize = Alignment / ScalarSize;
  532|       |  const Index AlignmentMask = AlignmentSize - 1;
  533|       |
  534|       |  if (AlignmentSize <= 1) {
  535|       |    // Either the requested alignment if smaller than a scalar, or it exactly match a 1 scalar
  536|       |    // so that all elements of the array have the same alignment.
  537|       |    return 0;
  538|       |  } else if ((std::uintptr_t(array) & (sizeof(Scalar) - 1)) || (Alignment % ScalarSize) != 0) {
  539|       |    // The array is not aligned to the size of a single scalar, or the requested alignment is not a multiple of the
  540|       |    // scalar size. Consequently, no element of the array is well aligned.
  541|       |    return size;
  542|       |  } else {
  543|       |    Index first = (AlignmentSize - (Index((std::uintptr_t(array) / sizeof(Scalar))) & AlignmentMask)) & AlignmentMask;
  544|       |    return (first < size) ? first : size;
  545|       |  }
  546|       |}
  547|       |
  548|       |/** \internal Returns the index of the first element of the array that is well aligned with respect the largest packet
  549|       | * requirement. \sa first_aligned(Scalar*,Index) and first_default_aligned(DenseBase<Derived>) */
  550|       |template <typename Scalar, typename Index>
  551|       |EIGEN_DEVICE_FUNC inline Index first_default_aligned(const Scalar* array, Index size) {
  552|       |  typedef typename packet_traits<Scalar>::type DefaultPacketType;
  553|       |  return first_aligned<unpacket_traits<DefaultPacketType>::alignment>(array, size);
  554|       |}
  555|       |
  556|       |/** \internal Returns the smallest integer multiple of \a base and greater or equal to \a size
  557|       | */
  558|       |template <typename Index>
  559|       |inline Index first_multiple(Index size, Index base) {
  560|       |  return ((size + base - 1) / base) * base;
  561|       |}
  562|       |
  563|       |// std::copy is much slower than memcpy, so let's introduce a smart_copy which
  564|       |// use memcpy on trivial types, i.e., on types that does not require an initialization ctor.
  565|       |template <typename T, bool UseMemcpy>
  566|       |struct smart_copy_helper;
  567|       |
  568|       |template <typename T>
  569|       |EIGEN_DEVICE_FUNC void smart_copy(const T* start, const T* end, T* target) {
  570|       |  smart_copy_helper<T, !NumTraits<T>::RequireInitialization>::run(start, end, target);
  571|       |}
  572|       |
  573|       |template <typename T>
  574|       |struct smart_copy_helper<T, true> {
  575|       |  EIGEN_DEVICE_FUNC static inline void run(const T* start, const T* end, T* target) {
  576|       |    std::intptr_t size = std::intptr_t(end) - std::intptr_t(start);
  577|       |    if (size == 0) return;
  578|       |    eigen_internal_assert(start != 0 && end != 0 && target != 0);
  579|       |    EIGEN_USING_STD(memcpy)
  580|       |    memcpy(target, start, size);
  581|       |  }
  582|       |};
  583|       |
  584|       |template <typename T>
  585|       |struct smart_copy_helper<T, false> {
  586|       |  EIGEN_DEVICE_FUNC static inline void run(const T* start, const T* end, T* target) { std::copy(start, end, target); }
  587|       |};
  588|       |
  589|       |// intelligent memmove. falls back to std::memmove for POD types, uses std::copy otherwise.
  590|       |template <typename T, bool UseMemmove>
  591|       |struct smart_memmove_helper;
  592|       |
  593|       |template <typename T>
  594|       |void smart_memmove(const T* start, const T* end, T* target) {
  595|       |  smart_memmove_helper<T, !NumTraits<T>::RequireInitialization>::run(start, end, target);
  596|       |}
  597|       |
  598|       |template <typename T>
  599|       |struct smart_memmove_helper<T, true> {
  600|       |  static inline void run(const T* start, const T* end, T* target) {
  601|       |    std::intptr_t size = std::intptr_t(end) - std::intptr_t(start);
  602|       |    if (size == 0) return;
  603|       |    eigen_internal_assert(start != 0 && end != 0 && target != 0);
  604|       |    std::memmove(target, start, size);
  605|       |  }
  606|       |};
  607|       |
  608|       |template <typename T>
  609|       |struct smart_memmove_helper<T, false> {
  610|       |  static inline void run(const T* start, const T* end, T* target) {
  611|       |    if (std::uintptr_t(target) < std::uintptr_t(start)) {
  612|       |      std::copy(start, end, target);
  613|       |    } else {
  614|       |      std::ptrdiff_t count = (std::ptrdiff_t(end) - std::ptrdiff_t(start)) / sizeof(T);
  615|       |      std::copy_backward(start, end, target + count);
  616|       |    }
  617|       |  }
  618|       |};
  619|       |
  620|       |template <typename T>
  621|       |EIGEN_DEVICE_FUNC T* smart_move(T* start, T* end, T* target) {
  622|       |  return std::move(start, end, target);
  623|       |}
  624|       |
  625|       |/*****************************************************************************
  626|       |*** Implementation of runtime stack allocation (falling back to malloc)    ***
  627|       |*****************************************************************************/
  628|       |
  629|       |// you can overwrite Eigen's default behavior regarding alloca by defining EIGEN_ALLOCA
  630|       |// to the appropriate stack allocation function
  631|       |#if !defined EIGEN_ALLOCA && !defined EIGEN_GPU_COMPILE_PHASE
  632|       |#if EIGEN_OS_LINUX || EIGEN_OS_MAC || (defined alloca)
  633|       |#define EIGEN_ALLOCA alloca
  634|       |#elif EIGEN_COMP_MSVC
  635|       |#define EIGEN_ALLOCA _alloca
  636|       |#endif
  637|       |#endif
  638|       |
  639|       |// With clang -Oz -mthumb, alloca changes the stack pointer in a way that is
  640|       |// not allowed in Thumb2. -DEIGEN_STACK_ALLOCATION_LIMIT=0 doesn't work because
  641|       |// the compiler still emits bad code because stack allocation checks use "<=".
  642|       |// TODO: Eliminate after https://bugs.llvm.org/show_bug.cgi?id=23772
  643|       |// is fixed.
  644|       |#if defined(__clang__) && defined(__thumb__)
  645|       |#undef EIGEN_ALLOCA
  646|       |#endif
  647|       |
  648|       |// This helper class construct the allocated memory, and takes care of destructing and freeing the handled data
  649|       |// at destruction time. In practice this helper class is mainly useful to avoid memory leak in case of exceptions.
  650|       |template <typename T>
  651|       |class aligned_stack_memory_handler : noncopyable {
  652|       | public:
  653|       |  /* Creates a stack_memory_handler responsible for the buffer \a ptr of size \a size.
  654|       |   * Note that \a ptr can be 0 regardless of the other parameters.
  655|       |   * This constructor takes care of constructing/initializing the elements of the buffer if required by the scalar type
  656|       |   *T (see NumTraits<T>::RequireInitialization). In this case, the buffer elements will also be destructed when this
  657|       |   *handler will be destructed. Finally, if \a dealloc is true, then the pointer \a ptr is freed.
  658|       |   **/
  659|       |  EIGEN_DEVICE_FUNC aligned_stack_memory_handler(T* ptr, std::size_t size, bool dealloc)
  660|       |      : m_ptr(ptr), m_size(size), m_deallocate(dealloc) {
  661|       |    if (NumTraits<T>::RequireInitialization && m_ptr) Eigen::internal::default_construct_elements_of_array(m_ptr, size);
  662|       |  }
  663|       |  EIGEN_DEVICE_FUNC ~aligned_stack_memory_handler() {
  664|       |    if (NumTraits<T>::RequireInitialization && m_ptr) Eigen::internal::destruct_elements_of_array<T>(m_ptr, m_size);
  665|       |    if (m_deallocate) Eigen::internal::aligned_free(m_ptr);
  666|       |  }
  667|       |
  668|       | protected:
  669|       |  T* m_ptr;
  670|       |  std::size_t m_size;
  671|       |  bool m_deallocate;
  672|       |};
  673|       |
  674|       |#ifdef EIGEN_ALLOCA
  675|       |
  676|       |template <typename Xpr, int NbEvaluations,
  677|       |          bool MapExternalBuffer = nested_eval<Xpr, NbEvaluations>::Evaluate && Xpr::MaxSizeAtCompileTime == Dynamic>
  678|       |struct local_nested_eval_wrapper {
  679|       |  static constexpr bool NeedExternalBuffer = false;
  680|       |  typedef typename Xpr::Scalar Scalar;
  681|       |  typedef typename nested_eval<Xpr, NbEvaluations>::type ObjectType;
  682|       |  ObjectType object;
  683|       |
  684|       |  EIGEN_DEVICE_FUNC local_nested_eval_wrapper(const Xpr& xpr, Scalar* ptr) : object(xpr) {
  685|       |    EIGEN_UNUSED_VARIABLE(ptr);
  686|       |    eigen_internal_assert(ptr == 0);
  687|       |  }
  688|       |};
  689|       |
  690|       |template <typename Xpr, int NbEvaluations>
  691|       |struct local_nested_eval_wrapper<Xpr, NbEvaluations, true> {
  692|       |  static constexpr bool NeedExternalBuffer = true;
  693|       |  typedef typename Xpr::Scalar Scalar;
  694|       |  typedef typename plain_object_eval<Xpr>::type PlainObject;
  695|       |  typedef Map<PlainObject, EIGEN_DEFAULT_ALIGN_BYTES> ObjectType;
  696|       |  ObjectType object;
  697|       |
  698|       |  EIGEN_DEVICE_FUNC local_nested_eval_wrapper(const Xpr& xpr, Scalar* ptr)
  699|       |      : object(ptr == 0 ? reinterpret_cast<Scalar*>(Eigen::internal::aligned_malloc(sizeof(Scalar) * xpr.size())) : ptr,
  700|       |               xpr.rows(), xpr.cols()),
  701|       |        m_deallocate(ptr == 0) {
  702|       |    if (NumTraits<Scalar>::RequireInitialization && object.data())
  703|       |      Eigen::internal::default_construct_elements_of_array(object.data(), object.size());
  704|       |    object = xpr;
  705|       |  }
  706|       |
  707|       |  EIGEN_DEVICE_FUNC ~local_nested_eval_wrapper() {
  708|       |    if (NumTraits<Scalar>::RequireInitialization && object.data())
  709|       |      Eigen::internal::destruct_elements_of_array(object.data(), object.size());
  710|       |    if (m_deallocate) Eigen::internal::aligned_free(object.data());
  711|       |  }
  712|       |
  713|       | private:
  714|       |  bool m_deallocate;
  715|       |};
  716|       |
  717|       |#endif  // EIGEN_ALLOCA
  718|       |
  719|       |template <typename T>
  720|       |class scoped_array : noncopyable {
  721|       |  T* m_ptr;
  722|       |
  723|       | public:
  724|       |  explicit scoped_array(std::ptrdiff_t size) { m_ptr = new T[size]; }
  725|       |  ~scoped_array() { delete[] m_ptr; }
  726|       |  T& operator[](std::ptrdiff_t i) { return m_ptr[i]; }
  727|       |  const T& operator[](std::ptrdiff_t i) const { return m_ptr[i]; }
  728|       |  T*& ptr() { return m_ptr; }
  729|       |  const T* ptr() const { return m_ptr; }
  730|       |  operator const T*() const { return m_ptr; }
  731|       |};
  732|       |
  733|       |template <typename T>
  734|       |void swap(scoped_array<T>& a, scoped_array<T>& b) {
  735|       |  std::swap(a.ptr(), b.ptr());
  736|       |}
  737|       |
  738|       |}  // end namespace internal
  739|       |
  740|       |/** \internal
  741|       | *
  742|       | * The macro ei_declare_aligned_stack_constructed_variable(TYPE,NAME,SIZE,BUFFER) declares, allocates,
  743|       | * and construct an aligned buffer named NAME of SIZE elements of type TYPE on the stack
  744|       | * if the size in bytes is smaller than EIGEN_STACK_ALLOCATION_LIMIT, and if stack allocation is supported by the
  745|       | * platform (currently, this is Linux, OSX and Visual Studio only). Otherwise the memory is allocated on the heap. The
  746|       | * allocated buffer is automatically deleted when exiting the scope of this declaration. If BUFFER is non null, then the
  747|       | * declared variable is simply an alias for BUFFER, and no allocation/deletion occurs. Here is an example: \code
  748|       | * {
  749|       | *   ei_declare_aligned_stack_constructed_variable(float,data,size,0);
  750|       | *   // use data[0] to data[size-1]
  751|       | * }
  752|       | * \endcode
  753|       | * The underlying stack allocation function can controlled with the EIGEN_ALLOCA preprocessor token.
  754|       | *
  755|       | * The macro ei_declare_local_nested_eval(XPR_T,XPR,N,NAME) is analogue to
  756|       | * \code
  757|       | *   typename internal::nested_eval<XPRT_T,N>::type NAME(XPR);
  758|       | * \endcode
  759|       | * with the advantage of using aligned stack allocation even if the maximal size of XPR at compile time is unknown.
  760|       | * This is accomplished through alloca if this later is supported and if the required number of bytes
  761|       | * is below EIGEN_STACK_ALLOCATION_LIMIT.
  762|       | */
  763|       |#ifdef EIGEN_ALLOCA
  764|       |
  765|       |#if EIGEN_DEFAULT_ALIGN_BYTES > 0
  766|       |// We always manually re-align the result of EIGEN_ALLOCA.
  767|       |// If alloca is already aligned, the compiler should be smart enough to optimize away the re-alignment.
  768|       |
  769|       |#if ((EIGEN_COMP_GNUC || EIGEN_COMP_CLANG) && !EIGEN_COMP_NVHPC)
  770|       |#define EIGEN_ALIGNED_ALLOCA(SIZE) __builtin_alloca_with_align(SIZE, CHAR_BIT* EIGEN_DEFAULT_ALIGN_BYTES)
  771|       |#else
  772|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void* eigen_aligned_alloca_helper(void* ptr) {
  773|       |  constexpr std::uintptr_t mask = EIGEN_DEFAULT_ALIGN_BYTES - 1;
  774|       |  std::uintptr_t ptr_int = std::uintptr_t(ptr);
  775|       |  std::uintptr_t aligned_ptr_int = (ptr_int + mask) & ~mask;
  776|       |  std::uintptr_t offset = aligned_ptr_int - ptr_int;
  777|       |  return static_cast<void*>(static_cast<uint8_t*>(ptr) + offset);
  778|       |}
  779|       |#define EIGEN_ALIGNED_ALLOCA(SIZE) eigen_aligned_alloca_helper(EIGEN_ALLOCA(SIZE + EIGEN_DEFAULT_ALIGN_BYTES - 1))
  780|       |#endif
  781|       |
  782|       |#else
  783|       |#define EIGEN_ALIGNED_ALLOCA(SIZE) EIGEN_ALLOCA(SIZE)
  784|       |#endif
  785|       |
  786|       |#define ei_declare_aligned_stack_constructed_variable(TYPE, NAME, SIZE, BUFFER)                                     \
  787|       |  Eigen::internal::check_size_for_overflow<TYPE>(SIZE);                                                             \
  788|       |  TYPE* NAME = (BUFFER) != 0 ? (BUFFER)                                                                             \
  789|       |                             : reinterpret_cast<TYPE*>((sizeof(TYPE) * SIZE <= EIGEN_STACK_ALLOCATION_LIMIT)        \
  790|       |                                                           ? EIGEN_ALIGNED_ALLOCA(sizeof(TYPE) * SIZE)              \
  791|       |                                                           : Eigen::internal::aligned_malloc(sizeof(TYPE) * SIZE)); \
  792|       |  Eigen::internal::aligned_stack_memory_handler<TYPE> EIGEN_CAT(NAME, _stack_memory_destructor)(                    \
  793|       |      (BUFFER) == 0 ? NAME : 0, SIZE, sizeof(TYPE) * SIZE > EIGEN_STACK_ALLOCATION_LIMIT)
  794|       |
  795|       |#define ei_declare_local_nested_eval(XPR_T, XPR, N, NAME)                                        \
  796|       |  Eigen::internal::local_nested_eval_wrapper<XPR_T, N> EIGEN_CAT(NAME, _wrapper)(                \
  797|       |      XPR, reinterpret_cast<typename XPR_T::Scalar*>(                                            \
  798|       |               ((Eigen::internal::local_nested_eval_wrapper<XPR_T, N>::NeedExternalBuffer) &&    \
  799|       |                ((sizeof(typename XPR_T::Scalar) * XPR.size()) <= EIGEN_STACK_ALLOCATION_LIMIT)) \
  800|       |                   ? EIGEN_ALIGNED_ALLOCA(sizeof(typename XPR_T::Scalar) * XPR.size())           \
  801|       |                   : 0));                                                                        \
  802|       |  typename Eigen::internal::local_nested_eval_wrapper<XPR_T, N>::ObjectType NAME(EIGEN_CAT(NAME, _wrapper).object)
  803|       |
  804|       |#else
  805|       |
  806|       |#define ei_declare_aligned_stack_constructed_variable(TYPE, NAME, SIZE, BUFFER)                                        \
  807|       |  Eigen::internal::check_size_for_overflow<TYPE>(SIZE);                                                                \
  808|       |  TYPE* NAME = (BUFFER) != 0 ? BUFFER : reinterpret_cast<TYPE*>(Eigen::internal::aligned_malloc(sizeof(TYPE) * SIZE)); \
  809|       |  Eigen::internal::aligned_stack_memory_handler<TYPE> EIGEN_CAT(NAME, _stack_memory_destructor)(                       \
  810|       |      (BUFFER) == 0 ? NAME : 0, SIZE, true)
  811|       |
  812|       |#define ei_declare_local_nested_eval(XPR_T, XPR, N, NAME) \
  813|       |  typename Eigen::internal::nested_eval<XPR_T, N>::type NAME(XPR)
  814|       |
  815|       |#endif
  816|       |
  817|       |/*****************************************************************************
  818|       |*** Implementation of EIGEN_MAKE_ALIGNED_OPERATOR_NEW [_IF]                ***
  819|       |*****************************************************************************/
  820|       |
  821|       |#if EIGEN_HAS_CXX17_OVERALIGN
  822|       |
  823|       |// C++17 -> no need to bother about alignment anymore :)
  824|       |
  825|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_NOTHROW(NeedsToAlign)
  826|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(NeedsToAlign)
  827|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW
  828|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF_VECTORIZABLE_FIXED_SIZE(Scalar, Size)
  829|       |
  830|       |#else
  831|       |
  832|       |// HIP does not support new/delete on device.
  833|       |#if EIGEN_MAX_ALIGN_BYTES != 0 && !defined(EIGEN_HIP_DEVICE_COMPILE)
  834|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_NOTHROW(NeedsToAlign)                                    \
  835|       |  EIGEN_DEVICE_FUNC void* operator new(std::size_t size, const std::nothrow_t&) EIGEN_NO_THROW { \
  836|       |    EIGEN_TRY { return Eigen::internal::conditional_aligned_malloc<NeedsToAlign>(size); }        \
  837|       |    EIGEN_CATCH(...) { return 0; }                                                               \
  838|       |  }
  839|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(NeedsToAlign)                                                             \
  840|       |  EIGEN_DEVICE_FUNC void* operator new(std::size_t size) {                                                           \
  841|       |    return Eigen::internal::conditional_aligned_malloc<NeedsToAlign>(size);                                          \
  842|       |  }                                                                                                                  \
  843|       |  EIGEN_DEVICE_FUNC void* operator new[](std::size_t size) {                                                         \
  844|       |    return Eigen::internal::conditional_aligned_malloc<NeedsToAlign>(size);                                          \
  845|       |  }                                                                                                                  \
  846|       |  EIGEN_DEVICE_FUNC void operator delete(void* ptr) EIGEN_NO_THROW {                                                 \
  847|       |    Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr);                                                    \
  848|       |  }                                                                                                                  \
  849|       |  EIGEN_DEVICE_FUNC void operator delete[](void* ptr) EIGEN_NO_THROW {                                               \
  850|       |    Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr);                                                    \
  851|       |  }                                                                                                                  \
  852|       |  EIGEN_DEVICE_FUNC void operator delete(void* ptr, std::size_t /* sz */) EIGEN_NO_THROW {                           \
  853|       |    Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr);                                                    \
  854|       |  }                                                                                                                  \
  855|       |  EIGEN_DEVICE_FUNC void operator delete[](void* ptr, std::size_t /* sz */) EIGEN_NO_THROW {                         \
  856|       |    Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr);                                                    \
  857|       |  }                                                                                                                  \
  858|       |  /* in-place new and delete. since (at least afaik) there is no actual   */                                         \
  859|       |  /* memory allocated we can safely let the default implementation handle */                                         \
  860|       |  /* this particular case. */                                                                                        \
  861|       |  EIGEN_DEVICE_FUNC static void* operator new(std::size_t size, void* ptr) { return ::operator new(size, ptr); }     \
  862|       |  EIGEN_DEVICE_FUNC static void* operator new[](std::size_t size, void* ptr) { return ::operator new[](size, ptr); } \
  863|       |  EIGEN_DEVICE_FUNC void operator delete(void* memory, void* ptr) EIGEN_NO_THROW {                                   \
  864|       |    return ::operator delete(memory, ptr);                                                                           \
  865|       |  }                                                                                                                  \
  866|       |  EIGEN_DEVICE_FUNC void operator delete[](void* memory, void* ptr) EIGEN_NO_THROW {                                 \
  867|       |    return ::operator delete[](memory, ptr);                                                                         \
  868|       |  }                                                                                                                  \
  869|       |  /* nothrow-new (returns zero instead of std::bad_alloc) */                                                         \
  870|       |  EIGEN_MAKE_ALIGNED_OPERATOR_NEW_NOTHROW(NeedsToAlign)                                                              \
  871|       |  EIGEN_DEVICE_FUNC void operator delete(void* ptr, const std::nothrow_t&) EIGEN_NO_THROW {                          \
  872|       |    Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr);                                                    \
  873|       |  }                                                                                                                  \
  874|       |  typedef void eigen_aligned_operator_new_marker_type;
  875|       |#else
  876|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(NeedsToAlign)
  877|       |#endif
  878|       |
  879|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(true)
  880|       |#define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF_VECTORIZABLE_FIXED_SIZE(Scalar, Size)                                 \
  881|       |  EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(                                                                            \
  882|       |      bool(((Size) != Eigen::Dynamic) &&                                                                         \
  883|       |           (((EIGEN_MAX_ALIGN_BYTES >= 16) && ((sizeof(Scalar) * (Size)) % (EIGEN_MAX_ALIGN_BYTES) == 0)) ||     \
  884|       |            ((EIGEN_MAX_ALIGN_BYTES >= 32) && ((sizeof(Scalar) * (Size)) % (EIGEN_MAX_ALIGN_BYTES / 2) == 0)) || \
  885|       |            ((EIGEN_MAX_ALIGN_BYTES >= 64) && ((sizeof(Scalar) * (Size)) % (EIGEN_MAX_ALIGN_BYTES / 4) == 0)))))
  886|       |
  887|       |#endif
  888|       |
  889|       |/****************************************************************************/
  890|       |
  891|       |/** \class aligned_allocator
  892|       | * \ingroup Core_Module
  893|       | *
  894|       | * \brief STL compatible allocator to use with types requiring a non-standard alignment.
  895|       | *
  896|       | * The memory is aligned as for dynamically aligned matrix/array types such as MatrixXd.
  897|       | * By default, it will thus provide at least 16 bytes alignment and more in following cases:
  898|       | *  - 32 bytes alignment if AVX is enabled.
  899|       | *  - 64 bytes alignment if AVX512 is enabled.
  900|       | *
  901|       | * This can be controlled using the \c EIGEN_MAX_ALIGN_BYTES macro as documented
  902|       | * \link TopicPreprocessorDirectivesPerformance there \endlink.
  903|       | *
  904|       | * Example:
  905|       | * \code
  906|       | * // Matrix4f requires 16 bytes alignment:
  907|       | * std::map< int, Matrix4f, std::less<int>,
  908|       | *           aligned_allocator<std::pair<const int, Matrix4f> > > my_map_mat4;
  909|       | * // Vector3f does not require 16 bytes alignment, no need to use Eigen's allocator:
  910|       | * std::map< int, Vector3f > my_map_vec3;
  911|       | * \endcode
  912|       | *
  913|       | * \sa \blank \ref TopicStlContainers.
  914|       | */
  915|       |template <class T>
  916|       |class aligned_allocator {
  917|       | public:
  918|       |  typedef std::size_t size_type;
  919|       |  typedef std::ptrdiff_t difference_type;
  920|       |  typedef T* pointer;
  921|       |  typedef const T* const_pointer;
  922|       |  typedef T& reference;
  923|       |  typedef const T& const_reference;
  924|       |  typedef T value_type;
  925|       |
  926|       |  template <class U>
  927|       |  struct rebind {
  928|       |    typedef aligned_allocator<U> other;
  929|       |  };
  930|       |
  931|       |  aligned_allocator() = default;
  932|       |
  933|       |  aligned_allocator(const aligned_allocator&) = default;
  934|       |
  935|       |  template <class U>
  936|       |  aligned_allocator(const aligned_allocator<U>&) {}
  937|       |
  938|       |  template <class U>
  939|       |  constexpr bool operator==(const aligned_allocator<U>&) const noexcept {
  940|       |    return true;
  941|       |  }
  942|       |  template <class U>
  943|       |  constexpr bool operator!=(const aligned_allocator<U>&) const noexcept {
  944|       |    return false;
  945|       |  }
  946|       |
  947|       |#if EIGEN_COMP_GNUC_STRICT && EIGEN_GNUC_STRICT_AT_LEAST(7, 0, 0)
  948|       |  // In gcc std::allocator::max_size() is bugged making gcc triggers a warning:
  949|       |  // eigen/Eigen/src/Core/util/Memory.h:189:12: warning: argument 1 value '18446744073709551612' exceeds maximum object
  950|       |  // size 9223372036854775807 See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87544
  951|       |  size_type max_size() const { return (std::numeric_limits<std::ptrdiff_t>::max)() / sizeof(T); }
  952|       |#endif
  953|       |
  954|       |  pointer allocate(size_type num, const void* /*hint*/ = 0) {
  955|       |    internal::check_size_for_overflow<T>(num);
  956|       |    return static_cast<pointer>(internal::aligned_malloc(num * sizeof(T)));
  957|       |  }
  958|       |
  959|       |  void deallocate(pointer p, size_type /*num*/) { internal::aligned_free(p); }
  960|       |};
  961|       |
  962|       |//---------- Cache sizes ----------
  963|       |
  964|       |#if !defined(EIGEN_NO_CPUID)
  965|       |#if EIGEN_COMP_GNUC && EIGEN_ARCH_i386_OR_x86_64
  966|       |#if defined(__PIC__) && EIGEN_ARCH_i386
  967|       |// Case for x86 with PIC
  968|       |#define EIGEN_CPUID(abcd, func, id)                                                  \
  969|       |  __asm__ __volatile__("xchgl %%ebx, %k1;cpuid; xchgl %%ebx,%k1"                     \
  970|       |                       : "=a"(abcd[0]), "=&r"(abcd[1]), "=c"(abcd[2]), "=d"(abcd[3]) \
  971|       |                       : "a"(func), "c"(id));
  972|       |#elif defined(__PIC__) && EIGEN_ARCH_x86_64
  973|       |// Case for x64 with PIC. In theory this is only a problem with recent gcc and with medium or large code model, not with
  974|       |// the default small code model. However, we cannot detect which code model is used, and the xchg overhead is negligible
  975|       |// anyway.
  976|       |#define EIGEN_CPUID(abcd, func, id)                                                  \
  977|       |  __asm__ __volatile__("xchg{q}\t{%%}rbx, %q1; cpuid; xchg{q}\t{%%}rbx, %q1"         \
  978|       |                       : "=a"(abcd[0]), "=&r"(abcd[1]), "=c"(abcd[2]), "=d"(abcd[3]) \
  979|       |                       : "0"(func), "2"(id));
  980|       |#else
  981|       |// Case for x86_64 or x86 w/o PIC
  982|       |#define EIGEN_CPUID(abcd, func, id) \
  983|       |  __asm__ __volatile__("cpuid" : "=a"(abcd[0]), "=b"(abcd[1]), "=c"(abcd[2]), "=d"(abcd[3]) : "0"(func), "2"(id));
  984|       |#endif
  985|       |#elif EIGEN_COMP_MSVC
  986|       |#if EIGEN_ARCH_i386_OR_x86_64
  987|       |#define EIGEN_CPUID(abcd, func, id) __cpuidex((int*)abcd, func, id)
  988|       |#endif
  989|       |#endif
  990|       |#endif
  991|       |
  992|       |namespace internal {
  993|       |
  994|       |#ifdef EIGEN_CPUID
  995|       |
  996|      0|inline bool cpuid_is_vendor(int abcd[4], const int vendor[3]) {
  997|      0|  return abcd[1] == vendor[0] && abcd[3] == vendor[1] && abcd[2] == vendor[2];
  998|      0|}
  999|       |
 1000|      0|inline void queryCacheSizes_intel_direct(int& l1, int& l2, int& l3) {
 1001|      0|  int abcd[4];
 1002|      0|  l1 = l2 = l3 = 0;
 1003|      0|  int cache_id = 0;
 1004|      0|  int cache_type = 0;
 1005|      0|  do {
 1006|      0|    abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
 1007|      0|    EIGEN_CPUID(abcd, 0x4, cache_id);
 1008|      0|    cache_type = (abcd[0] & 0x0F) >> 0;
 1009|      0|    if (cache_type == 1 || cache_type == 3)  // data or unified cache
 1010|      0|    {
 1011|      0|      int cache_level = (abcd[0] & 0xE0) >> 5;        // A[7:5]
 1012|      0|      int ways = (abcd[1] & 0xFFC00000) >> 22;        // B[31:22]
 1013|      0|      int partitions = (abcd[1] & 0x003FF000) >> 12;  // B[21:12]
 1014|      0|      int line_size = (abcd[1] & 0x00000FFF) >> 0;    // B[11:0]
 1015|      0|      int sets = (abcd[2]);                           // C[31:0]
 1016|      0|
 1017|      0|      int cache_size = (ways + 1) * (partitions + 1) * (line_size + 1) * (sets + 1);
 1018|      0|
 1019|      0|      switch (cache_level) {
 1020|      0|        case 1:
 1021|      0|          l1 = cache_size;
 1022|      0|          break;
 1023|      0|        case 2:
 1024|      0|          l2 = cache_size;
 1025|      0|          break;
 1026|      0|        case 3:
 1027|      0|          l3 = cache_size;
 1028|      0|          break;
 1029|      0|        default:
 1030|      0|          break;
 1031|      0|      }
 1032|      0|    }
 1033|      0|    cache_id++;
 1034|      0|  } while (cache_type > 0 && cache_id < 16);
 1035|      0|}
 1036|       |
 1037|      0|inline void queryCacheSizes_intel_codes(int& l1, int& l2, int& l3) {
 1038|      0|  int abcd[4];
 1039|      0|  abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
 1040|      0|  l1 = l2 = l3 = 0;
 1041|      0|  EIGEN_CPUID(abcd, 0x00000002, 0);
 1042|      0|  unsigned char* bytes = reinterpret_cast<unsigned char*>(abcd) + 2;
 1043|      0|  bool check_for_p2_core2 = false;
 1044|      0|  for (int i = 0; i < 14; ++i) {
 1045|      0|    switch (bytes[i]) {
 1046|      0|      case 0x0A:
 1047|      0|        l1 = 8;
 1048|      0|        break;  // 0Ah   data L1 cache, 8 KB, 2 ways, 32 byte lines
 1049|      0|      case 0x0C:
 1050|      0|        l1 = 16;
 1051|      0|        break;  // 0Ch   data L1 cache, 16 KB, 4 ways, 32 byte lines
 1052|      0|      case 0x0E:
 1053|      0|        l1 = 24;
 1054|      0|        break;  // 0Eh   data L1 cache, 24 KB, 6 ways, 64 byte lines
 1055|      0|      case 0x10:
 1056|      0|        l1 = 16;
 1057|      0|        break;  // 10h   data L1 cache, 16 KB, 4 ways, 32 byte lines (IA-64)
 1058|      0|      case 0x15:
 1059|      0|        l1 = 16;
 1060|      0|        break;  // 15h   code L1 cache, 16 KB, 4 ways, 32 byte lines (IA-64)
 1061|      0|      case 0x2C:
 1062|      0|        l1 = 32;
 1063|      0|        break;  // 2Ch   data L1 cache, 32 KB, 8 ways, 64 byte lines
 1064|      0|      case 0x30:
 1065|      0|        l1 = 32;
 1066|      0|        break;  // 30h   code L1 cache, 32 KB, 8 ways, 64 byte lines
 1067|      0|      case 0x60:
 1068|      0|        l1 = 16;
 1069|      0|        break;  // 60h   data L1 cache, 16 KB, 8 ways, 64 byte lines, sectored
 1070|      0|      case 0x66:
 1071|      0|        l1 = 8;
 1072|      0|        break;  // 66h   data L1 cache, 8 KB, 4 ways, 64 byte lines, sectored
 1073|      0|      case 0x67:
 1074|      0|        l1 = 16;
 1075|      0|        break;  // 67h   data L1 cache, 16 KB, 4 ways, 64 byte lines, sectored
 1076|      0|      case 0x68:
 1077|      0|        l1 = 32;
 1078|      0|        break;  // 68h   data L1 cache, 32 KB, 4 ways, 64 byte lines, sectored
 1079|      0|      case 0x1A:
 1080|      0|        l2 = 96;
 1081|      0|        break;  // code and data L2 cache, 96 KB, 6 ways, 64 byte lines (IA-64)
 1082|      0|      case 0x22:
 1083|      0|        l3 = 512;
 1084|      0|        break;  // code and data L3 cache, 512 KB, 4 ways (!), 64 byte lines, dual-sectored
 1085|      0|      case 0x23:
 1086|      0|        l3 = 1024;
 1087|      0|        break;  // code and data L3 cache, 1024 KB, 8 ways, 64 byte lines, dual-sectored
 1088|      0|      case 0x25:
 1089|      0|        l3 = 2048;
 1090|      0|        break;  // code and data L3 cache, 2048 KB, 8 ways, 64 byte lines, dual-sectored
 1091|      0|      case 0x29:
 1092|      0|        l3 = 4096;
 1093|      0|        break;  // code and data L3 cache, 4096 KB, 8 ways, 64 byte lines, dual-sectored
 1094|      0|      case 0x39:
 1095|      0|        l2 = 128;
 1096|      0|        break;  // code and data L2 cache, 128 KB, 4 ways, 64 byte lines, sectored
 1097|      0|      case 0x3A:
 1098|      0|        l2 = 192;
 1099|      0|        break;  // code and data L2 cache, 192 KB, 6 ways, 64 byte lines, sectored
 1100|      0|      case 0x3B:
 1101|      0|        l2 = 128;
 1102|      0|        break;  // code and data L2 cache, 128 KB, 2 ways, 64 byte lines, sectored
 1103|      0|      case 0x3C:
 1104|      0|        l2 = 256;
 1105|      0|        break;  // code and data L2 cache, 256 KB, 4 ways, 64 byte lines, sectored
 1106|      0|      case 0x3D:
 1107|      0|        l2 = 384;
 1108|      0|        break;  // code and data L2 cache, 384 KB, 6 ways, 64 byte lines, sectored
 1109|      0|      case 0x3E:
 1110|      0|        l2 = 512;
 1111|      0|        break;  // code and data L2 cache, 512 KB, 4 ways, 64 byte lines, sectored
 1112|      0|      case 0x40:
 1113|      0|        l2 = 0;
 1114|      0|        break;  // no integrated L2 cache (P6 core) or L3 cache (P4 core)
 1115|      0|      case 0x41:
 1116|      0|        l2 = 128;
 1117|      0|        break;  // code and data L2 cache, 128 KB, 4 ways, 32 byte lines
 1118|      0|      case 0x42:
 1119|      0|        l2 = 256;
 1120|      0|        break;  // code and data L2 cache, 256 KB, 4 ways, 32 byte lines
 1121|      0|      case 0x43:
 1122|      0|        l2 = 512;
 1123|      0|        break;  // code and data L2 cache, 512 KB, 4 ways, 32 byte lines
 1124|      0|      case 0x44:
 1125|      0|        l2 = 1024;
 1126|      0|        break;  // code and data L2 cache, 1024 KB, 4 ways, 32 byte lines
 1127|      0|      case 0x45:
 1128|      0|        l2 = 2048;
 1129|      0|        break;  // code and data L2 cache, 2048 KB, 4 ways, 32 byte lines
 1130|      0|      case 0x46:
 1131|      0|        l3 = 4096;
 1132|      0|        break;  // code and data L3 cache, 4096 KB, 4 ways, 64 byte lines
 1133|      0|      case 0x47:
 1134|      0|        l3 = 8192;
 1135|      0|        break;  // code and data L3 cache, 8192 KB, 8 ways, 64 byte lines
 1136|      0|      case 0x48:
 1137|      0|        l2 = 3072;
 1138|      0|        break;  // code and data L2 cache, 3072 KB, 12 ways, 64 byte lines
 1139|      0|      case 0x49:
 1140|      0|        if (l2 != 0)
 1141|      0|          l3 = 4096;
 1142|      0|        else {
 1143|      0|          check_for_p2_core2 = true;
 1144|      0|          l3 = l2 = 4096;
 1145|      0|        }
 1146|      0|        break;  // code and data L3 cache, 4096 KB, 16 ways, 64 byte lines (P4) or L2 for core2
 1147|      0|      case 0x4A:
 1148|      0|        l3 = 6144;
 1149|      0|        break;  // code and data L3 cache, 6144 KB, 12 ways, 64 byte lines
 1150|      0|      case 0x4B:
 1151|      0|        l3 = 8192;
 1152|      0|        break;  // code and data L3 cache, 8192 KB, 16 ways, 64 byte lines
 1153|      0|      case 0x4C:
 1154|      0|        l3 = 12288;
 1155|      0|        break;  // code and data L3 cache, 12288 KB, 12 ways, 64 byte lines
 1156|      0|      case 0x4D:
 1157|      0|        l3 = 16384;
 1158|      0|        break;  // code and data L3 cache, 16384 KB, 16 ways, 64 byte lines
 1159|      0|      case 0x4E:
 1160|      0|        l2 = 6144;
 1161|      0|        break;  // code and data L2 cache, 6144 KB, 24 ways, 64 byte lines
 1162|      0|      case 0x78:
 1163|      0|        l2 = 1024;
 1164|      0|        break;  // code and data L2 cache, 1024 KB, 4 ways, 64 byte lines
 1165|      0|      case 0x79:
 1166|      0|        l2 = 128;
 1167|      0|        break;  // code and data L2 cache, 128 KB, 8 ways, 64 byte lines, dual-sectored
 1168|      0|      case 0x7A:
 1169|      0|        l2 = 256;
 1170|      0|        break;  // code and data L2 cache, 256 KB, 8 ways, 64 byte lines, dual-sectored
 1171|      0|      case 0x7B:
 1172|      0|        l2 = 512;
 1173|      0|        break;  // code and data L2 cache, 512 KB, 8 ways, 64 byte lines, dual-sectored
 1174|      0|      case 0x7C:
 1175|      0|        l2 = 1024;
 1176|      0|        break;  // code and data L2 cache, 1024 KB, 8 ways, 64 byte lines, dual-sectored
 1177|      0|      case 0x7D:
 1178|      0|        l2 = 2048;
 1179|      0|        break;  // code and data L2 cache, 2048 KB, 8 ways, 64 byte lines
 1180|      0|      case 0x7E:
 1181|      0|        l2 = 256;
 1182|      0|        break;  // code and data L2 cache, 256 KB, 8 ways, 128 byte lines, sect. (IA-64)
 1183|      0|      case 0x7F:
 1184|      0|        l2 = 512;
 1185|      0|        break;  // code and data L2 cache, 512 KB, 2 ways, 64 byte lines
 1186|      0|      case 0x80:
 1187|      0|        l2 = 512;
 1188|      0|        break;  // code and data L2 cache, 512 KB, 8 ways, 64 byte lines
 1189|      0|      case 0x81:
 1190|      0|        l2 = 128;
 1191|      0|        break;  // code and data L2 cache, 128 KB, 8 ways, 32 byte lines
 1192|      0|      case 0x82:
 1193|      0|        l2 = 256;
 1194|      0|        break;  // code and data L2 cache, 256 KB, 8 ways, 32 byte lines
 1195|      0|      case 0x83:
 1196|      0|        l2 = 512;
 1197|      0|        break;  // code and data L2 cache, 512 KB, 8 ways, 32 byte lines
 1198|      0|      case 0x84:
 1199|      0|        l2 = 1024;
 1200|      0|        break;  // code and data L2 cache, 1024 KB, 8 ways, 32 byte lines
 1201|      0|      case 0x85:
 1202|      0|        l2 = 2048;
 1203|      0|        break;  // code and data L2 cache, 2048 KB, 8 ways, 32 byte lines
 1204|      0|      case 0x86:
 1205|      0|        l2 = 512;
 1206|      0|        break;  // code and data L2 cache, 512 KB, 4 ways, 64 byte lines
 1207|      0|      case 0x87:
 1208|      0|        l2 = 1024;
 1209|      0|        break;  // code and data L2 cache, 1024 KB, 8 ways, 64 byte lines
 1210|      0|      case 0x88:
 1211|      0|        l3 = 2048;
 1212|      0|        break;  // code and data L3 cache, 2048 KB, 4 ways, 64 byte lines (IA-64)
 1213|      0|      case 0x89:
 1214|      0|        l3 = 4096;
 1215|      0|        break;  // code and data L3 cache, 4096 KB, 4 ways, 64 byte lines (IA-64)
 1216|      0|      case 0x8A:
 1217|      0|        l3 = 8192;
 1218|      0|        break;  // code and data L3 cache, 8192 KB, 4 ways, 64 byte lines (IA-64)
 1219|      0|      case 0x8D:
 1220|      0|        l3 = 3072;
 1221|      0|        break;  // code and data L3 cache, 3072 KB, 12 ways, 128 byte lines (IA-64)
 1222|      0|
 1223|      0|      default:
 1224|      0|        break;
 1225|      0|    }
 1226|      0|  }
 1227|      0|  if (check_for_p2_core2 && l2 == l3) l3 = 0;
 1228|      0|  l1 *= 1024;
 1229|      0|  l2 *= 1024;
 1230|      0|  l3 *= 1024;
 1231|      0|}
 1232|       |
 1233|      0|inline void queryCacheSizes_intel(int& l1, int& l2, int& l3, int max_std_funcs) {
 1234|      0|  if (max_std_funcs >= 4)
 1235|      0|    queryCacheSizes_intel_direct(l1, l2, l3);
 1236|      0|  else if (max_std_funcs >= 2)
 1237|      0|    queryCacheSizes_intel_codes(l1, l2, l3);
 1238|      0|  else
 1239|      0|    l1 = l2 = l3 = 0;
 1240|      0|}
 1241|       |
 1242|      0|inline void queryCacheSizes_amd(int& l1, int& l2, int& l3) {
 1243|      0|  int abcd[4];
 1244|      0|  abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
 1245|      0|
 1246|      0|  // First query the max supported function.
 1247|      0|  EIGEN_CPUID(abcd, 0x80000000, 0);
 1248|      0|  if (static_cast<numext::uint32_t>(abcd[0]) >= static_cast<numext::uint32_t>(0x80000006)) {
 1249|      0|    EIGEN_CPUID(abcd, 0x80000005, 0);
 1250|      0|    l1 = (abcd[2] >> 24) * 1024;  // C[31:24] = L1 size in KB
 1251|      0|    abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
 1252|      0|    EIGEN_CPUID(abcd, 0x80000006, 0);
 1253|      0|    l2 = (abcd[2] >> 16) * 1024;                      // C[31;16] = l2 cache size in KB
 1254|      0|    l3 = ((abcd[3] & 0xFFFC000) >> 18) * 512 * 1024;  // D[31;18] = l3 cache size in 512KB
 1255|      0|  } else {
 1256|      0|    l1 = l2 = l3 = 0;
 1257|      0|  }
 1258|      0|}
 1259|       |#endif
 1260|       |
 1261|       |/** \internal
 1262|       | * Queries and returns the cache sizes in Bytes of the L1, L2, and L3 data caches respectively */
 1263|      0|inline void queryCacheSizes(int& l1, int& l2, int& l3) {
 1264|      0|#ifdef EIGEN_CPUID
 1265|      0|  int abcd[4];
 1266|      0|  const int GenuineIntel[] = {0x756e6547, 0x49656e69, 0x6c65746e};
 1267|      0|  const int AuthenticAMD[] = {0x68747541, 0x69746e65, 0x444d4163};
 1268|      0|  const int AMDisbetter_[] = {0x69444d41, 0x74656273, 0x21726574};  // "AMDisbetter!"
 1269|      0|
 1270|      0|  // identify the CPU vendor
 1271|      0|  EIGEN_CPUID(abcd, 0x0, 0);
 1272|      0|  int max_std_funcs = abcd[0];
 1273|      0|  if (cpuid_is_vendor(abcd, GenuineIntel))
 1274|      0|    queryCacheSizes_intel(l1, l2, l3, max_std_funcs);
 1275|      0|  else if (cpuid_is_vendor(abcd, AuthenticAMD) || cpuid_is_vendor(abcd, AMDisbetter_))
 1276|      0|    queryCacheSizes_amd(l1, l2, l3);
 1277|      0|  else
 1278|      0|    // by default let's use Intel's API
 1279|      0|    queryCacheSizes_intel(l1, l2, l3, max_std_funcs);
 1280|      0|
 1281|      0|    // here is the list of other vendors:
 1282|      0|    //   ||cpuid_is_vendor(abcd,"VIA VIA VIA ")
 1283|      0|    //   ||cpuid_is_vendor(abcd,"CyrixInstead")
 1284|      0|    //   ||cpuid_is_vendor(abcd,"CentaurHauls")
 1285|      0|    //   ||cpuid_is_vendor(abcd,"GenuineTMx86")
 1286|      0|    //   ||cpuid_is_vendor(abcd,"TransmetaCPU")
 1287|      0|    //   ||cpuid_is_vendor(abcd,"RiseRiseRise")
 1288|      0|    //   ||cpuid_is_vendor(abcd,"Geode by NSC")
 1289|      0|    //   ||cpuid_is_vendor(abcd,"SiS SiS SiS ")
 1290|      0|    //   ||cpuid_is_vendor(abcd,"UMC UMC UMC ")
 1291|      0|    //   ||cpuid_is_vendor(abcd,"NexGenDriven")
 1292|      0|#else
 1293|      0|  l1 = l2 = l3 = -1;
 1294|      0|#endif
 1295|      0|}
 1296|       |
 1297|       |/** \internal
 1298|       | * \returns the size in Bytes of the L1 data cache */
 1299|      0|inline int queryL1CacheSize() {
 1300|      0|  int l1(-1), l2, l3;
 1301|      0|  queryCacheSizes(l1, l2, l3);
 1302|      0|  return l1;
 1303|      0|}
 1304|       |
 1305|       |/** \internal
 1306|       | * \returns the size in Bytes of the L2 or L3 cache if this later is present */
 1307|      0|inline int queryTopLevelCacheSize() {
 1308|      0|  int l1, l2(-1), l3(-1);
 1309|      0|  queryCacheSizes(l1, l2, l3);
 1310|      0|  return (std::max)(l2, l3);
 1311|      0|}
 1312|       |
 1313|       |/** \internal
 1314|       | * This wraps C++20's std::construct_at, using placement new instead if it is not available.
 1315|       | */
 1316|       |
 1317|       |#if EIGEN_COMP_CXXVER >= 20
 1318|       |using std::construct_at;
 1319|       |#else
 1320|       |template <class T, class... Args>
 1321|       |EIGEN_DEVICE_FUNC T* construct_at(T* p, Args&&... args) {
 1322|       |  return ::new (const_cast<void*>(static_cast<const volatile void*>(p))) T(std::forward<Args>(args)...);
 1323|       |}
 1324|       |#endif
 1325|       |
 1326|       |/** \internal
 1327|       | * This wraps C++17's std::destroy_at.  If it's not available it calls the destructor.
 1328|       | * The wrapper is not a full replacement for C++20's std::destroy_at as it cannot
 1329|       | * be applied to std::array.
 1330|       | */
 1331|       |#if EIGEN_COMP_CXXVER >= 17
 1332|       |using std::destroy_at;
 1333|       |#else
 1334|       |template <class T>
 1335|       |EIGEN_DEVICE_FUNC void destroy_at(T* p) {
 1336|       |  p->~T();
 1337|       |}
 1338|       |#endif
 1339|       |
 1340|       |}  // end namespace internal
 1341|       |
 1342|       |}  // end namespace Eigen
 1343|       |
 1344|       |#endif  // EIGEN_MEMORY_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/Meta.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008-2015 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |// Copyright (C) 2006-2008 Benoit Jacob <jacob.benoit.1@gmail.com>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_META_H
   12|       |#define EIGEN_META_H
   13|       |
   14|       |// IWYU pragma: private
   15|       |#include "../InternalHeaderCheck.h"
   16|       |
   17|       |#if defined(EIGEN_GPU_COMPILE_PHASE)
   18|       |
   19|       |#include <cfloat>
   20|       |
   21|       |#if defined(EIGEN_CUDA_ARCH)
   22|       |#include <math_constants.h>
   23|       |#endif
   24|       |
   25|       |#if defined(EIGEN_HIP_DEVICE_COMPILE)
   26|       |#include "Eigen/src/Core/arch/HIP/hcc/math_constants.h"
   27|       |#endif
   28|       |
   29|       |#endif
   30|       |
   31|       |// Define portable (u)int{32,64} types
   32|       |#include <cstdint>
   33|       |
   34|       |namespace Eigen {
   35|       |namespace numext {
   36|       |typedef std::uint8_t uint8_t;
   37|       |typedef std::int8_t int8_t;
   38|       |typedef std::uint16_t uint16_t;
   39|       |typedef std::int16_t int16_t;
   40|       |typedef std::uint32_t uint32_t;
   41|       |typedef std::int32_t int32_t;
   42|       |typedef std::uint64_t uint64_t;
   43|       |typedef std::int64_t int64_t;
   44|       |
   45|       |template <size_t Size>
   46|       |struct get_integer_by_size {
   47|       |  typedef void signed_type;
   48|       |  typedef void unsigned_type;
   49|       |};
   50|       |template <>
   51|       |struct get_integer_by_size<1> {
   52|       |  typedef int8_t signed_type;
   53|       |  typedef uint8_t unsigned_type;
   54|       |};
   55|       |template <>
   56|       |struct get_integer_by_size<2> {
   57|       |  typedef int16_t signed_type;
   58|       |  typedef uint16_t unsigned_type;
   59|       |};
   60|       |template <>
   61|       |struct get_integer_by_size<4> {
   62|       |  typedef int32_t signed_type;
   63|       |  typedef uint32_t unsigned_type;
   64|       |};
   65|       |template <>
   66|       |struct get_integer_by_size<8> {
   67|       |  typedef int64_t signed_type;
   68|       |  typedef uint64_t unsigned_type;
   69|       |};
   70|       |}  // namespace numext
   71|       |}  // namespace Eigen
   72|       |
   73|       |namespace Eigen {
   74|       |
   75|       |typedef EIGEN_DEFAULT_DENSE_INDEX_TYPE DenseIndex;
   76|       |
   77|       |/**
   78|       | * \brief The Index type as used for the API.
   79|       | * \details To change this, \c \#define the preprocessor symbol \c EIGEN_DEFAULT_DENSE_INDEX_TYPE.
   80|       | * \sa \blank \ref TopicPreprocessorDirectives, StorageIndex.
   81|       | */
   82|       |
   83|       |typedef EIGEN_DEFAULT_DENSE_INDEX_TYPE Index;
   84|       |
   85|       |namespace internal {
   86|       |
   87|       |/** \internal
   88|       | * \file Meta.h
   89|       | * This file contains generic metaprogramming classes which are not specifically related to Eigen.
   90|       | * \note In case you wonder, yes we're aware that Boost already provides all these features,
   91|       | * we however don't want to add a dependency to Boost.
   92|       | */
   93|       |
   94|       |struct true_type {
   95|       |  enum { value = 1 };
   96|       |};
   97|       |struct false_type {
   98|       |  enum { value = 0 };
   99|       |};
  100|       |
  101|       |template <bool Condition>
  102|       |struct bool_constant;
  103|       |
  104|       |template <>
  105|       |struct bool_constant<true> : true_type {};
  106|       |
  107|       |template <>
  108|       |struct bool_constant<false> : false_type {};
  109|       |
  110|       |// Third-party libraries rely on these.
  111|       |using std::conditional;
  112|       |using std::remove_const;
  113|       |using std::remove_pointer;
  114|       |using std::remove_reference;
  115|       |
  116|       |template <typename T>
  117|       |struct remove_all {
  118|       |  typedef T type;
  119|       |};
  120|       |template <typename T>
  121|       |struct remove_all<const T> {
  122|       |  typedef typename remove_all<T>::type type;
  123|       |};
  124|       |template <typename T>
  125|       |struct remove_all<T const&> {
  126|       |  typedef typename remove_all<T>::type type;
  127|       |};
  128|       |template <typename T>
  129|       |struct remove_all<T&> {
  130|       |  typedef typename remove_all<T>::type type;
  131|       |};
  132|       |template <typename T>
  133|       |struct remove_all<T const*> {
  134|       |  typedef typename remove_all<T>::type type;
  135|       |};
  136|       |template <typename T>
  137|       |struct remove_all<T*> {
  138|       |  typedef typename remove_all<T>::type type;
  139|       |};
  140|       |
  141|       |template <typename T>
  142|       |using remove_all_t = typename remove_all<T>::type;
  143|       |
  144|       |template <typename T>
  145|       |struct is_arithmetic {
  146|       |  enum { value = false };
  147|       |};
  148|       |template <>
  149|       |struct is_arithmetic<float> {
  150|       |  enum { value = true };
  151|       |};
  152|       |template <>
  153|       |struct is_arithmetic<double> {
  154|       |  enum { value = true };
  155|       |};
  156|       |// GPU devices treat `long double` as `double`.
  157|       |#ifndef EIGEN_GPU_COMPILE_PHASE
  158|       |template <>
  159|       |struct is_arithmetic<long double> {
  160|       |  enum { value = true };
  161|       |};
  162|       |#endif
  163|       |template <>
  164|       |struct is_arithmetic<bool> {
  165|       |  enum { value = true };
  166|       |};
  167|       |template <>
  168|       |struct is_arithmetic<char> {
  169|       |  enum { value = true };
  170|       |};
  171|       |template <>
  172|       |struct is_arithmetic<signed char> {
  173|       |  enum { value = true };
  174|       |};
  175|       |template <>
  176|       |struct is_arithmetic<unsigned char> {
  177|       |  enum { value = true };
  178|       |};
  179|       |template <>
  180|       |struct is_arithmetic<signed short> {
  181|       |  enum { value = true };
  182|       |};
  183|       |template <>
  184|       |struct is_arithmetic<unsigned short> {
  185|       |  enum { value = true };
  186|       |};
  187|       |template <>
  188|       |struct is_arithmetic<signed int> {
  189|       |  enum { value = true };
  190|       |};
  191|       |template <>
  192|       |struct is_arithmetic<unsigned int> {
  193|       |  enum { value = true };
  194|       |};
  195|       |template <>
  196|       |struct is_arithmetic<signed long> {
  197|       |  enum { value = true };
  198|       |};
  199|       |template <>
  200|       |struct is_arithmetic<unsigned long> {
  201|       |  enum { value = true };
  202|       |};
  203|       |
  204|       |template <typename T, typename U>
  205|       |struct is_same {
  206|       |  enum { value = 0 };
  207|       |};
  208|       |template <typename T>
  209|       |struct is_same<T, T> {
  210|       |  enum { value = 1 };
  211|       |};
  212|       |
  213|       |template <class T>
  214|       |struct is_void : is_same<void, std::remove_const_t<T>> {};
  215|       |
  216|       |/** \internal
  217|       | * Implementation of std::void_t for SFINAE.
  218|       | *
  219|       | * Pre C++17:
  220|       | * Custom implementation.
  221|       | *
  222|       | * Post C++17: Uses std::void_t
  223|       | */
  224|       |#if EIGEN_COMP_CXXVER >= 17
  225|       |using std::void_t;
  226|       |#else
  227|       |template <typename...>
  228|       |using void_t = void;
  229|       |#endif
  230|       |
  231|       |template <>
  232|       |struct is_arithmetic<signed long long> {
  233|       |  enum { value = true };
  234|       |};
  235|       |template <>
  236|       |struct is_arithmetic<unsigned long long> {
  237|       |  enum { value = true };
  238|       |};
  239|       |using std::is_integral;
  240|       |
  241|       |using std::make_unsigned;
  242|       |
  243|       |template <typename T>
  244|       |struct is_const {
  245|       |  enum { value = 0 };
  246|       |};
  247|       |template <typename T>
  248|       |struct is_const<T const> {
  249|       |  enum { value = 1 };
  250|       |};
  251|       |
  252|       |template <typename T>
  253|       |struct add_const_on_value_type {
  254|       |  typedef const T type;
  255|       |};
  256|       |template <typename T>
  257|       |struct add_const_on_value_type<T&> {
  258|       |  typedef T const& type;
  259|       |};
  260|       |template <typename T>
  261|       |struct add_const_on_value_type<T*> {
  262|       |  typedef T const* type;
  263|       |};
  264|       |template <typename T>
  265|       |struct add_const_on_value_type<T* const> {
  266|       |  typedef T const* const type;
  267|       |};
  268|       |template <typename T>
  269|       |struct add_const_on_value_type<T const* const> {
  270|       |  typedef T const* const type;
  271|       |};
  272|       |
  273|       |template <typename T>
  274|       |using add_const_on_value_type_t = typename add_const_on_value_type<T>::type;
  275|       |
  276|       |using std::is_convertible;
  277|       |
  278|       |/** \internal
  279|       | * A base class do disable default copy ctor and copy assignment operator.
  280|       | */
  281|       |class noncopyable {
  282|       |  EIGEN_DEVICE_FUNC noncopyable(const noncopyable&);
  283|       |  EIGEN_DEVICE_FUNC const noncopyable& operator=(const noncopyable&);
  284|       |
  285|       | protected:
  286|      0|  EIGEN_DEVICE_FUNC noncopyable() {}
  287|      0|  EIGEN_DEVICE_FUNC ~noncopyable() {}
  288|       |};
  289|       |
  290|       |/** \internal
  291|       | * Provides access to the number of elements in the object of as a compile-time constant expression.
  292|       | * It "returns" Eigen::Dynamic if the size cannot be resolved at compile-time (default).
  293|       | *
  294|       | * Similar to std::tuple_size, but more general.
  295|       | *
  296|       | * It currently supports:
  297|       | *  - any types T defining T::SizeAtCompileTime
  298|       | *  - plain C arrays as T[N]
  299|       | *  - std::array (c++11)
  300|       | *  - some internal types such as SingleRange and AllRange
  301|       | *
  302|       | * The second template parameter eases SFINAE-based specializations.
  303|       | */
  304|       |template <typename T, typename EnableIf = void>
  305|       |struct array_size {
  306|       |  static constexpr Index value = Dynamic;
  307|       |};
  308|       |
  309|       |template <typename T>
  310|       |struct array_size<T, std::enable_if_t<((T::SizeAtCompileTime & 0) == 0)>> {
  311|       |  static constexpr Index value = T::SizeAtCompileTime;
  312|       |};
  313|       |
  314|       |template <typename T, int N>
  315|       |struct array_size<const T (&)[N]> {
  316|       |  static constexpr Index value = N;
  317|       |};
  318|       |template <typename T, int N>
  319|       |struct array_size<T (&)[N]> {
  320|       |  static constexpr Index value = N;
  321|       |};
  322|       |
  323|       |template <typename T, std::size_t N>
  324|       |struct array_size<const std::array<T, N>> {
  325|       |  static constexpr Index value = N;
  326|       |};
  327|       |template <typename T, std::size_t N>
  328|       |struct array_size<std::array<T, N>> {
  329|       |  static constexpr Index value = N;
  330|       |};
  331|       |
  332|       |/** \internal
  333|       | * Analogue of the std::ssize free function.
  334|       | * It returns the signed size of the container or view \a x of type \c T
  335|       | *
  336|       | * It currently supports:
  337|       | *  - any types T defining a member T::size() const
  338|       | *  - plain C arrays as T[N]
  339|       | *
  340|       | * For C++20, this function just forwards to `std::ssize`, or any ADL discoverable `ssize` function.
  341|       | */
  342|       |#if EIGEN_COMP_CXXVER < 20 || EIGEN_GNUC_STRICT_LESS_THAN(10, 0, 0)
  343|       |template <typename T>
  344|       |EIGEN_CONSTEXPR auto index_list_size(const T& x) {
  345|       |  using R = std::common_type_t<std::ptrdiff_t, std::make_signed_t<decltype(x.size())>>;
  346|       |  return static_cast<R>(x.size());
  347|       |}
  348|       |
  349|       |template <typename T, std::ptrdiff_t N>
  350|       |EIGEN_CONSTEXPR std::ptrdiff_t index_list_size(const T (&)[N]) {
  351|       |  return N;
  352|       |}
  353|       |#else
  354|       |template <typename T>
  355|       |EIGEN_CONSTEXPR auto index_list_size(T&& x) {
  356|       |  using std::ssize;
  357|       |  return ssize(std::forward<T>(x));
  358|       |}
  359|       |#endif  // EIGEN_COMP_CXXVER
  360|       |
  361|       |/** \internal
  362|       | * Convenient struct to get the result type of a nullary, unary, binary, or
  363|       | * ternary functor.
  364|       | *
  365|       | * Pre C++17:
  366|       | * This uses std::result_of. However, note the `type` member removes
  367|       | * const and converts references/pointers to their corresponding value type.
  368|       | *
  369|       | * Post C++17: Uses std::invoke_result
  370|       | */
  371|       |#if EIGEN_HAS_STD_INVOKE_RESULT
  372|       |template <typename T>
  373|       |struct result_of;
  374|       |
  375|       |template <typename F, typename... ArgTypes>
  376|       |struct result_of<F(ArgTypes...)> {
  377|       |  typedef typename std::invoke_result<F, ArgTypes...>::type type1;
  378|       |  typedef remove_all_t<type1> type;
  379|       |};
  380|       |
  381|       |template <typename F, typename... ArgTypes>
  382|       |struct invoke_result {
  383|       |  typedef typename std::invoke_result<F, ArgTypes...>::type type1;
  384|       |  typedef remove_all_t<type1> type;
  385|       |};
  386|       |#else
  387|       |template <typename T>
  388|       |struct result_of {
  389|       |  typedef typename std::result_of<T>::type type1;
  390|       |  typedef remove_all_t<type1> type;
  391|       |};
  392|       |
  393|       |template <typename F, typename... ArgTypes>
  394|       |struct invoke_result {
  395|       |  typedef typename result_of<F(ArgTypes...)>::type type1;
  396|       |  typedef remove_all_t<type1> type;
  397|       |};
  398|       |#endif
  399|       |
  400|       |// Reduces a sequence of bools to true if all are true, false otherwise.
  401|       |template <bool... values>
  402|       |using reduce_all =
  403|       |    std::is_same<std::integer_sequence<bool, values..., true>, std::integer_sequence<bool, true, values...>>;
  404|       |
  405|       |// Reduces a sequence of bools to true if any are true, false if all false.
  406|       |template <bool... values>
  407|       |using reduce_any = std::integral_constant<bool, !std::is_same<std::integer_sequence<bool, values..., false>,
  408|       |                                                              std::integer_sequence<bool, false, values...>>::value>;
  409|       |
  410|       |struct meta_yes {
  411|       |  char a[1];
  412|       |};
  413|       |struct meta_no {
  414|       |  char a[2];
  415|       |};
  416|       |
  417|       |// Check whether T::ReturnType does exist
  418|       |template <typename T>
  419|       |struct has_ReturnType {
  420|       |  template <typename C>
  421|       |  static meta_yes testFunctor(C const*, typename C::ReturnType const* = 0);
  422|       |  template <typename C>
  423|       |  static meta_no testFunctor(...);
  424|       |
  425|       |  enum { value = sizeof(testFunctor<T>(static_cast<T*>(0))) == sizeof(meta_yes) };
  426|       |};
  427|       |
  428|       |template <typename T>
  429|       |const T* return_ptr();
  430|       |
  431|       |template <typename T, typename IndexType = Index>
  432|       |struct has_nullary_operator {
  433|       |  template <typename C>
  434|       |  static meta_yes testFunctor(C const*, std::enable_if_t<(sizeof(return_ptr<C>()->operator()()) > 0)>* = 0);
  435|       |  static meta_no testFunctor(...);
  436|       |
  437|       |  enum { value = sizeof(testFunctor(static_cast<T*>(0))) == sizeof(meta_yes) };
  438|       |};
  439|       |
  440|       |template <typename T, typename IndexType = Index>
  441|       |struct has_unary_operator {
  442|       |  template <typename C>
  443|       |  static meta_yes testFunctor(C const*, std::enable_if_t<(sizeof(return_ptr<C>()->operator()(IndexType(0))) > 0)>* = 0);
  444|       |  static meta_no testFunctor(...);
  445|       |
  446|       |  enum { value = sizeof(testFunctor(static_cast<T*>(0))) == sizeof(meta_yes) };
  447|       |};
  448|       |
  449|       |template <typename T, typename IndexType = Index>
  450|       |struct has_binary_operator {
  451|       |  template <typename C>
  452|       |  static meta_yes testFunctor(
  453|       |      C const*, std::enable_if_t<(sizeof(return_ptr<C>()->operator()(IndexType(0), IndexType(0))) > 0)>* = 0);
  454|       |  static meta_no testFunctor(...);
  455|       |
  456|       |  enum { value = sizeof(testFunctor(static_cast<T*>(0))) == sizeof(meta_yes) };
  457|       |};
  458|       |
  459|       |/** \internal In short, it computes int(sqrt(\a Y)) with \a Y an integer.
  460|       | * Usage example: \code meta_sqrt<1023>::ret \endcode
  461|       | */
  462|       |template <int Y, int InfX = 0, int SupX = ((Y == 1) ? 1 : Y / 2),
  463|       |          bool Done = ((SupX - InfX) <= 1 || ((SupX * SupX <= Y) && ((SupX + 1) * (SupX + 1) > Y)))>
  464|       |class meta_sqrt {
  465|       |  enum {
  466|       |    MidX = (InfX + SupX) / 2,
  467|       |    TakeInf = MidX * MidX > Y ? 1 : 0,
  468|       |    NewInf = int(TakeInf) ? InfX : int(MidX),
  469|       |    NewSup = int(TakeInf) ? int(MidX) : SupX
  470|       |  };
  471|       |
  472|       | public:
  473|       |  enum { ret = meta_sqrt<Y, NewInf, NewSup>::ret };
  474|       |};
  475|       |
  476|       |template <int Y, int InfX, int SupX>
  477|       |class meta_sqrt<Y, InfX, SupX, true> {
  478|       | public:
  479|       |  enum { ret = (SupX * SupX <= Y) ? SupX : InfX };
  480|       |};
  481|       |
  482|       |/** \internal Computes the least common multiple of two positive integer A and B
  483|       | * at compile-time.
  484|       | */
  485|       |template <int A, int B, int K = 1, bool Done = ((A * K) % B) == 0, bool Big = (A >= B)>
  486|       |struct meta_least_common_multiple {
  487|       |  enum { ret = meta_least_common_multiple<A, B, K + 1>::ret };
  488|       |};
  489|       |template <int A, int B, int K, bool Done>
  490|       |struct meta_least_common_multiple<A, B, K, Done, false> {
  491|       |  enum { ret = meta_least_common_multiple<B, A, K>::ret };
  492|       |};
  493|       |template <int A, int B, int K>
  494|       |struct meta_least_common_multiple<A, B, K, true, true> {
  495|       |  enum { ret = A * K };
  496|       |};
  497|       |
  498|       |/** \internal determines whether the product of two numeric types is allowed and what the return type is */
  499|       |template <typename T, typename U>
  500|       |struct scalar_product_traits {
  501|       |  enum { Defined = 0 };
  502|       |};
  503|       |
  504|       |// FIXME quick workaround around current limitation of result_of
  505|       |// template<typename Scalar, typename ArgType0, typename ArgType1>
  506|       |// struct result_of<scalar_product_op<Scalar>(ArgType0,ArgType1)> {
  507|       |// typedef typename scalar_product_traits<remove_all_t<ArgType0>, remove_all_t<ArgType1>>::ReturnType type;
  508|       |// };
  509|       |
  510|       |/** \internal Obtains a POD type suitable to use as storage for an object of a size
  511|       | * of at most Len bytes, aligned as specified by \c Align.
  512|       | */
  513|       |template <unsigned Len, unsigned Align>
  514|       |struct aligned_storage {
  515|       |  struct type {
  516|       |    EIGEN_ALIGN_TO_BOUNDARY(Align) unsigned char data[Len];
  517|       |  };
  518|       |};
  519|       |
  520|       |}  // end namespace internal
  521|       |
  522|       |template <typename T>
  523|       |struct NumTraits;
  524|       |
  525|       |namespace numext {
  526|       |
  527|       |#if defined(EIGEN_GPU_COMPILE_PHASE)
  528|       |template <typename T>
  529|       |EIGEN_DEVICE_FUNC void swap(T& a, T& b) {
  530|       |  T tmp = b;
  531|       |  b = a;
  532|       |  a = tmp;
  533|       |}
  534|       |#else
  535|       |template <typename T>
  536|       |EIGEN_STRONG_INLINE void swap(T& a, T& b) {
  537|       |  std::swap(a, b);
  538|       |}
  539|       |#endif
  540|       |
  541|       |using std::numeric_limits;
  542|       |
  543|       |// Handle integer comparisons of different signedness.
  544|       |template <typename X, typename Y, bool XIsInteger = NumTraits<X>::IsInteger, bool XIsSigned = NumTraits<X>::IsSigned,
  545|       |          bool YIsInteger = NumTraits<Y>::IsInteger, bool YIsSigned = NumTraits<Y>::IsSigned>
  546|       |struct equal_strict_impl {
  547|      0|  static EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool run(const X& x, const Y& y) { return x == y; }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implIssLb1ELb1ELb1ELb1EE3runERKsS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implIttLb1ELb0ELb1ELb0EE3runERKtS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implIiiLb1ELb1ELb1ELb1EE3runERKiS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implIjjLb1ELb0ELb1ELb0EE3runERKjS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implIllLb1ELb1ELb1ELb1EE3runERKlS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implImmLb1ELb0ELb1ELb0EE3runERKmS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implIxxLb1ELb1ELb1ELb1EE3runERKxS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implIyyLb1ELb0ELb1ELb0EE3runERKyS4_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implINS_4halfES2_Lb0ELb1ELb0ELb1EE3runERKS2_S5_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext17equal_strict_implINS_8bfloat16ES2_Lb0ELb1ELb0ELb1EE3runERKS2_S5_
  ------------------
  548|       |};
  549|       |template <typename X, typename Y>
  550|       |struct equal_strict_impl<X, Y, true, false, true, true> {
  551|       |  // X is an unsigned integer
  552|       |  // Y is a signed integer
  553|       |  // if Y is non-negative, it may be represented exactly as its unsigned counterpart.
  554|       |  using UnsignedY = typename internal::make_unsigned<Y>::type;
  555|       |  static EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool run(const X& x, const Y& y) {
  556|       |    return y < Y(0) ? false : (x == static_cast<UnsignedY>(y));
  557|       |  }
  558|       |};
  559|       |template <typename X, typename Y>
  560|       |struct equal_strict_impl<X, Y, true, true, true, false> {
  561|       |  // X is a signed integer
  562|       |  // Y is an unsigned integer
  563|       |  static EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool run(const X& x, const Y& y) {
  564|       |    return equal_strict_impl<Y, X>::run(y, x);
  565|       |  }
  566|       |};
  567|       |
  568|       |// The aim of the following functions is to bypass -Wfloat-equal warnings
  569|       |// when we really want a strict equality comparison on floating points.
  570|       |template <typename X, typename Y>
  571|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool equal_strict(const X& x, const Y& y) {
  572|      0|  return equal_strict_impl<X, Y>::run(x, y);
  573|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictIssEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictIttEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictIiiEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictIjjEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictIllEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictImmEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictIxxEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictIyyEEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictINS_4halfES2_EEbRKT_RKT0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen6numext12equal_strictINS_8bfloat16ES2_EEbRKT_RKT0_
  ------------------
  574|       |
  575|       |#if !defined(EIGEN_GPU_COMPILE_PHASE) || (!defined(EIGEN_CUDA_ARCH) && defined(EIGEN_CONSTEXPR_ARE_DEVICE_FUNC))
  576|       |template <>
  577|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool equal_strict(const float& x, const float& y) {
  578|      0|  return std::equal_to<float>()(x, y);
  579|      0|}
  580|       |
  581|       |template <>
  582|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool equal_strict(const double& x, const double& y) {
  583|      0|  return std::equal_to<double>()(x, y);
  584|      0|}
  585|       |#endif
  586|       |
  587|       |/**
  588|       | * \internal Performs an exact comparison of x to zero, e.g. to decide whether a term can be ignored.
  589|       | * Use this to to bypass -Wfloat-equal warnings when exact zero is what needs to be tested.
  590|       | */
  591|       |template <typename X>
  592|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool is_exactly_zero(const X& x) {
  593|       |  return equal_strict(x, typename NumTraits<X>::Literal{0});
  594|       |}
  595|       |
  596|       |/**
  597|       | * \internal Performs an exact comparison of x to one, e.g. to decide whether a factor needs to be multiplied.
  598|       | * Use this to to bypass -Wfloat-equal warnings when exact one is what needs to be tested.
  599|       | */
  600|       |template <typename X>
  601|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool is_exactly_one(const X& x) {
  602|       |  return equal_strict(x, typename NumTraits<X>::Literal{1});
  603|       |}
  604|       |
  605|       |template <typename X, typename Y>
  606|       |EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool not_equal_strict(const X& x, const Y& y) {
  607|       |  return !equal_strict_impl<X, Y>::run(x, y);
  608|       |}
  609|       |
  610|       |#if !defined(EIGEN_GPU_COMPILE_PHASE) || (!defined(EIGEN_CUDA_ARCH) && defined(EIGEN_CONSTEXPR_ARE_DEVICE_FUNC))
  611|       |template <>
  612|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool not_equal_strict(const float& x, const float& y) {
  613|      0|  return std::not_equal_to<float>()(x, y);
  614|      0|}
  615|       |
  616|       |template <>
  617|      0|EIGEN_STRONG_INLINE EIGEN_DEVICE_FUNC bool not_equal_strict(const double& x, const double& y) {
  618|      0|  return std::not_equal_to<double>()(x, y);
  619|      0|}
  620|       |#endif
  621|       |
  622|       |}  // end namespace numext
  623|       |
  624|       |namespace internal {
  625|       |
  626|       |template <typename Scalar>
  627|       |struct is_identically_zero_impl {
  628|       |  static inline bool run(const Scalar& s) { return numext::is_exactly_zero(s); }
  629|       |};
  630|       |
  631|       |template <typename Scalar>
  632|       |EIGEN_STRONG_INLINE bool is_identically_zero(const Scalar& s) {
  633|       |  return is_identically_zero_impl<Scalar>::run(s);
  634|       |}
  635|       |
  636|       |/// \internal Returns true if its argument is of integer or enum type.
  637|       |/// FIXME this has the same purpose as `is_valid_index_type` in XprHelper.h
  638|       |template <typename A>
  639|       |constexpr bool is_int_or_enum_v = std::is_enum<A>::value || std::is_integral<A>::value;
  640|       |
  641|       |template <typename A, typename B>
  642|      0|inline constexpr void plain_enum_asserts(A, B) {
  643|      0|  static_assert(is_int_or_enum_v<A>, "Argument a must be an integer or enum");
  644|      0|  static_assert(is_int_or_enum_v<B>, "Argument b must be an integer or enum");
  645|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18plain_enum_assertsIiiEEvT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18plain_enum_assertsINS_9DenseBaseINS_6MatrixI14AnnoyingScalarLin1ELi1ELi0ELin1ELi1EEEEUt_ES7_EEvT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18plain_enum_assertsINS_9DenseBaseINS_6MatrixI14AnnoyingScalarLin1ELin1ELi0ELin1ELin1EEEEUt_ES7_EEvT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18plain_enum_assertsINS_9DenseBaseINS_5BlockINS_6MatrixI14AnnoyingScalarLin1ELin1ELi0ELin1ELin1EEELin1ELin1ELb0EEEEUt_ES9_EEvT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal18plain_enum_assertsINS_9DenseBaseINS_5BlockINS_6MatrixI14AnnoyingScalarLin1ELi1ELi0ELin1ELi1EEELin1ELi1ELb0EEEEUt_ES9_EEvT_T0_
  ------------------
  646|       |
  647|       |/// \internal Gets the minimum of two values which may be integers or enums
  648|       |template <typename A, typename B>
  649|      0|inline constexpr int plain_enum_min(A a, B b) {
  650|      0|  plain_enum_asserts(a, b);
  651|      0|  return ((int)a <= (int)b) ? (int)a : (int)b;
  652|      0|}
  653|       |
  654|       |/// \internal Gets the maximum of two values which may be integers or enums
  655|       |template <typename A, typename B>
  656|      0|inline constexpr int plain_enum_max(A a, B b) {
  657|      0|  plain_enum_asserts(a, b);
  658|      0|  return ((int)a >= (int)b) ? (int)a : (int)b;
  659|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14plain_enum_maxIiiEEiT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14plain_enum_maxINS_9DenseBaseINS_6MatrixI14AnnoyingScalarLin1ELi1ELi0ELin1ELi1EEEEUt_ES7_EEiT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14plain_enum_maxINS_9DenseBaseINS_6MatrixI14AnnoyingScalarLin1ELin1ELi0ELin1ELin1EEEEUt_ES7_EEiT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14plain_enum_maxINS_9DenseBaseINS_5BlockINS_6MatrixI14AnnoyingScalarLin1ELin1ELi0ELin1ELin1EEELin1ELin1ELb0EEEEUt_ES9_EEiT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal14plain_enum_maxINS_9DenseBaseINS_5BlockINS_6MatrixI14AnnoyingScalarLin1ELi1ELi0ELin1ELi1EEELin1ELi1ELb0EEEEUt_ES9_EEiT_T0_
  ------------------
  660|       |
  661|       |/**
  662|       | * \internal
  663|       | *  `min_size_prefer_dynamic` gives the min between compile-time sizes. 0 has absolute priority, followed by 1,
  664|       | *  followed by Dynamic, followed by other finite values. The reason for giving Dynamic the priority over
  665|       | *  finite values is that min(3, Dynamic) should be Dynamic, since that could be anything between 0 and 3.
  666|       | */
  667|       |template <typename A, typename B>
  668|       |inline constexpr int min_size_prefer_dynamic(A a, B b) {
  669|       |  plain_enum_asserts(a, b);
  670|       |  if ((int)a == 0 || (int)b == 0) return 0;
  671|       |  if ((int)a == 1 || (int)b == 1) return 1;
  672|       |  if ((int)a == Dynamic || (int)b == Dynamic) return Dynamic;
  673|       |  return plain_enum_min(a, b);
  674|       |}
  675|       |
  676|       |/**
  677|       | * \internal
  678|       | *  min_size_prefer_fixed is a variant of `min_size_prefer_dynamic` comparing MaxSizes. The difference is that finite
  679|       | * values now have priority over Dynamic, so that min(3, Dynamic) gives 3. Indeed, whatever the actual value is (between
  680|       | * 0 and 3), it is not more than 3.
  681|       | */
  682|       |template <typename A, typename B>
  683|      0|inline constexpr int min_size_prefer_fixed(A a, B b) {
  684|      0|  plain_enum_asserts(a, b);
  685|      0|  if ((int)a == 0 || (int)b == 0) return 0;
  686|      0|  if ((int)a == 1 || (int)b == 1) return 1;
  687|      0|  if ((int)a == Dynamic && (int)b == Dynamic) return Dynamic;
  688|      0|  if ((int)a == Dynamic) return (int)b;
  689|      0|  if ((int)b == Dynamic) return (int)a;
  690|      0|  return plain_enum_min(a, b);
  691|      0|}
  692|       |
  693|       |/// \internal see `min_size_prefer_fixed`. No need for a separate variant for MaxSizes here.
  694|       |template <typename A, typename B>
  695|      0|inline constexpr int max_size_prefer_dynamic(A a, B b) {
  696|      0|  plain_enum_asserts(a, b);
  697|      0|  if ((int)a == Dynamic || (int)b == Dynamic) return Dynamic;
  698|      0|  return plain_enum_max(a, b);
  699|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal23max_size_prefer_dynamicINS_9DenseBaseINS_6MatrixI14AnnoyingScalarLin1ELi1ELi0ELin1ELi1EEEEUt_ES7_EEiT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal23max_size_prefer_dynamicINS_9DenseBaseINS_6MatrixI14AnnoyingScalarLin1ELin1ELi0ELin1ELin1EEEEUt_ES7_EEiT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal23max_size_prefer_dynamicINS_9DenseBaseINS_5BlockINS_6MatrixI14AnnoyingScalarLin1ELin1ELi0ELin1ELin1EEELin1ELin1ELb0EEEEUt_ES9_EEiT_T0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen8internal23max_size_prefer_dynamicINS_9DenseBaseINS_5BlockINS_6MatrixI14AnnoyingScalarLin1ELi1ELi0ELin1ELi1EEELin1ELi1ELb0EEEEUt_ES9_EEiT_T0_
  ------------------
  700|       |
  701|       |template <typename A, typename B>
  702|       |inline constexpr bool enum_eq_not_dynamic(A a, B b) {
  703|       |  plain_enum_asserts(a, b);
  704|       |  if ((int)a == Dynamic || (int)b == Dynamic) return false;
  705|       |  return (int)a == (int)b;
  706|       |}
  707|       |
  708|       |template <typename A, typename B>
  709|       |inline constexpr bool enum_lt_not_dynamic(A a, B b) {
  710|       |  plain_enum_asserts(a, b);
  711|       |  if ((int)a == Dynamic || (int)b == Dynamic) return false;
  712|       |  return (int)a < (int)b;
  713|       |}
  714|       |
  715|       |template <typename A, typename B>
  716|       |inline constexpr bool enum_le_not_dynamic(A a, B b) {
  717|       |  plain_enum_asserts(a, b);
  718|       |  if ((int)a == Dynamic || (int)b == Dynamic) return false;
  719|       |  return (int)a <= (int)b;
  720|       |}
  721|       |
  722|       |template <typename A, typename B>
  723|       |inline constexpr bool enum_gt_not_dynamic(A a, B b) {
  724|       |  plain_enum_asserts(a, b);
  725|       |  if ((int)a == Dynamic || (int)b == Dynamic) return false;
  726|       |  return (int)a > (int)b;
  727|       |}
  728|       |
  729|       |template <typename A, typename B>
  730|       |inline constexpr bool enum_ge_not_dynamic(A a, B b) {
  731|       |  plain_enum_asserts(a, b);
  732|       |  if ((int)a == Dynamic || (int)b == Dynamic) return false;
  733|       |  return (int)a >= (int)b;
  734|       |}
  735|       |
  736|       |/// \internal Calculate logical XOR at compile time
  737|      0|inline constexpr bool logical_xor(bool a, bool b) { return a != b; }
  738|       |
  739|       |/// \internal Calculate logical IMPLIES at compile time
  740|      0|inline constexpr bool check_implication(bool a, bool b) { return !a || b; }
  741|       |
  742|       |/// \internal Provide fallback for std::is_constant_evaluated for pre-C++20.
  743|       |#if EIGEN_COMP_CXXVER >= 20
  744|       |using std::is_constant_evaluated;
  745|       |#else
  746|      0|constexpr bool is_constant_evaluated() { return false; }
  747|       |#endif
  748|       |
  749|       |}  // end namespace internal
  750|       |
  751|       |}  // end namespace Eigen
  752|       |
  753|       |#endif  // EIGEN_META_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/ReshapedHelper.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2017 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_RESHAPED_HELPER_H
   11|       |#define EIGEN_RESHAPED_HELPER_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |enum AutoSize_t { AutoSize };
   19|       |const int AutoOrder = 2;
   20|       |
   21|       |namespace internal {
   22|       |
   23|       |template <typename SizeType, typename OtherSize, int TotalSize>
   24|       |struct get_compiletime_reshape_size {
   25|       |  enum { value = get_fixed_value<SizeType>::value };
   26|       |};
   27|       |
   28|       |template <typename SizeType>
   29|       |Index get_runtime_reshape_size(SizeType size, Index /*other*/, Index /*total*/) {
   30|       |  return internal::get_runtime_value(size);
   31|       |}
   32|       |
   33|       |template <typename OtherSize, int TotalSize>
   34|       |struct get_compiletime_reshape_size<AutoSize_t, OtherSize, TotalSize> {
   35|       |  enum {
   36|       |    other_size = get_fixed_value<OtherSize>::value,
   37|       |    value = (TotalSize == Dynamic || other_size == Dynamic) ? Dynamic : TotalSize / other_size
   38|       |  };
   39|       |};
   40|       |
   41|      0|inline Index get_runtime_reshape_size(AutoSize_t /*size*/, Index other, Index total) { return total / other; }
   42|       |
   43|      0|constexpr inline int get_compiletime_reshape_order(int flags, int order) {
   44|      0|  return order == AutoOrder ? flags & RowMajorBit : order;
   45|      0|}
   46|       |
   47|       |}  // namespace internal
   48|       |
   49|       |}  // end namespace Eigen
   50|       |
   51|       |#endif  // EIGEN_RESHAPED_HELPER_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/Serializer.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2021 The Eigen Team
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_SERIALIZER_H
   11|       |#define EIGEN_SERIALIZER_H
   12|       |
   13|       |#include <type_traits>
   14|       |
   15|       |// The Serializer class encodes data into a memory buffer so it can be later
   16|       |// reconstructed. This is mainly used to send objects back-and-forth between
   17|       |// the CPU and GPU.
   18|       |
   19|       |namespace Eigen {
   20|       |
   21|       |/**
   22|       | * Serializes an object to a memory buffer.
   23|       | *
   24|       | * Useful for transferring data (e.g. back-and-forth to a device).
   25|       | */
   26|       |template <typename T, typename EnableIf = void>
   27|       |class Serializer;
   28|       |
   29|       |// Specialization for POD types.
   30|       |template <typename T>
   31|       |class Serializer<T, typename std::enable_if_t<std::is_trivial<T>::value && std::is_standard_layout<T>::value>> {
   32|       | public:
   33|       |  /**
   34|       |   * Determines the required size of the serialization buffer for a value.
   35|       |   *
   36|       |   * \param value the value to serialize.
   37|       |   * \return the required size.
   38|       |   */
   39|       |  EIGEN_DEVICE_FUNC size_t size(const T& value) const { return sizeof(value); }
   40|       |
   41|       |  /**
   42|       |   * Serializes a value to a byte buffer.
   43|       |   * \param dest the destination buffer; if this is nullptr, does nothing.
   44|       |   * \param end the end of the destination buffer.
   45|       |   * \param value the value to serialize.
   46|       |   * \return the next memory address past the end of the serialized data.
   47|       |   */
   48|       |  EIGEN_DEVICE_FUNC uint8_t* serialize(uint8_t* dest, uint8_t* end, const T& value) {
   49|       |    if (EIGEN_PREDICT_FALSE(dest == nullptr)) return nullptr;
   50|       |    if (EIGEN_PREDICT_FALSE(dest + sizeof(value) > end)) return nullptr;
   51|       |    EIGEN_USING_STD(memcpy)
   52|       |    memcpy(dest, &value, sizeof(value));
   53|       |    return dest + sizeof(value);
   54|       |  }
   55|       |
   56|       |  /**
   57|       |   * Deserializes a value from a byte buffer.
   58|       |   * \param src the source buffer; if this is nullptr, does nothing.
   59|       |   * \param end the end of the source buffer.
   60|       |   * \param value the value to populate.
   61|       |   * \return the next unprocessed memory address; nullptr if parsing errors are detected.
   62|       |   */
   63|       |  EIGEN_DEVICE_FUNC const uint8_t* deserialize(const uint8_t* src, const uint8_t* end, T& value) const {
   64|       |    if (EIGEN_PREDICT_FALSE(src == nullptr)) return nullptr;
   65|       |    if (EIGEN_PREDICT_FALSE(src + sizeof(value) > end)) return nullptr;
   66|       |    EIGEN_USING_STD(memcpy)
   67|       |    memcpy(&value, src, sizeof(value));
   68|       |    return src + sizeof(value);
   69|       |  }
   70|       |};
   71|       |
   72|       |// Specialization for DenseBase.
   73|       |// Serializes [rows, cols, data...].
   74|       |template <typename Derived>
   75|       |class Serializer<DenseBase<Derived>, void> {
   76|       | public:
   77|       |  typedef typename Derived::Scalar Scalar;
   78|       |
   79|       |  struct Header {
   80|       |    typename Derived::Index rows;
   81|       |    typename Derived::Index cols;
   82|       |  };
   83|       |
   84|       |  EIGEN_DEVICE_FUNC size_t size(const Derived& value) const { return sizeof(Header) + sizeof(Scalar) * value.size(); }
   85|       |
   86|       |  EIGEN_DEVICE_FUNC uint8_t* serialize(uint8_t* dest, uint8_t* end, const Derived& value) {
   87|       |    if (EIGEN_PREDICT_FALSE(dest == nullptr)) return nullptr;
   88|       |    if (EIGEN_PREDICT_FALSE(dest + size(value) > end)) return nullptr;
   89|       |    const size_t header_bytes = sizeof(Header);
   90|       |    const size_t data_bytes = sizeof(Scalar) * value.size();
   91|       |    Header header = {value.rows(), value.cols()};
   92|       |    EIGEN_USING_STD(memcpy)
   93|       |    memcpy(dest, &header, header_bytes);
   94|       |    dest += header_bytes;
   95|       |    memcpy(dest, value.data(), data_bytes);
   96|       |    return dest + data_bytes;
   97|       |  }
   98|       |
   99|       |  EIGEN_DEVICE_FUNC const uint8_t* deserialize(const uint8_t* src, const uint8_t* end, Derived& value) const {
  100|       |    if (EIGEN_PREDICT_FALSE(src == nullptr)) return nullptr;
  101|       |    if (EIGEN_PREDICT_FALSE(src + sizeof(Header) > end)) return nullptr;
  102|       |    const size_t header_bytes = sizeof(Header);
  103|       |    Header header;
  104|       |    EIGEN_USING_STD(memcpy)
  105|       |    memcpy(&header, src, header_bytes);
  106|       |    src += header_bytes;
  107|       |    const size_t data_bytes = sizeof(Scalar) * header.rows * header.cols;
  108|       |    if (EIGEN_PREDICT_FALSE(src + data_bytes > end)) return nullptr;
  109|       |    value.resize(header.rows, header.cols);
  110|       |    memcpy(value.data(), src, data_bytes);
  111|       |    return src + data_bytes;
  112|       |  }
  113|       |};
  114|       |
  115|       |template <typename Scalar, int Rows, int Cols, int Options, int MaxRows, int MaxCols>
  116|       |class Serializer<Matrix<Scalar, Rows, Cols, Options, MaxRows, MaxCols>>
  117|       |    : public Serializer<DenseBase<Matrix<Scalar, Rows, Cols, Options, MaxRows, MaxCols>>> {};
  118|       |
  119|       |template <typename Scalar, int Rows, int Cols, int Options, int MaxRows, int MaxCols>
  120|       |class Serializer<Array<Scalar, Rows, Cols, Options, MaxRows, MaxCols>>
  121|       |    : public Serializer<DenseBase<Array<Scalar, Rows, Cols, Options, MaxRows, MaxCols>>> {};
  122|       |
  123|       |namespace internal {
  124|       |
  125|       |// Recursive serialization implementation helper.
  126|       |template <size_t N, typename... Types>
  127|       |struct serialize_impl;
  128|       |
  129|       |template <size_t N, typename T1, typename... Ts>
  130|       |struct serialize_impl<N, T1, Ts...> {
  131|       |  using Serializer = Eigen::Serializer<typename std::decay<T1>::type>;
  132|       |
  133|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE size_t serialize_size(const T1& value, const Ts&... args) {
  134|       |    Serializer serializer;
  135|       |    size_t size = serializer.size(value);
  136|       |    return size + serialize_impl<N - 1, Ts...>::serialize_size(args...);
  137|       |  }
  138|       |
  139|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE uint8_t* serialize(uint8_t* dest, uint8_t* end, const T1& value,
  140|       |                                                                  const Ts&... args) {
  141|       |    Serializer serializer;
  142|       |    dest = serializer.serialize(dest, end, value);
  143|       |    return serialize_impl<N - 1, Ts...>::serialize(dest, end, args...);
  144|       |  }
  145|       |
  146|       |  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const uint8_t* deserialize(const uint8_t* src, const uint8_t* end,
  147|       |                                                                          T1& value, Ts&... args) {
  148|       |    Serializer serializer;
  149|       |    src = serializer.deserialize(src, end, value);
  150|       |    return serialize_impl<N - 1, Ts...>::deserialize(src, end, args...);
  151|       |  }
  152|       |};
  153|       |
  154|       |// Base case.
  155|       |template <>
  156|       |struct serialize_impl<0> {
  157|      0|  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE size_t serialize_size() { return 0; }
  158|       |
  159|      0|  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE uint8_t* serialize(uint8_t* dest, uint8_t* /*end*/) { return dest; }
  160|       |
  161|      0|  static EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const uint8_t* deserialize(const uint8_t* src, const uint8_t* /*end*/) {
  162|      0|    return src;
  163|      0|  }
  164|       |};
  165|       |
  166|       |}  // namespace internal
  167|       |
  168|       |/**
  169|       | * Determine the buffer size required to serialize a set of values.
  170|       | *
  171|       | * \param args ... arguments to serialize in sequence.
  172|       | * \return the total size of the required buffer.
  173|       | */
  174|       |template <typename... Args>
  175|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE size_t serialize_size(const Args&... args) {
  176|       |  return internal::serialize_impl<sizeof...(args), Args...>::serialize_size(args...);
  177|       |}
  178|       |
  179|       |/**
  180|       | * Serialize a set of values to the byte buffer.
  181|       | *
  182|       | * \param dest output byte buffer; if this is nullptr, does nothing.
  183|       | * \param end the end of the output byte buffer.
  184|       | * \param args ... arguments to serialize in sequence.
  185|       | * \return the next address after all serialized values.
  186|       | */
  187|       |template <typename... Args>
  188|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE uint8_t* serialize(uint8_t* dest, uint8_t* end, const Args&... args) {
  189|       |  return internal::serialize_impl<sizeof...(args), Args...>::serialize(dest, end, args...);
  190|       |}
  191|       |
  192|       |/**
  193|       | * Deserialize a set of values from the byte buffer.
  194|       | *
  195|       | * \param src input byte buffer; if this is nullptr, does nothing.
  196|       | * \param end the end of input byte buffer.
  197|       | * \param args ... arguments to deserialize in sequence.
  198|       | * \return the next address after all parsed values; nullptr if parsing errors are detected.
  199|       | */
  200|       |template <typename... Args>
  201|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const uint8_t* deserialize(const uint8_t* src, const uint8_t* end,
  202|       |                                                                 Args&... args) {
  203|       |  return internal::serialize_impl<sizeof...(args), Args...>::deserialize(src, end, args...);
  204|       |}
  205|       |
  206|       |}  // namespace Eigen
  207|       |
  208|       |#endif  // EIGEN_SERIALIZER_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/SymbolicIndex.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2017 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_SYMBOLIC_INDEX_H
   11|       |#define EIGEN_SYMBOLIC_INDEX_H
   12|       |
   13|       |// IWYU pragma: private
   14|       |#include "../InternalHeaderCheck.h"
   15|       |
   16|       |namespace Eigen {
   17|       |
   18|       |/** \namespace Eigen::symbolic
   19|       | * \ingroup Core_Module
   20|       | *
   21|       | * This namespace defines a set of classes and functions to build and evaluate symbolic expressions of scalar type
   22|       | * Index. Here is a simple example:
   23|       | *
   24|       | * \code
   25|       | * // First step, defines symbols:
   26|       | * struct x_tag {};  static const symbolic::SymbolExpr<x_tag> x;
   27|       | * struct y_tag {};  static const symbolic::SymbolExpr<y_tag> y;
   28|       | * struct z_tag {};  static const symbolic::SymbolExpr<z_tag> z;
   29|       | *
   30|       | * // Defines an expression:
   31|       | * auto expr = (x+3)/y+z;
   32|       | *
   33|       | * // And evaluate it: (c++14)
   34|       | * std::cout << expr.eval(x=6,y=3,z=-13) << "\n";
   35|       | *
   36|       | * \endcode
   37|       | *
   38|       | * It is currently only used internally to define and manipulate the
   39|       | * Eigen::placeholders::last and Eigen::placeholders::lastp1 symbols in
   40|       | * Eigen::seq and Eigen::seqN.
   41|       | *
   42|       | */
   43|       |namespace symbolic {
   44|       |
   45|       |template <typename Tag>
   46|       |class Symbol;
   47|       |template <typename Tag, typename Type>
   48|       |class SymbolValue;
   49|       |template <typename Arg0>
   50|       |class NegateExpr;
   51|       |template <typename Arg1, typename Arg2>
   52|       |class AddExpr;
   53|       |template <typename Arg1, typename Arg2>
   54|       |class ProductExpr;
   55|       |template <typename Arg1, typename Arg2>
   56|       |class QuotientExpr;
   57|       |template <typename IndexType = Index>
   58|       |class ValueExpr;
   59|       |
   60|       |/** \class BaseExpr
   61|       | * \ingroup Core_Module
   62|       | * Common base class of any symbolic expressions
   63|       | */
   64|       |template <typename Derived_>
   65|       |class BaseExpr {
   66|       | public:
   67|       |  using Derived = Derived_;
   68|      0|  constexpr const Derived& derived() const { return *static_cast<const Derived*>(this); }
   69|       |
   70|       |  /** Evaluate the expression given the \a values of the symbols.
   71|       |   *
   72|       |   * \param values defines the values of the symbols, as constructed by SymbolExpr::operator= operator.
   73|       |   *
   74|       |   */
   75|       |  template <typename... Tags, typename... Types>
   76|       |  constexpr Index eval(const SymbolValue<Tags, Types>&... values) const {
   77|       |    return derived().eval_impl(values...);
   78|       |  }
   79|       |
   80|       |  /** Evaluate the expression at compile time given the \a values of the symbols.
   81|       |   *
   82|       |   * If a value is not known at compile-time, returns Eigen::Undefined.
   83|       |   *
   84|       |   */
   85|       |  template <typename... Tags, typename... Types>
   86|       |  static constexpr Index eval_at_compile_time(const SymbolValue<Tags, Types>&...) {
   87|       |    return Derived::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
   88|       |  }
   89|       |
   90|       |  constexpr NegateExpr<Derived> operator-() const { return NegateExpr<Derived>(derived()); }
   91|       |
   92|       |  constexpr AddExpr<Derived, ValueExpr<>> operator+(Index b) const {
   93|       |    return AddExpr<Derived, ValueExpr<>>(derived(), b);
   94|       |  }
   95|       |  constexpr AddExpr<Derived, ValueExpr<>> operator-(Index a) const {
   96|       |    return AddExpr<Derived, ValueExpr<>>(derived(), -a);
   97|       |  }
   98|       |  constexpr ProductExpr<Derived, ValueExpr<>> operator*(Index a) const {
   99|       |    return ProductExpr<Derived, ValueExpr<>>(derived(), a);
  100|       |  }
  101|       |  constexpr QuotientExpr<Derived, ValueExpr<>> operator/(Index a) const {
  102|       |    return QuotientExpr<Derived, ValueExpr<>>(derived(), a);
  103|       |  }
  104|       |
  105|       |  friend constexpr AddExpr<Derived, ValueExpr<>> operator+(Index a, const BaseExpr& b) {
  106|       |    return AddExpr<Derived, ValueExpr<>>(b.derived(), a);
  107|       |  }
  108|       |  friend constexpr AddExpr<NegateExpr<Derived>, ValueExpr<>> operator-(Index a, const BaseExpr& b) {
  109|       |    return AddExpr<NegateExpr<Derived>, ValueExpr<>>(-b.derived(), a);
  110|       |  }
  111|       |  friend constexpr ProductExpr<ValueExpr<>, Derived> operator*(Index a, const BaseExpr& b) {
  112|       |    return ProductExpr<ValueExpr<>, Derived>(a, b.derived());
  113|       |  }
  114|       |  friend constexpr QuotientExpr<ValueExpr<>, Derived> operator/(Index a, const BaseExpr& b) {
  115|       |    return QuotientExpr<ValueExpr<>, Derived>(a, b.derived());
  116|       |  }
  117|       |
  118|       |  template <int N>
  119|      0|  constexpr AddExpr<Derived, ValueExpr<internal::FixedInt<N>>> operator+(internal::FixedInt<N>) const {
  120|      0|    return AddExpr<Derived, ValueExpr<internal::FixedInt<N>>>(derived(), ValueExpr<internal::FixedInt<N>>());
  121|      0|  }
  122|       |  template <int N>
  123|       |  constexpr AddExpr<Derived, ValueExpr<internal::FixedInt<-N>>> operator-(internal::FixedInt<N>) const {
  124|       |    return AddExpr<Derived, ValueExpr<internal::FixedInt<-N>>>(derived(), ValueExpr<internal::FixedInt<-N>>());
  125|       |  }
  126|       |  template <int N>
  127|       |  constexpr ProductExpr<Derived, ValueExpr<internal::FixedInt<N>>> operator*(internal::FixedInt<N>) const {
  128|       |    return ProductExpr<Derived, ValueExpr<internal::FixedInt<N>>>(derived(), ValueExpr<internal::FixedInt<N>>());
  129|       |  }
  130|       |  template <int N>
  131|       |  constexpr QuotientExpr<Derived, ValueExpr<internal::FixedInt<N>>> operator/(internal::FixedInt<N>) const {
  132|       |    return QuotientExpr<Derived, ValueExpr<internal::FixedInt<N>>>(derived(), ValueExpr<internal::FixedInt<N>>());
  133|       |  }
  134|       |
  135|       |  template <int N>
  136|       |  friend constexpr AddExpr<Derived, ValueExpr<internal::FixedInt<N>>> operator+(internal::FixedInt<N>,
  137|       |                                                                                const BaseExpr& b) {
  138|       |    return AddExpr<Derived, ValueExpr<internal::FixedInt<N>>>(b.derived(), ValueExpr<internal::FixedInt<N>>());
  139|       |  }
  140|       |  template <int N>
  141|       |  friend constexpr AddExpr<NegateExpr<Derived>, ValueExpr<internal::FixedInt<N>>> operator-(internal::FixedInt<N>,
  142|       |                                                                                            const BaseExpr& b) {
  143|       |    return AddExpr<NegateExpr<Derived>, ValueExpr<internal::FixedInt<N>>>(-b.derived(),
  144|       |                                                                          ValueExpr<internal::FixedInt<N>>());
  145|       |  }
  146|       |  template <int N>
  147|       |  friend constexpr ProductExpr<ValueExpr<internal::FixedInt<N>>, Derived> operator*(internal::FixedInt<N>,
  148|       |                                                                                    const BaseExpr& b) {
  149|       |    return ProductExpr<ValueExpr<internal::FixedInt<N>>, Derived>(ValueExpr<internal::FixedInt<N>>(), b.derived());
  150|       |  }
  151|       |  template <int N>
  152|       |  friend constexpr QuotientExpr<ValueExpr<internal::FixedInt<N>>, Derived> operator/(internal::FixedInt<N>,
  153|       |                                                                                     const BaseExpr& b) {
  154|       |    return QuotientExpr<ValueExpr<internal::FixedInt<N>>, Derived>(ValueExpr<internal::FixedInt<N>>(), b.derived());
  155|       |  }
  156|       |
  157|       |  template <typename OtherDerived>
  158|       |  constexpr AddExpr<Derived, OtherDerived> operator+(const BaseExpr<OtherDerived>& b) const {
  159|       |    return AddExpr<Derived, OtherDerived>(derived(), b.derived());
  160|       |  }
  161|       |
  162|       |  template <typename OtherDerived>
  163|       |  constexpr AddExpr<Derived, NegateExpr<OtherDerived>> operator-(const BaseExpr<OtherDerived>& b) const {
  164|       |    return AddExpr<Derived, NegateExpr<OtherDerived>>(derived(), -b.derived());
  165|       |  }
  166|       |
  167|       |  template <typename OtherDerived>
  168|       |  constexpr ProductExpr<Derived, OtherDerived> operator*(const BaseExpr<OtherDerived>& b) const {
  169|       |    return ProductExpr<Derived, OtherDerived>(derived(), b.derived());
  170|       |  }
  171|       |
  172|       |  template <typename OtherDerived>
  173|       |  constexpr QuotientExpr<Derived, OtherDerived> operator/(const BaseExpr<OtherDerived>& b) const {
  174|       |    return QuotientExpr<Derived, OtherDerived>(derived(), b.derived());
  175|       |  }
  176|       |};
  177|       |
  178|       |template <typename T>
  179|       |struct is_symbolic {
  180|       |  // BaseExpr has no conversion ctor, so we only have to check whether T can be statically cast to its base class
  181|       |  // BaseExpr<T>.
  182|       |  enum { value = internal::is_convertible<T, BaseExpr<T>>::value };
  183|       |};
  184|       |
  185|       |// A simple wrapper around an integral value to provide the eval method.
  186|       |// We could also use a free-function symbolic_eval...
  187|       |template <typename IndexType>
  188|       |class ValueExpr : BaseExpr<ValueExpr<IndexType>> {
  189|       | public:
  190|       |  constexpr ValueExpr() = default;
  191|       |  constexpr ValueExpr(IndexType val) : value_(val) {}
  192|       |  template <typename... Tags, typename... Types>
  193|       |  constexpr IndexType eval_impl(const SymbolValue<Tags, Types>&...) const {
  194|       |    return value_;
  195|       |  }
  196|       |  template <typename... Tags, typename... Types>
  197|       |  static constexpr IndexType eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  198|       |    return IndexType(Undefined);
  199|       |  }
  200|       |
  201|       | protected:
  202|       |  IndexType value_;
  203|       |};
  204|       |
  205|       |// Specialization for compile-time value,
  206|       |// It is similar to ValueExpr(N) but this version helps the compiler to generate better code.
  207|       |template <int N>
  208|       |class ValueExpr<internal::FixedInt<N>> : public BaseExpr<ValueExpr<internal::FixedInt<N>>> {
  209|       | public:
  210|       |  constexpr ValueExpr() = default;
  211|       |  constexpr ValueExpr(internal::FixedInt<N>) {}
  212|       |  template <typename... Tags, typename... Types>
  213|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&...) const {
  214|       |    return Index(N);
  215|       |  }
  216|       |  template <typename... Tags, typename... Types>
  217|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  218|       |    return Index(N);
  219|       |  }
  220|       |};
  221|       |
  222|       |/** Represents the actual value of a symbol identified by its tag
  223|       | *
  224|       | * It is the return type of SymbolValue::operator=, and most of the time this is only way it is used.
  225|       | */
  226|       |template <typename Tag, typename Type>
  227|       |class SymbolValue : public BaseExpr<SymbolValue<Tag, Type>> {};
  228|       |
  229|       |template <typename Tag>
  230|       |class SymbolValue<Tag, Index> : public BaseExpr<SymbolValue<Tag, Index>> {
  231|       | public:
  232|       |  constexpr SymbolValue() = default;
  233|       |
  234|       |  /** Default constructor from the value \a val */
  235|       |  constexpr SymbolValue(Index val) : value_(val) {}
  236|       |
  237|       |  /** \returns the stored value of the symbol */
  238|       |  constexpr Index value() const { return value_; }
  239|       |
  240|       |  /** \returns the stored value of the symbol at compile time, or Undefined if not known. */
  241|       |  static constexpr Index value_at_compile_time() { return Index(Undefined); }
  242|       |
  243|       |  template <typename... Tags, typename... Types>
  244|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&...) const {
  245|       |    return value();
  246|       |  }
  247|       |
  248|       |  template <typename... Tags, typename... Types>
  249|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  250|       |    return value_at_compile_time();
  251|       |  }
  252|       |
  253|       | protected:
  254|       |  Index value_;
  255|       |};
  256|       |
  257|       |template <typename Tag, int N>
  258|       |class SymbolValue<Tag, internal::FixedInt<N>> : public BaseExpr<SymbolValue<Tag, internal::FixedInt<N>>> {
  259|       | public:
  260|       |  constexpr SymbolValue() = default;
  261|       |
  262|       |  /** Default constructor from the value \a val */
  263|       |  constexpr SymbolValue(internal::FixedInt<N>) {}
  264|       |
  265|       |  /** \returns the stored value of the symbol */
  266|       |  constexpr Index value() const { return static_cast<Index>(N); }
  267|       |
  268|       |  /** \returns the stored value of the symbol at compile time, or Undefined if not known. */
  269|       |  static constexpr Index value_at_compile_time() { return static_cast<Index>(N); }
  270|       |
  271|       |  template <typename... Tags, typename... Types>
  272|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&...) const {
  273|       |    return value();
  274|       |  }
  275|       |
  276|       |  template <typename... Tags, typename... Types>
  277|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  278|       |    return value_at_compile_time();
  279|       |  }
  280|       |};
  281|       |
  282|       |// Find and return a symbol value based on the tag.
  283|       |template <typename Tag, typename... Types>
  284|       |struct EvalSymbolValueHelper;
  285|       |
  286|       |// Empty base case, symbol not found.
  287|       |template <typename Tag>
  288|       |struct EvalSymbolValueHelper<Tag> {
  289|       |  static constexpr Index eval_impl() {
  290|       |    eigen_assert(false && "Symbol not found.");
  291|       |    return Index(Undefined);
  292|       |  }
  293|       |  static constexpr Index eval_at_compile_time_impl() { return Index(Undefined); }
  294|       |};
  295|       |
  296|       |// We found a symbol value matching the provided Tag!
  297|       |template <typename Tag, typename Type, typename... OtherTypes>
  298|       |struct EvalSymbolValueHelper<Tag, SymbolValue<Tag, Type>, OtherTypes...> {
  299|       |  static constexpr Index eval_impl(const SymbolValue<Tag, Type>& symbol, const OtherTypes&...) {
  300|       |    return symbol.value();
  301|       |  }
  302|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tag, Type>& symbol, const OtherTypes&...) {
  303|       |    return symbol.value_at_compile_time();
  304|       |  }
  305|       |};
  306|       |
  307|       |// No symbol value in first value, recursive search starting with next.
  308|       |template <typename Tag, typename T1, typename... OtherTypes>
  309|       |struct EvalSymbolValueHelper<Tag, T1, OtherTypes...> {
  310|       |  static constexpr Index eval_impl(const T1&, const OtherTypes&... values) {
  311|       |    return EvalSymbolValueHelper<Tag, OtherTypes...>::eval_impl(values...);
  312|       |  }
  313|       |  static constexpr Index eval_at_compile_time_impl(const T1&, const OtherTypes&...) {
  314|       |    return EvalSymbolValueHelper<Tag, OtherTypes...>::eval_at_compile_time_impl(OtherTypes{}...);
  315|       |  }
  316|       |};
  317|       |
  318|       |/** Expression of a symbol uniquely identified by the template parameter type \c tag */
  319|       |template <typename tag>
  320|       |class SymbolExpr : public BaseExpr<SymbolExpr<tag>> {
  321|       | public:
  322|       |  /** Alias to the template parameter \c tag */
  323|       |  typedef tag Tag;
  324|       |
  325|       |  constexpr SymbolExpr() = default;
  326|       |
  327|       |  /** Associate the value \a val to the given symbol \c *this, uniquely identified by its \c Tag.
  328|       |   *
  329|       |   * The returned object should be passed to ExprBase::eval() to evaluate a given expression with this specified
  330|       |   * runtime-time value.
  331|       |   */
  332|      0|  constexpr SymbolValue<Tag, Index> operator=(Index val) const { return SymbolValue<Tag, Index>(val); }
  333|       |
  334|       |  template <int N>
  335|       |  constexpr SymbolValue<Tag, internal::FixedInt<N>> operator=(internal::FixedInt<N>) const {
  336|       |    return SymbolValue<Tag, internal::FixedInt<N>>{internal::FixedInt<N>{}};
  337|       |  }
  338|       |
  339|       |  template <typename... Tags, typename... Types>
  340|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&... values) const {
  341|       |    return EvalSymbolValueHelper<Tag, SymbolValue<Tags, Types>...>::eval_impl(values...);
  342|       |  }
  343|       |
  344|       |  template <typename... Tags, typename... Types>
  345|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  346|       |    return EvalSymbolValueHelper<Tag, SymbolValue<Tags, Types>...>::eval_at_compile_time_impl(
  347|       |        SymbolValue<Tags, Types>{}...);
  348|       |  }
  349|       |};
  350|       |
  351|       |template <typename Arg0>
  352|       |class NegateExpr : public BaseExpr<NegateExpr<Arg0>> {
  353|       | public:
  354|       |  constexpr NegateExpr() = default;
  355|       |  constexpr NegateExpr(const Arg0& arg0) : m_arg0(arg0) {}
  356|       |
  357|       |  template <typename... Tags, typename... Types>
  358|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&... values) const {
  359|       |    return -m_arg0.eval_impl(values...);
  360|       |  }
  361|       |
  362|       |  template <typename... Tags, typename... Types>
  363|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  364|       |    constexpr Index v = Arg0::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
  365|       |    return (v == Undefined) ? Undefined : -v;
  366|       |  }
  367|       |
  368|       | protected:
  369|       |  Arg0 m_arg0;
  370|       |};
  371|       |
  372|       |template <typename Arg0, typename Arg1>
  373|       |class AddExpr : public BaseExpr<AddExpr<Arg0, Arg1>> {
  374|       | public:
  375|       |  constexpr AddExpr() = default;
  376|       |  constexpr AddExpr(const Arg0& arg0, const Arg1& arg1) : m_arg0(arg0), m_arg1(arg1) {}
  377|       |
  378|       |  template <typename... Tags, typename... Types>
  379|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&... values) const {
  380|       |    return m_arg0.eval_impl(values...) + m_arg1.eval_impl(values...);
  381|       |  }
  382|       |
  383|       |  template <typename... Tags, typename... Types>
  384|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  385|       |    constexpr Index v0 = Arg0::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
  386|       |    constexpr Index v1 = Arg1::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
  387|       |    return (v0 == Undefined || v1 == Undefined) ? Undefined : v0 + v1;
  388|       |  }
  389|       |
  390|       | protected:
  391|       |  Arg0 m_arg0;
  392|       |  Arg1 m_arg1;
  393|       |};
  394|       |
  395|       |template <typename Arg0, typename Arg1>
  396|       |class ProductExpr : public BaseExpr<ProductExpr<Arg0, Arg1>> {
  397|       | public:
  398|       |  constexpr ProductExpr() = default;
  399|       |  constexpr ProductExpr(const Arg0& arg0, const Arg1& arg1) : m_arg0(arg0), m_arg1(arg1) {}
  400|       |
  401|       |  template <typename... Tags, typename... Types>
  402|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&... values) const {
  403|       |    return m_arg0.eval_impl(values...) * m_arg1.eval_impl(values...);
  404|       |  }
  405|       |
  406|       |  template <typename... Tags, typename... Types>
  407|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  408|       |    constexpr Index v0 = Arg0::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
  409|       |    constexpr Index v1 = Arg1::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
  410|       |    return (v0 == Undefined || v1 == Undefined) ? Undefined : v0 * v1;
  411|       |  }
  412|       |
  413|       | protected:
  414|       |  Arg0 m_arg0;
  415|       |  Arg1 m_arg1;
  416|       |};
  417|       |
  418|       |template <typename Arg0, typename Arg1>
  419|       |class QuotientExpr : public BaseExpr<QuotientExpr<Arg0, Arg1>> {
  420|       | public:
  421|       |  constexpr QuotientExpr() = default;
  422|       |  constexpr QuotientExpr(const Arg0& arg0, const Arg1& arg1) : m_arg0(arg0), m_arg1(arg1) {}
  423|       |
  424|       |  template <typename... Tags, typename... Types>
  425|       |  constexpr Index eval_impl(const SymbolValue<Tags, Types>&... values) const {
  426|       |    return m_arg0.eval_impl(values...) / m_arg1.eval_impl(values...);
  427|       |  }
  428|       |
  429|       |  template <typename... Tags, typename... Types>
  430|       |  static constexpr Index eval_at_compile_time_impl(const SymbolValue<Tags, Types>&...) {
  431|       |    constexpr Index v0 = Arg0::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
  432|       |    constexpr Index v1 = Arg1::eval_at_compile_time_impl(SymbolValue<Tags, Types>{}...);
  433|       |    return (v0 == Undefined || v1 == Undefined) ? Undefined : v0 / v1;
  434|       |  }
  435|       |
  436|       | protected:
  437|       |  Arg0 m_arg0;
  438|       |  Arg1 m_arg1;
  439|       |};
  440|       |
  441|       |}  // end namespace symbolic
  442|       |
  443|       |}  // end namespace Eigen
  444|       |
  445|       |#endif  // EIGEN_SYMBOLIC_INDEX_H

/home/kidus/Desktop/GNNs/eigen/eigen/Eigen/src/Core/util/XprHelper.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2008 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |// Copyright (C) 2006-2008 Benoit Jacob <jacob.benoit.1@gmail.com>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#ifndef EIGEN_XPRHELPER_H
   12|       |#define EIGEN_XPRHELPER_H
   13|       |
   14|       |// IWYU pragma: private
   15|       |#include "../InternalHeaderCheck.h"
   16|       |
   17|       |namespace Eigen {
   18|       |
   19|       |namespace internal {
   20|       |
   21|       |// useful for unsigned / signed integer comparisons when idx is intended to be non-negative
   22|       |template <typename IndexType>
   23|       |EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE typename make_unsigned<IndexType>::type returnUnsignedIndexValue(
   24|       |    const IndexType& idx) {
   25|       |  EIGEN_STATIC_ASSERT((NumTraits<IndexType>::IsInteger), THIS FUNCTION IS FOR INTEGER TYPES)
   26|       |  eigen_internal_assert(idx >= 0 && "Index value is negative and target type is unsigned");
   27|       |  using UnsignedType = typename make_unsigned<IndexType>::type;
   28|       |  return static_cast<UnsignedType>(idx);
   29|       |}
   30|       |
   31|       |template <typename IndexDest, typename IndexSrc, bool IndexDestIsInteger = NumTraits<IndexDest>::IsInteger,
   32|       |          bool IndexDestIsSigned = NumTraits<IndexDest>::IsSigned,
   33|       |          bool IndexSrcIsInteger = NumTraits<IndexSrc>::IsInteger,
   34|       |          bool IndexSrcIsSigned = NumTraits<IndexSrc>::IsSigned>
   35|       |struct convert_index_impl {
   36|       |  static inline EIGEN_DEVICE_FUNC IndexDest run(const IndexSrc& idx) {
   37|       |    eigen_internal_assert(idx <= NumTraits<IndexDest>::highest() && "Index value is too big for target type");
   38|       |    return static_cast<IndexDest>(idx);
   39|       |  }
   40|       |};
   41|       |template <typename IndexDest, typename IndexSrc>
   42|       |struct convert_index_impl<IndexDest, IndexSrc, true, true, true, false> {
   43|       |  // IndexDest is a signed integer
   44|       |  // IndexSrc is an unsigned integer
   45|       |  static inline EIGEN_DEVICE_FUNC IndexDest run(const IndexSrc& idx) {
   46|       |    eigen_internal_assert(idx <= returnUnsignedIndexValue(NumTraits<IndexDest>::highest()) &&
   47|       |                          "Index value is too big for target type");
   48|       |    return static_cast<IndexDest>(idx);
   49|       |  }
   50|       |};
   51|       |template <typename IndexDest, typename IndexSrc>
   52|       |struct convert_index_impl<IndexDest, IndexSrc, true, false, true, true> {
   53|       |  // IndexDest is an unsigned integer
   54|       |  // IndexSrc is a signed integer
   55|       |  static inline EIGEN_DEVICE_FUNC IndexDest run(const IndexSrc& idx) {
   56|       |    eigen_internal_assert(returnUnsignedIndexValue(idx) <= NumTraits<IndexDest>::highest() &&
   57|       |                          "Index value is too big for target type");
   58|       |    return static_cast<IndexDest>(idx);
   59|       |  }
   60|       |};
   61|       |
   62|       |template <typename IndexDest, typename IndexSrc>
   63|       |EIGEN_DEVICE_FUNC inline IndexDest convert_index(const IndexSrc& idx) {
   64|       |  return convert_index_impl<IndexDest, IndexSrc>::run(idx);
   65|       |}
   66|       |
   67|       |// true if T can be considered as an integral index (i.e., and integral type or enum)
   68|       |template <typename T>
   69|       |struct is_valid_index_type {
   70|       |  enum { value = internal::is_integral<T>::value || std::is_enum<T>::value };
   71|       |};
   72|       |
   73|       |// true if both types are not valid index types
   74|       |template <typename RowIndices, typename ColIndices>
   75|       |struct valid_indexed_view_overload {
   76|       |  enum {
   77|       |    value = !(internal::is_valid_index_type<RowIndices>::value && internal::is_valid_index_type<ColIndices>::value)
   78|       |  };
   79|       |};
   80|       |
   81|       |// promote_scalar_arg is an helper used in operation between an expression and a scalar, like:
   82|       |//    expression * scalar
   83|       |// Its role is to determine how the type T of the scalar operand should be promoted given the scalar type ExprScalar of
   84|       |// the given expression. The IsSupported template parameter must be provided by the caller as:
   85|       |// internal::has_ReturnType<ScalarBinaryOpTraits<ExprScalar,T,op> >::value using the proper order for ExprScalar and T.
   86|       |// Then the logic is as follows:
   87|       |//  - if the operation is natively supported as defined by IsSupported, then the scalar type is not promoted, and T is
   88|       |//  returned.
   89|       |//  - otherwise, NumTraits<ExprScalar>::Literal is returned if T is implicitly convertible to
   90|       |//  NumTraits<ExprScalar>::Literal AND that this does not imply a float to integer conversion.
   91|       |//  - otherwise, ExprScalar is returned if T is implicitly convertible to ExprScalar AND that this does not imply a
   92|       |//  float to integer conversion.
   93|       |//  - In all other cases, the promoted type is not defined, and the respective operation is thus invalid and not
   94|       |//  available (SFINAE).
   95|       |template <typename ExprScalar, typename T, bool IsSupported>
   96|       |struct promote_scalar_arg;
   97|       |
   98|       |template <typename S, typename T>
   99|       |struct promote_scalar_arg<S, T, true> {
  100|       |  typedef T type;
  101|       |};
  102|       |
  103|       |// Recursively check safe conversion to PromotedType, and then ExprScalar if they are different.
  104|       |template <typename ExprScalar, typename T, typename PromotedType,
  105|       |          bool ConvertibleToLiteral = internal::is_convertible<T, PromotedType>::value,
  106|       |          bool IsSafe = NumTraits<T>::IsInteger || !NumTraits<PromotedType>::IsInteger>
  107|       |struct promote_scalar_arg_unsupported;
  108|       |
  109|       |// Start recursion with NumTraits<ExprScalar>::Literal
  110|       |template <typename S, typename T>
  111|       |struct promote_scalar_arg<S, T, false> : promote_scalar_arg_unsupported<S, T, typename NumTraits<S>::Literal> {};
  112|       |
  113|       |// We found a match!
  114|       |template <typename S, typename T, typename PromotedType>
  115|       |struct promote_scalar_arg_unsupported<S, T, PromotedType, true, true> {
  116|       |  typedef PromotedType type;
  117|       |};
  118|       |
  119|       |// No match, but no real-to-integer issues, and ExprScalar and current PromotedType are different,
  120|       |// so let's try to promote to ExprScalar
  121|       |template <typename ExprScalar, typename T, typename PromotedType>
  122|       |struct promote_scalar_arg_unsupported<ExprScalar, T, PromotedType, false, true>
  123|       |    : promote_scalar_arg_unsupported<ExprScalar, T, ExprScalar> {};
  124|       |
  125|       |// Unsafe real-to-integer, let's stop.
  126|       |template <typename S, typename T, typename PromotedType, bool ConvertibleToLiteral>
  127|       |struct promote_scalar_arg_unsupported<S, T, PromotedType, ConvertibleToLiteral, false> {};
  128|       |
  129|       |// T is not even convertible to ExprScalar, let's stop.
  130|       |template <typename S, typename T>
  131|       |struct promote_scalar_arg_unsupported<S, T, S, false, true> {};
  132|       |
  133|       |// classes inheriting no_assignment_operator don't generate a default operator=.
  134|       |class no_assignment_operator {
  135|       | private:
  136|       |  no_assignment_operator& operator=(const no_assignment_operator&);
  137|       |
  138|       | protected:
  139|       |  EIGEN_DEFAULT_COPY_CONSTRUCTOR(no_assignment_operator)
  140|       |  EIGEN_DEFAULT_EMPTY_CONSTRUCTOR_AND_DESTRUCTOR(no_assignment_operator)
  141|       |};
  142|       |
  143|       |/** \internal return the index type with the largest number of bits */
  144|       |template <typename I1, typename I2>
  145|       |struct promote_index_type {
  146|       |  typedef std::conditional_t<(sizeof(I1) < sizeof(I2)), I2, I1> type;
  147|       |};
  148|       |
  149|       |/** \internal If the template parameter Value is Dynamic, this class is just a wrapper around a T variable that
  150|       | * can be accessed using value() and setValue().
  151|       | * Otherwise, this class is an empty structure and value() just returns the template parameter Value.
  152|       | */
  153|       |template <typename T, int Value>
  154|       |class variable_if_dynamic {
  155|       | public:
  156|       |  EIGEN_DEFAULT_EMPTY_CONSTRUCTOR_AND_DESTRUCTOR(variable_if_dynamic)
  157|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE explicit variable_if_dynamic(T v) {
  158|       |    EIGEN_ONLY_USED_FOR_DEBUG(v);
  159|       |    eigen_assert(v == T(Value));
  160|       |  }
  161|       |  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE EIGEN_CONSTEXPR T value() { return T(Value); }
  162|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE EIGEN_CONSTEXPR operator T() const { return T(Value); }
  163|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void setValue(T v) const {
  164|       |    EIGEN_ONLY_USED_FOR_DEBUG(v);
  165|       |    eigen_assert(v == T(Value));
  166|       |  }
  167|       |};
  168|       |
  169|       |template <typename T>
  170|       |class variable_if_dynamic<T, Dynamic> {
  171|       |  T m_value;
  172|       |
  173|       | public:
  174|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE explicit variable_if_dynamic(T value = 0) EIGEN_NO_THROW : m_value(value) {}
  175|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T value() const { return m_value; }
  176|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE operator T() const { return m_value; }
  177|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void setValue(T value) { m_value = value; }
  178|       |};
  179|       |
  180|       |/** \internal like variable_if_dynamic but for DynamicIndex
  181|       | */
  182|       |template <typename T, int Value>
  183|       |class variable_if_dynamicindex {
  184|       | public:
  185|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE explicit variable_if_dynamicindex(T v) {
  186|       |    EIGEN_ONLY_USED_FOR_DEBUG(v);
  187|       |    eigen_assert(v == T(Value));
  188|       |  }
  189|       |  EIGEN_DEVICE_FUNC static EIGEN_STRONG_INLINE EIGEN_CONSTEXPR T value() { return T(Value); }
  190|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void setValue(T) {}
  191|       |};
  192|       |
  193|       |template <typename T>
  194|       |class variable_if_dynamicindex<T, DynamicIndex> {
  195|       |  T m_value;
  196|       |  EIGEN_DEVICE_FUNC variable_if_dynamicindex() { eigen_assert(false); }
  197|       |
  198|       | public:
  199|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE explicit variable_if_dynamicindex(T value) : m_value(value) {}
  200|       |  EIGEN_DEVICE_FUNC T EIGEN_STRONG_INLINE value() const { return m_value; }
  201|       |  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE void setValue(T value) { m_value = value; }
  202|       |};
  203|       |
  204|       |template <typename T>
  205|       |struct functor_traits {
  206|       |  enum { Cost = 10, PacketAccess = false, IsRepeatable = false };
  207|       |};
  208|       |
  209|       |// estimates the cost of lazily evaluating a generic functor by unwinding the expression
  210|       |template <typename Xpr>
  211|       |struct nested_functor_cost {
  212|       |  static constexpr Index Cost = static_cast<Index>(functor_traits<Xpr>::Cost);
  213|       |};
  214|       |
  215|       |template <typename Scalar, int Rows, int Cols, int Options, int MaxRows, int MaxCols>
  216|       |struct nested_functor_cost<Matrix<Scalar, Rows, Cols, Options, MaxRows, MaxCols>> {
  217|       |  static constexpr Index Cost = 1;
  218|       |};
  219|       |
  220|       |template <typename Scalar, int Rows, int Cols, int Options, int MaxRows, int MaxCols>
  221|       |struct nested_functor_cost<Array<Scalar, Rows, Cols, Options, MaxRows, MaxCols>> {
  222|       |  static constexpr Index Cost = 1;
  223|       |};
  224|       |
  225|       |// TODO: assign a cost to the stride type?
  226|       |template <typename PlainObjectType, int MapOptions, typename StrideType>
  227|       |struct nested_functor_cost<Map<PlainObjectType, MapOptions, StrideType>> : nested_functor_cost<PlainObjectType> {};
  228|       |
  229|       |template <typename Func, typename Xpr>
  230|       |struct nested_functor_cost<CwiseUnaryOp<Func, Xpr>> {
  231|       |  using XprCleaned = remove_all_t<Xpr>;
  232|       |  using FuncCleaned = remove_all_t<Func>;
  233|       |  static constexpr Index Cost = nested_functor_cost<FuncCleaned>::Cost + nested_functor_cost<XprCleaned>::Cost;
  234|       |};
  235|       |
  236|       |template <typename Func, typename Xpr>
  237|       |struct nested_functor_cost<CwiseNullaryOp<Func, Xpr>> {
  238|       |  using XprCleaned = remove_all_t<Xpr>;
  239|       |  using FuncCleaned = remove_all_t<Func>;
  240|       |  static constexpr Index Cost = nested_functor_cost<FuncCleaned>::Cost + nested_functor_cost<XprCleaned>::Cost;
  241|       |};
  242|       |
  243|       |template <typename Func, typename LhsXpr, typename RhsXpr>
  244|       |struct nested_functor_cost<CwiseBinaryOp<Func, LhsXpr, RhsXpr>> {
  245|       |  using LhsXprCleaned = remove_all_t<LhsXpr>;
  246|       |  using RhsXprCleaned = remove_all_t<RhsXpr>;
  247|       |  using FuncCleaned = remove_all_t<Func>;
  248|       |  static constexpr Index Cost = nested_functor_cost<FuncCleaned>::Cost + nested_functor_cost<LhsXprCleaned>::Cost +
  249|       |                                nested_functor_cost<RhsXprCleaned>::Cost;
  250|       |};
  251|       |
  252|       |template <typename Func, typename LhsXpr, typename MidXpr, typename RhsXpr>
  253|       |struct nested_functor_cost<CwiseTernaryOp<Func, LhsXpr, MidXpr, RhsXpr>> {
  254|       |  using LhsXprCleaned = remove_all_t<LhsXpr>;
  255|       |  using MidXprCleaned = remove_all_t<MidXpr>;
  256|       |  using RhsXprCleaned = remove_all_t<RhsXpr>;
  257|       |  using FuncCleaned = remove_all_t<Func>;
  258|       |  static constexpr Index Cost = nested_functor_cost<FuncCleaned>::Cost + nested_functor_cost<LhsXprCleaned>::Cost +
  259|       |                                nested_functor_cost<MidXprCleaned>::Cost + nested_functor_cost<RhsXprCleaned>::Cost;
  260|       |};
  261|       |
  262|       |template <typename Xpr>
  263|       |struct functor_cost {
  264|       |  static constexpr Index Cost = plain_enum_max(nested_functor_cost<Xpr>::Cost, 1);
  265|       |};
  266|       |
  267|       |template <typename T>
  268|       |struct packet_traits;
  269|       |
  270|       |template <typename T>
  271|       |struct unpacket_traits;
  272|       |
  273|       |template <int Size, typename PacketType,
  274|       |          bool Stop = Size == Dynamic || (Size % unpacket_traits<PacketType>::size) == 0 ||
  275|       |                      is_same<PacketType, typename unpacket_traits<PacketType>::half>::value>
  276|       |struct find_best_packet_helper;
  277|       |
  278|       |template <int Size, typename PacketType>
  279|       |struct find_best_packet_helper<Size, PacketType, true> {
  280|       |  typedef PacketType type;
  281|       |};
  282|       |
  283|       |template <int Size, typename PacketType>
  284|       |struct find_best_packet_helper<Size, PacketType, false> {
  285|       |  typedef typename find_best_packet_helper<Size, typename unpacket_traits<PacketType>::half>::type type;
  286|       |};
  287|       |
  288|       |template <typename T, int Size>
  289|       |struct find_best_packet {
  290|       |  typedef typename find_best_packet_helper<Size, typename packet_traits<T>::type>::type type;
  291|       |};
  292|       |
  293|       |template <int Size, typename PacketType,
  294|       |          bool Stop = (Size == unpacket_traits<PacketType>::size) ||
  295|       |                      is_same<PacketType, typename unpacket_traits<PacketType>::half>::value>
  296|       |struct find_packet_by_size_helper;
  297|       |template <int Size, typename PacketType>
  298|       |struct find_packet_by_size_helper<Size, PacketType, true> {
  299|       |  using type = PacketType;
  300|       |};
  301|       |template <int Size, typename PacketType>
  302|       |struct find_packet_by_size_helper<Size, PacketType, false> {
  303|       |  using type = typename find_packet_by_size_helper<Size, typename unpacket_traits<PacketType>::half>::type;
  304|       |};
  305|       |
  306|       |template <typename T, int Size>
  307|       |struct find_packet_by_size {
  308|       |  using type = typename find_packet_by_size_helper<Size, typename packet_traits<T>::type>::type;
  309|       |  static constexpr bool value = (Size == unpacket_traits<type>::size);
  310|       |};
  311|       |template <typename T>
  312|       |struct find_packet_by_size<T, 1> {
  313|       |  using type = typename unpacket_traits<T>::type;
  314|       |  static constexpr bool value = (unpacket_traits<type>::size == 1);
  315|       |};
  316|       |
  317|       |#if EIGEN_MAX_STATIC_ALIGN_BYTES > 0
  318|      0|constexpr inline int compute_default_alignment_helper(int ArrayBytes, int AlignmentBytes) {
  319|      0|  if ((ArrayBytes % AlignmentBytes) == 0) {
  320|      0|    return AlignmentBytes;
  321|      0|  } else if (EIGEN_MIN_ALIGN_BYTES < AlignmentBytes) {
  322|      0|    return compute_default_alignment_helper(ArrayBytes, AlignmentBytes / 2);
  323|      0|  } else {
  324|      0|    return 0;
  325|      0|  }
  326|      0|}
  327|       |#else
  328|       |// If static alignment is disabled, no need to bother.
  329|       |// This also avoids a division by zero
  330|       |constexpr inline int compute_default_alignment_helper(int ArrayBytes, int AlignmentBytes) {
  331|       |  EIGEN_UNUSED_VARIABLE(ArrayBytes);
  332|       |  EIGEN_UNUSED_VARIABLE(AlignmentBytes);
  333|       |  return 0;
  334|       |}
  335|       |#endif
  336|       |
  337|       |template <typename T, int Size>
  338|       |struct compute_default_alignment {
  339|       |  enum { value = compute_default_alignment_helper(Size * sizeof(T), EIGEN_MAX_STATIC_ALIGN_BYTES) };
  340|       |};
  341|       |
  342|       |template <typename T>
  343|       |struct compute_default_alignment<T, Dynamic> {
  344|       |  enum { value = EIGEN_MAX_ALIGN_BYTES };
  345|       |};
  346|       |
  347|       |template <typename Scalar_, int Rows_, int Cols_,
  348|       |          int Options_ = AutoAlign | ((Rows_ == 1 && Cols_ != 1)   ? RowMajor
  349|       |                                      : (Cols_ == 1 && Rows_ != 1) ? ColMajor
  350|       |                                                                   : EIGEN_DEFAULT_MATRIX_STORAGE_ORDER_OPTION),
  351|       |          int MaxRows_ = Rows_, int MaxCols_ = Cols_>
  352|       |class make_proper_matrix_type {
  353|       |  enum {
  354|       |    IsColVector = Cols_ == 1 && Rows_ != 1,
  355|       |    IsRowVector = Rows_ == 1 && Cols_ != 1,
  356|       |    Options = IsColVector   ? (Options_ | ColMajor) & ~RowMajor
  357|       |              : IsRowVector ? (Options_ | RowMajor) & ~ColMajor
  358|       |                            : Options_
  359|       |  };
  360|       |
  361|       | public:
  362|       |  typedef Matrix<Scalar_, Rows_, Cols_, Options, MaxRows_, MaxCols_> type;
  363|       |};
  364|       |
  365|      0|constexpr inline unsigned compute_matrix_flags(int Options) {
  366|      0|  unsigned row_major_bit = Options & RowMajor ? RowMajorBit : 0;
  367|      0|  // FIXME currently we still have to handle DirectAccessBit at the expression level to handle DenseCoeffsBase<>
  368|      0|  // and then propagate this information to the evaluator's flags.
  369|      0|  // However, I (Gael) think that DirectAccessBit should only matter at the evaluation stage.
  370|      0|  return DirectAccessBit | LvalueBit | NestByRefBit | row_major_bit;
  371|      0|}
  372|       |
  373|      0|constexpr inline int size_at_compile_time(int rows, int cols) {
  374|      0|  if (rows == 0 || cols == 0) return 0;
  375|      0|  if (rows == Dynamic || cols == Dynamic) return Dynamic;
  376|      0|  return rows * cols;
  377|      0|}
  378|       |
  379|       |template <typename XprType>
  380|       |struct size_of_xpr_at_compile_time {
  381|       |  enum { ret = size_at_compile_time(traits<XprType>::RowsAtCompileTime, traits<XprType>::ColsAtCompileTime) };
  382|       |};
  383|       |
  384|       |/* plain_matrix_type : the difference from eval is that plain_matrix_type is always a plain matrix type,
  385|       | * whereas eval is a const reference in the case of a matrix
  386|       | */
  387|       |
  388|       |template <typename T, typename StorageKind = typename traits<T>::StorageKind>
  389|       |struct plain_matrix_type;
  390|       |template <typename T, typename BaseClassType, int Flags>
  391|       |struct plain_matrix_type_dense;
  392|       |template <typename T>
  393|       |struct plain_matrix_type<T, Dense> {
  394|       |  typedef typename plain_matrix_type_dense<T, typename traits<T>::XprKind, traits<T>::Flags>::type type;
  395|       |};
  396|       |template <typename T>
  397|       |struct plain_matrix_type<T, DiagonalShape> {
  398|       |  typedef typename T::PlainObject type;
  399|       |};
  400|       |
  401|       |template <typename T>
  402|       |struct plain_matrix_type<T, SkewSymmetricShape> {
  403|       |  typedef typename T::PlainObject type;
  404|       |};
  405|       |
  406|       |template <typename T, int Flags>
  407|       |struct plain_matrix_type_dense<T, MatrixXpr, Flags> {
  408|       |  typedef Matrix<typename traits<T>::Scalar, traits<T>::RowsAtCompileTime, traits<T>::ColsAtCompileTime,
  409|       |                 AutoAlign | (Flags & RowMajorBit ? RowMajor : ColMajor), traits<T>::MaxRowsAtCompileTime,
  410|       |                 traits<T>::MaxColsAtCompileTime>
  411|       |      type;
  412|       |};
  413|       |
  414|       |template <typename T, int Flags>
  415|       |struct plain_matrix_type_dense<T, ArrayXpr, Flags> {
  416|       |  typedef Array<typename traits<T>::Scalar, traits<T>::RowsAtCompileTime, traits<T>::ColsAtCompileTime,
  417|       |                AutoAlign | (Flags & RowMajorBit ? RowMajor : ColMajor), traits<T>::MaxRowsAtCompileTime,
  418|       |                traits<T>::MaxColsAtCompileTime>
  419|       |      type;
  420|       |};
  421|       |
  422|       |/* eval : the return type of eval(). For matrices, this is just a const reference
  423|       | * in order to avoid a useless copy
  424|       | */
  425|       |
  426|       |template <typename T, typename StorageKind = typename traits<T>::StorageKind>
  427|       |struct eval;
  428|       |
  429|       |template <typename T>
  430|       |struct eval<T, Dense> {
  431|       |  typedef typename plain_matrix_type<T>::type type;
  432|       |  //   typedef typename T::PlainObject type;
  433|       |  //   typedef T::Matrix<typename traits<T>::Scalar,
  434|       |  //                 traits<T>::RowsAtCompileTime,
  435|       |  //                 traits<T>::ColsAtCompileTime,
  436|       |  //                 AutoAlign | (traits<T>::Flags&RowMajorBit ? RowMajor : ColMajor),
  437|       |  //                 traits<T>::MaxRowsAtCompileTime,
  438|       |  //                 traits<T>::MaxColsAtCompileTime
  439|       |  //           > type;
  440|       |};
  441|       |
  442|       |template <typename T>
  443|       |struct eval<T, DiagonalShape> {
  444|       |  typedef typename plain_matrix_type<T>::type type;
  445|       |};
  446|       |
  447|       |template <typename T>
  448|       |struct eval<T, SkewSymmetricShape> {
  449|       |  typedef typename plain_matrix_type<T>::type type;
  450|       |};
  451|       |
  452|       |// for matrices, no need to evaluate, just use a const reference to avoid a useless copy
  453|       |template <typename Scalar_, int Rows_, int Cols_, int Options_, int MaxRows_, int MaxCols_>
  454|       |struct eval<Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>, Dense> {
  455|       |  typedef const Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>& type;
  456|       |};
  457|       |
  458|       |template <typename Scalar_, int Rows_, int Cols_, int Options_, int MaxRows_, int MaxCols_>
  459|       |struct eval<Array<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>, Dense> {
  460|       |  typedef const Array<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>& type;
  461|       |};
  462|       |
  463|       |/* similar to plain_matrix_type, but using the evaluator's Flags */
  464|       |template <typename T, typename StorageKind = typename traits<T>::StorageKind>
  465|       |struct plain_object_eval;
  466|       |
  467|       |template <typename T>
  468|       |struct plain_object_eval<T, Dense> {
  469|       |  typedef typename plain_matrix_type_dense<T, typename traits<T>::XprKind, evaluator<T>::Flags>::type type;
  470|       |};
  471|       |
  472|       |/* plain_matrix_type_column_major : same as plain_matrix_type but guaranteed to be column-major
  473|       | */
  474|       |template <typename T>
  475|       |struct plain_matrix_type_column_major {
  476|       |  enum {
  477|       |    Rows = traits<T>::RowsAtCompileTime,
  478|       |    Cols = traits<T>::ColsAtCompileTime,
  479|       |    MaxRows = traits<T>::MaxRowsAtCompileTime,
  480|       |    MaxCols = traits<T>::MaxColsAtCompileTime
  481|       |  };
  482|       |  typedef Matrix<typename traits<T>::Scalar, Rows, Cols, (MaxRows == 1 && MaxCols != 1) ? RowMajor : ColMajor, MaxRows,
  483|       |                 MaxCols>
  484|       |      type;
  485|       |};
  486|       |
  487|       |/* plain_matrix_type_row_major : same as plain_matrix_type but guaranteed to be row-major
  488|       | */
  489|       |template <typename T>
  490|       |struct plain_matrix_type_row_major {
  491|       |  enum {
  492|       |    Rows = traits<T>::RowsAtCompileTime,
  493|       |    Cols = traits<T>::ColsAtCompileTime,
  494|       |    MaxRows = traits<T>::MaxRowsAtCompileTime,
  495|       |    MaxCols = traits<T>::MaxColsAtCompileTime
  496|       |  };
  497|       |  typedef Matrix<typename traits<T>::Scalar, Rows, Cols, (MaxCols == 1 && MaxRows != 1) ? ColMajor : RowMajor, MaxRows,
  498|       |                 MaxCols>
  499|       |      type;
  500|       |};
  501|       |
  502|       |/** \internal The reference selector for template expressions. The idea is that we don't
  503|       | * need to use references for expressions since they are light weight proxy
  504|       | * objects which should generate no copying overhead. */
  505|       |template <typename T>
  506|       |struct ref_selector {
  507|       |  typedef std::conditional_t<bool(traits<T>::Flags& NestByRefBit), T const&, const T> type;
  508|       |
  509|       |  typedef std::conditional_t<bool(traits<T>::Flags& NestByRefBit), T&, T> non_const_type;
  510|       |};
  511|       |
  512|       |/** \internal Adds the const qualifier on the value-type of T2 if and only if T1 is a const type */
  513|       |template <typename T1, typename T2>
  514|       |struct transfer_constness {
  515|       |  typedef std::conditional_t<bool(internal::is_const<T1>::value), add_const_on_value_type_t<T2>, T2> type;
  516|       |};
  517|       |
  518|       |// However, we still need a mechanism to detect whether an expression which is evaluated multiple time
  519|       |// has to be evaluated into a temporary.
  520|       |// That's the purpose of this new nested_eval helper:
  521|       |/** \internal Determines how a given expression should be nested when evaluated multiple times.
  522|       | * For example, when you do a * (b+c), Eigen will determine how the expression b+c should be
  523|       | * evaluated into the bigger product expression. The choice is between nesting the expression b+c as-is, or
  524|       | * evaluating that expression b+c into a temporary variable d, and nest d so that the resulting expression is
  525|       | * a*d. Evaluating can be beneficial for example if every coefficient access in the resulting expression causes
  526|       | * many coefficient accesses in the nested expressions -- as is the case with matrix product for example.
  527|       | *
  528|       | * \tparam T the type of the expression being nested.
  529|       | * \tparam n the number of coefficient accesses in the nested expression for each coefficient access in the bigger
  530|       | * expression. \tparam PlainObject the type of the temporary if needed.
  531|       | */
  532|       |template <typename T, int n, typename PlainObject = typename plain_object_eval<T>::type>
  533|       |struct nested_eval {
  534|       |  enum {
  535|       |    ScalarReadCost = NumTraits<typename traits<T>::Scalar>::ReadCost,
  536|       |    CoeffReadCost =
  537|       |        evaluator<T>::CoeffReadCost,  // NOTE What if an evaluator evaluate itself into a temporary?
  538|       |                                      //      Then CoeffReadCost will be small (e.g., 1) but we still have to evaluate,
  539|       |                                      //      especially if n>1. This situation is already taken care by the
  540|       |                                      //      EvalBeforeNestingBit flag, which is turned ON for all evaluator creating a
  541|       |                                      //      temporary. This flag is then propagated by the parent evaluators. Another
  542|       |                                      //      solution could be to count the number of temps?
  543|       |    NAsInteger = n == Dynamic ? HugeCost : n,
  544|       |    CostEval = (NAsInteger + 1) * ScalarReadCost + CoeffReadCost,
  545|       |    CostNoEval = int(NAsInteger) * int(CoeffReadCost),
  546|       |    Evaluate = (int(evaluator<T>::Flags) & EvalBeforeNestingBit) || (int(CostEval) < int(CostNoEval))
  547|       |  };
  548|       |
  549|       |  typedef std::conditional_t<Evaluate, PlainObject, typename ref_selector<T>::type> type;
  550|       |};
  551|       |
  552|       |template <typename T>
  553|       |EIGEN_DEVICE_FUNC inline T* const_cast_ptr(const T* ptr) {
  554|       |  return const_cast<T*>(ptr);
  555|       |}
  556|       |
  557|       |template <typename Derived, typename XprKind = typename traits<Derived>::XprKind>
  558|       |struct dense_xpr_base {
  559|       |  /* dense_xpr_base should only ever be used on dense expressions, thus falling either into the MatrixXpr or into the
  560|       |   * ArrayXpr cases */
  561|       |};
  562|       |
  563|       |template <typename Derived>
  564|       |struct dense_xpr_base<Derived, MatrixXpr> {
  565|       |  typedef MatrixBase<Derived> type;
  566|       |};
  567|       |
  568|       |template <typename Derived>
  569|       |struct dense_xpr_base<Derived, ArrayXpr> {
  570|       |  typedef ArrayBase<Derived> type;
  571|       |};
  572|       |
  573|       |template <typename Derived, typename XprKind = typename traits<Derived>::XprKind,
  574|       |          typename StorageKind = typename traits<Derived>::StorageKind>
  575|       |struct generic_xpr_base;
  576|       |
  577|       |template <typename Derived, typename XprKind>
  578|       |struct generic_xpr_base<Derived, XprKind, Dense> {
  579|       |  typedef typename dense_xpr_base<Derived, XprKind>::type type;
  580|       |};
  581|       |
  582|       |template <typename XprType, typename CastType>
  583|       |struct cast_return_type {
  584|       |  typedef typename XprType::Scalar CurrentScalarType;
  585|       |  typedef remove_all_t<CastType> CastType_;
  586|       |  typedef typename CastType_::Scalar NewScalarType;
  587|       |  typedef std::conditional_t<is_same<CurrentScalarType, NewScalarType>::value, const XprType&, CastType> type;
  588|       |};
  589|       |
  590|       |template <typename A, typename B>
  591|       |struct promote_storage_type;
  592|       |
  593|       |template <typename A>
  594|       |struct promote_storage_type<A, A> {
  595|       |  typedef A ret;
  596|       |};
  597|       |template <typename A>
  598|       |struct promote_storage_type<A, const A> {
  599|       |  typedef A ret;
  600|       |};
  601|       |template <typename A>
  602|       |struct promote_storage_type<const A, A> {
  603|       |  typedef A ret;
  604|       |};
  605|       |
  606|       |/** \internal Specify the "storage kind" of applying a coefficient-wise
  607|       | * binary operations between two expressions of kinds A and B respectively.
  608|       | * The template parameter Functor permits to specialize the resulting storage kind wrt to
  609|       | * the functor.
  610|       | * The default rules are as follows:
  611|       | * \code
  612|       | * A      op A      -> A
  613|       | * A      op dense  -> dense
  614|       | * dense  op B      -> dense
  615|       | * sparse op dense  -> sparse
  616|       | * dense  op sparse -> sparse
  617|       | * \endcode
  618|       | */
  619|       |template <typename A, typename B, typename Functor>
  620|       |struct cwise_promote_storage_type;
  621|       |
  622|       |template <typename A, typename Functor>
  623|       |struct cwise_promote_storage_type<A, A, Functor> {
  624|       |  typedef A ret;
  625|       |};
  626|       |template <typename Functor>
  627|       |struct cwise_promote_storage_type<Dense, Dense, Functor> {
  628|       |  typedef Dense ret;
  629|       |};
  630|       |template <typename A, typename Functor>
  631|       |struct cwise_promote_storage_type<A, Dense, Functor> {
  632|       |  typedef Dense ret;
  633|       |};
  634|       |template <typename B, typename Functor>
  635|       |struct cwise_promote_storage_type<Dense, B, Functor> {
  636|       |  typedef Dense ret;
  637|       |};
  638|       |template <typename Functor>
  639|       |struct cwise_promote_storage_type<Sparse, Dense, Functor> {
  640|       |  typedef Sparse ret;
  641|       |};
  642|       |template <typename Functor>
  643|       |struct cwise_promote_storage_type<Dense, Sparse, Functor> {
  644|       |  typedef Sparse ret;
  645|       |};
  646|       |
  647|       |template <typename LhsKind, typename RhsKind, int LhsOrder, int RhsOrder>
  648|       |struct cwise_promote_storage_order {
  649|       |  enum { value = LhsOrder };
  650|       |};
  651|       |
  652|       |template <typename LhsKind, int LhsOrder, int RhsOrder>
  653|       |struct cwise_promote_storage_order<LhsKind, Sparse, LhsOrder, RhsOrder> {
  654|       |  enum { value = RhsOrder };
  655|       |};
  656|       |template <typename RhsKind, int LhsOrder, int RhsOrder>
  657|       |struct cwise_promote_storage_order<Sparse, RhsKind, LhsOrder, RhsOrder> {
  658|       |  enum { value = LhsOrder };
  659|       |};
  660|       |template <int Order>
  661|       |struct cwise_promote_storage_order<Sparse, Sparse, Order, Order> {
  662|       |  enum { value = Order };
  663|       |};
  664|       |
  665|       |/** \internal Specify the "storage kind" of multiplying an expression of kind A with kind B.
  666|       | * The template parameter ProductTag permits to specialize the resulting storage kind wrt to
  667|       | * some compile-time properties of the product: GemmProduct, GemvProduct, OuterProduct, InnerProduct.
  668|       | * The default rules are as follows:
  669|       | * \code
  670|       | *  K * K            -> K
  671|       | *  dense * K        -> dense
  672|       | *  K * dense        -> dense
  673|       | *  diag * K         -> K
  674|       | *  K * diag         -> K
  675|       | *  Perm * K         -> K
  676|       | * K * Perm          -> K
  677|       | * \endcode
  678|       | */
  679|       |template <typename A, typename B, int ProductTag>
  680|       |struct product_promote_storage_type;
  681|       |
  682|       |template <typename A, int ProductTag>
  683|       |struct product_promote_storage_type<A, A, ProductTag> {
  684|       |  typedef A ret;
  685|       |};
  686|       |template <int ProductTag>
  687|       |struct product_promote_storage_type<Dense, Dense, ProductTag> {
  688|       |  typedef Dense ret;
  689|       |};
  690|       |template <typename A, int ProductTag>
  691|       |struct product_promote_storage_type<A, Dense, ProductTag> {
  692|       |  typedef Dense ret;
  693|       |};
  694|       |template <typename B, int ProductTag>
  695|       |struct product_promote_storage_type<Dense, B, ProductTag> {
  696|       |  typedef Dense ret;
  697|       |};
  698|       |
  699|       |template <typename A, int ProductTag>
  700|       |struct product_promote_storage_type<A, DiagonalShape, ProductTag> {
  701|       |  typedef A ret;
  702|       |};
  703|       |template <typename B, int ProductTag>
  704|       |struct product_promote_storage_type<DiagonalShape, B, ProductTag> {
  705|       |  typedef B ret;
  706|       |};
  707|       |template <int ProductTag>
  708|       |struct product_promote_storage_type<Dense, DiagonalShape, ProductTag> {
  709|       |  typedef Dense ret;
  710|       |};
  711|       |template <int ProductTag>
  712|       |struct product_promote_storage_type<DiagonalShape, Dense, ProductTag> {
  713|       |  typedef Dense ret;
  714|       |};
  715|       |
  716|       |template <typename A, int ProductTag>
  717|       |struct product_promote_storage_type<A, SkewSymmetricShape, ProductTag> {
  718|       |  typedef A ret;
  719|       |};
  720|       |template <typename B, int ProductTag>
  721|       |struct product_promote_storage_type<SkewSymmetricShape, B, ProductTag> {
  722|       |  typedef B ret;
  723|       |};
  724|       |template <int ProductTag>
  725|       |struct product_promote_storage_type<Dense, SkewSymmetricShape, ProductTag> {
  726|       |  typedef Dense ret;
  727|       |};
  728|       |template <int ProductTag>
  729|       |struct product_promote_storage_type<SkewSymmetricShape, Dense, ProductTag> {
  730|       |  typedef Dense ret;
  731|       |};
  732|       |template <int ProductTag>
  733|       |struct product_promote_storage_type<SkewSymmetricShape, SkewSymmetricShape, ProductTag> {
  734|       |  typedef Dense ret;
  735|       |};
  736|       |
  737|       |template <typename A, int ProductTag>
  738|       |struct product_promote_storage_type<A, PermutationStorage, ProductTag> {
  739|       |  typedef A ret;
  740|       |};
  741|       |template <typename B, int ProductTag>
  742|       |struct product_promote_storage_type<PermutationStorage, B, ProductTag> {
  743|       |  typedef B ret;
  744|       |};
  745|       |template <int ProductTag>
  746|       |struct product_promote_storage_type<Dense, PermutationStorage, ProductTag> {
  747|       |  typedef Dense ret;
  748|       |};
  749|       |template <int ProductTag>
  750|       |struct product_promote_storage_type<PermutationStorage, Dense, ProductTag> {
  751|       |  typedef Dense ret;
  752|       |};
  753|       |
  754|       |/** \internal gives the plain matrix or array type to store a row/column/diagonal of a matrix type.
  755|       | * \tparam Scalar optional parameter allowing to pass a different scalar type than the one of the MatrixType.
  756|       | */
  757|       |template <typename ExpressionType, typename Scalar = typename ExpressionType::Scalar>
  758|       |struct plain_row_type {
  759|       |  typedef Matrix<Scalar, 1, ExpressionType::ColsAtCompileTime,
  760|       |                 int(ExpressionType::PlainObject::Options) | int(RowMajor), 1, ExpressionType::MaxColsAtCompileTime>
  761|       |      MatrixRowType;
  762|       |  typedef Array<Scalar, 1, ExpressionType::ColsAtCompileTime, int(ExpressionType::PlainObject::Options) | int(RowMajor),
  763|       |                1, ExpressionType::MaxColsAtCompileTime>
  764|       |      ArrayRowType;
  765|       |
  766|       |  typedef std::conditional_t<is_same<typename traits<ExpressionType>::XprKind, MatrixXpr>::value, MatrixRowType,
  767|       |                             ArrayRowType>
  768|       |      type;
  769|       |};
  770|       |
  771|       |template <typename ExpressionType, typename Scalar = typename ExpressionType::Scalar>
  772|       |struct plain_col_type {
  773|       |  typedef Matrix<Scalar, ExpressionType::RowsAtCompileTime, 1, ExpressionType::PlainObject::Options & ~RowMajor,
  774|       |                 ExpressionType::MaxRowsAtCompileTime, 1>
  775|       |      MatrixColType;
  776|       |  typedef Array<Scalar, ExpressionType::RowsAtCompileTime, 1, ExpressionType::PlainObject::Options & ~RowMajor,
  777|       |                ExpressionType::MaxRowsAtCompileTime, 1>
  778|       |      ArrayColType;
  779|       |
  780|       |  typedef std::conditional_t<is_same<typename traits<ExpressionType>::XprKind, MatrixXpr>::value, MatrixColType,
  781|       |                             ArrayColType>
  782|       |      type;
  783|       |};
  784|       |
  785|       |template <typename ExpressionType, typename Scalar = typename ExpressionType::Scalar>
  786|       |struct plain_diag_type {
  787|       |  enum {
  788|       |    diag_size = internal::min_size_prefer_dynamic(ExpressionType::RowsAtCompileTime, ExpressionType::ColsAtCompileTime),
  789|       |    max_diag_size = min_size_prefer_fixed(ExpressionType::MaxRowsAtCompileTime, ExpressionType::MaxColsAtCompileTime)
  790|       |  };
  791|       |  typedef Matrix<Scalar, diag_size, 1, ExpressionType::PlainObject::Options & ~RowMajor, max_diag_size, 1>
  792|       |      MatrixDiagType;
  793|       |  typedef Array<Scalar, diag_size, 1, ExpressionType::PlainObject::Options & ~RowMajor, max_diag_size, 1> ArrayDiagType;
  794|       |
  795|       |  typedef std::conditional_t<is_same<typename traits<ExpressionType>::XprKind, MatrixXpr>::value, MatrixDiagType,
  796|       |                             ArrayDiagType>
  797|       |      type;
  798|       |};
  799|       |
  800|       |template <typename Expr, typename Scalar = typename Expr::Scalar>
  801|       |struct plain_constant_type {
  802|       |  enum { Options = (traits<Expr>::Flags & RowMajorBit) ? RowMajor : 0 };
  803|       |
  804|       |  typedef Array<Scalar, traits<Expr>::RowsAtCompileTime, traits<Expr>::ColsAtCompileTime, Options,
  805|       |                traits<Expr>::MaxRowsAtCompileTime, traits<Expr>::MaxColsAtCompileTime>
  806|       |      array_type;
  807|       |
  808|       |  typedef Matrix<Scalar, traits<Expr>::RowsAtCompileTime, traits<Expr>::ColsAtCompileTime, Options,
  809|       |                 traits<Expr>::MaxRowsAtCompileTime, traits<Expr>::MaxColsAtCompileTime>
  810|       |      matrix_type;
  811|       |
  812|       |  typedef CwiseNullaryOp<
  813|       |      scalar_constant_op<Scalar>,
  814|       |      const std::conditional_t<is_same<typename traits<Expr>::XprKind, MatrixXpr>::value, matrix_type, array_type>>
  815|       |      type;
  816|       |};
  817|       |
  818|       |template <typename ExpressionType>
  819|       |struct is_lvalue {
  820|       |  enum { value = (!bool(is_const<ExpressionType>::value)) && bool(traits<ExpressionType>::Flags & LvalueBit) };
  821|       |};
  822|       |
  823|       |template <typename T>
  824|       |struct is_diagonal {
  825|       |  enum { ret = false };
  826|       |};
  827|       |
  828|       |template <typename T>
  829|       |struct is_diagonal<DiagonalBase<T>> {
  830|       |  enum { ret = true };
  831|       |};
  832|       |
  833|       |template <typename T>
  834|       |struct is_diagonal<DiagonalWrapper<T>> {
  835|       |  enum { ret = true };
  836|       |};
  837|       |
  838|       |template <typename T, int S>
  839|       |struct is_diagonal<DiagonalMatrix<T, S>> {
  840|       |  enum { ret = true };
  841|       |};
  842|       |
  843|       |template <typename T>
  844|       |struct is_identity {
  845|       |  enum { value = false };
  846|       |};
  847|       |
  848|       |template <typename T>
  849|       |struct is_identity<CwiseNullaryOp<internal::scalar_identity_op<typename T::Scalar>, T>> {
  850|       |  enum { value = true };
  851|       |};
  852|       |
  853|       |template <typename S1, typename S2>
  854|       |struct glue_shapes;
  855|       |template <>
  856|       |struct glue_shapes<DenseShape, TriangularShape> {
  857|       |  typedef TriangularShape type;
  858|       |};
  859|       |
  860|       |template <typename T1, typename T2>
  861|       |struct possibly_same_dense {
  862|       |  enum {
  863|       |    value = has_direct_access<T1>::ret && has_direct_access<T2>::ret &&
  864|       |            is_same<typename T1::Scalar, typename T2::Scalar>::value
  865|       |  };
  866|       |};
  867|       |
  868|       |template <typename T1, typename T2>
  869|       |EIGEN_DEVICE_FUNC bool is_same_dense(const T1& mat1, const T2& mat2,
  870|       |                                     std::enable_if_t<possibly_same_dense<T1, T2>::value>* = 0) {
  871|       |  return (mat1.data() == mat2.data()) && (mat1.innerStride() == mat2.innerStride()) &&
  872|       |         (mat1.outerStride() == mat2.outerStride());
  873|       |}
  874|       |
  875|       |template <typename T1, typename T2>
  876|       |EIGEN_DEVICE_FUNC bool is_same_dense(const T1&, const T2&, std::enable_if_t<!possibly_same_dense<T1, T2>::value>* = 0) {
  877|       |  return false;
  878|       |}
  879|       |
  880|       |// Internal helper defining the cost of a scalar division for the type T.
  881|       |// The default heuristic can be specialized for each scalar type and architecture.
  882|       |template <typename T, bool Vectorized = false, typename EnableIf = void>
  883|       |struct scalar_div_cost {
  884|       |  enum { value = 8 * NumTraits<T>::MulCost };
  885|       |};
  886|       |
  887|       |template <typename T, bool Vectorized>
  888|       |struct scalar_div_cost<std::complex<T>, Vectorized> {
  889|       |  enum { value = 2 * scalar_div_cost<T>::value + 6 * NumTraits<T>::MulCost + 3 * NumTraits<T>::AddCost };
  890|       |};
  891|       |
  892|       |template <bool Vectorized>
  893|       |struct scalar_div_cost<signed long, Vectorized, std::conditional_t<sizeof(long) == 8, void, false_type>> {
  894|       |  enum { value = 24 };
  895|       |};
  896|       |template <bool Vectorized>
  897|       |struct scalar_div_cost<unsigned long, Vectorized, std::conditional_t<sizeof(long) == 8, void, false_type>> {
  898|       |  enum { value = 21 };
  899|       |};
  900|       |
  901|       |#ifdef EIGEN_DEBUG_ASSIGN
  902|       |std::string demangle_traversal(int t) {
  903|       |  if (t == DefaultTraversal) return "DefaultTraversal";
  904|       |  if (t == LinearTraversal) return "LinearTraversal";
  905|       |  if (t == InnerVectorizedTraversal) return "InnerVectorizedTraversal";
  906|       |  if (t == LinearVectorizedTraversal) return "LinearVectorizedTraversal";
  907|       |  if (t == SliceVectorizedTraversal) return "SliceVectorizedTraversal";
  908|       |  return "?";
  909|       |}
  910|       |std::string demangle_unrolling(int t) {
  911|       |  if (t == NoUnrolling) return "NoUnrolling";
  912|       |  if (t == InnerUnrolling) return "InnerUnrolling";
  913|       |  if (t == CompleteUnrolling) return "CompleteUnrolling";
  914|       |  return "?";
  915|       |}
  916|       |std::string demangle_flags(int f) {
  917|       |  std::string res;
  918|       |  if (f & RowMajorBit) res += " | RowMajor";
  919|       |  if (f & PacketAccessBit) res += " | Packet";
  920|       |  if (f & LinearAccessBit) res += " | Linear";
  921|       |  if (f & LvalueBit) res += " | Lvalue";
  922|       |  if (f & DirectAccessBit) res += " | Direct";
  923|       |  if (f & NestByRefBit) res += " | NestByRef";
  924|       |  if (f & NoPreferredStorageOrderBit) res += " | NoPreferredStorageOrderBit";
  925|       |
  926|       |  return res;
  927|       |}
  928|       |#endif
  929|       |
  930|       |template <typename XprType>
  931|       |struct is_block_xpr : std::false_type {};
  932|       |
  933|       |template <typename XprType, int BlockRows, int BlockCols, bool InnerPanel>
  934|       |struct is_block_xpr<Block<XprType, BlockRows, BlockCols, InnerPanel>> : std::true_type {};
  935|       |
  936|       |template <typename XprType, int BlockRows, int BlockCols, bool InnerPanel>
  937|       |struct is_block_xpr<const Block<XprType, BlockRows, BlockCols, InnerPanel>> : std::true_type {};
  938|       |
  939|       |// Helper utility for constructing non-recursive block expressions.
  940|       |template <typename XprType>
  941|       |struct block_xpr_helper {
  942|       |  using BaseType = XprType;
  943|       |
  944|       |  // For regular block expressions, simply forward along the InnerPanel argument,
  945|       |  // which is set when calling row/column expressions.
  946|       |  static constexpr bool is_inner_panel(bool inner_panel) { return inner_panel; }
  947|       |
  948|       |  // Only enable non-const base function if XprType is not const (otherwise we get a duplicate definition).
  949|       |  template <typename T = XprType, typename EnableIf = std::enable_if_t<!std::is_const<T>::value>>
  950|       |  static EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE BaseType& base(XprType& xpr) {
  951|       |    return xpr;
  952|       |  }
  953|       |  static EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE const BaseType& base(const XprType& xpr) { return xpr; }
  954|       |  static constexpr EIGEN_ALWAYS_INLINE Index row(const XprType& /*xpr*/, Index r) { return r; }
  955|       |  static constexpr EIGEN_ALWAYS_INLINE Index col(const XprType& /*xpr*/, Index c) { return c; }
  956|       |};
  957|       |
  958|       |template <typename XprType, int BlockRows, int BlockCols, bool InnerPanel>
  959|       |struct block_xpr_helper<Block<XprType, BlockRows, BlockCols, InnerPanel>> {
  960|       |  using BlockXprType = Block<XprType, BlockRows, BlockCols, InnerPanel>;
  961|       |  // Recursive helper in case of explicit block-of-block expression.
  962|       |  using NestedXprHelper = block_xpr_helper<XprType>;
  963|       |  using BaseType = typename NestedXprHelper::BaseType;
  964|       |
  965|       |  // For block-of-block expressions, we need to combine the InnerPannel trait
  966|       |  // with that of the block subexpression.
  967|       |  static constexpr bool is_inner_panel(bool inner_panel) { return InnerPanel && inner_panel; }
  968|       |
  969|       |  // Only enable non-const base function if XprType is not const (otherwise we get a duplicates definition).
  970|       |  template <typename T = XprType, typename EnableIf = std::enable_if_t<!std::is_const<T>::value>>
  971|       |  static EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE BaseType& base(BlockXprType& xpr) {
  972|       |    return NestedXprHelper::base(xpr.nestedExpression());
  973|       |  }
  974|       |  static EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE const BaseType& base(const BlockXprType& xpr) {
  975|       |    return NestedXprHelper::base(xpr.nestedExpression());
  976|       |  }
  977|       |  static constexpr EIGEN_ALWAYS_INLINE Index row(const BlockXprType& xpr, Index r) {
  978|       |    return xpr.startRow() + NestedXprHelper::row(xpr.nestedExpression(), r);
  979|       |  }
  980|       |  static constexpr EIGEN_ALWAYS_INLINE Index col(const BlockXprType& xpr, Index c) {
  981|       |    return xpr.startCol() + NestedXprHelper::col(xpr.nestedExpression(), c);
  982|       |  }
  983|       |};
  984|       |
  985|       |template <typename XprType, int BlockRows, int BlockCols, bool InnerPanel>
  986|       |struct block_xpr_helper<const Block<XprType, BlockRows, BlockCols, InnerPanel>>
  987|       |    : block_xpr_helper<Block<XprType, BlockRows, BlockCols, InnerPanel>> {};
  988|       |
  989|       |template <typename XprType>
  990|       |struct is_matrix_base_xpr : std::is_base_of<MatrixBase<remove_all_t<XprType>>, remove_all_t<XprType>> {};
  991|       |
  992|       |template <typename XprType>
  993|       |struct is_permutation_base_xpr : std::is_base_of<PermutationBase<remove_all_t<XprType>>, remove_all_t<XprType>> {};
  994|       |
  995|       |}  // end namespace internal
  996|       |
  997|       |/** \class ScalarBinaryOpTraits
  998|       |  * \ingroup Core_Module
  999|       |  *
 1000|       |  * \brief Determines whether the given binary operation of two numeric types is allowed and what the scalar return type
 1001|       |  is.
 1002|       |  *
 1003|       |  * This class permits to control the scalar return type of any binary operation performed on two different scalar types
 1004|       |  through (partial) template specializations.
 1005|       |  *
 1006|       |  * For instance, let \c U1, \c U2 and \c U3 be three user defined scalar types for which most operations between
 1007|       |  instances of \c U1 and \c U2 returns an \c U3.
 1008|       |  * You can let %Eigen knows that by defining:
 1009|       |    \code
 1010|       |    template<typename BinaryOp>
 1011|       |    struct ScalarBinaryOpTraits<U1,U2,BinaryOp> { typedef U3 ReturnType;  };
 1012|       |    template<typename BinaryOp>
 1013|       |    struct ScalarBinaryOpTraits<U2,U1,BinaryOp> { typedef U3 ReturnType;  };
 1014|       |    \endcode
 1015|       |  * You can then explicitly disable some particular operations to get more explicit error messages:
 1016|       |    \code
 1017|       |    template<>
 1018|       |    struct ScalarBinaryOpTraits<U1,U2,internal::scalar_max_op<U1,U2> > {};
 1019|       |    \endcode
 1020|       |  * Or customize the return type for individual operation:
 1021|       |    \code
 1022|       |    template<>
 1023|       |    struct ScalarBinaryOpTraits<U1,U2,internal::scalar_sum_op<U1,U2> > { typedef U1 ReturnType; };
 1024|       |    \endcode
 1025|       |  *
 1026|       |  * By default, the following generic combinations are supported:
 1027|       |  <table class="manual">
 1028|       |  <tr><th>ScalarA</th><th>ScalarB</th><th>BinaryOp</th><th>ReturnType</th><th>Note</th></tr>
 1029|       |  <tr            ><td>\c T </td><td>\c T </td><td>\c * </td><td>\c T </td><td></td></tr>
 1030|       |  <tr class="alt"><td>\c NumTraits<T>::Real </td><td>\c T </td><td>\c * </td><td>\c T </td><td>Only if \c
 1031|       |  NumTraits<T>::IsComplex </td></tr> <tr            ><td>\c T </td><td>\c NumTraits<T>::Real </td><td>\c * </td><td>\c T
 1032|       |  </td><td>Only if \c NumTraits<T>::IsComplex </td></tr>
 1033|       |  </table>
 1034|       |  *
 1035|       |  * \sa CwiseBinaryOp
 1036|       |  */
 1037|       |template <typename ScalarA, typename ScalarB, typename BinaryOp = internal::scalar_product_op<ScalarA, ScalarB>>
 1038|       |struct ScalarBinaryOpTraits
 1039|       |#ifndef EIGEN_PARSED_BY_DOXYGEN
 1040|       |    // for backward compatibility, use the hints given by the (deprecated) internal::scalar_product_traits class.
 1041|       |    : internal::scalar_product_traits<ScalarA, ScalarB>
 1042|       |#endif  // EIGEN_PARSED_BY_DOXYGEN
 1043|       |{
 1044|       |};
 1045|       |
 1046|       |template <typename T, typename BinaryOp>
 1047|       |struct ScalarBinaryOpTraits<T, T, BinaryOp> {
 1048|       |  typedef T ReturnType;
 1049|       |};
 1050|       |
 1051|       |template <typename T, typename BinaryOp>
 1052|       |struct ScalarBinaryOpTraits<T, typename NumTraits<std::enable_if_t<NumTraits<T>::IsComplex, T>>::Real, BinaryOp> {
 1053|       |  typedef T ReturnType;
 1054|       |};
 1055|       |template <typename T, typename BinaryOp>
 1056|       |struct ScalarBinaryOpTraits<typename NumTraits<std::enable_if_t<NumTraits<T>::IsComplex, T>>::Real, T, BinaryOp> {
 1057|       |  typedef T ReturnType;
 1058|       |};
 1059|       |
 1060|       |// For Matrix * Permutation
 1061|       |template <typename T, typename BinaryOp>
 1062|       |struct ScalarBinaryOpTraits<T, void, BinaryOp> {
 1063|       |  typedef T ReturnType;
 1064|       |};
 1065|       |
 1066|       |// For Permutation * Matrix
 1067|       |template <typename T, typename BinaryOp>
 1068|       |struct ScalarBinaryOpTraits<void, T, BinaryOp> {
 1069|       |  typedef T ReturnType;
 1070|       |};
 1071|       |
 1072|       |// for Permutation*Permutation
 1073|       |template <typename BinaryOp>
 1074|       |struct ScalarBinaryOpTraits<void, void, BinaryOp> {
 1075|       |  typedef void ReturnType;
 1076|       |};
 1077|       |
 1078|       |// We require Lhs and Rhs to have "compatible" scalar types.
 1079|       |// It is tempting to always allow mixing different types but remember that this is often impossible in the vectorized
 1080|       |// paths. So allowing mixing different types gives very unexpected errors when enabling vectorization, when the user
 1081|       |// tries to add together a float matrix and a double matrix.
 1082|       |#define EIGEN_CHECK_BINARY_COMPATIBILIY(BINOP, LHS, RHS)                               \
 1083|       |  EIGEN_STATIC_ASSERT(                                                                 \
 1084|       |      (Eigen::internal::has_ReturnType<ScalarBinaryOpTraits<LHS, RHS, BINOP>>::value), \
 1085|       |      YOU_MIXED_DIFFERENT_NUMERIC_TYPES__YOU_NEED_TO_USE_THE_CAST_METHOD_OF_MATRIXBASE_TO_CAST_NUMERIC_TYPES_EXPLICITLY)
 1086|       |
 1087|       |}  // end namespace Eigen
 1088|       |
 1089|       |#endif  // EIGEN_XPRHELPER_H

/home/kidus/Desktop/GNNs/eigen/eigen/test/AnnoyingScalar.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2011-2018 Gael Guennebaud <gael.guennebaud@inria.fr>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#ifndef EIGEN_TEST_ANNOYING_SCALAR_H
   11|       |#define EIGEN_TEST_ANNOYING_SCALAR_H
   12|       |
   13|       |#include <ostream>
   14|       |
   15|       |#if EIGEN_COMP_GNUC
   16|       |#pragma GCC diagnostic ignored "-Wshadow"
   17|       |#endif
   18|       |
   19|       |#ifndef EIGEN_TEST_ANNOYING_SCALAR_DONT_THROW
   20|       |struct my_exception {
   21|      0|  my_exception() {}
   22|      0|  ~my_exception() {}
   23|       |};
   24|       |#endif
   25|       |
   26|       |// An AnnoyingScalar is a pseudo scalar type that:
   27|       |// - can randomly through an exception in operator +
   28|       |// - randomly allocate on the heap or initialize a reference to itself making it non trivially copyable, nor movable,
   29|       |// nor relocatable.
   30|       |
   31|       |class AnnoyingScalar {
   32|       | public:
   33|      0|  AnnoyingScalar() {
   34|      0|    init();
   35|      0|    *v = 0;
   36|      0|  }
   37|      0|  AnnoyingScalar(long double _v) {
   38|      0|    init();
   39|      0|    *v = static_cast<float>(_v);
   40|      0|  }
   41|      0|  AnnoyingScalar(double _v) {
   42|      0|    init();
   43|      0|    *v = static_cast<float>(_v);
   44|      0|  }
   45|      0|  AnnoyingScalar(float _v) {
   46|      0|    init();
   47|      0|    *v = _v;
   48|      0|  }
   49|      0|  AnnoyingScalar(int _v) {
   50|      0|    init();
   51|      0|    *v = static_cast<float>(_v);
   52|      0|  }
   53|      0|  AnnoyingScalar(long _v) {
   54|      0|    init();
   55|      0|    *v = static_cast<float>(_v);
   56|      0|  }
   57|      0|  AnnoyingScalar(long long _v) {
   58|      0|    init();
   59|      0|    *v = static_cast<float>(_v);
   60|      0|  }
   61|      0|  AnnoyingScalar(const AnnoyingScalar& other) {
   62|      0|    init();
   63|      0|    *v = *(other.v);
   64|      0|  }
   65|      0|  ~AnnoyingScalar() {
   66|      0|    if (v != &data) delete v;
   67|      0|    instances--;
   68|      0|  }
   69|       |
   70|      0|  void init() {
   71|      0|    if (internal::random<bool>())
   72|      0|      v = new float;
   73|      0|    else
   74|      0|      v = &data;
   75|      0|    instances++;
   76|      0|  }
   77|       |
   78|      0|  AnnoyingScalar operator+(const AnnoyingScalar& other) const {
   79|      0|#ifndef EIGEN_TEST_ANNOYING_SCALAR_DONT_THROW
   80|      0|    countdown--;
   81|      0|    if (countdown <= 0 && !dont_throw) throw my_exception();
   82|      0|#endif
   83|      0|    return AnnoyingScalar(*v + *other.v);
   84|      0|  }
   85|       |
   86|      0|  AnnoyingScalar operator-() const { return AnnoyingScalar(-*v); }
   87|       |
   88|      0|  AnnoyingScalar operator-(const AnnoyingScalar& other) const { return AnnoyingScalar(*v - *other.v); }
   89|       |
   90|      0|  AnnoyingScalar operator*(const AnnoyingScalar& other) const { return AnnoyingScalar((*v) * (*other.v)); }
   91|       |
   92|      0|  AnnoyingScalar operator/(const AnnoyingScalar& other) const { return AnnoyingScalar((*v) / (*other.v)); }
   93|       |
   94|      0|  AnnoyingScalar& operator+=(const AnnoyingScalar& other) {
   95|      0|    *v += *other.v;
   96|      0|    return *this;
   97|      0|  }
   98|      0|  AnnoyingScalar& operator-=(const AnnoyingScalar& other) {
   99|      0|    *v -= *other.v;
  100|      0|    return *this;
  101|      0|  }
  102|      0|  AnnoyingScalar& operator*=(const AnnoyingScalar& other) {
  103|      0|    *v *= *other.v;
  104|      0|    return *this;
  105|      0|  }
  106|      0|  AnnoyingScalar& operator/=(const AnnoyingScalar& other) {
  107|      0|    *v /= *other.v;
  108|      0|    return *this;
  109|      0|  }
  110|      0|  AnnoyingScalar& operator=(const AnnoyingScalar& other) {
  111|      0|    *v = *other.v;
  112|      0|    return *this;
  113|      0|  }
  114|       |
  115|      0|  bool operator==(const AnnoyingScalar& other) const { return numext::equal_strict(*v, *other.v); }
  116|      0|  bool operator!=(const AnnoyingScalar& other) const { return numext::not_equal_strict(*v, *other.v); }
  117|      0|  bool operator<=(const AnnoyingScalar& other) const { return *v <= *other.v; }
  118|      0|  bool operator<(const AnnoyingScalar& other) const { return *v < *other.v; }
  119|      0|  bool operator>=(const AnnoyingScalar& other) const { return *v >= *other.v; }
  120|      0|  bool operator>(const AnnoyingScalar& other) const { return *v > *other.v; }
  121|       |
  122|       |  float* v;
  123|       |  float data;
  124|       |  static int instances;
  125|       |#ifndef EIGEN_TEST_ANNOYING_SCALAR_DONT_THROW
  126|       |  static int countdown;
  127|       |  static bool dont_throw;
  128|       |#endif
  129|       |};
  130|       |
  131|      0|AnnoyingScalar real(const AnnoyingScalar& x) { return x; }
  132|      0|AnnoyingScalar imag(const AnnoyingScalar&) { return 0; }
  133|      0|AnnoyingScalar conj(const AnnoyingScalar& x) { return x; }
  134|      0|AnnoyingScalar sqrt(const AnnoyingScalar& x) { return std::sqrt(*x.v); }
  135|      0|AnnoyingScalar abs(const AnnoyingScalar& x) { return std::abs(*x.v); }
  136|      0|AnnoyingScalar cos(const AnnoyingScalar& x) { return std::cos(*x.v); }
  137|      0|AnnoyingScalar sin(const AnnoyingScalar& x) { return std::sin(*x.v); }
  138|      0|AnnoyingScalar acos(const AnnoyingScalar& x) { return std::acos(*x.v); }
  139|      0|AnnoyingScalar atan2(const AnnoyingScalar& y, const AnnoyingScalar& x) { return std::atan2(*y.v, *x.v); }
  140|       |
  141|      0|std::ostream& operator<<(std::ostream& stream, const AnnoyingScalar& x) {
  142|      0|  stream << (*(x.v));
  143|      0|  return stream;
  144|      0|}
  145|       |
  146|       |int AnnoyingScalar::instances = 0;
  147|       |
  148|       |#ifndef EIGEN_TEST_ANNOYING_SCALAR_DONT_THROW
  149|       |int AnnoyingScalar::countdown = 0;
  150|       |bool AnnoyingScalar::dont_throw = false;
  151|       |#endif
  152|       |
  153|       |namespace Eigen {
  154|       |template <>
  155|       |struct NumTraits<AnnoyingScalar> : NumTraits<float> {
  156|       |  enum {
  157|       |    RequireInitialization = 1,
  158|       |  };
  159|       |  typedef AnnoyingScalar Real;
  160|       |  typedef AnnoyingScalar Nested;
  161|       |  typedef AnnoyingScalar Literal;
  162|       |  typedef AnnoyingScalar NonInteger;
  163|       |};
  164|       |
  165|       |template <>
  166|      0|inline AnnoyingScalar test_precision<AnnoyingScalar>() {
  167|      0|  return test_precision<float>();
  168|      0|}
  169|       |
  170|       |namespace numext {
  171|       |template <>
  172|      0|EIGEN_DEVICE_FUNC EIGEN_ALWAYS_INLINE bool(isfinite)(const AnnoyingScalar& x) {
  173|      0|  return (numext::isfinite)(*x.v);
  174|      0|}
  175|       |}  // namespace numext
  176|       |
  177|       |namespace internal {
  178|       |template <>
  179|      0|EIGEN_STRONG_INLINE double cast(const AnnoyingScalar& x) {
  180|      0|  return double(*x.v);
  181|      0|}
  182|       |template <>
  183|      0|EIGEN_STRONG_INLINE float cast(const AnnoyingScalar& x) {
  184|      0|  return *x.v;
  185|      0|}
  186|       |
  187|       |}  // namespace internal
  188|       |}  // namespace Eigen
  189|       |
  190|      0|AnnoyingScalar get_test_precision(const AnnoyingScalar&) { return Eigen::test_precision<AnnoyingScalar>(); }
  191|       |
  192|      0|AnnoyingScalar test_relative_error(const AnnoyingScalar& a, const AnnoyingScalar& b) {
  193|      0|  return test_relative_error(*a.v, *b.v);
  194|      0|}
  195|       |
  196|      0|inline bool test_isApprox(const AnnoyingScalar& a, const AnnoyingScalar& b) {
  197|      0|  return internal::isApprox(*a.v, *b.v, test_precision<float>());
  198|      0|}
  199|       |
  200|      0|inline bool test_isMuchSmallerThan(const AnnoyingScalar& a, const AnnoyingScalar& b) {
  201|      0|  return test_isMuchSmallerThan(*a.v, *b.v);
  202|      0|}
  203|       |
  204|       |#endif  // EIGEN_TEST_ANNOYING_SCALAR_H

/home/kidus/Desktop/GNNs/eigen/eigen/test/conservative_resize.cpp:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2009 Hauke Heibel <hauke.heibel@gmail.com>
    5|       |//
    6|       |// This Source Code Form is subject to the terms of the Mozilla
    7|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    8|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
    9|       |
   10|       |#include "main.h"
   11|       |
   12|       |#include <Eigen/Core>
   13|       |#include "AnnoyingScalar.h"
   14|       |
   15|       |using namespace Eigen;
   16|       |
   17|       |template <typename Scalar, int Storage>
   18|       |void run_matrix_tests() {
   19|       |  typedef Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Storage> MatrixType;
   20|       |
   21|       |  MatrixType m, n;
   22|       |
   23|       |  // boundary cases ...
   24|       |  m = n = MatrixType::Random(50, 50);
   25|       |  m.conservativeResize(1, 50);
   26|       |  VERIFY_IS_APPROX(m, n.block(0, 0, 1, 50));
   27|       |
   28|       |  m = n = MatrixType::Random(50, 50);
   29|       |  m.conservativeResize(50, 1);
   30|       |  VERIFY_IS_APPROX(m, n.block(0, 0, 50, 1));
   31|       |
   32|       |  m = n = MatrixType::Random(50, 50);
   33|       |  m.conservativeResize(50, 50);
   34|       |  VERIFY_IS_APPROX(m, n.block(0, 0, 50, 50));
   35|       |
   36|       |  // random shrinking ...
   37|       |  for (int i = 0; i < 25; ++i) {
   38|       |    const Index rows = internal::random<Index>(1, 50);
   39|       |    const Index cols = internal::random<Index>(1, 50);
   40|       |    m = n = MatrixType::Random(50, 50);
   41|       |    m.conservativeResize(rows, cols);
   42|       |    VERIFY_IS_APPROX(m, n.block(0, 0, rows, cols));
   43|       |  }
   44|       |
   45|       |  // random growing with zeroing ...
   46|       |  for (int i = 0; i < 25; ++i) {
   47|       |    const Index rows = internal::random<Index>(50, 75);
   48|       |    const Index cols = internal::random<Index>(50, 75);
   49|       |    m = n = MatrixType::Random(50, 50);
   50|       |    m.conservativeResizeLike(MatrixType::Zero(rows, cols));
   51|       |    VERIFY_IS_APPROX(m.block(0, 0, n.rows(), n.cols()), n);
   52|       |    VERIFY(rows <= 50 || m.block(50, 0, rows - 50, cols).sum() == Scalar(0));
   53|       |    VERIFY(cols <= 50 || m.block(0, 50, rows, cols - 50).sum() == Scalar(0));
   54|       |  }
   55|       |}
   56|       |
   57|       |template <typename Scalar>
   58|       |void run_vector_tests() {
   59|       |  typedef Matrix<Scalar, 1, Eigen::Dynamic> VectorType;
   60|       |
   61|       |  VectorType m, n;
   62|       |
   63|       |  // boundary cases ...
   64|       |  m = n = VectorType::Random(50);
   65|       |  m.conservativeResize(1);
   66|       |  VERIFY_IS_APPROX(m, n.segment(0, 1));
   67|       |
   68|       |  m = n = VectorType::Random(50);
   69|       |  m.conservativeResize(50);
   70|       |  VERIFY_IS_APPROX(m, n.segment(0, 50));
   71|       |
   72|       |  m = n = VectorType::Random(50);
   73|       |  m.conservativeResize(m.rows(), 1);
   74|       |  VERIFY_IS_APPROX(m, n.segment(0, 1));
   75|       |
   76|       |  m = n = VectorType::Random(50);
   77|       |  m.conservativeResize(m.rows(), 50);
   78|       |  VERIFY_IS_APPROX(m, n.segment(0, 50));
   79|       |
   80|       |  // random shrinking ...
   81|       |  for (int i = 0; i < 50; ++i) {
   82|       |    const int size = internal::random<int>(1, 50);
   83|       |    m = n = VectorType::Random(50);
   84|       |    m.conservativeResize(size);
   85|       |    VERIFY_IS_APPROX(m, n.segment(0, size));
   86|       |
   87|       |    m = n = VectorType::Random(50);
   88|       |    m.conservativeResize(m.rows(), size);
   89|       |    VERIFY_IS_APPROX(m, n.segment(0, size));
   90|       |  }
   91|       |
   92|       |  // random growing with zeroing ...
   93|       |  for (int i = 0; i < 50; ++i) {
   94|       |    const int size = internal::random<int>(50, 100);
   95|       |    m = n = VectorType::Random(50);
   96|       |    m.conservativeResizeLike(VectorType::Zero(size));
   97|       |    VERIFY_IS_APPROX(m.segment(0, 50), n);
   98|       |    VERIFY(size <= 50 || m.segment(50, size - 50).sum() == Scalar(0));
   99|       |
  100|       |    m = n = VectorType::Random(50);
  101|       |    m.conservativeResizeLike(Matrix<Scalar, Dynamic, Dynamic>::Zero(1, size));
  102|       |    VERIFY_IS_APPROX(m.segment(0, 50), n);
  103|       |    VERIFY(size <= 50 || m.segment(50, size - 50).sum() == Scalar(0));
  104|       |  }
  105|       |}
  106|       |
  107|       |// Basic memory leak check with a non-copyable scalar type
  108|       |template <int>
  109|       |void noncopyable() {
  110|       |  typedef Eigen::Matrix<AnnoyingScalar, Dynamic, 1> VectorType;
  111|       |  typedef Eigen::Matrix<AnnoyingScalar, Dynamic, Dynamic> MatrixType;
  112|       |
  113|       |  {
  114|       |#ifndef EIGEN_TEST_ANNOYING_SCALAR_DONT_THROW
  115|       |    AnnoyingScalar::dont_throw = true;
  116|       |#endif
  117|       |    int n = 50;
  118|       |    VectorType v0(n), v1(n);
  119|       |    MatrixType m0(n, n), m1(n, n), m2(n, n);
  120|       |    v0.setOnes();
  121|       |    v1.setOnes();
  122|       |    m0.setOnes();
  123|       |    m1.setOnes();
  124|       |    m2.setOnes();
  125|       |    VERIFY(m0 == m1);
  126|       |    m0.conservativeResize(2 * n, 2 * n);
  127|       |    VERIFY(m0.topLeftCorner(n, n) == m1);
  128|       |
  129|       |    VERIFY(v0.head(n) == v1);
  130|       |    v0.conservativeResize(2 * n);
  131|       |    VERIFY(v0.head(n) == v1);
  132|       |  }
  133|       |  VERIFY(AnnoyingScalar::instances == 0 && "global memory leak detected in noncopyable");
  134|       |}
  135|       |
  136|      1|EIGEN_DECLARE_TEST(conservative_resize) {
  137|     11|  for (int i = 0; i < g_repeat; ++i) {
  138|     10|    CALL_SUBTEST_1((run_matrix_tests<int, Eigen::RowMajor>()));
  139|     10|    CALL_SUBTEST_1((run_matrix_tests<int, Eigen::ColMajor>()));
  140|     10|    CALL_SUBTEST_2((run_matrix_tests<float, Eigen::RowMajor>()));
  141|     10|    CALL_SUBTEST_2((run_matrix_tests<float, Eigen::ColMajor>()));
  142|     10|    CALL_SUBTEST_3((run_matrix_tests<double, Eigen::RowMajor>()));
  143|     10|    CALL_SUBTEST_3((run_matrix_tests<double, Eigen::ColMajor>()));
  144|     10|    CALL_SUBTEST_4((run_matrix_tests<std::complex<float>, Eigen::RowMajor>()));
  145|     10|    CALL_SUBTEST_4((run_matrix_tests<std::complex<float>, Eigen::ColMajor>()));
  146|     10|    CALL_SUBTEST_5((run_matrix_tests<std::complex<double>, Eigen::RowMajor>()));
  147|     10|    CALL_SUBTEST_5((run_matrix_tests<std::complex<double>, Eigen::ColMajor>()));
  148|     10|    CALL_SUBTEST_1((run_matrix_tests<int, Eigen::RowMajor | Eigen::DontAlign>()));
  149|       |
  150|     10|    CALL_SUBTEST_1((run_vector_tests<int>()));
  151|     10|    CALL_SUBTEST_2((run_vector_tests<float>()));
  152|     10|    CALL_SUBTEST_3((run_vector_tests<double>()));
  153|     10|    CALL_SUBTEST_4((run_vector_tests<std::complex<float> >()));
  154|     10|    CALL_SUBTEST_5((run_vector_tests<std::complex<double> >()));
  155|       |
  156|     10|#ifndef EIGEN_TEST_ANNOYING_SCALAR_DONT_THROW
  157|     10|    AnnoyingScalar::dont_throw = true;
  158|     10|#endif
  159|     10|    CALL_SUBTEST_6((run_vector_tests<AnnoyingScalar>()));
  160|     10|    CALL_SUBTEST_6((noncopyable<0>()));
  161|     10|  }
  162|      1|}

/home/kidus/Desktop/GNNs/eigen/eigen/test/main.h:
    1|       |// This file is part of Eigen, a lightweight C++ template library
    2|       |// for linear algebra.
    3|       |//
    4|       |// Copyright (C) 2006-2008 Benoit Jacob <jacob.benoit.1@gmail.com>
    5|       |// Copyright (C) 2008 Gael Guennebaud <gael.guennebaud@inria.fr>
    6|       |//
    7|       |// This Source Code Form is subject to the terms of the Mozilla
    8|       |// Public License v. 2.0. If a copy of the MPL was not distributed
    9|       |// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.
   10|       |
   11|       |#include <cstdlib>
   12|       |#include <cerrno>
   13|       |#include <ctime>
   14|       |#include <iostream>
   15|       |#include <iomanip>
   16|       |#include <fstream>
   17|       |#include <string>
   18|       |#include <sstream>
   19|       |#include <vector>
   20|       |#include <typeinfo>
   21|       |#include <type_traits>
   22|       |#include <functional>
   23|       |#ifdef EIGEN_USE_SYCL
   24|       |#include <CL/sycl.hpp>
   25|       |#endif
   26|       |
   27|       |// The following includes of STL headers have to be done _before_ the
   28|       |// definition of macros min() and max().  The reason is that many STL
   29|       |// implementations will not work properly as the min and max symbols collide
   30|       |// with the STL functions std::min() and std::max().  The STL headers may check
   31|       |// for the macro definition of min/max and issue a warning or undefine the
   32|       |// macros.
   33|       |//
   34|       |// Still, Windows defines min() and max() in windef.h as part of the regular
   35|       |// Windows system interfaces and many other Windows APIs depend on these
   36|       |// macros being available.  To prevent the macro expansion of min/max and to
   37|       |// make Eigen compatible with the Windows environment all function calls of
   38|       |// std::min() and std::max() have to be written with parenthesis around the
   39|       |// function name.
   40|       |//
   41|       |// All STL headers used by Eigen should be included here.  Because main.h is
   42|       |// included before any Eigen header and because the STL headers are guarded
   43|       |// against multiple inclusions, no STL header will see our own min/max macro
   44|       |// definitions.
   45|       |#include <limits>
   46|       |#include <algorithm>
   47|       |// Disable ICC's std::complex operator specializations so we can use our own.
   48|       |#define _OVERRIDE_COMPLEX_SPECIALIZATION_ 1
   49|       |#include <complex>
   50|       |#include <deque>
   51|       |#include <queue>
   52|       |#include <cassert>
   53|       |#include <list>
   54|       |#if __cplusplus >= 201103L || (defined(_MSVC_LANG) && _MSVC_LANG >= 201103L)
   55|       |#include <random>
   56|       |#include <chrono>
   57|       |#endif
   58|       |#if __cplusplus > 201703L
   59|       |// libstdc++ 9's <memory> indirectly uses max() via <bit>.
   60|       |// libstdc++ 10's <memory> indirectly uses max() via ranges headers.
   61|       |#include <memory>
   62|       |// libstdc++ 11's <thread> indirectly uses max() via semaphore headers.
   63|       |#include <thread>
   64|       |#endif
   65|       |
   66|       |// Configure GPU.
   67|       |#if defined(EIGEN_USE_HIP)
   68|       |#if defined(__HIPCC__) && !defined(EIGEN_NO_HIP)
   69|       |#define EIGEN_HIPCC __HIPCC__
   70|       |#include <hip/hip_runtime.h>
   71|       |#include <hip/hip_runtime_api.h>
   72|       |#endif
   73|       |#elif defined(__CUDACC__) && !defined(EIGEN_NO_CUDA)
   74|       |#define EIGEN_CUDACC __CUDACC__
   75|       |#include <cuda.h>
   76|       |#include <cuda_runtime.h>
   77|       |#include <cuda_runtime_api.h>
   78|       |#if CUDA_VERSION >= 7050
   79|       |#include <cuda_fp16.h>
   80|       |#endif
   81|       |#endif
   82|       |
   83|       |#if defined(EIGEN_CUDACC) || defined(EIGEN_HIPCC)
   84|       |#define EIGEN_TEST_NO_LONGDOUBLE
   85|       |#define EIGEN_DEFAULT_DENSE_INDEX_TYPE int
   86|       |#endif
   87|       |
   88|       |// To test that all calls from Eigen code to std::min() and std::max() are
   89|       |// protected by parenthesis against macro expansion, the min()/max() macros
   90|       |// are defined here and any not-parenthesized min/max call will cause a
   91|       |// compiler error.
   92|       |#if !defined(__HIPCC__) && !defined(EIGEN_USE_SYCL) && !defined(EIGEN_POCKETFFT_DEFAULT)
   93|       |//
   94|       |// HIP header files include the following files
   95|       |//  <thread>
   96|       |//  <regex>
   97|       |//  <unordered_map>
   98|       |// which seem to contain not-parenthesized calls to "max"/"min", triggering the following check and causing the compile
   99|       |// to fail
  100|       |//
  101|       |// Including those header files before the following macro definition for "min" / "max", only partially resolves the
  102|       |// issue This is because other HIP header files also define "isnan" / "isinf" / "isfinite" functions, which are needed
  103|       |// in other headers.
  104|       |//
  105|       |// So instead choosing to simply disable this check for HIP
  106|       |//
  107|       |#define min(A, B) please_protect_your_min_with_parentheses
  108|       |#define max(A, B) please_protect_your_max_with_parentheses
  109|       |#define isnan(X) please_protect_your_isnan_with_parentheses
  110|       |#define isinf(X) please_protect_your_isinf_with_parentheses
  111|       |#define isfinite(X) please_protect_your_isfinite_with_parentheses
  112|       |#endif
  113|       |
  114|       |// test possible conflicts
  115|       |struct real {};
  116|       |struct imag {};
  117|       |
  118|       |#ifdef M_PI
  119|       |#undef M_PI
  120|       |#endif
  121|       |#define M_PI please_use_EIGEN_PI_instead_of_M_PI
  122|       |
  123|       |#define FORBIDDEN_IDENTIFIER \
  124|       |  (this_identifier_is_forbidden_to_avoid_clashes) this_identifier_is_forbidden_to_avoid_clashes
  125|       |// B0 is defined in POSIX header termios.h
  126|       |#define B0 FORBIDDEN_IDENTIFIER
  127|       |#define I FORBIDDEN_IDENTIFIER
  128|       |
  129|       |// _res is defined by resolv.h
  130|       |#define _res FORBIDDEN_IDENTIFIER
  131|       |
  132|       |// Unit tests calling Eigen's blas library must preserve the default blocking size
  133|       |// to avoid troubles.
  134|       |#ifndef EIGEN_NO_DEBUG_SMALL_PRODUCT_BLOCKS
  135|       |#define EIGEN_DEBUG_SMALL_PRODUCT_BLOCKS
  136|       |#endif
  137|       |
  138|       |// shuts down ICC's remark #593: variable "XXX" was set but never used
  139|       |#define TEST_SET_BUT_UNUSED_VARIABLE(X) EIGEN_UNUSED_VARIABLE(X)
  140|       |
  141|       |#ifdef TEST_ENABLE_TEMPORARY_TRACKING
  142|       |
  143|       |static long int nb_temporaries;
  144|       |static long int nb_temporaries_on_assert = -1;
  145|       |
  146|       |#ifdef TEST_IGNORE_STACK_ALLOCATED_TEMPORARY
  147|       |inline void on_temporary_creation(long int size, int SizeAtCompileTime) {
  148|       |  // ignore stack-allocated temporaries
  149|       |  if (SizeAtCompileTime != -1) return;
  150|       |#else
  151|       |inline void on_temporary_creation(long int size, int) {
  152|       |#endif
  153|       |  // here's a great place to set a breakpoint when debugging failures in this test!
  154|       |  if (size != 0) nb_temporaries++;
  155|       |  if (nb_temporaries_on_assert > 0) assert(nb_temporaries < nb_temporaries_on_assert);
  156|       |}
  157|       |
  158|       |#define EIGEN_DENSE_STORAGE_CTOR_PLUGIN \
  159|       |  { on_temporary_creation(size, Size); }
  160|       |
  161|       |#define VERIFY_EVALUATION_COUNT(XPR, N)                            \
  162|       |  {                                                                \
  163|       |    nb_temporaries = 0;                                            \
  164|       |    XPR;                                                           \
  165|       |    if (nb_temporaries != (N)) {                                   \
  166|       |      std::cerr << "nb_temporaries == " << nb_temporaries << "\n"; \
  167|       |    }                                                              \
  168|       |    VERIFY((#XPR) && nb_temporaries == (N));                       \
  169|       |  }
  170|       |
  171|       |#endif
  172|       |
  173|       |#include "split_test_helper.h"
  174|       |
  175|       |#ifdef NDEBUG
  176|       |#undef NDEBUG
  177|       |#endif
  178|       |
  179|       |// On windows CE, NDEBUG is automatically defined <assert.h> if NDEBUG is not defined.
  180|       |#ifndef DEBUG
  181|       |#define DEBUG
  182|       |#endif
  183|       |
  184|      1|#define DEFAULT_REPEAT 10
  185|       |
  186|       |namespace Eigen {
  187|       |static std::vector<std::string> g_test_stack;
  188|       |// level == 0 <=> abort if test fail
  189|       |// level >= 1 <=> warning message to std::cerr if test fail
  190|       |static int g_test_level = 0;
  191|       |static int g_repeat = 1;
  192|       |static unsigned int g_seed = 0;
  193|       |static bool g_has_set_repeat = false, g_has_set_seed = false;
  194|       |
  195|       |class EigenTest {
  196|       | public:
  197|      0|  EigenTest() : m_func(0) {}
  198|      1|  EigenTest(const char* a_name, void (*func)(void)) : m_name(a_name), m_func(func) {
  199|      1|    get_registered_tests().push_back(this);
  200|      1|  }
  201|      1|  const std::string& name() const { return m_name; }
  202|      1|  void operator()() const { m_func(); }
  203|       |
  204|      4|  static const std::vector<EigenTest*>& all() { return get_registered_tests(); }
  205|       |
  206|       | protected:
  207|      5|  static std::vector<EigenTest*>& get_registered_tests() {
  208|      5|    static std::vector<EigenTest*>* ms_registered_tests = new std::vector<EigenTest*>();
  209|      5|    return *ms_registered_tests;
  210|      5|  }
  211|       |  std::string m_name;
  212|       |  void (*m_func)(void);
  213|       |};
  214|       |
  215|       |// Declare and register a test, e.g.:
  216|       |//    EIGEN_DECLARE_TEST(mytest) { ... }
  217|       |// will create a function:
  218|       |//    void test_mytest() { ... }
  219|       |// that will be automatically called.
  220|       |#define EIGEN_DECLARE_TEST(X)                                                              \
  221|       |  void EIGEN_CAT(test_, X)();                                                              \
  222|       |  static EigenTest EIGEN_CAT(test_handler_, X)(EIGEN_MAKESTRING(X), &EIGEN_CAT(test_, X)); \
  223|       |  void EIGEN_CAT(test_, X)()
  224|       |}  // namespace Eigen
  225|       |
  226|       |#define TRACK std::cerr << __FILE__ << " " << __LINE__ << std::endl
  227|       |
  228|       |#define EIGEN_DEFAULT_IO_FORMAT IOFormat(4, 0, "  ", "\n", "", "", "", "")
  229|       |
  230|       |#if (defined(_CPPUNWIND) || defined(__EXCEPTIONS)) && !defined(__CUDA_ARCH__) && !defined(__HIP_DEVICE_COMPILE__) && \
  231|       |    !defined(__SYCL_DEVICE_ONLY__)
  232|       |#define EIGEN_EXCEPTIONS
  233|       |#endif
  234|       |
  235|       |#ifndef EIGEN_NO_ASSERTION_CHECKING
  236|       |
  237|       |namespace Eigen {
  238|       |static const bool should_raise_an_assert = false;
  239|       |
  240|       |// Used to avoid to raise two exceptions at a time in which
  241|       |// case the exception is not properly caught.
  242|       |// This may happen when a second exceptions is triggered in a destructor.
  243|       |static bool no_more_assert = false;
  244|       |static bool report_on_cerr_on_assert_failure = true;
  245|       |
  246|       |struct eigen_assert_exception {
  247|      0|  eigen_assert_exception(void) {}
  248|      0|  ~eigen_assert_exception() { Eigen::no_more_assert = false; }
  249|       |};
  250|       |
  251|       |struct eigen_static_assert_exception {
  252|      0|  eigen_static_assert_exception(void) {}
  253|      0|  ~eigen_static_assert_exception() { Eigen::no_more_assert = false; }
  254|       |};
  255|       |}  // namespace Eigen
  256|       |// If EIGEN_DEBUG_ASSERTS is defined and if no assertion is triggered while
  257|       |// one should have been, then the list of executed assertions is printed out.
  258|       |//
  259|       |// EIGEN_DEBUG_ASSERTS is not enabled by default as it
  260|       |// significantly increases the compilation time
  261|       |// and might even introduce side effects that would hide
  262|       |// some memory errors.
  263|       |#ifdef EIGEN_DEBUG_ASSERTS
  264|       |
  265|       |namespace Eigen {
  266|       |namespace internal {
  267|       |static bool push_assert = false;
  268|       |}
  269|       |static std::vector<std::string> eigen_assert_list;
  270|       |}  // namespace Eigen
  271|       |#define eigen_assert(a)                                                                                             \
  272|       |  if ((!(a)) && (!no_more_assert)) {                                                                                \
  273|       |    if (report_on_cerr_on_assert_failure) std::cerr << #a << " " __FILE__ << "(" << __LINE__ << ")\n";              \
  274|       |    Eigen::no_more_assert = true;                                                                                   \
  275|       |    EIGEN_THROW_X(Eigen::eigen_assert_exception());                                                                 \
  276|       |  } else if (Eigen::internal::push_assert) {                                                                        \
  277|       |    eigen_assert_list.push_back(std::string(EIGEN_MAKESTRING(__FILE__) " (" EIGEN_MAKESTRING(__LINE__) ") : " #a)); \
  278|       |  }
  279|       |
  280|       |#ifdef EIGEN_EXCEPTIONS
  281|       |#define VERIFY_RAISES_ASSERT(a)                                                                                  \
  282|       |  {                                                                                                              \
  283|       |    Eigen::no_more_assert = false;                                                                               \
  284|       |    Eigen::eigen_assert_list.clear();                                                                            \
  285|       |    Eigen::internal::push_assert = true;                                                                         \
  286|       |    Eigen::report_on_cerr_on_assert_failure = false;                                                             \
  287|       |    try {                                                                                                        \
  288|       |      a;                                                                                                         \
  289|       |      std::cerr << "One of the following asserts should have been triggered:\n";                                 \
  290|       |      for (uint ai = 0; ai < eigen_assert_list.size(); ++ai) std::cerr << "  " << eigen_assert_list[ai] << "\n"; \
  291|       |      VERIFY(Eigen::should_raise_an_assert&& #a);                                                                \
  292|       |    } catch (Eigen::eigen_assert_exception) {                                                                    \
  293|       |      Eigen::internal::push_assert = false;                                                                      \
  294|       |      VERIFY(true);                                                                                              \
  295|       |    }                                                                                                            \
  296|       |    Eigen::report_on_cerr_on_assert_failure = true;                                                              \
  297|       |    Eigen::internal::push_assert = false;                                                                        \
  298|       |  }
  299|       |#endif  // EIGEN_EXCEPTIONS
  300|       |
  301|       |#elif !defined(__CUDACC__) && !defined(__HIPCC__) && !defined(__SYCL_DEVICE_ONLY__)  // EIGEN_DEBUG_ASSERTS
  302|       |#define eigen_assert(a)                               \
  303|      0|  if ((!(a)) && (!no_more_assert)) {                  \
  304|      0|    Eigen::no_more_assert = true;                     \
  305|      0|    if (report_on_cerr_on_assert_failure) {           \
  306|      0|      eigen_plain_assert(a);                          \
  307|      0|    } else {                                          \
  308|      0|      EIGEN_THROW_X(Eigen::eigen_assert_exception()); \
  309|      0|    }                                                 \
  310|      0|  }
  311|       |
  312|       |#ifdef EIGEN_EXCEPTIONS
  313|       |#define VERIFY_RAISES_ASSERT(a)                      \
  314|       |  {                                                  \
  315|       |    Eigen::no_more_assert = false;                   \
  316|       |    Eigen::report_on_cerr_on_assert_failure = false; \
  317|       |    try {                                            \
  318|       |      a;                                             \
  319|       |      VERIFY(Eigen::should_raise_an_assert&& #a);    \
  320|       |    } catch (Eigen::eigen_assert_exception&) {       \
  321|       |      VERIFY(true);                                  \
  322|       |    }                                                \
  323|       |    Eigen::report_on_cerr_on_assert_failure = true;  \
  324|       |  }
  325|       |#endif  // EIGEN_EXCEPTIONS
  326|       |#endif  // EIGEN_DEBUG_ASSERTS
  327|       |
  328|       |#ifndef VERIFY_RAISES_ASSERT
  329|       |#define VERIFY_RAISES_ASSERT(a) std::cout << "Can't VERIFY_RAISES_ASSERT( " #a " ) with exceptions disabled\n";
  330|       |#endif
  331|       |
  332|       |#if !defined(__CUDACC__) && !defined(__HIPCC__) && !defined(SYCL_DEVICE_ONLY)
  333|       |#define EIGEN_USE_CUSTOM_ASSERT
  334|       |#endif
  335|       |
  336|       |#else  // EIGEN_NO_ASSERTION_CHECKING
  337|       |
  338|       |#define VERIFY_RAISES_ASSERT(a) \
  339|       |  {}
  340|       |
  341|       |#endif  // EIGEN_NO_ASSERTION_CHECKING
  342|       |
  343|       |#if !defined(EIGEN_TESTING_CONSTEXPR) && !defined(EIGEN_TESTING_PLAINOBJECT_CTOR)
  344|       |#define EIGEN_INTERNAL_DEBUGGING
  345|       |#endif
  346|       |#include <Eigen/QR>  // required for createRandomPIMatrixOfRank and generateRandomMatrixSvs
  347|       |
  348|       |inline void verify_impl(bool condition, const char* testname, const char* file, int line,
  349|      1|                        const char* condition_as_string) {
  350|      1|  if (!condition) {
  351|      0|    if (Eigen::g_test_level > 0) std::cerr << "WARNING: ";
  352|      0|    std::cerr << "Test " << testname << " failed in " << file << " (" << line << ")" << std::endl
  353|      0|              << "    " << condition_as_string << std::endl;
  354|      0|    std::cerr << "Stack:\n";
  355|      0|    const int test_stack_size = static_cast<int>(Eigen::g_test_stack.size());
  356|      0|    for (int i = test_stack_size - 1; i >= 0; --i) std::cerr << "  - " << Eigen::g_test_stack[i] << "\n";
  357|      0|    std::cerr << "\n";
  358|      0|    if (Eigen::g_test_level == 0) abort();
  359|      0|  }
  360|      1|}
  361|       |
  362|      1|#define VERIFY(a) ::verify_impl(a, g_test_stack.back().c_str(), __FILE__, __LINE__, EIGEN_MAKESTRING(a))
  363|       |
  364|       |#define VERIFY_GE(a, b) ::verify_impl(a >= b, g_test_stack.back().c_str(), __FILE__, __LINE__, EIGEN_MAKESTRING(a >= b))
  365|       |#define VERIFY_LE(a, b) ::verify_impl(a <= b, g_test_stack.back().c_str(), __FILE__, __LINE__, EIGEN_MAKESTRING(a <= b))
  366|       |
  367|       |#define VERIFY_IS_EQUAL(a, b) VERIFY(test_is_equal(a, b, true))
  368|       |#define VERIFY_IS_NOT_EQUAL(a, b) VERIFY(test_is_equal(a, b, false))
  369|       |#define VERIFY_IS_APPROX(a, b) VERIFY(verifyIsApprox(a, b))
  370|       |#define VERIFY_IS_NOT_APPROX(a, b) VERIFY(!test_isApprox(a, b))
  371|       |#define VERIFY_IS_MUCH_SMALLER_THAN(a, b) VERIFY(test_isMuchSmallerThan(a, b))
  372|       |#define VERIFY_IS_NOT_MUCH_SMALLER_THAN(a, b) VERIFY(!test_isMuchSmallerThan(a, b))
  373|       |#define VERIFY_IS_APPROX_OR_LESS_THAN(a, b) VERIFY(test_isApproxOrLessThan(a, b))
  374|       |#define VERIFY_IS_NOT_APPROX_OR_LESS_THAN(a, b) VERIFY(!test_isApproxOrLessThan(a, b))
  375|       |#define VERIFY_IS_CWISE_EQUAL(a, b) VERIFY(verifyIsCwiseApprox(a, b, true))
  376|       |#define VERIFY_IS_CWISE_APPROX(a, b) VERIFY(verifyIsCwiseApprox(a, b, false))
  377|       |
  378|       |#define VERIFY_IS_UNITARY(a) VERIFY(test_isUnitary(a))
  379|       |
  380|       |#define STATIC_CHECK(COND) EIGEN_STATIC_ASSERT((COND), EIGEN_INTERNAL_ERROR_PLEASE_FILE_A_BUG_REPORT)
  381|       |
  382|       |#define CALL_SUBTEST(FUNC)                          \
  383|       |  do {                                              \
  384|       |    g_test_stack.push_back(EIGEN_MAKESTRING(FUNC)); \
  385|       |    FUNC;                                           \
  386|       |    g_test_stack.pop_back();                        \
  387|       |  } while (0)
  388|       |
  389|       |// Forward declarations to avoid ICC warnings
  390|       |#if EIGEN_COMP_ICC
  391|       |
  392|       |template <typename T>
  393|       |std::string type_name();
  394|       |
  395|       |namespace Eigen {
  396|       |
  397|       |template <typename T, typename U>
  398|       |bool test_is_equal(const T& actual, const U& expected, bool expect_equal = true);
  399|       |
  400|       |}  // end namespace Eigen
  401|       |
  402|       |#endif  // EIGEN_COMP_ICC
  403|       |
  404|       |namespace Eigen {
  405|       |
  406|       |template <typename T1, typename T2>
  407|       |std::enable_if_t<internal::is_same<T1, T2>::value, bool> is_same_type(const T1&, const T2&) {
  408|       |  return true;
  409|       |}
  410|       |
  411|       |template <typename T>
  412|      0|inline typename NumTraits<T>::Real test_precision() {
  413|      0|  return NumTraits<T>::dummy_precision();
  414|      0|}
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionIsEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionItEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionIiEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionIjEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionIlEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionImEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionIxEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionIyEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionINS_4halfEEENS_9NumTraitsIT_E4RealEv
  ------------------
  | Unexecuted instantiation: _ZN5Eigen14test_precisionINS_8bfloat16EEENS_9NumTraitsIT_E4RealEv
  ------------------
  415|       |template <>
  416|      0|inline float test_precision<float>() {
  417|      0|  return 1e-3f;
  418|      0|}
  419|       |template <>
  420|      0|inline double test_precision<double>() {
  421|      0|  return 1e-6;
  422|      0|}
  423|       |template <>
  424|      0|inline long double test_precision<long double>() {
  425|      0|  return 1e-6l;
  426|      0|}
  427|       |template <>
  428|      0|inline float test_precision<std::complex<float> >() {
  429|      0|  return test_precision<float>();
  430|      0|}
  431|       |template <>
  432|      0|inline double test_precision<std::complex<double> >() {
  433|      0|  return test_precision<double>();
  434|      0|}
  435|       |template <>
  436|      0|inline long double test_precision<std::complex<long double> >() {
  437|      0|  return test_precision<long double>();
  438|      0|}
  439|       |
  440|       |#define EIGEN_TEST_SCALAR_TEST_OVERLOAD(TYPE)                                          \
  441|      0|  inline bool test_isApprox(TYPE a, TYPE b) {                                          \
  442|      0|    return numext::equal_strict(a, b) || ((numext::isnan)(a) && (numext::isnan)(b)) || \
  443|      0|           (internal::isApprox(a, b, test_precision<TYPE>()));                         \
  444|      0|  }                                                                                    \
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEss
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEtt
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEii
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEjj
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEll
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEmm
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxExx
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEyy
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEff
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxEdd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxENS_4halfES0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen13test_isApproxENS_8bfloat16ES0_
  ------------------
  445|      0|  inline bool test_isCwiseApprox(TYPE a, TYPE b, bool exact) {                         \
  446|      0|    return numext::equal_strict(a, b) || ((numext::isnan)(a) && (numext::isnan)(b)) || \
  447|      0|           (!exact && internal::isApprox(a, b, test_precision<TYPE>()));               \
  448|      0|  }                                                                                    \
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEssb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEttb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEiib
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEjjb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEllb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEmmb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxExxb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEyyb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEffb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxEddb
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxENS_4halfES0_b
  ------------------
  | Unexecuted instantiation: _ZN5Eigen18test_isCwiseApproxENS_8bfloat16ES0_b
  ------------------
  449|      0|  inline bool test_isMuchSmallerThan(TYPE a, TYPE b) {                                 \
  450|      0|    return internal::isMuchSmallerThan(a, b, test_precision<TYPE>());                  \
  451|      0|  }                                                                                    \
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEss
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEtt
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEii
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEjj
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEll
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEmm
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanExx
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEyy
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEff
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanEdd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanENS_4halfES0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen22test_isMuchSmallerThanENS_8bfloat16ES0_
  ------------------
  452|      0|  inline bool test_isApproxOrLessThan(TYPE a, TYPE b) {                                \
  453|      0|    return internal::isApproxOrLessThan(a, b, test_precision<TYPE>());                 \
  454|      0|  }
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEss
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEtt
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEii
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEjj
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEll
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEmm
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanExx
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEyy
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEff
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanEdd
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanENS_4halfES0_
  ------------------
  | Unexecuted instantiation: _ZN5Eigen23test_isApproxOrLessThanENS_8bfloat16ES0_
  ------------------
  455|       |
  456|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(short)
  457|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(unsigned short)
  458|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(int)
  459|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(unsigned int)
  460|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(long)
  461|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(unsigned long)
  462|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(long long)
  463|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(unsigned long long)
  464|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(float)
  465|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(double)
  466|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(half)
  467|       |EIGEN_TEST_SCALAR_TEST_OVERLOAD(bfloat16)
  468|       |
  469|       |#undef EIGEN_TEST_SCALAR_TEST_OVERLOAD
  470|       |
  471|       |#ifndef EIGEN_TEST_NO_COMPLEX
  472|      0|inline bool test_isApprox(const std::complex<float>& a, const std::complex<float>& b) {
  473|      0|  return internal::isApprox(a, b, test_precision<std::complex<float> >());
  474|      0|}
  475|      0|inline bool test_isMuchSmallerThan(const std::complex<float>& a, const std::complex<float>& b) {
  476|      0|  return internal::isMuchSmallerThan(a, b, test_precision<std::complex<float> >());
  477|      0|}
  478|       |
  479|      0|inline bool test_isApprox(const std::complex<double>& a, const std::complex<double>& b) {
  480|      0|  return internal::isApprox(a, b, test_precision<std::complex<double> >());
  481|      0|}
  482|      0|inline bool test_isMuchSmallerThan(const std::complex<double>& a, const std::complex<double>& b) {
  483|      0|  return internal::isMuchSmallerThan(a, b, test_precision<std::complex<double> >());
  484|      0|}
  485|       |
  486|       |#ifndef EIGEN_TEST_NO_LONGDOUBLE
  487|      0|inline bool test_isApprox(const std::complex<long double>& a, const std::complex<long double>& b) {
  488|      0|  return internal::isApprox(a, b, test_precision<std::complex<long double> >());
  489|      0|}
  490|      0|inline bool test_isMuchSmallerThan(const std::complex<long double>& a, const std::complex<long double>& b) {
  491|      0|  return internal::isMuchSmallerThan(a, b, test_precision<std::complex<long double> >());
  492|      0|}
  493|       |#endif
  494|       |#endif
  495|       |
  496|       |#ifndef EIGEN_TEST_NO_LONGDOUBLE
  497|      0|inline bool test_isApprox(const long double& a, const long double& b) {
  498|      0|  bool ret = internal::isApprox(a, b, test_precision<long double>());
  499|      0|  if (!ret)
  500|      0|    std::cerr << std::endl << "    actual   = " << a << std::endl << "    expected = " << b << std::endl << std::endl;
  501|      0|  return ret;
  502|      0|}
  503|       |
  504|      0|inline bool test_isMuchSmallerThan(const long double& a, const long double& b) {
  505|      0|  return internal::isMuchSmallerThan(a, b, test_precision<long double>());
  506|      0|}
  507|      0|inline bool test_isApproxOrLessThan(const long double& a, const long double& b) {
  508|      0|  return internal::isApproxOrLessThan(a, b, test_precision<long double>());
  509|      0|}
  510|       |#endif  // EIGEN_TEST_NO_LONGDOUBLE
  511|       |
  512|       |// test_relative_error returns the relative difference between a and b as a real scalar as used in isApprox.
  513|       |template <typename T1, typename T2>
  514|       |typename NumTraits<typename T1::RealScalar>::NonInteger test_relative_error(const EigenBase<T1>& a,
  515|       |                                                                            const EigenBase<T2>& b) {
  516|       |  using std::sqrt;
  517|       |  typedef typename NumTraits<typename T1::RealScalar>::NonInteger RealScalar;
  518|       |  typename internal::nested_eval<T1, 2>::type ea(a.derived());
  519|       |  typename internal::nested_eval<T2, 2>::type eb(b.derived());
  520|       |  return sqrt(RealScalar((ea.matrix() - eb.matrix()).cwiseAbs2().sum()) /
  521|       |              RealScalar((std::min)(eb.cwiseAbs2().sum(), ea.cwiseAbs2().sum())));
  522|       |}
  523|       |
  524|       |template <typename T1, typename T2>
  525|       |typename T1::RealScalar test_relative_error(const T1& a, const T2& b, const typename T1::Coefficients* = 0) {
  526|       |  return test_relative_error(a.coeffs(), b.coeffs());
  527|       |}
  528|       |
  529|       |template <typename T1, typename T2>
  530|       |typename T1::Scalar test_relative_error(const T1& a, const T2& b, const typename T1::MatrixType* = 0) {
  531|       |  return test_relative_error(a.matrix(), b.matrix());
  532|       |}
  533|       |
  534|       |template <typename S, int D>
  535|       |S test_relative_error(const Translation<S, D>& a, const Translation<S, D>& b) {
  536|       |  return test_relative_error(a.vector(), b.vector());
  537|       |}
  538|       |
  539|       |template <typename S, int D, int O>
  540|       |S test_relative_error(const ParametrizedLine<S, D, O>& a, const ParametrizedLine<S, D, O>& b) {
  541|       |  return (std::max)(test_relative_error(a.origin(), b.origin()), test_relative_error(a.origin(), b.origin()));
  542|       |}
  543|       |
  544|       |template <typename S, int D>
  545|       |S test_relative_error(const AlignedBox<S, D>& a, const AlignedBox<S, D>& b) {
  546|       |  return (std::max)(test_relative_error((a.min)(), (b.min)()), test_relative_error((a.max)(), (b.max)()));
  547|       |}
  548|       |
  549|       |template <typename Derived>
  550|       |class SparseMatrixBase;
  551|       |template <typename T1, typename T2>
  552|       |typename T1::RealScalar test_relative_error(const MatrixBase<T1>& a, const SparseMatrixBase<T2>& b) {
  553|       |  return test_relative_error(a, b.toDense());
  554|       |}
  555|       |
  556|       |template <typename Derived>
  557|       |class SparseMatrixBase;
  558|       |template <typename T1, typename T2>
  559|       |typename T1::RealScalar test_relative_error(const SparseMatrixBase<T1>& a, const MatrixBase<T2>& b) {
  560|       |  return test_relative_error(a.toDense(), b);
  561|       |}
  562|       |
  563|       |template <typename Derived>
  564|       |class SparseMatrixBase;
  565|       |template <typename T1, typename T2>
  566|       |typename T1::RealScalar test_relative_error(const SparseMatrixBase<T1>& a, const SparseMatrixBase<T2>& b) {
  567|       |  return test_relative_error(a.toDense(), b.toDense());
  568|       |}
  569|       |
  570|       |template <typename T1, typename T2>
  571|       |typename NumTraits<typename NumTraits<T1>::Real>::NonInteger test_relative_error(
  572|      0|    const T1& a, const T2& b, std::enable_if_t<internal::is_arithmetic<typename NumTraits<T1>::Real>::value, T1>* = 0) {
  573|      0|  typedef typename NumTraits<typename NumTraits<T1>::Real>::NonInteger RealScalar;
  574|      0|  return numext::sqrt(RealScalar(numext::abs2(a - b)) /
  575|      0|                      (numext::mini)(RealScalar(numext::abs2(a)), RealScalar(numext::abs2(b))));
  576|      0|}
  577|       |
  578|       |template <typename T>
  579|       |T test_relative_error(const Rotation2D<T>& a, const Rotation2D<T>& b) {
  580|       |  return test_relative_error(a.angle(), b.angle());
  581|       |}
  582|       |
  583|       |template <typename T>
  584|       |T test_relative_error(const AngleAxis<T>& a, const AngleAxis<T>& b) {
  585|       |  return (std::max)(test_relative_error(a.angle(), b.angle()), test_relative_error(a.axis(), b.axis()));
  586|       |}
  587|       |
  588|       |template <typename Type1, typename Type2>
  589|       |inline bool test_isApprox(const Type1& a, const Type2& b, typename Type1::Scalar* = 0)  // Enabled for Eigen's type only
  590|       |{
  591|       |  return a.isApprox(b, test_precision<typename Type1::Scalar>());
  592|       |}
  593|       |
  594|       |// get_test_precision is a small wrapper to test_precision allowing to return the scalar precision for either scalars or
  595|       |// expressions
  596|       |template <typename T>
  597|       |typename NumTraits<typename T::Scalar>::Real get_test_precision(const T&, const typename T::Scalar* = 0) {
  598|       |  return test_precision<typename NumTraits<typename T::Scalar>::Real>();
  599|       |}
  600|       |
  601|       |template <typename T>
  602|       |typename NumTraits<T>::Real get_test_precision(
  603|       |    const T&, std::enable_if_t<internal::is_arithmetic<typename NumTraits<T>::Real>::value, T>* = 0) {
  604|       |  return test_precision<typename NumTraits<T>::Real>();
  605|       |}
  606|       |
  607|       |// verifyIsApprox is a wrapper to test_isApprox that outputs the relative difference magnitude if the test fails.
  608|       |template <typename Type1, typename Type2>
  609|       |inline bool verifyIsApprox(const Type1& a, const Type2& b) {
  610|       |  bool ret = test_isApprox(a, b);
  611|       |  if (!ret) {
  612|       |    std::cerr << "Difference too large wrt tolerance " << get_test_precision(a)
  613|       |              << ", relative error is: " << test_relative_error(a, b) << std::endl;
  614|       |  }
  615|       |  return ret;
  616|       |}
  617|       |
  618|       |// verifyIsCwiseApprox is a wrapper to test_isCwiseApprox that outputs the relative difference magnitude if the test
  619|       |// fails.
  620|       |template <typename Type1, typename Type2>
  621|       |inline bool verifyIsCwiseApprox(const Type1& a, const Type2& b, bool exact) {
  622|       |  bool ret = test_isCwiseApprox(a, b, exact);
  623|       |  if (!ret) {
  624|       |    if (exact) {
  625|       |      std::cerr << "Values are not an exact match";
  626|       |    } else {
  627|       |      std::cerr << "Difference too large wrt tolerance " << get_test_precision(a);
  628|       |    }
  629|       |    std::cerr << ", relative error is: " << test_relative_error(a, b) << std::endl;
  630|       |  }
  631|       |  return ret;
  632|       |}
  633|       |
  634|       |// The idea behind this function is to compare the two scalars a and b where
  635|       |// the scalar ref is a hint about the expected order of magnitude of a and b.
  636|       |// WARNING: the scalar a and b must be positive
  637|       |// Therefore, if for some reason a and b are very small compared to ref,
  638|       |// we won't issue a false negative.
  639|       |// This test could be: abs(a-b) <= eps * ref
  640|       |// However, it seems that simply comparing a+ref and b+ref is more sensitive to true error.
  641|       |template <typename Scalar, typename ScalarRef>
  642|       |inline bool test_isApproxWithRef(const Scalar& a, const Scalar& b, const ScalarRef& ref) {
  643|       |  return test_isApprox(a + ref, b + ref);
  644|       |}
  645|       |
  646|       |template <typename Derived1, typename Derived2>
  647|       |inline bool test_isMuchSmallerThan(const MatrixBase<Derived1>& m1, const MatrixBase<Derived2>& m2) {
  648|       |  return m1.isMuchSmallerThan(m2, test_precision<typename internal::traits<Derived1>::Scalar>());
  649|       |}
  650|       |
  651|       |template <typename Derived>
  652|       |inline bool test_isMuchSmallerThan(const MatrixBase<Derived>& m,
  653|       |                                   const typename NumTraits<typename internal::traits<Derived>::Scalar>::Real& s) {
  654|       |  return m.isMuchSmallerThan(s, test_precision<typename internal::traits<Derived>::Scalar>());
  655|       |}
  656|       |
  657|       |template <typename Derived>
  658|       |inline bool test_isUnitary(const MatrixBase<Derived>& m) {
  659|       |  return m.isUnitary(test_precision<typename internal::traits<Derived>::Scalar>());
  660|       |}
  661|       |
  662|       |// Checks component-wise, works with infs and nans.
  663|       |template <typename Derived1, typename Derived2>
  664|       |bool test_isCwiseApprox(const DenseBase<Derived1>& m1, const DenseBase<Derived2>& m2, bool exact) {
  665|       |  if (m1.rows() != m2.rows()) {
  666|       |    return false;
  667|       |  }
  668|       |  if (m1.cols() != m2.cols()) {
  669|       |    return false;
  670|       |  }
  671|       |  for (Index r = 0; r < m1.rows(); ++r) {
  672|       |    for (Index c = 0; c < m1.cols(); ++c) {
  673|       |      if (m1(r, c) != m2(r, c) && !((numext::isnan)(m1(r, c)) && (numext::isnan)(m2(r, c))) &&
  674|       |          (exact || !test_isApprox(m1(r, c), m2(r, c)))) {
  675|       |        return false;
  676|       |      }
  677|       |    }
  678|       |  }
  679|       |  return true;
  680|       |}
  681|       |
  682|       |template <typename Derived1, typename Derived2>
  683|       |bool test_isCwiseApprox(const SparseMatrixBase<Derived1>& m1, const SparseMatrixBase<Derived2>& m2, bool exact) {
  684|       |  return test_isCwiseApprox(m1.toDense(), m2.toDense(), exact);
  685|       |}
  686|       |
  687|       |template <typename T, typename U>
  688|       |bool test_is_equal(const T& actual, const U& expected, bool expect_equal) {
  689|       |  if (numext::equal_strict(actual, expected) == expect_equal) return true;
  690|       |  // false:
  691|       |  std::cerr << "\n    actual   = " << actual << "\n    expected " << (expect_equal ? "= " : "!=") << expected << "\n\n";
  692|       |  return false;
  693|       |}
  694|       |
  695|       |/**
  696|       | * Check if number is "not a number" (NaN).
  697|       | *
  698|       | * @tparam T input type
  699|       | * @param x input value
  700|       | * @return true, if input value is "not a number" (NaN)
  701|       | */
  702|       |template <typename T>
  703|       |bool isNotNaN(const T& x) {
  704|       |  return x == x;
  705|       |}
  706|       |
  707|       |/**
  708|       | * Check if number is plus infinity.
  709|       | *
  710|       | * @tparam T input type
  711|       | * @param x input value
  712|       | * @return true, if input value is plus infinity
  713|       | */
  714|       |template <typename T>
  715|       |bool isPlusInf(const T& x) {
  716|       |  return x > NumTraits<T>::highest();
  717|       |}
  718|       |
  719|       |/**
  720|       | * Check if number is minus infinity.
  721|       | *
  722|       | * @tparam T input type
  723|       | * @param x input value
  724|       | * @return true, if input value is minus infinity
  725|       | */
  726|       |template <typename T>
  727|       |bool isMinusInf(const T& x) {
  728|       |  return x < NumTraits<T>::lowest();
  729|       |}
  730|       |
  731|       |}  // end namespace Eigen
  732|       |
  733|       |#include "random_matrix_helper.h"
  734|       |
  735|       |template <typename T>
  736|       |struct GetDifferentType;
  737|       |
  738|       |template <>
  739|       |struct GetDifferentType<float> {
  740|       |  typedef double type;
  741|       |};
  742|       |template <>
  743|       |struct GetDifferentType<double> {
  744|       |  typedef float type;
  745|       |};
  746|       |template <typename T>
  747|       |struct GetDifferentType<std::complex<T> > {
  748|       |  typedef std::complex<typename GetDifferentType<T>::type> type;
  749|       |};
  750|       |
  751|       |template <typename T>
  752|       |std::string type_name(T) {
  753|       |  return typeid(T).name();
  754|       |}
  755|       |template <>
  756|      0|std::string type_name<float>(float) {
  757|      0|  return "float";
  758|      0|}
  759|       |template <>
  760|      0|std::string type_name<double>(double) {
  761|      0|  return "double";
  762|      0|}
  763|       |template <>
  764|      0|std::string type_name<long double>(long double) {
  765|      0|  return "long double";
  766|      0|}
  767|       |template <>
  768|      0|std::string type_name<Eigen::half>(Eigen::half) {
  769|      0|  return "half";
  770|      0|}
  771|       |template <>
  772|      0|std::string type_name<Eigen::bfloat16>(Eigen::bfloat16) {
  773|      0|  return "bfloat16";
  774|      0|}
  775|       |template <>
  776|      0|std::string type_name<int8_t>(int8_t) {
  777|      0|  return "int8_t";
  778|      0|}
  779|       |template <>
  780|      0|std::string type_name<int16_t>(int16_t) {
  781|      0|  return "int16_t";
  782|      0|}
  783|       |template <>
  784|      0|std::string type_name<int32_t>(int32_t) {
  785|      0|  return "int32_t";
  786|      0|}
  787|       |template <>
  788|      0|std::string type_name<int64_t>(int64_t) {
  789|      0|  return "int64_t";
  790|      0|}
  791|       |template <>
  792|      0|std::string type_name<uint8_t>(uint8_t) {
  793|      0|  return "uint8_t";
  794|      0|}
  795|       |template <>
  796|      0|std::string type_name<uint16_t>(uint16_t) {
  797|      0|  return "uint16_t";
  798|      0|}
  799|       |template <>
  800|      0|std::string type_name<uint32_t>(uint32_t) {
  801|      0|  return "uint32_t";
  802|      0|}
  803|       |template <>
  804|      0|std::string type_name<uint64_t>(uint64_t) {
  805|      0|  return "uint64_t";
  806|      0|}
  807|       |template <>
  808|      0|std::string type_name<std::complex<float> >(std::complex<float>) {
  809|      0|  return "complex<float>";
  810|      0|}
  811|       |template <>
  812|      0|std::string type_name<std::complex<double> >(std::complex<double>) {
  813|      0|  return "complex<double>";
  814|      0|}
  815|       |template <>
  816|      0|std::string type_name<std::complex<long double> >(std::complex<long double>) {
  817|      0|  return "complex<long double>";
  818|      0|}
  819|       |template <>
  820|      0|std::string type_name<std::complex<int> >(std::complex<int>) {
  821|      0|  return "complex<int>";
  822|      0|}
  823|       |template <typename T>
  824|       |std::string type_name() {
  825|       |  return type_name(T());
  826|       |}
  827|       |
  828|       |using namespace Eigen;
  829|       |
  830|       |/**
  831|       | * Set number of repetitions for unit test from input string.
  832|       | *
  833|       | * @param str input string
  834|       | */
  835|      0|inline void set_repeat_from_string(const char* str) {
  836|      0|  errno = 0;
  837|      0|  g_repeat = int(strtoul(str, 0, 10));
  838|      0|  if (errno || g_repeat <= 0) {
  839|      0|    std::cout << "Invalid repeat value " << str << std::endl;
  840|      0|    exit(EXIT_FAILURE);
  841|      0|  }
  842|      0|  g_has_set_repeat = true;
  843|      0|}
  844|       |
  845|       |/**
  846|       | * Set seed for randomized unit tests from input string.
  847|       | *
  848|       | * @param str input string
  849|       | */
  850|      0|inline void set_seed_from_string(const char* str) {
  851|      0|  errno = 0;
  852|      0|  g_seed = int(strtoul(str, 0, 10));
  853|      0|  if (errno || g_seed == 0) {
  854|      0|    std::cout << "Invalid seed value " << str << std::endl;
  855|      0|    exit(EXIT_FAILURE);
  856|      0|  }
  857|      0|  g_has_set_seed = true;
  858|      0|}
  859|       |
  860|      1|int main(int argc, char* argv[]) {
  861|      1|  g_has_set_repeat = false;
  862|      1|  g_has_set_seed = false;
  863|      1|  bool need_help = false;
  864|       |
  865|      1|  for (int i = 1; i < argc; i++) {
  866|      0|    if (argv[i][0] == 'r') {
  867|      0|      if (g_has_set_repeat) {
  868|      0|        std::cout << "Argument " << argv[i] << " conflicting with a former argument" << std::endl;
  869|      0|        return 1;
  870|      0|      }
  871|      0|      set_repeat_from_string(argv[i] + 1);
  872|      0|    } else if (argv[i][0] == 's') {
  873|      0|      if (g_has_set_seed) {
  874|      0|        std::cout << "Argument " << argv[i] << " conflicting with a former argument" << std::endl;
  875|      0|        return 1;
  876|      0|      }
  877|      0|      set_seed_from_string(argv[i] + 1);
  878|      0|    } else {
  879|      0|      need_help = true;
  880|      0|    }
  881|      0|  }
  882|       |
  883|      1|  if (need_help) {
  884|      0|    std::cout << "This test application takes the following optional arguments:" << std::endl;
  885|      0|    std::cout << "  rN     Repeat each test N times (default: " << DEFAULT_REPEAT << ")" << std::endl;
  886|      0|    std::cout << "  sN     Use N as seed for random numbers (default: based on current time)" << std::endl;
  887|      0|    std::cout << std::endl;
  888|      0|    std::cout << "If defined, the environment variables EIGEN_REPEAT and EIGEN_SEED" << std::endl;
  889|      0|    std::cout << "will be used as default values for these parameters." << std::endl;
  890|      0|    return 1;
  891|      0|  }
  892|       |
  893|      1|  char* env_EIGEN_REPEAT = getenv("EIGEN_REPEAT");
  894|      1|  if (!g_has_set_repeat && env_EIGEN_REPEAT) set_repeat_from_string(env_EIGEN_REPEAT);
  895|      1|  char* env_EIGEN_SEED = getenv("EIGEN_SEED");
  896|      1|  if (!g_has_set_seed && env_EIGEN_SEED) set_seed_from_string(env_EIGEN_SEED);
  897|       |
  898|      1|  if (!g_has_set_seed) g_seed = (unsigned int)time(NULL);
  899|      1|  if (!g_has_set_repeat) g_repeat = DEFAULT_REPEAT;
  900|       |
  901|      1|  std::cout << "Initializing random number generator with seed " << g_seed << std::endl;
  902|      1|  std::stringstream ss;
  903|      1|  ss << "Seed: " << g_seed;
  904|      1|  g_test_stack.push_back(ss.str());
  905|      1|  srand(g_seed);
  906|      1|  std::cout << "Repeating each test " << g_repeat << " times" << std::endl;
  907|       |
  908|      1|  VERIFY(EigenTest::all().size() > 0);
  909|       |
  910|      2|  for (std::size_t i = 0; i < EigenTest::all().size(); ++i) {
  911|      1|    const EigenTest& current_test = *EigenTest::all()[i];
  912|      1|    Eigen::g_test_stack.push_back(current_test.name());
  913|      1|    current_test();
  914|      1|    Eigen::g_test_stack.pop_back();
  915|      1|  }
  916|       |
  917|      1|  return 0;
  918|      1|}
  919|       |
  920|       |// These warning are disabled here such that they are still ON when parsing Eigen's header files.
  921|       |#if defined __INTEL_COMPILER
  922|       |// remark #383: value copied to temporary, reference to temporary used
  923|       |//  -> this warning is raised even for legal usage as: g_test_stack.push_back("foo"); where g_test_stack is a
  924|       |//  std::vector<std::string>
  925|       |// remark #1418: external function definition with no prior declaration
  926|       |//  -> this warning is raised for all our test functions. Declaring them static would fix the issue.
  927|       |// warning #279: controlling expression is constant
  928|       |// remark #1572: floating-point equality and inequality comparisons are unreliable
  929|       |#pragma warning disable 279 383 1418 1572
  930|       |#endif
  931|       |
  932|       |#ifdef _MSC_VER
  933|       |// 4503 - decorated name length exceeded, name was truncated
  934|       |#pragma warning(disable : 4503)
  935|       |#endif
  936|       |
  937|       |#include "gpu_test_helper.h"

